keyword,git_name,owner_name,git_url,star_count,forks_count,contributor_count,contributors_list,code_language,last_updated_date,readme_url,readme_text
Synthetic+Data,Belval/TextRecognitionDataGenerator,Belval,https://api.github.com/repos/Belval/TextRecognitionDataGenerator,2635,842,30,"['https://api.github.com/users/Belval', 'https://api.github.com/users/FHainzl', 'https://api.github.com/users/Enzodtz', 'https://api.github.com/users/hendraet', 'https://api.github.com/users/nicolasmetallo', 'https://api.github.com/users/jtwsmeal', 'https://api.github.com/users/AghilesAzzoug', 'https://api.github.com/users/bakrianoo', 'https://api.github.com/users/astrocket', 'https://api.github.com/users/dc-chengchao', 'https://api.github.com/users/elahe-dastan', 'https://api.github.com/users/euihyun-lee', 'https://api.github.com/users/iknoorjobs', 'https://api.github.com/users/jinmingteo', 'https://api.github.com/users/junxnone', 'https://api.github.com/users/mohamadmansourX', 'https://api.github.com/users/JulienCoutault', 'https://api.github.com/users/PyaePhyoKhant', 'https://api.github.com/users/rkcosmos', 'https://api.github.com/users/Hrazhan', 'https://api.github.com/users/SunHaozhe', 'https://api.github.com/users/luangtatipsy', 'https://api.github.com/users/YacobBY', 'https://api.github.com/users/FLming', 'https://api.github.com/users/bact', 'https://api.github.com/users/edwardpwtsoi', 'https://api.github.com/users/gachiemchiep', 'https://api.github.com/users/wangershi', 'https://api.github.com/users/yifeitao', 'https://api.github.com/users/zhenglilei']",Python,2023-04-09T19:48:06Z,https://raw.githubusercontent.com/Belval/TextRecognitionDataGenerator/master/README.md,"['# TextRecognitionDataGenerator [![CircleCI](https://circleci.com/gh/Belval/TextRecognitionDataGenerator/tree/master.svg?style=svg)](https://circleci.com/gh/Belval/TextRecognitionDataGenerator/tree/master) [![PyPI version](https://badge.fury.io/py/trdg.svg)](https://badge.fury.io/py/trdg) [![codecov](https://codecov.io/gh/Belval/TextRecognitionDataGenerator/branch/master/graph/badge.svg)](https://codecov.io/gh/Belval/TextRecognitionDataGenerator) [![Documentation Status](https://readthedocs.org/projects/textrecognitiondatagenerator/badge/?version=latest)](https://textrecognitiondatagenerator.readthedocs.io/en/latest/?badge=latest)\n', '\n', 'A synthetic data generator for text recognition\n', '\n', '## What is it for?\n', '\n', 'Generating text image samples to train an OCR software. Now supporting non-latin text! For a more thorough tutorial see [the official documentation](https://textrecognitiondatagenerator.readthedocs.io/en/latest/index.html).\n', '\n', '## What do I need to make it work?\n', '\n', 'Install the pypi package\n', '\n', '```\n', 'pip install trdg\n', '```\n', '\n', 'Afterwards, you can use `trdg` from the CLI. I recommend using a virtualenv instead of installing with `sudo`.\n', '\n', 'If you want to add another language, you can clone the repository instead. Simply run `pip install -r requirements.txt`\n', '\n', '## Docker image\n', '\n', 'If you would rather not have to install anything to use TextRecognitionDataGenerator, you can pull the docker image.\n', '\n', '```\n', 'docker pull belval/trdg:latest\n', '\n', 'docker run -v /output/path/:/app/out/ -t belval/trdg:latest trdg [args]\n', '```\n', '\n', 'The path (`/output/path/`) must be absolute.\n', '\n', '## New\n', '- Add `--stroke_width` argument to set the width of the text stroke (Thank you [@SunHaozhe](https://github.com/SunHaozhe))\n', '- Add `--stroke_fill` argument to set the color of the text contour if stroke > 0 (Thank you [@SunHaozhe](https://github.com/SunHaozhe))\n', '- Add `--word_split` argument to split on word instead of per-character. This is useful for ligature-based languages\n', '- Add `--dict` argument to specify a custom dictionary (Thank you [@luh0907](https://github.com/luh0907))\n', '- Add `--font_dir` argument to specify the fonts to use\n', '- Add `--output_mask` to output character-level mask for each image\n', '- Add `--character_spacing` to control space between characters (in pixels)\n', '- Add python module\n', '- Add `--font` to use only one font for all the generated images (Thank you [@JulienCoutault](https://github.com/JulienCoutault)!)\n', '- Add `--fit` and `--margins` for finer layout control\n', '- Change the text orientation using the `-or` parameter\n', ""- Specify text color range using `-tc '#000000,#FFFFFF'`, please note that the quotes are **necessary**\n"", '- Add support for Simplified and Traditional Chinese\n', '\n', '## How does it work?\n', '\n', 'Words will be randomly chosen from a dictionary of a specific language. Then an image of those words will be generated by using font, background, and modifications (skewing, blurring, etc.) as specified.\n', '\n', '### Basic (Python module)\n', '\n', 'The usage as a Python module is very similar to the CLI, but it is more flexible if you want to include it directly in your training pipeline, and will consume less space and memory. There are 4 generators that can be used.\n', '\n', '```py\n', 'from trdg.generators import (\n', '    GeneratorFromDict,\n', '    GeneratorFromRandom,\n', '    GeneratorFromStrings,\n', '    GeneratorFromWikipedia,\n', ')\n', '\n', '# The generators use the same arguments as the CLI, only as parameters\n', 'generator = GeneratorFromStrings(\n', ""    ['Test1', 'Test2', 'Test3'],\n"", '    blur=2,\n', '    random_blur=True\n', ')\n', '\n', 'for img, lbl in generator:\n', '    # Do something with the pillow images here.\n', '```\n', '\n', 'You can see the full class definition here:\n', '\n', '- [`GeneratorFromDict`](trdg/generators/from_dict.py)\n', '- [`GeneratorFromRandom`](trdg/generators/from_random.py)\n', '- [`GeneratorFromStrings`](trdg/generators/from_strings.py)\n', '- [`GeneratorFromWikipedia`](trdg/generators/from_wikipedia.py)\n', '\n', '### Basic (CLI)\n', '\n', '`trdg -c 1000 -w 5 -f 64`\n', '\n', 'You get 1,000 randomly generated images with random text on them like:\n', '\n', '![1](samples/1.jpg ""1"")\n', '![2](samples/2.jpg ""2"")\n', '![3](samples/3.jpg ""3"")\n', '![4](samples/4.jpg ""4"")\n', '![5](samples/5.jpg ""5"")\n', '\n', 'By default, they will be generated to `out/` in the current working directory.\n', '\n', '### Text skewing\n', '\n', 'What if you want random skewing? Add `-k` and `-rk` (`trdg -c 1000 -w 5 -f 64 -k 5 -rk`)\n', '\n', '![6](samples/6.jpg ""6"")\n', '![7](samples/7.jpg ""7"")\n', '![8](samples/8.jpg ""8"")\n', '![9](samples/9.jpg ""9"")\n', '![10](samples/10.jpg ""10"")\n', '\n', '### Text distortion\n', 'You can also add distorsion to the generated text with `-d` and `-do`\n', '\n', '![23](samples/24.jpg ""0"")\n', '![24](samples/25.jpg ""1"")\n', '![25](samples/26.jpg ""2"")\n', '\n', '### Text blurring\n', '\n', ""But scanned document usually aren't that clear are they? Add `-bl` and `-rbl` to get gaussian blur on the generated image with user-defined radius (here 0, 1, 2, 4):\n"", '\n', '![11](samples/11.jpg ""0"")\n', '![12](samples/12.jpg ""1"")\n', '![13](samples/13.jpg ""2"")\n', '![14](samples/14.jpg ""4"")\n', '\n', '### Background\n', '\n', 'Maybe you want another background? Add `-b` to define one of the three available backgrounds: gaussian noise (0), plain white (1), quasicrystal (2) or image (3).\n', '\n', '![15](samples/15.jpg ""0"")\n', '![16](samples/16.jpg ""1"")\n', '![17](samples/17.jpg ""2"")\n', '![23](samples/23.jpg ""3"")\n', '\n', 'When using image background (3). A image from the images/ folder will be randomly selected and the text will be written on it.\n', '\n', '### Handwritten\n', '\n', 'Or maybe you are working on an OCR for handwritten text? Add `-hw`! (Experimental)\n', '\n', '![18](samples/18.jpg ""0"")\n', '![19](samples/19.jpg ""1"")\n', '![20](samples/20.jpg ""2"")\n', '![21](samples/21.jpg ""3"")\n', '![22](samples/22.jpg ""4"")\n', '\n', 'It uses a Tensorflow model trained using [this excellent project](https://github.com/Grzego/handwriting-generation) by Grzego.\n', '\n', ""**The project does not require TensorFlow to run if you aren't using this feature**\n"", '\n', '### Dictionary\n', '\n', 'The text is chosen at random in a dictionary file (that can be found in the *dicts* folder) and drawn on a white background made with Gaussian noise. The resulting image is saved as [text]\\_[index].jpg\n', '\n', 'There are a lot of parameters that you can tune to get the results you want, therefore I recommend checking out `trdg -h` for more information.\n', '\n', '## Create images with Chinese text\n', '\n', 'It is simple! Just do `trdg -l cn -c 1000 -w 5`!\n', '\n', 'Generated texts come both in simplified and traditional Chinese scripts.\n', '\n', 'Traditional:\n', '\n', '![27](samples/27.jpg ""0"")\n', '\n', 'Simplified:\n', '\n', '![28](samples/28.jpg ""1"")\n', '\n', '## Create images with Japanese text \n', '\n', 'It is simple! Just do `trdg -l ja -c 1000 -w 5`!\n', '\n', 'Output \n', '\n', '![29](samples/29.jpg ""2"")\n', '\n', '\n', '## Add new fonts\n', '\n', 'The script picks a font at random from the *fonts* directory.\n', '\n', '| Directory | Languages |\n', '|:----|:-----|\n', '| fonts/latin | English, French, Spanish, German |\n', '| fonts/cn | Chinese |\n', '| fonts/ko | Korean |\n', '| fonts/ja | Japanese |\n', '| fonts/th | Thai |\n', '\n', 'Simply add/remove fonts until you get the desired output.\n', '\n', 'If you want to add a new non-latin language, the amount of work is minimal.\n', '\n', '1. Create a new folder with your language [two-letters code](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes)\n', '2. Add a .ttf font in it\n', '3. Edit `run.py` to add an if statement in `load_fonts()`\n', '4. Add a text file in `dicts` with the same two-letters code\n', '5. Run the tool as you normally would but add `-l` with your two-letters code\n', '\n', 'It only supports .ttf for now.\n', '\n', '## Benchmarks\n', '\n', 'Number of images generated per second.\n', '\n', '- Intel Core i7-4710HQ @ 2.50Ghz + SSD (-c 1000 -w 1)\n', '    - `-t 1` : 363 img/s\n', '    - `-t 2` : 694 img/s\n', '    - `-t 4` : 1300 img/s\n', '    - `-t 8` : 1500 img/s\n', '- AMD Ryzen 7 1700 @ 4.0Ghz + SSD (-c 1000 -w 1)\n', '    - `-t 1` : 558 img/s\n', '    - `-t 2` : 1045 img/s\n', '    - `-t 4` : 2107 img/s\n', '    - `-t 8` : 3297 img/s\n', '\n', '## Contributing\n', '\n', ""1. Create an issue describing the feature you'll be working on\n"", '2. Code said feature\n', '3. Create a pull request\n', '\n', '## Feature request & issues\n', '\n', 'If anything is missing, unclear, or simply not working, open an issue on the repository.\n', '\n', '## What is left to do?\n', '- Better background generation\n', '- Better handwritten text generation\n', '- More customization parameters (mostly regarding background)\n']"
Synthetic+Data,ydataai/ydata-synthetic,ydataai,https://api.github.com/repos/ydataai/ydata-synthetic,916,198,19,"['https://api.github.com/users/fabclmnt', 'https://api.github.com/users/jfsantos-ds', 'https://api.github.com/users/dependabot%5Bbot%5D', 'https://api.github.com/users/renovate%5Bbot%5D', 'https://api.github.com/users/gmartinsribeiro', 'https://api.github.com/users/portellaa', 'https://api.github.com/users/aquemy', 'https://api.github.com/users/vascoalramos', 'https://api.github.com/users/miriamspsantos', 'https://api.github.com/users/ubabe53', 'https://api.github.com/users/arunnthevapalan', 'https://api.github.com/users/strickvl', 'https://api.github.com/users/archity', 'https://api.github.com/users/ceshine', 'https://api.github.com/users/fanconic', 'https://api.github.com/users/crownpku', 'https://api.github.com/users/ricardodcpereira', 'https://api.github.com/users/rajeshai', 'https://api.github.com/users/mglcampos']",Python,2023-04-09T17:09:54Z,https://raw.githubusercontent.com/ydataai/ydata-synthetic/dev/README.md,"['![](https://img.shields.io/github/workflow/status/ydataai/ydata-synthetic/prerelease)\n', '![](https://img.shields.io/pypi/status/ydata-synthetic)\n', '[![](https://pepy.tech/badge/ydata-synthetic)](https://pypi.org/project/ydata-synthetic/)\n', '![](https://img.shields.io/badge/python-3.9%20%7C%203.10-blue)\n', '[![](https://img.shields.io/pypi/v/ydata-synthetic)](https://pypi.org/project/ydata-synthetic/)\n', '![](https://img.shields.io/github/license/ydataai/ydata-synthetic)\n', '\n', '<p align=""center""><img width=""200"" src=""https://user-images.githubusercontent.com/3348134/177604157-11181f6c-57e5-44b1-8f6c-774edbba5512.png"" alt=""Synthetic Data Logo""></p>\n', '\n', 'Join us on [![Discord](https://img.shields.io/badge/Discord-7289DA?style=for-the-badge&logo=discord&logoColor=white)](https://discord.gg/mw7xjJ7b7s)\n', '\n', '# YData Synthetic\n', 'A package to generate synthetic tabular and time-series data leveraging the state of the art generative models.\n', '\n', '## 🎊 We have **big news**: v1.0.0 is here\n', '> We have exciting news for you. The new version of `ydata-synthetic` include new and exciting features:\n', '  > - A conditional architecture for tabular data: CTGAN, which will make the process of synthetic data generation easier and with higher quality!\n', '  > - A new streamlit app that delivers the synthetic data generation experience with a UI interface\n', '\n', '## Synthetic data\n', '### What is synthetic data?\n', ""Synthetic data is artificially generated data that is not collected from real world events. It replicates the statistical components of real data without containing any identifiable information, ensuring individuals' privacy.\n"", '\n', '### Why Synthetic Data?\n', 'Synthetic data can be used for many applications:\n', '  - Privacy\n', '  - Remove bias\n', '  - Balance datasets\n', '  - Augment datasets\n', '\n', '# ydata-synthetic\n', 'This repository contains material related with Generative Adversarial Networks for synthetic data generation, in particular regular tabular data and time-series.\n', 'It consists a set of different GANs architectures developed using Tensorflow 2.0. Several example Jupyter Notebooks and Python scripts are included, to show how to use the different architectures.\n', '\n', '## Quickstart\n', 'The source code is currently hosted on GitHub at: https://github.com/ydataai/ydata-synthetic\n', '\n', 'Binary installers for the latest released version are available at the [Python Package Index (PyPI).](https://pypi.org/project/ydata-synthetic/)\n', '```commandline\n', 'pip install ydata-synthetic\n', '```\n', '\n', '### The UI guide for synthetic data generation\n', '\n', 'YData synthetic has now a UI interface to guide you through the steps and inputs to generate structure tabular data.\n', 'The streamlit app is available form *v1.0.0* onwards, and supports the following flows:\n', '- Train a synthesizer model\n', '- Generate & profile synthetic data samples\n', '\n', '#### Installation\n', '\n', '```commandline\n', 'pip install ydata-syntehtic[streamlit]\n', '```\n', '#### Quickstart\n', 'Use the code snippet below in a python file (Jupyter Notebooks are not supported):\n', '```python\n', 'from ydata_synthetic import streamlit_app\n', '\n', 'streamlit_app.run()\n', '```\n', '\n', 'Or use the file streamlit_app.py that can be found in the [examples folder](https://github.com/ydataai/ydata-synthetic/tree/master/examples/streamlit_app.py).\n', '\n', '```commandline\n', 'python -m streamlit_app\n', '```\n', '\n', 'The below models are supported:\n', '  - CGAN\n', '  - WGAN\n', '  - WGANGP\n', '  - DRAGAN\n', '  - CRAMER\n', '  - CTGAN\n', '\n', '[![Watch the video](assets/streamlit_app.png)](https://youtu.be/ep0PhwsFx0A)\n', '\n', '### Examples\n', 'Here you can find usage examples of the package and models to synthesize tabular data.\n', '  \n', '  - Tabular synthetic data generation with CTGAN on adult census income dataset [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ydataai/ydata-synthetic/blob/master/examples/regular/models/CTGAN_Adult_Census_Income_Data.ipynb)\n', '  - Time Series synthetic data generation with TimeGAN on stock dataset [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ydataai/ydata-synthetic/blob/master/examples/timeseries/TimeGAN_Synthetic_stock_data.ipynb)\n', '  - More examples are continuously added and can be found in `/examples` directory.\n', '\n', '### Datasets for you to experiment\n', 'Here are some example datasets for you to try with the synthesizers:\n', '#### Tabular datasets\n', '- [Adult Census Income](https://www.kaggle.com/datasets/uciml/adult-census-income)\n', '- [Credit card fraud](https://www.kaggle.com/mlg-ulb/creditcardfraud)\n', '- [Cardiovascular Disease dataset](https://www.kaggle.com/datasets/sulianova/cardiovascular-disease-dataset)\n', '\n', '#### Sequential datasets\n', '- [Stock data](https://github.com/ydataai/ydata-synthetic/tree/master/data)\n', '\n', '## Project Resources\n', '\n', 'In this repository you can find the several GAN architectures that are used to create synthesizers:\n', '\n', '### Tabular data\n', '  - [GAN](https://arxiv.org/abs/1406.2661)\n', '  - [CGAN (Conditional GAN)](https://arxiv.org/abs/1411.1784)\n', '  - [WGAN (Wasserstein GAN)](https://arxiv.org/abs/1701.07875)\n', '  - [WGAN-GP (Wassertein GAN with Gradient Penalty)](https://arxiv.org/abs/1704.00028)\n', '  - [DRAGAN (On Convergence and stability of GANS)](https://arxiv.org/pdf/1705.07215.pdf)\n', '  - [Cramer GAN (The Cramer Distance as a Solution to Biased Wasserstein Gradients)](https://arxiv.org/abs/1705.10743)\n', '  - [CWGAN-GP (Conditional Wassertein GAN with Gradient Penalty)](https://cameronfabbri.github.io/papers/conditionalWGAN.pdf)\n', '  - [CTGAN (Conditional Tabular GAN)](https://arxiv.org/pdf/1907.00503.pdf)\n', '\n', '### Sequential data\n', '  - [TimeGAN](https://papers.nips.cc/paper/2019/file/c9efe5f26cd17ba6216bbe2a7d26d490-Paper.pdf)\n', '\n', '## Contributing\n', 'We are open to collaboration! If you want to start contributing you only need to:\n', '  1. Search for an issue in which you would like to work. Issues for newcomers are labeled with good first issue.\n', '  2. Create a PR solving the issue.\n', '  3. We would review every PRs and either accept or ask for revisions.\n', '\n', '## Support\n', 'For support in using this library, please join our Discord server. Our Discord community is very friendly and great about quickly answering questions about the use and development of the library. [Click here to join our Discord community!](https://discord.com/invite/mw7xjJ7b7s)\n', '\n', '## License\n', '[MIT License](https://github.com/ydataai/ydata-synthetic/blob/master/LICENSE)\n']"
Synthetic+Data,sdv-dev/CTGAN,sdv-dev,https://api.github.com/repos/sdv-dev/CTGAN,848,225,17,"['https://api.github.com/users/csala', 'https://api.github.com/users/fealho', 'https://api.github.com/users/pvk-developer', 'https://api.github.com/users/leix28', 'https://api.github.com/users/amontanez24', 'https://api.github.com/users/katxiao', 'https://api.github.com/users/kevinykuo', 'https://api.github.com/users/npatki', 'https://api.github.com/users/oregonpillow', 'https://api.github.com/users/JDTheRipperPC', 'https://api.github.com/users/tejuafonja', 'https://api.github.com/users/Baukebrenninkmeijer', 'https://api.github.com/users/lurosenb', 'https://api.github.com/users/matheusccouto', 'https://api.github.com/users/Deathn0t', 'https://api.github.com/users/timvink', 'https://api.github.com/users/mfhbree']",Python,2023-04-08T08:18:22Z,https://raw.githubusercontent.com/sdv-dev/CTGAN/master/README.md,"['<div align=""center"">\n', '<br/>\n', '<p align=""center"">\n', '    <i>This repository is part of <a href=""https://sdv.dev"">The Synthetic Data Vault Project</a>, a project from <a href=""https://datacebo.com"">DataCebo</a>.</i>\n', '</p>\n', '\n', '[![Development Status](https://img.shields.io/badge/Development%20Status-2%20--%20Pre--Alpha-yellow)](https://pypi.org/search/?c=Development+Status+%3A%3A+2+-+Pre-Alpha)\n', '[![PyPI Shield](https://img.shields.io/pypi/v/ctgan.svg)](https://pypi.python.org/pypi/ctgan)\n', '[![Unit Tests](https://github.com/sdv-dev/CTGAN/actions/workflows/unit.yml/badge.svg)](https://github.com/sdv-dev/CTGAN/actions/workflows/unit.yml)\n', '[![Downloads](https://pepy.tech/badge/ctgan)](https://pepy.tech/project/ctgan)\n', '[![Coverage Status](https://codecov.io/gh/sdv-dev/CTGAN/branch/master/graph/badge.svg)](https://codecov.io/gh/sdv-dev/CTGAN)\n', '\n', '<div align=""left"">\n', '<br/>\n', '<p align=""center"">\n', '<a href=""https://github.com/sdv-dev/CTGAN"">\n', '<img align=""center"" width=40% src=""https://github.com/sdv-dev/SDV/blob/master/docs/images/CTGAN-DataCebo.png""></img>\n', '</a>\n', '</p>\n', '</div>\n', '\n', '</div>\n', '\n', '# Overview\n', '\n', 'CTGAN\xa0is a collection of Deep Learning based\xa0synthetic data generators\xa0for\xa0single table\xa0data, which are able to learn from real data and generate synthetic data with high fidelity.\n', '\n', '| Important Links                               |                                                                      |\n', '| --------------------------------------------- | -------------------------------------------------------------------- |\n', '| :computer: **[Website]**                      | Check out the SDV Website for more information about our overall synthetic data ecosystem.|\n', '| :orange_book: **[Blog]**                      | A deeper look at open source, synthetic data creation and evaluation.|\n', '| :book: **[Documentation]**                    | Quickstarts, User and Development Guides, and API Reference.         |\n', '| :octocat: **[Repository]**                    | The link to the Github Repository of this library.                   |\n', '| :keyboard: **[Development Status]**           | This software is in its Pre-Alpha stage.                             |\n', '| [![][Slack Logo] **Community**][Community]    | Join our Slack Workspace for announcements and discussions.          |\n', '\n', '[Website]: https://sdv.dev\n', '[Blog]: https://datacebo.com/blog\n', '[Documentation]: https://bit.ly/sdv-docs\n', '[Repository]: https://github.com/sdv-dev/CTGAN\n', '[License]: https://github.com/sdv-dev/CTGAN/blob/master/LICENSE\n', '[Development Status]: https://pypi.org/search/?c=Development+Status+%3A%3A+2+-+Pre-Alpha\n', '[Slack Logo]: https://github.com/sdv-dev/SDV/blob/master/docs/images/slack.png\n', '[Community]: https://bit.ly/sdv-slack-invite\n', '\n', 'Currently, this library implements the **CTGAN** and **TVAE** models described in the [Modeling Tabular data using Conditional GAN](https://arxiv.org/abs/1907.00503) paper, presented at the 2019 NeurIPS conference.\n', '\n', '# Install\n', '\n', '## Use CTGAN through the SDV library\n', '\n', "":warning: If you're just getting started with synthetic data, we recommend installing the SDV library which provides user-friendly APIs for accessing CTGAN. :warning:\n"", '\n', 'The SDV library provides wrappers for preprocessing your data as well as additional usability features like constraints. See the [SDV documentation](https://bit.ly/sdv-docs) to get started.\n', '\n', '## Use the CTGAN standalone library\n', '\n', 'Alternatively, you can also install and use **CTGAN** directly, as a standalone library:\n', '\n', '**Using `pip`:**\n', '\n', '```bash\n', 'pip install ctgan\n', '```\n', '\n', '**Using `conda`:**\n', '\n', '```bash\n', 'conda install -c pytorch -c conda-forge ctgan\n', '```\n', '\n', 'When using the CTGAN library directly, you may need to manually preprocess your data into the correct format, for example:\n', '\n', '* Continuous data must be represented as floats\n', '* Discrete data must be represented as ints or strings\n', '* The data should not contain any missing values\n', '\n', '# Usage Example\n', '\n', 'In this example we load the [Adult Census Dataset](https://archive.ics.uci.edu/ml/datasets/adult)* which is a built-in demo dataset. We use CTGAN to learn from the real data and then generate some synthetic data.\n', '\n', '```python3\n', 'from ctgan import CTGAN\n', 'from ctgan import load_demo\n', '\n', 'real_data = load_demo()\n', '\n', '# Names of the columns that are discrete\n', 'discrete_columns = [\n', ""    'workclass',\n"", ""    'education',\n"", ""    'marital-status',\n"", ""    'occupation',\n"", ""    'relationship',\n"", ""    'race',\n"", ""    'sex',\n"", ""    'native-country',\n"", ""    'income'\n"", ']\n', '\n', 'ctgan = CTGAN(epochs=10)\n', 'ctgan.fit(real_data, discrete_columns)\n', '\n', '# Create synthetic data\n', 'synthetic_data = ctgan.sample(1000)\n', '```\n', '\n', '*For more information about the dataset see:\n', 'Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml].\n', 'Irvine, CA: University of California, School of Information and Computer Science.\n', '\n', '# Join our community\n', '\n', 'Join our [Slack channel](https://bit.ly/sdv-slack-invite) to discuss more about CTGAN and synthetic data. If you find a bug or have a feature request, you can also [open an issue](https://github.com/sdv-dev/CTGAN/issues) on our GitHub.\n', '\n', '**Interested in contributing to CTGAN?** Read our [Contribution Guide](CONTRIBUTING.rst) to get started.\n', '\n', '# Citing CTGAN\n', '\n', 'If you use CTGAN, please cite the following work:\n', '\n', '*Lei Xu, Maria Skoularidou, Alfredo Cuesta-Infante, Kalyan Veeramachaneni.* **Modeling Tabular data using Conditional GAN**. NeurIPS, 2019.\n', '\n', '```LaTeX\n', '@inproceedings{ctgan,\n', '  title={Modeling Tabular data using Conditional GAN},\n', '  author={Xu, Lei and Skoularidou, Maria and Cuesta-Infante, Alfredo and Veeramachaneni, Kalyan},\n', '  booktitle={Advances in Neural Information Processing Systems},\n', '  year={2019}\n', '}\n', '```\n', '\n', '# Related Projects\n', 'Please note that these projects are external to the SDV Ecosystem. They are not affiliated with or maintained by DataCebo.\n', '\n', '* **R Interface for CTGAN**: A wrapper around **CTGAN** that brings the functionalities to **R** users.\n', 'More details can be found in the corresponding repository: https://github.com/kasaai/ctgan\n', '* **CTGAN Server CLI**: A package to easily deploy CTGAN onto a remote server. Created by Timothy Pillow @oregonpillow at: https://github.com/oregonpillow/ctgan-server-cli\n', '\n', '---\n', '\n', '\n', '<div align=""center"">\n', '<a href=""https://datacebo.com""><img align=""center"" width=40% src=""https://github.com/sdv-dev/SDV/blob/master/docs/images/DataCebo.png""></img></a>\n', '</div>\n', '<br/>\n', '<br/>\n', '\n', ""[The Synthetic Data Vault Project](https://sdv.dev) was first created at MIT's [Data to AI Lab](\n"", 'https://dai.lids.mit.edu/) in 2016. After 4 years of research and traction with enterprise, we\n', 'created [DataCebo](https://datacebo.com) in 2020 with the goal of growing the project.\n', 'Today, DataCebo is the proud developer of SDV, the largest ecosystem for\n', 'synthetic data generation & evaluation. It is home to multiple libraries that support synthetic\n', 'data, including:\n', '\n', '* 🔄 Data discovery & transformation. Reverse the transforms to reproduce realistic data.\n', '* 🧠 Multiple machine learning models -- ranging from Copulas to Deep Learning -- to create tabular,\n', '  multi table and time series data.\n', '* 📊 Measuring quality and privacy of synthetic data, and comparing different synthetic data\n', '  generation models.\n', '\n', '[Get started using the SDV package](https://sdv.dev/SDV/getting_started/install.html) -- a fully\n', 'integrated solution and your one-stop shop for synthetic data. Or, use the standalone libraries\n', 'for specific needs.\n']"
Synthetic+Data,sdv-dev/SDV,sdv-dev,https://api.github.com/repos/sdv-dev/SDV,1349,214,19,"['https://api.github.com/users/csala', 'https://api.github.com/users/ManuelAlvarezC', 'https://api.github.com/users/amontanez24', 'https://api.github.com/users/JDTheRipperPC', 'https://api.github.com/users/katxiao', 'https://api.github.com/users/fealho', 'https://api.github.com/users/pvk-developer', 'https://api.github.com/users/frances-h', 'https://api.github.com/users/npatki', 'https://api.github.com/users/kveerama', 'https://api.github.com/users/xamm', 'https://api.github.com/users/sarahmish', 'https://api.github.com/users/Aylr', 'https://api.github.com/users/R-Palazzo', 'https://api.github.com/users/dyuliu', 'https://api.github.com/users/ludovicc', 'https://api.github.com/users/Deathn0t', 'https://api.github.com/users/rollervan', 'https://api.github.com/users/tssbas']",Python,2023-04-08T08:59:13Z,https://raw.githubusercontent.com/sdv-dev/SDV/master/README.md,"['<div align=""center"">\n', '<br/>\n', '<p align=""center"">\n', '    <i>This repository is part of <a href=""https://sdv.dev"">The Synthetic Data Vault Project</a>, a project from <a href=""https://datacebo.com"">DataCebo</a>.</i>\n', '</p>\n', '\n', '[![Dev Status](https://img.shields.io/badge/Dev%20Status-5%20--%20Production%2fStable-green)](https://pypi.org/search/?c=Development+Status+%3A%3A+5+-+Production%2FStable)\n', '[![PyPi Shield](https://img.shields.io/pypi/v/SDV.svg)](https://pypi.python.org/pypi/SDV)\n', '[![Unit Tests](https://github.com/sdv-dev/SDV/actions/workflows/unit.yml/badge.svg?branch=master)](https://github.com/sdv-dev/SDV/actions/workflows/unit.yml?query=branch%3Amaster)\n', '[![Integration Tests](https://github.com/sdv-dev/SDV/actions/workflows/integration.yml/badge.svg?branch=master)](https://github.com/sdv-dev/SDV/actions/workflows/integration.yml?query=branch%3Amaster)\n', '[![Coverage Status](https://codecov.io/gh/sdv-dev/SDV/branch/master/graph/badge.svg)](https://codecov.io/gh/sdv-dev/SDV)\n', '[![Downloads](https://static.pepy.tech/personalized-badge/sdv?period=total&units=international_system&left_color=grey&right_color=blue&left_text=Downloads)](https://pepy.tech/project/sdv)\n', '[![Colab](https://img.shields.io/badge/Tutorials-Try%20now!-orange?logo=googlecolab)](https://docs.sdv.dev/sdv/demos)\n', '[![Slack](https://img.shields.io/badge/Slack-Join%20now!-36C5F0?logo=slack)](https://bit.ly/sdv-slack-invite)\n', '\n', '<div align=""left"">\n', '<br/>\n', '<p align=""center"">\n', '<a href=""https://github.com/sdv-dev/SDV"">\n', '<img align=""center"" width=40% src=""https://github.com/sdv-dev/SDV/blob/master/docs/images/SDV-logo.png""></img>\n', '</a>\n', '</p>\n', '</div>\n', '\n', '</div>\n', '\n', '# Overview\n', '\n', 'The **Synthetic Data Vault** (SDV) is a Python library designed to be your one-stop shop for\n', 'creating tabular synthetic data. The SDV uses a variety of machine learning algorithms to learn\n', 'patterns from your real data and emulate them in synthetic data.\n', '\n', '## Features\n', ':brain: **Create synthetic data using machine learning.** The SDV offers multiple models, ranging\n', 'from classical statistical methods (GaussianCopula) to deep learning methods (CTGAN). Generate\n', 'data for single tables, multiple connected tables or sequential tables.\n', '\n', ':bar_chart: **Evaluate and visualize data.** Compare the synthetic data to the real data against a\n', 'variety of measures. Diagnose problems and generate a quality report to get more insights.\n', '\n', ':arrows_counterclockwise: **Preprocess, anonymize and define constraints.** Control data\n', 'processing to improve the quality of synthetic data, choose from different types of anonymization\n', 'and define business rules in the form of logical constraints.\n', '\n', '| Important Links                               |                                                                                                     |\n', '| --------------------------------------------- | ----------------------------------------------------------------------------------------------------|\n', '| [![][Colab Logo] **Tutorials**][Tutorials]    | Get some hands-on experience with the SDV. Launch the tutorial notebooks and run the code yourself. |\n', '| :book: **[Docs]**                             | Learn how to use the SDV library with user guides and API references.                               |\n', '| :orange_book: **[Blog]**                      | Get more insights about using the SDV, deploying models and our synthetic data community.          |\n', '| [![][Slack Logo] **Community**][Community]    | Join our Slack workspace for announcements and discussions.                                         |\n', '| :computer: **[Website]**                      | Check out the SDV website for more information about the project.                                   |\n', '\n', '[Website]: https://sdv.dev\n', '[Blog]: https://datacebo.com/blog\n', '[Docs]: https://bit.ly/sdv-docs\n', '[Repository]: https://github.com/sdv-dev/SDV\n', '[License]: https://github.com/sdv-dev/SDV/blob/master/LICENSE\n', '[Development Status]: https://pypi.org/search/?c=Development+Status+%3A%3A+5+-+Production%2FStable\n', '[Slack Logo]: https://github.com/sdv-dev/SDV/blob/master/docs/images/slack.png\n', '[Community]: https://bit.ly/sdv-slack-invite\n', '[Colab Logo]: https://github.com/sdv-dev/SDV/blob/master/docs/images/google_colab.png\n', '[Tutorials]: https://docs.sdv.dev/sdv/demos\n', '\n', '# Install\n', 'The SDV is publicly available under the [Business Source License](https://github.com/sdv-dev/SDV/blob/master/LICENSE).\n', 'Install SDV using pip or conda. We recommend using a virtual environment to avoid conflicts with\n', 'other software on your device.\n', '\n', '```bash\n', 'pip install sdv\n', '```\n', '\n', '```bash\n', 'conda install -c pytorch -c conda-forge sdv\n', '```\n', '\n', '# Getting Started\n', 'Load a demo dataset to get started. This dataset is a single table describing guests staying at a\n', 'fictional hotel.\n', '\n', '```python\n', 'from sdv.datasets.demo import download_demo\n', '\n', 'real_data, metadata = download_demo(\n', ""    modality='single_table',\n"", ""    dataset_name='fake_hotel_guests')\n"", '```\n', '\n', '![Single Table Metadata Example](https://github.com/sdv-dev/SDV/blob/master/docs/images/Single-Table-Metadata-Example.png)\n', '\n', 'The demo also includes **metadata**, a description of the dataset, including the data types in each\n', 'column and the primary key (`guest_email`).\n', '\n', '## Synthesizing Data\n', 'Next, we can create an **SDV synthesizer**,  an object that you can use to create synthetic data.\n', ""It learns patterns from the real data and replicates them to generate synthetic data. Let's use\n"", 'the `FAST_ML` preset synthesizer, which is optimized for performance.\n', '\n', '```python\n', 'from sdv.lite import SingleTablePreset\n', '\n', ""synthesizer = SingleTablePreset(metadata, name='FAST_ML')\n"", 'synthesizer.fit(data=real_data)\n', '```\n', '\n', 'And now the synthesizer is ready to create synthetic data!\n', '\n', '```python\n', 'synthetic_data = synthesizer.sample(num_rows=500)\n', '```\n', '\n', 'The synthetic data will have the following properties:\n', '- **Sensitive columns are fully anonymized.** The email, billing address and credit card number\n', ""columns contain new data so you don't expose the real values.\n"", '- **Other columns follow statistical patterns.** For example, the proportion of room types, the\n', 'distribution of check in dates and the correlations between room rate and room type are preserved.\n', '- **Keys and other relationships are intact.** The primary key (guest email) is unique for each row.\n', 'If you have multiple tables, the connection between a primary and foreign keys makes sense.\n', '\n', '## Evaluating Synthetic Data\n', 'The SDV library allows you to evaluate the synthetic data by comparing it to the real data. Get\n', 'started by generating a quality report.\n', '\n', '```python\n', 'from sdv.evaluation.single_table import evaluate_quality\n', '\n', 'quality_report = evaluate_quality(\n', '    real_data,\n', '    synthetic_data,\n', '    metadata)\n', '```\n', '\n', '```\n', 'Creating report: 100%|██████████| 4/4 [00:00<00:00, 19.30it/s]\n', 'Overall Quality Score: 89.12%\n', 'Properties:\n', 'Column Shapes: 90.27%\n', 'Column Pair Trends: 87.97%\n', '```\n', '\n', 'This object computes an overall quality score on a scale of 0 to 100% (100 being the best) as well\n', 'as detailed breakdowns. For more insights, you can also visualize the synthetic vs. real data.\n', '\n', '```python\n', 'from sdv.evaluation.single_table import get_column_plot\n', '\n', 'fig = get_column_plot(\n', '    real_data=real_data,\n', '    synthetic_data=synthetic_data,\n', ""    column_name='amenities_fee',\n"", '    metadata=metadata\n', ')\n', '    \n', 'fig.show()\n', '```\n', '\n', '![Real vs. Synthetic Data](https://github.com/sdv-dev/SDV/blob/master/docs/images/Real-vs-Synthetic-Evaluation.png)\n', '\n', ""# What's Next?\n"", 'Using the SDV library, you can synthesize single table, multi table and sequential data. You can\n', 'also customize the full synthetic data workflow, including preprocessing, anonymization and adding\n', 'constraints.\n', '\n', 'To learn more, visit the [SDV Demo page](https://docs.sdv.dev/sdv/demos).\n', '\n', '# Credits\n', 'Thank you to our team of contributors who have built and maintained the SDV ecosystem over the\n', 'years!\n', '\n', '[View Contributors](https://github.com/sdv-dev/SDV/graphs/contributors)\n', '\n', '## Citation\n', 'If you use SDV for your research, please cite the following paper:\n', '\n', '*Neha Patki, Roy Wedge, Kalyan Veeramachaneni*. [The Synthetic Data Vault](https://dai.lids.mit.edu/wp-content/uploads/2018/03/SDV.pdf). [IEEE DSAA 2016](https://ieeexplore.ieee.org/document/7796926).\n', '\n', '```\n', '@inproceedings{\n', '    SDV,\n', '    title={The Synthetic data vault},\n', '    author={Patki, Neha and Wedge, Roy and Veeramachaneni, Kalyan},\n', '    booktitle={IEEE International Conference on Data Science and Advanced Analytics (DSAA)},\n', '    year={2016},\n', '    pages={399-410},\n', '    doi={10.1109/DSAA.2016.49},\n', '    month={Oct}\n', '}\n', '```\n', '\n', '---\n', '\n', '\n', '<div align=""center"">\n', '  <a href=""https://datacebo.com""><picture>\n', '      <source media=""(prefers-color-scheme: dark)"" srcset=""https://github.com/sdv-dev/SDV/blob/master/docs/images/datacebo-logo-dark-mode.png"">\n', '      <img align=""center"" width=40% src=""https://github.com/sdv-dev/SDV/blob/master/docs/images/datacebo-logo.png""></img>\n', '  </picture></a>\n', '</div>\n', '<br/>\n', '<br/>\n', '\n', ""[The Synthetic Data Vault Project](https://sdv.dev) was first created at MIT's [Data to AI Lab](\n"", 'https://dai.lids.mit.edu/) in 2016. After 4 years of research and traction with enterprise, we\n', 'created [DataCebo](https://datacebo.com) in 2020 with the goal of growing the project.\n', 'Today, DataCebo is the proud developer of SDV, the largest ecosystem for\n', 'synthetic data generation & evaluation. It is home to multiple libraries that support synthetic\n', 'data, including:\n', '\n', '* 🔄 Data discovery & transformation. Reverse the transforms to reproduce realistic data.\n', '* 🧠 Multiple machine learning models -- ranging from Copulas to Deep Learning -- to create tabular,\n', '  multi table and time series data.\n', '* 📊 Measuring quality and privacy of synthetic data, and comparing different synthetic data\n', '  generation models.\n', '\n', '[Get started using the SDV package](https://bit.ly/sdv-docs) -- a fully\n', 'integrated solution and your one-stop shop for synthetic data. Or, use the standalone libraries\n', 'for specific needs.\n']"
Synthetic+Data,wang-tf/Chinese_OCR_synthetic_data,wang-tf,https://api.github.com/repos/wang-tf/Chinese_OCR_synthetic_data,256,86,0,[],Python,2023-04-06T09:36:31Z,https://raw.githubusercontent.com/wang-tf/Chinese_OCR_synthetic_data/master/README.md,"['# Chinese_OCR_synthetic_data\n', '---\n', '## The progress was used to generate synthetic dataset for Chinese OCR.\n', 'Here we used [Augmenter](https://github.com/mdbloice/Augmentor) to augment out output characters in images, including rotate, skew, shear and distort.\n', 'And you can change characters.txt file to use other characters.\n', 'The main function can be found in the synthetic_data.py file.\n', '\n', 'The python package you may need:\n', '- tqdm\n', '- PIL(pillow)\n', '- pathlib\n', '- cv2(opencv)\n', '- numpy\n', '- codecs\n', '- glob\n', '\n', '---\n', '## 本程序用于合成中文OCR数据库。\n', '本程序使用了[Augmenter](https://github.com/mdbloice/Augmentor)库，以对输出的图像进行增强图片中的文本，其中包括旋转、倾斜、剪切和扭曲。这些形变的参数可以在utils.py中找到并修改。\n', '在characters.txt中存放着所有的中文字符，如果想更换训练的字符请替换该文件。\n', 'main函数在synthetic_data.py中，可以按需要做修改。\n', '\n', '使用之前可能需要安装一下的包：\n', '- tqdm\n', '- PIL(pillow)\n', '- pathlib\n', '- cv2(opencv)\n', '- numpy\n', '- codecs\n', '- glob\n', '\n', '\n', '![test](https://github.com/wang-tf/Chinese_OCR_synthetic_data/blob/master/test_ocrdataset/train_part_image/0_0.jpg)\n']"
Synthetic+Data,GeostatsGuy/GeoDataSets,GeostatsGuy,https://api.github.com/repos/GeostatsGuy/GeoDataSets,46,108,1,['https://api.github.com/users/GeostatsGuy'],,2023-03-24T08:25:41Z,https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/README.md,"['<p>\n', '    <img src=""https://github.com/GeostatsGuy/GeostatsPy/blob/master/TCG_color_logo.png"" width=""220"" height=""200"" />\n', '</p>\n', '\n', '# GeoDataSets: Synthetic Subsurface Data Repository (1.0.0) \n', '\n', '## Prof. Michael J. Pyrcz, Associate Professor, The University of Texas at Austin\n', '\n', 'A collection of synthetic subsurface datasets to support education, publications, and prototyping.\n', '\n', 'Please cite as:\n', '\n', 'Pyrcz, Michael J. (2021). GeoDataSets: Synthetic Subsurface Data Repository (1.0.0). Zenodo. https://doi.org/10.5281/zenodo.5564874\n', '\n', 'This repository includes a wide variety of synthetic, subsurface datasets with a variety of:\n', '\n', '#### Data Dimensionality\n', '\n', 'To support education with easy visualization and interactivity the datasets are 1D and 2D.\n', '\n', '* 1D cores from wells and 2D seismic maps. \n', '\n', '#### Number of Features\n', '\n', 'For multivariate analysis some of the datasets include up to 6 features with a variety of structures.\n', '\n', '* linear and nonlinear, homoscedastic and heteroscedastic, and multivariate constraints \n', '\n', '#### Data Issues\n', '\n', 'The datasets attempt to include typical issues such as non-physical values, random and structured noise\n', '\n', '#### Use and Attribution\n', '\n', 'You are welcome to use these datasets for any purpose. Please cite the repository as:\n', '\n', 'Pyrcz, M.J., 2021, GeoDataSets Repository, GitHub Respository, https://github.com/GeostatsGuy/GeoDataSets/.\n', '\n', 'I hope this is helpful,\n', '\n', 'Michael\n']"
Synthetic+Data,gretelai/gretel-synthetics,gretelai,https://api.github.com/repos/gretelai/gretel-synthetics,383,61,21,"['https://api.github.com/users/zredlined', 'https://api.github.com/users/johntmyers', 'https://api.github.com/users/drew', 'https://api.github.com/users/kboyd', 'https://api.github.com/users/misberner', 'https://api.github.com/users/tylersbray', 'https://api.github.com/users/pimlock', 'https://api.github.com/users/lipikaramaswamy', 'https://api.github.com/users/santhosh97', 'https://api.github.com/users/anthager', 'https://api.github.com/users/lememta', 'https://api.github.com/users/andrewnc', 'https://api.github.com/users/arronhunt', 'https://api.github.com/users/Marjan-emd', 'https://api.github.com/users/mckornfield', 'https://api.github.com/users/anastasia-nesterenko', 'https://api.github.com/users/dni138', 'https://api.github.com/users/hgascon', 'https://api.github.com/users/Jeesh96', 'https://api.github.com/users/csbailey5t', 'https://api.github.com/users/theonlyrob']",Python,2023-04-07T07:17:55Z,https://raw.githubusercontent.com/gretelai/gretel-synthetics/master/README.md,"['# Gretel Synthetics\n', '\n', '<p align=""center"">\n', '    <a href=""https://gretel.ai""><img width=""128px"" src=""https://gretel-public-website.s3.amazonaws.com/assets/gobs_the_cat_@1x.png"" alt=""Gobs the Gretel.ai cat"" /></a><br />\n', '    <i>A permissive synthetic data library from Gretel.ai</i>\n', '</p>\n', '\n', '[![Documentation Status](https://readthedocs.org/projects/gretel-synthetics/badge/?version=stable)](https://gretel-synthetics.readthedocs.io/en/stable/?badge=stable)\n', '[![CLA assistant](https://cla-assistant.io/readme/badge/gretelai/gretel-synthetics)](https://cla-assistant.io/gretelai/gretel-synthetics)\n', '[![PyPI](https://badge.fury.io/py/gretel-synthetics.svg)](https://badge.fury.io/py/gretel-synthetics)\n', '[![Python](https://img.shields.io/pypi/pyversions/gretel-synthetics.svg)](https://github.com/gretelai/gretel-synthetics)\n', '[![Downloads](https://pepy.tech/badge/gretel-synthetics)](https://pepy.tech/project/gretel-synthetics)\n', '[![GitHub stars](https://img.shields.io/github/stars/gretelai/gretel-synthetics?style=social)](https://github.com/gretelai/gretel-synthetics)\n', '[![Discord](https://img.shields.io/discord/1007817822614847500?label=Discord&logo=Discord)](https://gretel.ai/discord)\n', '\n', '## Documentation\n', '\n', '- [Get started with gretel-synthetics](https://gretel-synthetics.readthedocs.io/en/stable/)\n', '- [Configuration](https://gretel-synthetics.readthedocs.io/en/stable/api/config.html)\n', '- [Train your model](https://gretel-synthetics.readthedocs.io/en/stable/api/train.html)\n', '- [Generate synthetic records](https://gretel-synthetics.readthedocs.io/en/stable/api/generate.html)\n', '\n', '## Try it out now!\n', '\n', 'If you want to quickly discover gretel-synthetics, simply click the button below and follow the tutorials!\n', '\n', '[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gretelai/gretel-synthetics/blob/master/examples/synthetic_records.ipynb)\n', '\n', 'Check out additional examples [here](https://github.com/gretelai/gretel-synthetics/tree/master/examples).\n', '\n', '## Getting Started\n', '\n', 'This section will guide you through installation of `gretel-synthetics` and dependencies that are not directly installed by the Python package manager.\n', '\n', '### Dependency Requirements\n', '\n', 'By default, we do not install certain core requirements, the following dependencies should be installed _external to the installation_\n', 'of `gretel-synthetics`, depending on which model(s) you plan to use.\n', '\n', '- Tensorflow: Used by the LSTM model, we recommend version 2.8.x\n', '- Torch: Used by Timeseries DGAN and ACTGAN (for ACTGAN, Torch is installed by SDV)\n', '- SDV (Synthetic Data Vault): Used by ACTGAN, we recommned version 0.17.x\n', '\n', 'These dependencies can be installed by doing the following:\n', '\n', '```\n', 'pip install tensorflow==2.8 # for LSTM\n', 'pip install sdv<0.18 # for ACTGAN\n', 'pip install torch==1.13.1 # for Timeseries DGAN\n', '```\n', '\n', 'To install the actual `gretel-synthetics` package, first clone the repo and then...\n', '\n', '```\n', 'pip install -U .\n', '```\n', '\n', '_or_\n', '\n', '```\n', 'pip install gretel-synthetics\n', '```\n', '\n', '_then..._\n', '\n', '```\n', '$ pip install jupyter\n', '$ jupyter notebook\n', '```\n', '\n', 'When the UI launches in your browser, navigate to `examples/synthetic_records.ipynb` and get generating!\n', '\n', 'If you want to install `gretel-synthetics` locally and use a GPU (recommended):\n', '\n', '1. Create a virtual environment (e.g. using `conda`)\n', '\n', '```\n', '$ conda create --name tf python=3.9\n', '```\n', '\n', '2. Activate the virtual environment\n', '\n', '```\n', '$ conda activate tf\n', '```\n', '\n', '3. Run the setup script `./setup-utils/setup-gretel-synthetics-tensorflow24-with-gpu.sh`\n', '\n', 'The last step will install all the necessary software packages for GPU usage, `tensorflow=2.8` and `gretel-synthetics`.\n', 'Note that this script works only for Ubuntu 18.04. You might need to modify it for other OS versions.\n', '\n', '## Timeseries DGAN Overview\n', '\n', 'The [timeseries DGAN module](https://synthetics.docs.gretel.ai/en/stable/models/timeseries_dgan.html#timeseries-dgan) contains a PyTorch implementation of a DoppelGANger model that is optimized for timeseries data. Similar to tensorflow, you will need to manually install pytorch:\n', '\n', '```\n', 'pip install torch==1.13.1\n', '```\n', '\n', '[This notebook](https://github.com/gretelai/gretel-synthetics/blob/master/examples/timeseries_dgan.ipynb) shows basic usage on a small data set of smart home sensor readings.\n', '\n', '## ACTGAN Overview\n', '\n', 'ACTGAN (Anyway CTGAN) is an extension of the popular [CTGAN implementation](https://sdv.dev/SDV/user_guides/single_table/ctgan.html) that provides\n', 'some additiona functionality to improve memory usage, autodetection and transformation of columns, and more.\n', '\n', 'To use this model, you will need to manually install SDV:\n', '\n', '```\n', 'pip install sdv<0.18\n', '```\n', '\n', 'Keep in mind that this will also install several dependencies like PyTorch that SDV relies on, which may conflict with PyTorch\n', 'versions installed for use with other models like Timeseries DGAN.\n', '\n', 'The ACTGAN interface is a superset of the CTGAN interface. To see the additional features, please take a look at the ACTGAN demo notebook in the `examples` directory of this repo.\n', '\n', '## LSTM Overview\n', '\n', 'This package allows developers to quickly get immersed with synthetic data generation through the use of neural networks. The more complex pieces of working with libraries like Tensorflow and differential privacy are bundled into friendly Python classes and functions. There are two high level modes that can be utilized.\n', '\n', '### Simple Mode\n', '\n', 'The simple mode will train line-per-line on an input file of text. When generating data, the generator will yield a custom object that can be used a variety of different ways based on your use case. [This notebook](https://github.com/gretelai/gretel-synthetics/blob/master/examples/tensorflow/simple-character-model.ipynb) demonstrates this mode.\n', '\n', '### DataFrame Mode\n', '\n', 'This library supports CSV / DataFrames natively using the DataFrame ""batch"" mode. This module provided a wrapper around our simple mode that is geared for working with tabular data. Additionally, it is capabable of handling a high number of columns by breaking the input DataFrame up into ""batches"" of columns and training a model on each batch. [This notebook](https://github.com/gretelai/gretel-synthetics/blob/master/examples/dataframe_batch.ipynb) shows an overview of using this library with DataFrames natively.\n', '\n', '### Components\n', '\n', 'There are four primary components to be aware of when using this library.\n', '\n', '1. Configurations. Configurations are classes that are specific to an underlying ML engine used to train and generate data. An example would be using `TensorFlowConfig` to create all the necessary parameters to train a model based on TF. `LocalConfig` is aliased to `TensorFlowConfig` for backwards compatability with older versions of the library. A model is saved to a designated directory, which can optionally be archived and utilized later.\n', '\n', '2. Tokenizers. Tokenizers convert input text into integer based IDs that are used by the underlying ML engine. These tokenizers can be created and sent to the training input. This is optional, and if no specific tokenizer is specified then a default one will be used. You can find [an example](https://github.com/gretelai/gretel-synthetics/blob/master/examples/tensorflow/batch-df-char-tokenizer.ipynb) here that uses a simple char-by-char tokenizer to build a model from an input CSV. When training in a non-differentially private mode, we suggest using the default `SentencePiece` tokenizer, an unsupervised tokenizer that learns subword units (e.g., **byte-pair-encoding (BPE)** [[Sennrich et al.](http://www.aclweb.org/anthology/P16-1162)]) and **unigram language model** [[Kudo.](https://arxiv.org/abs/1804.10959)]) for faster training and increased accuracy of the synthetic model.\n', '\n', '3. Training. Training a model combines the configuration and tokenizer and builds a model, which is stored in the designated directory, that can be used to generate new records.\n', '\n', '4. Generation. Once a model is trained, any number of new lines or records can be generated. Optionally, a record validator can be provided to ensure that the generated data meets any constraints that are necessary. See our notebooks for examples on validators.\n', '\n', '### Utilities\n', '\n', 'In addition to the four primary components, the `gretel-synthetics` package also ships with a set of utilities that are helpful for training advanced synthetics models and evaluating synthetic datasets.\n', '\n', 'Some of this functionality carries large dependencies, so they are shipped as an extra called `utils`. To install these dependencies, you may run\n', '\n', '```\n', 'pip install gretel-synthetics[utils]\n', '```\n', '\n', 'For additional details, please refer to the [Utility module API docs](https://synthetics.docs.gretel.ai/en/latest/utils/index.html).\n', '\n', '### Differential Privacy\n', '\n', 'Differential privacy support for our TensorFlow mode is built on the great work being done by the Google TF team and their [TensorFlow Privacy library](https://github.com/tensorflow/privacy).\n', '\n', 'When utilizing DP, we currently recommend using the character tokenizer as it will only create a vocabulary of single tokens and removes the risk of sensitive data being memorized as actual tokens that can be replayed during generation.\n', '\n', 'There are also a few configuration options that are notable such as:\n', '\n', '- `predict_batch_size` should be set to 1\n', '- `dp` should be enabled\n', '- `learning_rate`, `dp_noise_multiplier`, `dp_l2_norm_clip`, and `dp_microbatches` can be adjusted to achieve various epsilon values.\n', '- `reset_states` should be disabled\n', '\n', 'Please see our [example Notebook](https://github.com/gretelai/gretel-synthetics/blob/master/examples/tensorflow/diff_privacy.ipynb) for training a DP model based on the [Netflix Prize](https://en.wikipedia.org/wiki/Netflix_Prize) dataset.\n']"
Synthetic+Data,sdv-dev/SDGym,sdv-dev,https://api.github.com/repos/sdv-dev/SDGym,204,55,13,"['https://api.github.com/users/csala', 'https://api.github.com/users/katxiao', 'https://api.github.com/users/leix28', 'https://api.github.com/users/fealho', 'https://api.github.com/users/pvk-developer', 'https://api.github.com/users/ManuelAlvarezC', 'https://api.github.com/users/amontanez24', 'https://api.github.com/users/Elesa', 'https://api.github.com/users/tejuafonja', 'https://api.github.com/users/Baukebrenninkmeijer', 'https://api.github.com/users/JDTheRipperPC', 'https://api.github.com/users/k15z', 'https://api.github.com/users/sbrugman']",Python,2023-04-09T16:37:22Z,https://raw.githubusercontent.com/sdv-dev/SDGym/master/README.md,"['<div align=""center"">\n', '<br/>\n', '<p align=""center"">\n', '    <i>This repository is part of <a href=""https://sdv.dev"">The Synthetic Data Vault Project</a>, a project from <a href=""https://datacebo.com"">DataCebo</a>.</i>\n', '</p>\n', '\n', '[![Development Status](https://img.shields.io/badge/Development%20Status-2%20--%20Pre--Alpha-yellow)](https://pypi.org/search/?c=Development+Status+%3A%3A+2+-+Pre-Alpha)\n', '[![Travis](https://travis-ci.org/sdv-dev/SDGym.svg?branch=master)](https://travis-ci.org/sdv-dev/SDGym)\n', '[![PyPi Shield](https://img.shields.io/pypi/v/sdgym.svg)](https://pypi.python.org/pypi/sdgym)\n', '[![Downloads](https://pepy.tech/badge/sdgym)](https://pepy.tech/project/sdgym)\n', '[![Slack](https://img.shields.io/badge/Community-Slack-blue?style=plastic&logo=slack)](https://bit.ly/sdv-slack-invite)\n', '\n', '<div align=""left"">\n', '<br/>\n', '<p align=""center"">\n', '<a href=""https://github.com/sdv-dev/SDGym"">\n', '<img align=""center"" width=40% src=""https://github.com/sdv-dev/SDV/blob/master/docs/images/SDGym-DataCebo.png""></img>\n', '</a>\n', '</p>\n', '</div>\n', '\n', '</div>\n', '\n', '# Overview\n', '\n', 'The Synthetic Data Gym (SDGym) is a benchmarking framework for modeling and generating\n', 'synthetic data. Measure performance and memory usage across different synthetic data modeling\n', 'techniques – classical statistics, deep learning and more!\n', '\n', '<img align=""center"" src=""docs/images/SDGym_Results.png""></img>\n', '\n', 'The SDGym library integrates with the Synthetic Data Vault ecosystem. You can use any of its\n', 'synthesizers, datasets or metrics for benchmarking. You also customize the process to include\n', 'your own work.\n', '\n', '* **Datasets**: Select any of the publicly available datasets from the SDV project, or input your own data.\n', '* **Synthesizers**: Choose from any of the SDV synthesizers and baselines. Or write your own custom\n', 'machine learning model.\n', '* **Evaluation**: In addition to performance and memory usage, you can also measure synthetic data\n', 'quality and privacy through a variety of metrics\n', '\n', '# Install\n', '\n', 'Install SDGym using pip or conda. We recommend using a virtual environment to avoid conflicts with other software on your device.\n', '\n', '```bash\n', 'pip install sdgym\n', '```\n', '\n', '```bash\n', 'conda install -c pytorch -c conda-forge sdgym\n', '```\n', '\n', 'For more information about using SDGym, visit the [SDGym Documentation](https://docs.sdv.dev/sdgym).\n', '\n', '# Usage\n', '\n', ""Let's benchmark synthetic data generation for single tables. First, let's define which modeling\n"", ""techniques we want to use. Let's choose a few synthesizers from the SDV library and a few others\n"", 'to use as baselines.\n', '\n', '```python\n', '# these synthesizers come from the SDV library\n', '# each one uses different modeling techniques\n', ""sdv_synthesizers = ['GaussianCopulaSynthesizer', 'CTGANSynthesizer']\n"", '\n', '# these basic synthesizers are available in SDGym\n', '# as baselines\n', ""baseline_synthesizers = ['UniformSynthesizer']\n"", '```\n', '\n', 'Now, we can benchmark the different techniques:\n', '```python\n', 'import sdgym\n', '\n', 'sdgym.benchmark_single_table(\n', '    synthesizers=(sdv_synthesizers + baseline_synthesizers)\n', ')\n', '```\n', '\n', 'The result is a detailed performance, memory and quality evaluation across the synthesizers\n', 'on a variety of publicly available datasets.\n', '\n', '## Supplying a custom synthesizer\n', '\n', 'Benchmark your own synthetic data generation techniques. Define your synthesizer by\n', 'specifying the training logic (using machine learning) and the sampling logic.\n', '\n', '```python\n', 'def my_training_logic(data, metadata):\n', '    # create an object to represent your synthesizer\n', '    # train it using the data\n', '    return synthesizer\n', '\n', 'def my_sampling_logic(trained_synthesizer, num_rows):\n', '    # use the trained synthesizer to create\n', '    # num_rows of synthetic data\n', '    return synthetic_data\n', '```\n', '\n', 'Learn more in the [Custom Synthesizers Guide](https://docs.sdv.dev/sdgym/customization/synthesizers/custom-synthesizers).\n', '\n', '## Customizing your datasets\n', '\n', 'The SDGym library includes many publicly available datasets that you can include right away.\n', 'List these using the ``get_available_datasets`` feature.\n', '\n', '```python\n', 'sdgym.get_available_datasets()\n', '```\n', '\n', '```\n', 'dataset_name   size_MB     num_tables\n', 'KRK_v1         0.072128    1\n', 'adult          3.907448    1\n', 'alarm          4.520128    1\n', 'asia           1.280128    1\n', '...\n', '```\n', '\n', 'You can also include any custom, private datasets that are stored on your computer on an\n', 'Amazon S3 bucket.\n', '\n', '```\n', ""my_datasets_folder = 's3://my-datasets-bucket'\n"", '```\n', '\n', 'For more information, see the docs for [Customized Datasets](https://docs.sdv.dev/sdgym/customization/datasets).\n', '\n', ""# What's next?\n"", '\n', 'Visit the [SDGym Documentation](https://docs.sdv.dev/sdgym) to learn more!\n', '\n', '---\n', '\n', '\n', '<div align=""center"">\n', '<a href=""https://datacebo.com""><img align=""center"" width=40% src=""https://github.com/sdv-dev/SDV/blob/master/docs/images/DataCebo.png""></img></a>\n', '</div>\n', '<br/>\n', '<br/>\n', '\n', ""[The Synthetic Data Vault Project](https://sdv.dev) was first created at MIT's [Data to AI Lab](\n"", 'https://dai.lids.mit.edu/) in 2016. After 4 years of research and traction with enterprise, we\n', 'created [DataCebo](https://datacebo.com) in 2020 with the goal of growing the project.\n', 'Today, DataCebo is the proud developer of SDV, the largest ecosystem for\n', 'synthetic data generation & evaluation. It is home to multiple libraries that support synthetic\n', 'data, including:\n', '\n', '* 🔄 Data discovery & transformation. Reverse the transforms to reproduce realistic data.\n', '* 🧠 Multiple machine learning models -- ranging from Copulas to Deep Learning -- to create tabular,\n', '  multi table and time series data.\n', '* 📊 Measuring quality and privacy of synthetic data, and comparing different synthetic data\n', '  generation models.\n', '\n', '[Get started using the SDV package](https://sdv.dev/SDV/getting_started/install.html) -- a fully\n', 'integrated solution and your one-stop shop for synthetic data. Or, use the standalone libraries\n', 'for specific needs.\n']"
Synthetic+Data,ankush-me/SynthText,ankush-me,https://api.github.com/repos/ankush-me/SynthText,1877,608,3,"['https://api.github.com/users/ankush-me', 'https://api.github.com/users/carandraug', 'https://api.github.com/users/codeVerySlow']",Python,2023-04-05T14:11:34Z,https://raw.githubusercontent.com/ankush-me/SynthText/master/README.md,"['# SynthText\n', 'Code for generating synthetic text images as described in [""Synthetic Data for Text Localisation in Natural Images"", Ankush Gupta, Andrea Vedaldi, Andrew Zisserman, CVPR 2016](https://www.robots.ox.ac.uk/~vgg/data/scenetext/).\n', '\n', '\n', '**Synthetic Scene-Text Image Samples**\n', '![Synthetic Scene-Text Samples](samples.png ""Synthetic Samples"")\n', '\n', 'The code in the `master` branch is for Python2. Python3 is supported in the `python3` branch.\n', '\n', 'The main dependencies are:\n', '\n', '```\n', 'pygame==2.0.0, opencv (cv2), PIL (Image), numpy, matplotlib, h5py, scipy\n', '```\n', '\n', '### Generating samples\n', '\n', '```\n', 'python gen.py --viz [--datadir <path-to-dowloaded-renderer-data>]\n', '```\n', 'where, `--datadir` points to the `renderer_data` directory included in the\n', '[data torrent](https://academictorrents.com/details/2dba9518166cbd141534cbf381aa3e99a087e83c).\n', 'Specifying this `datadir` is optional, and if not specified, the script will\n', 'automatically download and extract the same `renderer.tar.gz` data file (~24 M).\n', 'This data file includes:\n', '\n', '  - **sample.h5**: This is a sample h5 file which contains a set of 5 images along with their depth and segmentation information. Note, this is just given as an example; you are encouraged to add more images (along with their depth and segmentation information) to this database for your own use.\n', '  - **fonts**: three sample fonts (add more fonts to this folder and then update `fonts/fontlist.txt` with their paths).\n', '  - **newsgroup**: Text-source (from the News Group dataset). This can be subsituted with any text file. Look inside `text_utils.py` to see how the text inside this file is used by the renderer.\n', '  - **models/colors_new.cp**: Color-model (foreground/background text color model), learnt from the IIIT-5K word dataset.\n', '  - **models**: Other cPickle files (**char\\_freq.cp**: frequency of each character in the text dataset; **font\\_px2pt.cp**: conversion from pt to px for various fonts: If you add a new font, make sure that the corresponding model is present in this file, if not you can add it by adapting `invert_font_size.py`).\n', '\n', 'This script will generate random scene-text image samples and store them in an h5 file in `results/SynthText.h5`. If the `--viz` option is specified, the generated output will be visualized as the script is being run; omit the `--viz` option to turn-off the visualizations. If you want to visualize the results stored in  `results/SynthText.h5` later, run:\n', '\n', '```\n', 'python visualize_results.py\n', '```\n', '### Pre-generated Dataset\n', 'A dataset with approximately 800000 synthetic scene-text images generated with this code can be found [here](https://www.robots.ox.ac.uk/~vgg/data/scenetext/).\n', '\n', '### Adding New Images\n', 'Segmentation and depth-maps are required to use new images as background. Sample scripts for obtaining these are available [here](https://github.com/ankush-me/SynthText/tree/master/prep_scripts).\n', '\n', '* `predict_depth.m` MATLAB script to regress a depth mask for a given RGB image; uses the network of [Liu etal.](https://bitbucket.org/fayao/dcnf-fcsp/) However, more recent works (e.g., [this](https://github.com/iro-cp/FCRN-DepthPrediction)) might give better results.\n', '* `run_ucm.m` and `floodFill.py` for getting segmentation masks using [gPb-UCM](https://github.com/jponttuset/mcg).\n', '\n', 'For an explanation of the fields in `sample.h5` (e.g.: `seg`,`area`,`label`), please check this [comment](https://github.com/ankush-me/SynthText/issues/5#issuecomment-274490044).\n', '\n', '### Pre-processed Background Images\n', '\n', 'The 8,000 background images used in the paper, along with their\n', 'segmentation and depth masks, are included in the [same\n', 'torrent](https://academictorrents.com/details/2dba9518166cbd141534cbf381aa3e99a087e83c)\n', 'as the pre-generated dataset under the `bg_data` directory.  The files are:\n', '\n', '|    filenames    |                      description                     |\n', '|:--------------- |:---------------------------------------------------- |\n', '| `imnames.cp`    | names of images which do not contain background text |\n', '| `bg_img.tar.gz` | images (filter these using `imnames.cp`)             |\n', '| `depth.h5`      | depth maps                                           |\n', '| `seg.h5`        | segmentation maps                                    |\n', '\n', '#### Downloading without BitTorrent\n', '\n', 'Downloading with BitTorrent is strongly recommended.  If that is not\n', 'possible, the files are also available to download over http from\n', '`https://thor.robots.ox.ac.uk/~vgg/data/scenetext/preproc/<filename>`,\n', 'where, `<filename>` can be:\n', '\n', '|    filenames    | size |             md5 hash             |\n', '|:--------------- | ----:|:-------------------------------- |\n', '| `imnames.cp`    | 180K |                                  |\n', '| `bg_img.tar.gz` | 8.9G | 3eac26af5f731792c9d95838a23b5047 |\n', '| `depth.h5`      |  15G | af97f6e6c9651af4efb7b1ff12a5dc1b |\n', '| `seg.h5`        | 6.9G | 1605f6e629b2524a3902a5ea729e86b2 |\n', '\n', 'Note: due to large size, `depth.h5` is also available for download as 3-part split-files of 5G each.\n', 'These part files are named: `depth.h5-00, depth.h5-01, depth.h5-02`. Download using the path above, and put them together using `cat depth.h5-0* > depth.h5`.\n', 'To download, use the something like the following:\n', '```\n', 'wget --continue https://thor.robots.ox.ac.uk/~vgg/data/scenetext/preproc/<filename>\n', '```\n', '[`use_preproc_bg.py`](https://github.com/ankush-me/SynthText/blob/master/use_preproc_bg.py) provides sample code for reading this data.\n', '\n', 'Note: I do not own the copyright to these images.\n', '\n', '### Generating Samples with Text in non-Latin (English) Scripts\n', '- @JarveeLee has modified the pipeline for generating samples with Chinese text [here](https://github.com/JarveeLee/SynthText_Chinese_version).\n', '- @adavoudi has modified it for arabic/persian script, which flows from right-to-left [here](https://github.com/adavoudi/SynthText).\n', '- @MichalBusta has adapted it for a number of languages (e.g. Bangla, Arabic, Chinese, Japanese, Korean) [here](https://github.com/MichalBusta/E2E-MLT).\n', '- @gachiemchiep has adapted for Japanese [here](https://github.com/gachiemchiep/SynthText).\n', '- @gungui98 has adapted for Vietnamese [here](https://github.com/gungui98/SynthText).\n', '- @youngkyung has adapted for Korean [here](https://github.com/youngkyung/SynthText_kr).\n', '- @kotomiDu has developed an interactive UI for generating images with text [here](https://github.com/kotomiDu/GameSynthText).\n', '- @LaJoKoch has adapted for German [here](https://github.com/LaJoKoch/SynthTextGerman).\n', '\n', '### Further Information\n', 'Please refer to the paper for more information, or contact me (email address in the paper).\n']"
Synthetic+Data,tirthajyoti/Synthetic-data-gen,tirthajyoti,https://api.github.com/repos/tirthajyoti/Synthetic-data-gen,68,37,1,['https://api.github.com/users/tirthajyoti'],Python,2022-11-08T22:55:34Z,https://raw.githubusercontent.com/tirthajyoti/Synthetic-data-gen/master/README.md,"['# Synthetic-data-gen\n', 'Various methods for generating synthetic data for data science and ML.\n', '\n', 'Read my article on Medium **""[Synthetic data generation — a must-have skill for new data scientists](https://towardsdatascience.com/synthetic-data-generation-a-must-have-skill-for-new-data-scientists-915896c0c1ae)""**\n', '\n', 'Also, a related article on generating random variables from scratch: **""[How to generate random variables from scratch (no library used)](https://towardsdatascience.com/how-to-generate-random-variables-from-scratch-no-library-used-4b71eb3c8dc7)""**\n', '\n', '---\n', '## Notebooks\n', '\n', '* [Scikit-learn data generation (regression/classification/clustering) methods](https://github.com/tirthajyoti/Synthetic-data-gen/blob/master/Notebooks/Scikit-learn-data-generation.ipynb)\n', '* [Random regression and classification problem generation from symbolic expressions (using `SymPy`)](https://github.com/tirthajyoti/Synthetic-data-gen/blob/master/Notebooks/Symbolic%20regression%20classification%20generator.ipynb)\n', '* [Synthesizing time series](https://github.com/tirthajyoti/Synthetic-data-gen/blob/master/Notebooks/Synth_Time_series.ipynb)\n', '* [Generating Gaussian mixture model data](https://github.com/tirthajyoti/Synthetic-data-gen/blob/master/Notebooks/GMM_generator.ipynb)\n', '\n', '## Why do you need the skill of synthetic data generation?\n', '\n', 'Imagine you are tinkering with a cool machine learning algorithm like SVM or a deep neural net. What kind of dataset you should practice them on? If you are learning from scratch, the advice is to start with simple, small-scale datasets which you can plot in two dimensions to understand the patterns visually and see for yourself the working of the ML algorithm in an intuitive fashion. For example, [here is an excellent article](https://www.analyticsvidhya.com/blog/2018/05/24-ultimate-data-science-projects-to-boost-your-knowledge-and-skills/) on various datasets you can try at various level of learning.\n', '\n', 'This is a great start. But it is not all.\n', '\n', 'Sure, you can go up a level and find yourself a real-life large dataset to practice the algorithm on. But that is still a fixed dataset, with a fixed number of samples, a fixed pattern, and a fixed degree of class separation between positive and negative samples (if we assume it to be a classification problem). Are you learning all the intricacies of the algorithm in terms of\n', '- sample complexity,\n', '- computational efficiency,\n', '- ability to handle class imbalance,\n', '- robustness of the metrics in the face of varying degree of class separation\n', '- bias-variance trade-off as a function of data complexity\n', '\n', 'Probably not. **Perhaps, no single dataset can lend all these deep insights for a given ML algorithm**. But, these are extremely important insights to master for you to become a true expert practitioner of machine learning. So, you will need an **extremely rich and sufficiently large dataset, which is amenable enough for all these experimentation**.\n', '\n', 'So, what can you do in this situation? Scour the internet for more datasets and just hope that some of them will bring out the limitations and challenges, associated with a particular algorithm, and help you learn?\n', '\n', 'Yes, it is a possible approach but may not be the most viable or optimal one in terms of time and effort. Good datasets may not be clean or easily obtainable. You may spend much more time looking for, extracting, and wrangling with a suitable dataset than putting that effort to understand the ML algorithm.\n', '\n', 'Make no mistake. **The experience of searching for a real life dataset, extracting it, running exploratory data analysis, and wrangling with it to make it suitably prepared for a machine learning based modeling is invaluable**. I know because I wrote a book about it :-)\n', '\n', 'But that can be taught and practiced separately. In many situations, however, **you may just want to have access to a flexible dataset (or several of them) to ‘teach’ you the ML algorithm in all its gory details**.\n', '\n', 'Surprisingly enough, in many cases, such teaching can be done with **synthetic datasets**.\n', '\n', '## What is a synthetic dataset?\n', 'As the name suggests, quite obviously, a synthetic dataset is a repository of data that is generated programmatically. So, it is not collected by any real-life survey or experiment. Its main purpose, therefore, is **to be flexible and rich enough to help an ML practitioner conduct fascinating experiments with various classification, regression, and clustering algorithms**. Desired properties are,\n', '\n', '* It can be numerical, binary, or categorical (ordinal or non-ordinal),\n', '* The number of features and length of the dataset should be arbitrary\n', '* It should preferably be random and the user should be able to choose a wide variety of statistical distribution to base this data upon i.e. the underlying random process can be precisely controlled and tuned,\n', '* If it is used for classification algorithms, then the degree of class separation should be controllable to make the learning problem easy or hard,\n', '* Random noise can be interjected in a controllable manner\n', '* For a regression problem, a complex, non-linear generative process can be used for sourcing the data\n']"
Synthetic+Data,stefan-jansen/synthetic-data-for-finance,stefan-jansen,https://api.github.com/repos/stefan-jansen/synthetic-data-for-finance,79,29,1,['https://api.github.com/users/stefan-jansen'],Python,2023-04-05T12:46:11Z,https://raw.githubusercontent.com/stefan-jansen/synthetic-data-for-finance/main/README.md,"['# Generative Adversarial Nets for Synthetic Time Series Data\n', '\n', 'This repo shows how to create synthetic time-series data using generative adversarial networks (GAN). GANs train a generator and a discriminator network in a competitive setting so that the generator learns to produce samples that the discriminator cannot distinguish from a given class of training data. The goal is to yield a generative model capable of producing synthetic samples representative of this class.\n', 'While most popular with image data, GANs have also been used to generate synthetic time-series data in the medical domain. Subsequent experiments with financial data explored whether GANs can produce alternative price trajectories useful for ML training or strategy backtests. \n', '\n', 'We replicate the 2019 NeurIPS [Time-Series GAN](https://proceedings.neurips.cc/paper/2019/file/c9efe5f26cd17ba6216bbe2a7d26d490-Paper.pdf) paper by Jinsung Yoon, et al., to illustrate the approach and demonstrate the results. The material is based on the 2<sup>nd</sup> edition of my book on [Machine Learning for Trading]((https://www.amazon.com/Machine-Learning-Algorithmic-Trading-alternative/dp/1839217715?pf_rd_r=GZH2XZ35GB3BET09PCCA&pf_rd_p=c5b6893a-24f2-4a59-9d4b-aff5065c90ec&pd_rd_r=91a679c7-f069-4a6e-bdbb-a2b3f548f0c8&pd_rd_w=2B0Q0&pd_rd_wg=GMY5S&ref_=pd_gw_ci_mcx_mr_hp_d)) (see [GitHub repo](https://github.com/stefan-jansen/machine-learning-for-trading)).  \n', '\n', '<p align=""center"">\n', '<img src=""https://i.imgur.com/W1Rp89K.png"" width=""60%"">\n', '</p>\n', '\n', '## Content\n', '\n', '1. [Generative adversarial networks for synthetic data](#generative-adversarial-networks-for-synthetic-data)\n', '    * [Comparing generative and discriminative models](#comparing-generative-and-discriminative-models)\n', '    * [Adversarial training: a zero-sum game of trickery](#adversarial-training-a-zero-sum-game-of-trickery)\n', '2. [Code example: TimeGAN: Adversarial Training for Synthetic Financial Data](#code-example-timegan-adversarial-training-for-synthetic-financial-data)\n', '    * [Learning the data generation process across features and time](#learning-the-data-generation-process-across-features-and-time)\n', '    * [Combining adversarial and supervised training with time-series embedding](#combining-adversarial-and-supervised-training-with-time-series-embedding)\n', '    * [The four components of the TimeGAN architecture](#the-four-components-of-the-timegan-architecture)\n', '    * [Implementing TimeGAN using TensorFlow 2](#implementing-timegan-using-tensorflow-2)\n', '    * [Evaluating the quality of synthetic time-series data](#evaluating-the-quality-of-synthetic-time-series-data)\n', '3. [Resources](#resources)\n', ""    * [How GAN's work](#how-gans-work)\n"", '    * [Implementation](#implementation)\n', '    * [The rapid evolution of the GAN architecture zoo](#the-rapid-evolution-of-the-gan-architecture-zoo)\n', '    * [Applications](#applications)\n', '\n', '## Generative adversarial networks for synthetic data\n', '\n', 'The [book](https://www.amazon.com/Machine-Learning-Algorithmic-Trading-alternative/dp/1839217715?pf_rd_r=GZH2XZ35GB3BET09PCCA&pf_rd_p=c5b6893a-24f2-4a59-9d4b-aff5065c90ec&pd_rd_r=91a679c7-f069-4a6e-bdbb-a2b3f548f0c8&pd_rd_w=2B0Q0&pd_rd_wg=GMY5S&ref_=pd_gw_ci_mcx_mr_hp_d) mostly focuses on supervised learning algorithms that receive input data and predict an outcome, which we can compare to the ground truth to evaluate their performance. Such algorithms are also called discriminative models because they learn to differentiate between different output values.\n', 'Generative adversarial networks (GANs) are an instance of generative models like the variational autoencoder covered in [Chapter 20](https://github.com/stefan-jansen/machine-learning-for-trading/tree/master/20_autoencoders_for_conditional_risk_factors).\n', '\n', '### Comparing generative and discriminative models\n', '\n', 'Discriminative models learn how to differentiate among outcomes y, given input data X. In other words, they learn the probability of the outcome given the data: p(y | X). Generative models, on the other hand, learn the joint distribution of inputs and outcome p(y, X). \n', '\n', 'While generative models can be used as discriminative models using Bayes Rule to compute which class is most likely (see [Chapter 10](https://github.com/stefan-jansen/machine-learning-for-trading/tree/master/10_bayesian_machine_learning)), it appears often preferable to solve the prediction problem directly rather than by solving the more general generative challenge first.\n', '\n', '### Adversarial training: a zero-sum game of trickery\n', '\n', 'The key innovation of GANs is a new way of learning the data-generating probability distribution. The algorithm sets up a competitive, or adversarial game between two neural networks called the generator and the discriminator.\n', '\n', '<p align=""center"">\n', '<img src=""https://i.imgur.com/0vuUsY0.png"" width=""80%"">\n', '</p>\n', '\n', '## Code example: How to build a GAN using TensorFlow 2\n', '\n', 'To illustrate the implementation of a generative adversarial network using Python, we use the deep convolutional GAN (DCGAN) example discussed earlier in this section to synthesize images from the fashion MNIST dataset that we first encountered in Chapter 13. \n', '\n', 'The notebook [deep_convolutional_generative_adversarial_network](https://github.com/stefan-jansen/machine-learning-for-trading/blob/master/21_gans_for_synthetic_time_series/01_deep_convolutional_generative_adversarial_network.ipynb) illustrates the implementation of a GAN using Python. It uses the Deep Convolutional GAN (DCGAN) example to synthesize images from the fashion MNIST dataset\n', '\n', '## Code example: TimeGAN: Adversarial Training for Synthetic Financial Data\n', '\n', 'Generating synthetic time-series data poses specific challenges above and beyond those encountered when designing GANs for images. \n', 'In addition to the distribution over variables at any given point, such as pixel values or the prices of numerous stocks, a generative model for time-series data should also learn the temporal dynamics that shapes how one sequence of observations follows another (see also discussion in Chapter 9: [Time Series Models for Volatility Forecasts and Statistical Arbitrage](../09_time_series_models)).\n', '\n', 'Very recent and promising research by Yoon, Jarrett, and van der Schaar, presented at NeurIPS in December 2019, introduces a novel [Time-Series Generative Adversarial Network](https://papers.nips.cc/paper/8789-time-series-generative-adversarial-networks.pdf) (TimeGAN) framework that aims to account for temporal correlations by combining supervised and unsupervised training. \n', 'The model learns a time-series embedding space while optimizing both supervised and adversarial objectives that encourage it to adhere to the dynamics observed while sampling from historical data during training. \n', 'The authors test the model on various time series, including historical stock prices, and find that the quality of the synthetic data significantly outperforms that of available alternatives.\n', '\n', '### Learning the data generation process across features and time\n', '\n', 'A successful generative model for time-series data needs to capture both the cross-sectional distribution of features at each point in time and the longitudinal relationships among these features over time. \n', 'Expressed in the image context we just discussed, the model needs to learn not only what a realistic image looks like, but also how one image evolves from the next as in a video.\n', '\n', '### Combining adversarial and supervised training with time-series embedding\n', '\n', 'Prior attempts at generating time-series data like the recurrent (conditional) GAN relied on recurrent neural networks (RNN, see Chapter 19, [RNN for Multivariate Time Series and Sentiment Analysis](../19_recurrent_neural_nets)) in the roles of generator and discriminator. \n', '\n', 'TimeGAN explicitly incorporates the autoregressive nature of time series by combining the unsupervised adversarial loss on both real and synthetic sequences familiar from the DCGAN example with a stepwise supervised loss with respect to the original data. \n', 'The goal is to reward the model for learning the distribution over transitions from one point in time to the next present in the historical data.\n', '\n', '### The four components of the TimeGAN architecture\n', '\n', 'The TimeGAN architecture combines an adversarial network with an autoencoder and has thus four network components as depicted in Figure 21.4:\n', 'Autoencoder: embedding and recovery networks\n', 'Adversarial Network: sequence generator and sequence discriminator components\n', '<p align=""center"">\n', '<img src=""https://i.imgur.com/WqoXbr8.png"" width=""80%"">\n', '</p>\n', '\n', '### Implementing TimeGAN using TensorFlow 2\n', '\n', 'In this section, we implement the TimeGAN architecture just described. The authors provide sample code using TensorFlow 1 that we port to TensorFlow 2. Building and training TimeGAN requires several steps:\n', '1. Selecting and preparing real and random time series inputs\n', '2. Creating the key TimeGAN model components\n', '3. Defining the various loss functions and train steps used during the three training phases\n', '4. Running the training loops and logging the results\n', '5. Generating synthetic time series and evaluating the results\n', '\n', 'The notebook [TimeGAN_TF2](02_TimeGAN_TF2.ipynb) shows how to implement these steps.\n', '\n', '### Installation\n', '\n', 'Using a GPU is recommended to speed up training. There are several options to run the notebook:\n', '1) Use a [Docker](https://docs.docker.com/get-started/overview/) image provided by TensorFlow with either CPU or GPU support. See [instructions](https://www.tensorflow.org/install/docker). \n', ""    - To start the container configured with TensorFlow and mount the project directory in the `/home` directory, run the following command in this repo's root folder on your machine:\n"", '        - With GPU support (using [nvidia-docker](https://github.com/NVIDIA/nvidia-docker) as describe in the linked [instructions](https://www.tensorflow.org/install/docker)):\n', '            ```bash\n', '            docker run --gpus all -it -v $(pwd):/home -p 8888:8888 --name ml4t tensorflow/tensorflow:latest-gpu-jupyter bash\n', '          ```\n', '      - With CPU support:\n', '          ```bash\n', '          docker run -it -v $(pwd):/home -p 8888:8888 --name ml4t tensorflow/tensorflow:latest-gpu-jupyter bash\n', '          ```\n', '    - Change into the `/home` folder of your container using `cd /home`\n', '    - Run the install script to get some requisite packages: `./install.sh`\n', '    - Then, launch the jupyter server to work with the notebooks as usual:\n', '        ```bash\n', '        jupyter notebook --ip 0.0.0.0 --no-browser --allow-root\n', '        ```\n', '2) Create a virtual environment using the `requirements.txt` file (Ubuntu only; other OS requires modifying the content).\n', '\n', '### Evaluating the quality of synthetic time-series data\n', '\n', 'The TimeGAN authors assess the quality of the generated data with respect to three practical criteria:\n', '1. **Diversity**: the distribution of the synthetic samples should roughly match that of the real data\n', '2. **Fidelity**: the sample series should be indistinguishable from the real data, and \n', '3. **Usefulness**: the synthetic data should be as useful as their real counterparts for solving a predictive task\n', '\n', 'The authors apply three methods to evaluate whether the synthetic data actually exhibits these characteristics:\n', '1. **Visualization**: for a qualitative diversity assessment of diversity, we use dimensionality reduction (principal components analysis (PCA) and t-SNE, see Chapter 13) to visually inspect how closely the distribution of the synthetic samples resembles that of the original data\n', '2. **Discriminative Score**: for a quantitative assessment of fidelity, the test error of a time-series classifier such as a 2-layer LSTM (see Chapter 18) let’s us evaluate whether real and synthetic time series can be differentiated or are, in fact, indistinguishable.\n', '3. **Predictive Score**: for a quantitative measure of usefulness, we can compare the test errors of a sequence prediction model trained on, alternatively, real or synthetic data to predict the next time step for the real data.\n', '\n', 'The notebook [evaluating_synthetic_data](03_evaluating_synthetic_data.ipynb) contains the relevant code samples.\n', '\n', '## Resources\n', '\n', ""### How GAN's work\n"", '\n', '- [NIPS 2016 Tutorial: Generative Adversarial Networks](https://arxiv.org/pdf/1701.00160.pdf), Ian Goodfellow, 2017\n', '- [Why is unsupervised learning important?](https://www.quora.com/Why-is-unsupervised-learning-important), Yoshua Bengio on Quora, 2018\n', '- [GAN Lab: Understanding Complex Deep Generative Models using Interactive Visual Experimentation](https://www.groundai.com/project/gan-lab-understanding-complex-deep-generative-models-using-interactive-visual-experimentation/), Minsuk Kahng, Nikhil Thorat, Duen Horng (Polo) Chau, Fernanda B. Viégas, and Martin Wattenberg, IEEE Transactions on Visualization and Computer Graphics, 25(1) (VAST 2018), Jan. 2019\n', '    - [GitHub](https://poloclub.github.io/ganlab/)\n', '- [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661), Ian Goodfellow, et al, 2014\n', '- [Generative Adversarial Networks: an Overview](https://arxiv.org/pdf/1710.07035.pdf), Antonia Creswell, et al, 2017\n', '- [Generative Models](https://blog.openai.com/generative-models/), OpenAI Blog\n', '\n', '### Implementation\n', '\n', '- [Deep Convolutional Generative Adversarial Network](https://www.tensorflow.org/tutorials/generative/dcgan)\n', '- [CycleGAN](https://www.tensorflow.org/tutorials/generative/cyclegan)\n', '- [Keras-GAN](https://github.com/eriklindernoren/Keras-GAN), numerous Keras GAN implementations\n', '- [PyTorch-GAN](https://github.com/eriklindernoren/PyTorch-GAN), numerous PyTorch GAN implementations\n', '\n', '\n', '### The rapid evolution of the GAN architecture zoo\n', '\n', '- [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (DCGAN)](https://arxiv.org/pdf/1511.06434.pdf), Luke Metz et al, 2016\n', '- [Conditional Generative Adversarial Net](https://arxiv.org/pdf/1411.1784.pdf), Medhi Mirza and Simon Osindero, 2014\n', '- [Infogan: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets](https://arxiv.org/pdf/1606.03657.pdf), Xi Chen et al, 2016\n', '- [Stackgan: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks](https://arxiv.org/pdf/1612.03242.pdf), Shaoting Zhang et al, 2016\n', '- [Photo-realistic Single Image Super-resolution Using a Generative Adversarial Network](https://arxiv.org/pdf/1609.04802.pdf), Alejando Acosta et al, 2016\n', '- [Unpaired Image-to-image Translation Using Cycle-consistent Adversarial Networks](https://arxiv.org/pdf/1703.10593.pdf), Juan-Yan Zhu et al, 2018\n', '- [Learning What and Where to Draw](https://arxiv.org/abs/1610.02454), Scott Reed, et al 2016\n', '- [Fantastic GANs and where to find them](http://guimperarnau.com/blog/2017/03/Fantastic-GANs-and-where-to-find-them)\n', '\n', '### Applications\n', '\n', '- [Real-valued (Medical) Time Series Generation with Recurrent Conditional GANs](https://arxiv.org/abs/1706.02633), Cristóbal Esteban, Stephanie L. Hyland, Gunnar Rätsch, 2016\n', '    - [GitHub Repo](https://github.com/ratschlab/RGAN)\n', '- [MAD-GAN: Multivariate Anomaly Detection for Time Series Data with Generative Adversarial Networks](https://arxiv.org/pdf/1901.04997.pdf), Dan Li, Dacheng Chen, Jonathan Goh, and See-Kiong Ng, 2019\n', '    - [GitHub Repo](https://github.com/LiDan456/MAD-GANs)\n', '- [GAN\u200a—\u200aSome cool applications](https://medium.com/@jonathan_hui/gan-some-cool-applications-of-gans-4c9ecca35900), Jonathan Hui, 2018\n', '- [gans-awesome-applications](https://github.com/nashory/gans-awesome-applications), curated list of awesome GAN applications\n', '\n', '\n', '\n']"
Synthetic+Data,microsoft/synthetic-data-showcase,microsoft,https://api.github.com/repos/microsoft/synthetic-data-showcase,76,25,8,"['https://api.github.com/users/rracanicci', 'https://api.github.com/users/dworthen', 'https://api.github.com/users/natoverse', 'https://api.github.com/users/darrenedge', 'https://api.github.com/users/katua', 'https://api.github.com/users/andresmor-ms', 'https://api.github.com/users/microsoftopensource', 'https://api.github.com/users/microsoft-github-operations%5Bbot%5D']",Rust,2023-04-09T06:50:54Z,https://raw.githubusercontent.com/microsoft/synthetic-data-showcase/main/README.md,"['[![Rust CI](https://github.com/microsoft/synthetic-data-showcase/actions/workflows/rust-ci.yml/badge.svg?branch=main&event=push)](https://github.com/microsoft/synthetic-data-showcase/actions/workflows/rust-ci.yml)\n', '[![Javascript CI](https://github.com/microsoft/synthetic-data-showcase/actions/workflows/javascript-ci.yml/badge.svg?branch=main&event=push)](https://github.com/microsoft/synthetic-data-showcase/actions/workflows/javascript-ci.yml)\n', '[![Python CI](https://github.com/microsoft/synthetic-data-showcase/actions/workflows/python-ci.yml/badge.svg?branch=main&event=push)](https://github.com/microsoft/synthetic-data-showcase/actions/workflows/python-ci.yml)\n', '\n', '# Synthetic data showcase\n', '\n', '> Generates synthetic data and user interfaces for privacy-preserving data sharing and analysis.\n', '\n', '> Free-to-use web application for private data release: https://microsoft.github.io/synthetic-data-showcase/\n', '\n', '# Overview\n', '\n', 'In many cases, the best way to share sensitive datasets is not to share the actual sensitive datasets, but user interfaces to derived datasets that are inherently anonymous. Our name for such an interface is a _data showcase_. In this project, we provide an automated set of tools for generating the three elements of a _synthetic data showcase_:\n', '\n', '1. _Synthetic data_ representing the overall structure and statistics of the input data, without describing actual identifiable individuals.\n', '2. _Aggregate data_ reporting the number of individuals with different combinations of attributes, without disclosing exact counts.\n', '3. _Data dashboards_ enabling exploratory visual analysis of both datasets, without the need for custom data science or interface development.\n', '\n', 'To generate these elements, our tool provides two approaches to create anonymous datasets that are safe to release: (i) differential privacy and (ii) k-anonymity.\n', '\n', '# Differential privacy\n', '\n', '## Privacy guarantees\n', '\n', 'The paradigm of differential privacy (DP) offers ""safety in noise"" &ndash; just enough calibrated noise is added to the data to control the maximum possible privacy loss, $\\varepsilon$ (epsilon). When applied in the context of private data release, $\\varepsilon$ bounds the ratio of probabilities of getting an arbitrary result to an arbitrary computation when using two synthetic datasets &ndash; one generated from the sensitive dataset itself and the other from a neighboring dataset missing a single arbitrary record.\n', '\n', 'Our approach to synthesizing data with differential privacy first protects attribute combination counts in the aggregate data using our [DP Marginals](./docs/dp/dp_marginals.pdf) algorithm and then uses the resulting DP aggregate counts to derive synthetic records that retain differential privacy under the post-processing property.\n', '\n', '> For a detailed explanation of how SDS uses differential privacy, please check our [DP documentation](./docs/dp/README.md).\n', '\n', '## Usage\n', '\n', 'Use of our differential privacy synthesizer is recommended for **repeated data releases** where cumulative privacy loss must be quantified and controlled and where provable guarantees against all possible privacy attacks are desired.\n', '\n', 'Any differentially-private dataset should be evaluated for potential risks in situations where missing, fabricated, or inaccurate counts of attribute combinations could trigger inappropriate downstream decisions or actions. Our DP synthesizer prioritises the release of accurate combination counts (with minimal noise) of actual combinations (with minimal fabrication).\n', '\n', '# K-anonymity\n', '\n', '## Privacy guarantees\n', '\n', 'The paradigm of k-anonymity offers ""safety in numbers"" &ndash; combinations of attributes are only released when they occur at least k times in the sensitive dataset. When applied in the context of private data release, we interpret k as a privacy resolution determining the minimum group size that will be (a) reported explicitly in the aggregate dataset and (b) represented implicitly by the records of the synthetic dataset. This makes it possible to offer privacy guarantees in clearly understandable terms, e.g.:\n', '\n', '""All attribute combinations in this synthetic dataset describe groups of 10 or more individuals in the original sensitive dataset, therefore may never be used to infer the presence of individuals or groups smaller than 10.""\n', '\n', 'Our approach to synthesizing data with k-anonymity overcomes many of the limitations of standard [k-anonymization](https://en.wikipedia.org/wiki/K-anonymity), in which attributes of sensitive data records are generalized and suppressed until k-anonymity is reached, and only for those attributes determined in advance to be potentially identifying when used in combination (so-called quasi-identifiers). In this standard approach, all remaining sensitive attributes are released so long as k-anonymity holds for the designated quasi-identifiers. This makes the records (and thus subjects) of k-anonymized datasets susceptible to linking attacks based on auxiliary data or background knowledge.\n', '\n', 'In contrast, our k-anonymity synthesizers generate synthetic records that do not represent actual individuals, yet are composed exclusively from common combinations of attributes in the sensitive dataset. The k-anonymity guarantee therefore holds for all data columns and all combinations of attributes.\n', '\n', '## Usage\n', '\n', 'Use of our k-anonymity synthesizers is recommended only for **one-off data releases** where there is a need for precise counts of attribute combinations (at a given privacy resolution).\n', '\n', 'These synthesizers are designed to offer strong group-level protection against membership inference, i.e., preventing an adversary from inferring whether a known individual or small group of individuals is present in the sensitive dataset.\n', '\n', 'They should not be used in situations where attribute inference from homogeneity attacks are a concern, i.e., when an adversary knows that a certain individual is present in the sensitive dataset, identifies them as part of a group sharing known attributes, and then infers previously unknown attributes of the individual because those attributes are common to the group.\n', '\n', '# Quick setup\n', '\n', 'The easiest way to start is to [run the web application locally with docker](./packages/webapp/README.md#locally-run-the-web-application-with-docker). You will be able to experiment with your data and see the result in real time using the UI.\n', '\n', 'If you are looking for faster alternatives to process bigger datasets, please refer to our [python pipeline tool](./packages/python-pipeline/README.md), [CLI application tool](./packages/cli/README.md) or [python synthesizer library](./packages/lib-pacsynth/README.md).\n', '\n', '# All available tools\n', '\n', 'We provide a set of tools to synthesize, aggregate and evaluate your data, which can be used according to your use case/preference. The available tools are described below:\n', '\n', '- **Python pipeline**: if you want to synthesize, aggregate your data and also generate the dashboards for visual analysis with a single command line command in python, please check the [python pipeline tool](./packages/python-pipeline/README.md).\n', '- **Web application**: if you want to locally run a web application capable of synthesize, aggregate and evaluate your data directly on your browser using Javascript and Web Assembly, this is the tool for you. The data is processed locally and never leaves your machine. Please check the [web application tool](./packages/webapp/README.md).\n', '- **Raw CLI application**: if you only want a command line interface (CLI) around our [core Rust library](./packages/core/README.md) for data synthesis and aggregation, please check the [CLI application tool](./packages/cli/README.md).\n', '- **pac-synth library**: if want to aggregate and synthesize data locally with python, please check the [python synthesizer library](./packages/lib-pacsynth/README.md).\n', '\n', '# Quick references\n', '\n', '- [python-pipeline](./packages/python-pipeline/README.md)\n', '- [webapp](./packages/webapp/README.md)\n', '- [cli](./packages/cli/README.md)\n', '- [core](./packages/core/README.md)\n', '- [lib-wasm](./packages/lib-wasm/README.md)\n', '- [lib-python](./packages/lib-python/README.md)\n', '- [lib-pacsynth](./packages/lib-pacsynth/README.md)\n', '\n', '# License\n', '\n', 'Synthetic data showcase\n', '\n', 'MIT License\n', '\n', 'Copyright (c) Microsoft Corporation.\n', '\n', 'Permission is hereby granted, free of charge, to any person obtaining a copy\n', 'of this software and associated documentation files (the ""Software""), to deal\n', 'in the Software without restriction, including without limitation the rights\n', 'to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n', 'copies of the Software, and to permit persons to whom the Software is\n', 'furnished to do so, subject to the following conditions:\n', '\n', 'The above copyright notice and this permission notice shall be included in all\n', 'copies or substantial portions of the Software.\n', '\n', 'THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n', 'IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n', 'FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n', 'AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n', 'LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n', 'OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n', 'SOFTWARE\n', '\n', '# Contributing\n', '\n', 'This project welcomes contributions and suggestions. Most contributions require you to agree to a\n', 'Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\n', 'the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n', '\n', 'When you submit a pull request, a CLA bot will automatically determine whether you need to provide\n', 'a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\n', 'provided by the bot. You will only need to do this once across all repos using our CLA.\n', '\n', 'This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\n', 'For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\n', 'contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n', '\n', '# Acknowledgements\n', '\n', 'This project resulted from a [Tech Against Trafficking (TAT)](https://techagainsttrafficking.org/) accelerator program with the [Counter Trafficking Data Collaborative (CTDC)](https://www.ctdatacollaborative.org/) and the [International Organization for Migration (IOM)](https://www.iom.int/) on how to safely share data on identified victims of human trafficking. Read more in this [TAT blog post](https://techagainsttrafficking.org/accelerating-toward-data-insights-tech-against-trafficking-successfully-concludes-its-pilot-accelerator/).\n', '\n', '# Contact\n', '\n', 'Feedback and suggestions are welcome via email to sds-team@microsoft.com.\n']"
Synthetic+Data,theodi/synthetic-data-tutorial,theodi,https://api.github.com/repos/theodi/synthetic-data-tutorial,71,26,2,"['https://api.github.com/users/fionntan', 'https://api.github.com/users/olivierthereaux']",Python,2023-02-03T09:27:35Z,https://raw.githubusercontent.com/theodi/synthetic-data-tutorial/master/README.md,"['_Last tested: 2022-04-14. Updated the requirements and ran in Python 3.10 (although a few warnings from Pandas)._\n', '\n', '# Anonymisation with Synthetic Data Tutorial\n', '\n', '## Some questions\n', '\n', '**What is this?**\n', '\n', 'A hands-on tutorial showing how to use Python to create synthetic data.\n', '\n', '**Wait, what is this ""synthetic data"" you speak of?**\n', '\n', ""It's data that is created by an automated process which contains many of the statistical patterns of an original dataset. It is also sometimes used as a way to release data that has no personal information in it, even if the original did contain lots of data that could identify people. This means programmers and data scientists can crack on with building software and algorithms that they know will work similarly on the real data.\n"", '\n', '**Who is this tutorial for?**\n', '\n', 'For any person who programs who wants to learn about data anonymisation in general or more specifically about synthetic data.\n', '\n', '**What is it not for?**\n', '\n', ""Non-programmers. Although we think this tutorial is still worth a browse to get some of the main ideas in what goes in to anonymising a dataset. However, if you're looking for info on how to create synthetic data using the latest and greatest deep learning techniques, this is not the tutorial for you.\n"", '\n', '**Who are you?**\n', '\n', ""We're the Open Data Institute. We work with companies and governments to build an open, trustworthy data ecosystem. Anonymisation and synthetic data are some of the many, many ways we can responsibly increase access to data. If you want to learn more, [check out our site](http://theodi.org).\n"", '\n', '**Why did you make this?**\n', '\n', ""We have an [R&D program](https://theodi.org/project/data-innovation-for-uk-research-and-development/) that has a number of projects looking in to how to support innovation, improve data infrastructure and encourage ethical data sharing. One of our projects is about [managing the risks of re-identification](https://theodi.org/project/rd-broaden-access-to-personal-data-while-protecting-privacy-and-creating-a-fair-market/) in shared and open data. As you can see in the *Key outputs* section, we have other material from the project, but we thought it'd be good to have something specifically aimed at programmers who are interested in learning by doing.\n"", '\n', '**Speaking of which, can I just get to the tutorial now?**\n', '\n', ""Sure! Let's go.\n"", '\n', '## Overview\n', '\n', ""In this tutorial you are aiming to create a safe version of accident and emergency (A&E) admissions data, collected from multiple hospitals. This data contains some sensitive personal information about people's health and can't be openly shared. By removing and altering certain identifying information in the data we can greatly reduce the risk that patients can be re-identified and therefore hope to release the data.\n"", '\n', ""Just to be clear, we're not using actual A&E data but are creating our own simple, mock, version of it.\n"", '\n', 'The practical steps involve:\n', '\n', '1. Create an A&E admissions dataset which will contain (pretend) personal information.\n', '2. Run some anonymisation steps over this dataset to generate a new dataset with much less re-identification risk.\n', '3. Take this de-identified dataset and generate multiple synthetic datasets from it to reduce the re-identification risk even further.\n', '4. Analyse the synthetic datasets to see how similar they are to the original data.\n', '\n', ""You may be wondering, why can't we just do synthetic data step? If it's synthetic surely it won't contain any personal information?\n"", '\n', 'Not exactly. Patterns picked up in the original data can be transferred to the synthetic data. This is especially true for outliers. For instance if there is only one person from an certain area over 85 and this shows up in the synthetic data, we would be able to re-identify them.\n', '\n', '## Credit to others\n', '\n', ""This tutorial is inspired by the [NHS England and ODI Leeds' research](https://odileeds.org/events/synae/) in creating a synthetic dataset from NHS England's accident and emergency admissions. Please do read about their project, as it's really interesting and great for learning about the benefits and risks in creating synthetic data.\n"", '\n', ""Also, the synthetic data generating library we use is [DataSynthetizer](https://homes.cs.washington.edu/~billhowe//projects/2017/07/20/Data-Synthesizer.html) and comes as part of this codebase. Coming from researchers in Drexel University and University of Washington, it's an excellent piece of software and their research and papers are well worth checking out. It's available as a [repo on Github](https://github.com/DataResponsibly/DataSynthesizer) which includes some short tutorials on how to use the toolkit and an accompanying research paper describing the theory behind it.\n"", '\n', '---\n', '\n', '## Setup\n', '\n', 'First, make sure you have [Python3 installed](https://www.python.org/downloads/). Minimum Python 3.6.\n', '\n', 'Download this repository either as a zip or clone using Git.\n', '\n', 'Install required dependent libraries. You can do that, for example, with a _virtualenv_.\n', '\n', '```bash\n', 'cd /path/to/repo/synthetic_data_tutorial/\n', 'pip install -r requirements.txt\n', '```\n', '\n', ""Next we'll go through how to create, de-identify and synthesise the code. We'll show this using code snippets but the full code is contained within the `/tutorial` directory.\n"", '\n', ""There's small differences between the code presented here and what's in the Python scripts but it's mostly down to variable naming. I'd encourage you to run, edit and play with the code locally.\n"", '\n', '## Generate mock NHS A&E dataset\n', '\n', 'The data already exists in `data/nhs_ae_mock.csv` so feel free to browse that. But you should generate your own fresh dataset using the `tutorial/generate.py` script.\n', '\n', ""To do this, you'll need to download one dataset first. It's a list of all postcodes in London. You can find it at this page on [doogal.co.uk](https://www.doogal.co.uk/PostcodeDownloads.php), at the _London_ link under the _By English region_ section. Or just download it directly at [this link](https://www.doogal.co.uk/UKPostcodesCSV.ashx?region=E12000007) (just take note, it's 133MB in size), then place the `London postcodes.csv` file in to the `data/` directory.\n"", '\n', 'Or you can just do it using `curl`.\n', '\n', '```bash\n', 'curl -o ""./data/London postcodes.csv"" https://www.doogal.co.uk/UKPostcodesCSV.ashx?region=E12000007\n', '```\n', '\n', 'Then, to generate the data, from the project root directory run the `generate.py` script.\n', '\n', '```bash\n', 'python tutorial/generate.py\n', '```\n', '\n', ""Voila! You'll now see a new `hospital_ae_data.csv` file in the `/data` directory. Open it up and have a browse. It's contains the following columns:\n"", '\n', '- **Health Service ID**: NHS number of the admitted patient  \n', '- **Age**: age of patient\n', '- **Time in A&E (mins)**: time in minutes of how long the patient spent in A&E. This is generated to correlate with the age of the patient.\n', '- **Hospital**: which hospital admitted the patient - with some hospitals being more prevalent in the data than others\n', '- **Arrival Time**: what time and date the patient was admitted - with weekends as busier and and a different peak time for each day\n', '- **Treatment**: what the person was treated for - with certain treatments being more common than others\n', '- **Gender**: patient gender - based on [NHS patient gender codes](https://www.datadictionary.nhs.uk/data_dictionary/attributes/p/person/person_gender_code_de.asp?shownav=1)\n', '- **Postcode**: postcode of patient - random, in use, London postcodes extracted from the `London postcodes.csv` file.\n', '\n', ""We can see this dataset obviously contains some personal information. For instance, if we knew roughly the time a neighbour went to A&E we could use their postcode to figure out exactly what ailment they went in with. Or, if a list of people's Health Service ID's were to be leaked in future, lots of people could be re-identified.\n"", '\n', ""Because of this, we'll need to take some de-identification steps.\n"", '\n', '---\n', '\n', '## De-identification\n', '\n', ""For this stage, we're going to be loosely following the de-identification techniques used by Jonathan Pearson of NHS England, and described in a blog post about [creating its own synthetic data](https://odileeds.org/blog/2019-01-24-exploring-methods-for-creating-synthetic-a-e-data).\n"", '\n', ""If you look in `tutorial/deidentify.py` you'll see the full code of all de-identification steps. You can run this code easily.\n"", '\n', '```bash\n', 'python tutorial/deidentify.py\n', '```\n', '\n', 'It takes the `data/hospital_ae_data.csv` file, run the steps, and saves the new dataset to `data/hospital_ae_data_deidentify.csv`.\n', '\n', 'Breaking down each of these steps. It first loads the `data/nhs_ae_data.csv` file in to the Pandas DataFrame as `hospital_ae_df`.\n', '\n', '```python\n', '# _df is a common way to refer to a Pandas DataFrame object\n', 'hospital_ae_df = pd.read_csv(filepaths.hospital_ae_data)\n', '```\n', '\n', '(`filepaths.py` is, surprise, surprise, where all the filepaths are listed)\n', '\n', '### Remove Health Service ID numbers\n', '\n', ""Health Service ID numbers are direct identifiers and should be removed. So we'll simply drop the entire column.\n"", '\n', '```python\n', ""hospital_ae_df = hospital_ae_df.drop('Health Service ID', 1)\n"", '```\n', '\n', '### Where a patient lives\n', '\n', ""Pseudo-identifiers, also known as [quasi-identifiers](https://en.wikipedia.org/wiki/Quasi-identifier), are pieces of information that don't directly identify people but can used with other information to identify a person. If we were to take the age, postcode and gender of a person we could combine these and check the dataset to see what that person was treated for in A&E.\n"", '\n', 'The data scientist from NHS England, Jonathan Pearson, describes this in the blog post:\n', '\n', '> I started with the postcode of the patients resident lower super output area (LSOA). This is a geographical definition with an average of 1500 residents created to make reporting in England and Wales easier. I wanted to keep some basic information about the area where the patient lives whilst completely removing any information regarding any actual postcode. A key variable in health care inequalities is the patients Index of Multiple deprivation (IMD) decile (broad measure of relative deprivation) which gives an average ranked value for each LSOA. By replacing the patients resident postcode with an IMD decile I have kept a key bit of information whilst making this field non-identifiable.\n', '\n', ""We'll do just the same with our dataset.\n"", '\n', ""First we'll map the rows' postcodes to their LSOA and then drop the postcodes column.\n"", '\n', '```python\n', 'postcodes_df = pd.read_csv(filepaths.postcodes_london)\n', 'hospital_ae_df = pd.merge(\n', '    hospital_ae_df,\n', ""    postcodes_df[['Postcode', 'Lower layer super output area']],\n"", ""    on='Postcode'\n"", ')\n', ""hospital_ae_df = hospital_ae_df.drop('Postcode', 1)\n"", '```\n', '\n', 'Then we\'ll add a mapped column of ""Index of Multiple Deprivation"" column for each entry\'s LSOA.\n', '\n', '```python\n', 'hospital_ae_df = pd.merge(\n', '    hospital_ae_df,\n', ""    postcodes_df[['Lower layer super output area', 'Index of Multiple Deprivation']].drop_duplicates(),\n"", ""    on='Lower layer super output area'\n"", ')\n', '```\n', '\n', ""Next calculate the decile bins for the IMDs by taking all the IMDs from large list of London. We'll use the Pandas `qcut` (quantile cut), function for this.\n"", '\n', '```python\n', '_, bins = pd.qcut(\n', ""    postcodes_df['Index of Multiple Deprivation'],\n"", '    10,\n', '    retbins=True,\n', '    labels=False\n', ')\n', '```\n', '\n', ""Then we'll use those decile `bins` to map each row's IMD to its IMD decile.\n"", '\n', '```python\n', '# add +1 to get deciles from 1 to 10 (not 0 to 9)\n', ""hospital_ae_df['Index of Multiple Deprivation Decile'] = pd.cut(\n"", ""    hospital_ae_df['Index of Multiple Deprivation'],\n"", '    bins=bins,\n', '    labels=False,\n', '    include_lowest=True) + 1\n', '```\n', '\n', 'And finally drop the columns we no longer need.\n', '\n', '```python\n', ""hospital_ae_df = hospital_ae_df.drop('Index of Multiple Deprivation', 1)\n"", ""hospital_ae_df = hospital_ae_df.drop('Lower layer super output area', 1)\n"", '```\n', '\n', '### Individual hospitals\n', '\n', 'The data scientist at NHS England masked individual hospitals giving the following reason.\n', '\n', '> As each hospital has its own complex case mix and health system, using these data to identify poor performance or possible improvements would be invalid and un-helpful. Therefore, I decided to replace the hospital code with a random number.\n', '\n', ""So we'll do as they did, replacing hospitals with a random six-digit ID.\n"", '\n', '```python\n', ""hospitals = hospital_ae_df['Hospital'].unique().tolist()\n"", 'random.shuffle(hospitals)\n', 'hospitals_map = {\n', ""    hospital : ''.join(random.choices(string.digits, k=6))\n"", '    for hospital in hospitals\n', '}\n', ""hospital_ae_df['Hospital ID'] = hospital_ae_df['Hospital'].map(hospitals_map)\n"", '```\n', '\n', 'And remove the `Hospital` column.\n', '\n', '```python\n', ""hospital_ae_df = hospital_ae_df.drop('Hospital', 1)\n"", '```\n', '\n', '### Time in the data\n', '\n', ""> The next obvious step was to simplify some of the time information I have available as health care system analysis doesn't need to be responsive enough to work on a second and minute basis. Thus, I removed the time information from the 'arrival date', mapped the 'arrival time' into 4-hour chunks\n"", '\n', ""First we'll split the `Arrival Time` column in to `Arrival Date` and `Arrival Hour`.\n"", '\n', '```python\n', ""arrival_times = pd.to_datetime(hospital_ae_df['Arrival Time'])\n"", ""hospital_ae_df['Arrival Date'] = arrival_times.dt.strftime('%Y-%m-%d')\n"", ""hospital_ae_df['Arrival Hour'] = arrival_times.dt.hour\n"", ""hospital_ae_df = hospital_ae_df.drop('Arrival Time', 1)\n"", '```\n', '\n', ""Then we'll map the hours to 4-hour chunks and drop the `Arrival Hour` column.\n"", '\n', '```python\n', ""hospital_ae_df['Arrival hour range'] = pd.cut(\n"", ""    hospital_ae_df['Arrival Hour'],\n"", '    bins=[0, 4, 8, 12, 16, 20, 24],\n', ""    labels=['00-03', '04-07', '08-11', '12-15', '16-19', '20-23'],\n"", '    include_lowest=True\n', ')\n', ""hospital_ae_df = hospital_ae_df.drop('Arrival Hour', 1)\n"", '```\n', '\n', '### Patient demographics\n', '\n', '> I decided to only include records with a sex of male or female in order to reduce risk of re identification through low numbers.\n', '\n', '```python\n', ""hospital_ae_df = hospital_ae_df[hospital_ae_df['Gender'].isin(['Male', 'Female'])]\n"", '```\n', '\n', ""> For the patients age it is common practice to group these into bands and so I've used a standard set - 1-17, 18-24, 25-44, 45-64, 65-84, and 85+ - which although are non-uniform are well used segments defining different average health care usage.\n"", '\n', '```python\n', ""hospital_ae_df['Age bracket'] = pd.cut(\n"", ""    hospital_ae_df['Age'],\n"", '    bins=[0, 18, 25, 45, 65, 85, 150],\n', ""    labels=['0-17', '18-24', '25-44', '45-64', '65-84', '85-'],\n"", '    include_lowest=True\n', ')\n', ""hospital_ae_df = hospital_ae_df.drop('Age', 1)\n"", '```\n', '\n', ""That's all the steps we'll take. We'll finally save our new de-identified dataset.\n"", '\n', '```python\n', 'hospital_ae_df.to_csv(filepaths.hospital_ae_data_deidentify, index=False)\n', '```\n', '\n', '---\n', '\n', '## Synthesise\n', '\n', 'Synthetic data exists on a spectrum from merely the same columns and datatypes as the original data all the way to carrying nearly all of the statistical patterns of the original dataset.\n', '\n', ""The UK's Office of National Statistics has a great report on synthetic data and the [_Synthetic Data Spectrum_](https://www.ons.gov.uk/methodology/methodologicalpublications/generalmethodology/onsworkingpaperseries/onsmethodologyworkingpaperseriesnumber16syntheticdatapilot?utm_campaign=201903_UK_DataPolicyNetwork&utm_source=hs_email&utm_medium=email&utm_content=70377606&_hsenc=p2ANqtz-9W6ByBext_HsgkTPG1lw2JJ_utRoJSTIeVC5Z2lz3QkzwFQpZ0dp2ns9SZLPqxLJrgWzsjC_zt7FQcBvtIGoeSjZtwNg&_hsmi=70377606#synthetic-dataset-spectrum) section is very good in explaining the nuances in more detail.\n"", '\n', ""In this tutorial we'll create not one, not two, but *three* synthetic datasets, that are on a range across the synthetic data spectrum: *Random*, *Independent* and *Correlated*.\n"", '\n', '> In **correlated attribute mode**, we learn a differentially private Bayesian network capturing the correlation structure between attributes, then draw samples from this model to construct the result dataset.\n', '>\n', '> In cases where the correlated attribute mode is too computationally expensive or when there is insufficient data to derive a reasonable model, one can use **independent attribute mode**. In this mode, a histogram is derived for each attribute, noise is added to the histogram to achieve differential privacy, and then samples are drawn for each attribute.\n', '>\n', '> Finally, for cases of extremely sensitive data, one can use **random mode** that simply generates type-consistent random values for each attribute.\n', '\n', ""We'll go through each of these now, moving along the synthetic data spectrum, in the order of random to independent to correlated.\n"", '\n', 'The toolkit we will be using to generate the three synthetic datasets is DataSynthetizer.\n', '\n', '### DataSynthesizer\n', '\n', ""As described in the introduction, this is an open-source toolkit for generating synthetic data. And I'd like to lavish much praise on the researchers who made it as it's excellent.\n"", '\n', ""Instead of explaining it myself, I'll use the researchers' own words from their paper:\n"", '\n', '> DataSynthesizer infers the domain of each attribute and derives a description of the distribution of attribute values in the private dataset. This information is saved in a dataset description file, to which we refer as data summary. Then DataSynthesizer is able to generate synthetic datasets of arbitrary size by sampling from the probabilistic model in the dataset description file.\n', '\n', ""We'll create and inspect our synthetic datasets using three modules within it.\n"", '\n', '> DataSynthesizer consists of three high-level modules:\n', '>\n', '> 1. **DataDescriber**: investigates the data types, correlations and distributions of the attributes in the private dataset, and produces a data summary.\n', '> 2. **DataGenerator**: samples from the summary computed by DataDescriber and outputs synthetic data\n', '> 3. **ModelInspector**: shows an intuitive description of the data summary that was computed by DataDescriber, allowing the data owner to evaluate the accuracy of the summarization process and adjust any parameters, if desired.\n', '\n', 'If you want to browse the code for each of these modules, you can find the Python classes for in the `DataSynthetizer` directory (all code in here from the [original repo](https://github.com/DataResponsibly/DataSynthesizer)).\n', '\n', '\n', '### An aside about differential privacy and Bayesian networks\n', '\n', 'You might have seen the phrase ""differentially private Bayesian network"" in the *correlated mode* description earlier, and got slightly panicked. But fear not! You don\'t need to worry *too* much about these to get DataSynthesizer working.\n', '\n', ""First off, while DataSynthesizer has the option of using differential privacy for anonymisation, we are turning it off and won't be using it in this tutorial. So you can ignore that part. However, if you care about anonymisation you really should read up on differential privacy. I've read a lot of explainers on it and the best I found was [this article from Access Now](https://www.accessnow.org/understanding-differential-privacy-matters-digital-rights/).\n"", '\n', 'Now the next term, Bayesian networks. These are graphs with directions which model the statistical relationship between a dataset\'s variables. It does this by saying certain variables are ""parents"" of others, that is, their value influences their ""children"" variables. Parent variables can influence children but children can\'t influence parents. In our case, if patient age is a parent of waiting time, it means the age of patient influences how long they wait, but how long they doesn\'t influence their age. So by using Bayesian Networks, DataSynthesizer can model these influences and use this model in generating the synthetic data.\n', '\n', 'It can be a slightly tricky topic to grasp but a nice, introductory tutorial on them is at the [Probabilistic World site](https://www.probabilisticworld.com/bayesian-belief-networks-part-1/). Give it a read.\n', '\n', '### Random mode\n', '\n', ""If we were just to generate A&E data for testing our software, we wouldn't care too much about the statistical patterns within the data. Just that it was roughly a similar size and that the datatypes and columns aligned.\n"", '\n', 'In this case, we can just generate the data at random using the `generate_dataset_in_random_mode` function within the `DataGenerator` class.\n', '\n', '#### Data Description: Random\n', '\n', 'The first step is to create a description of the data, defining the datatypes and which are the categorical variables.\n', '\n', '```python\n', 'attribute_to_datatype = {\n', ""    'Time in A&E (mins)': 'Integer',\n"", ""    'Treatment': 'String',\n"", ""    'Gender': 'String',\n"", ""    'Index of Multiple Deprivation Decile': 'Integer',\n"", ""    'Hospital ID': 'String',\n"", ""    'Arrival Date': 'String',\n"", ""    'Arrival hour range': 'String',  \n"", ""    'Age bracket': 'String'\n"", '}\n', '\n', 'attribute_is_categorical = {\n', ""    'Hospital ID': True,\n"", ""    'Time in A&E (mins)': False,\n"", ""    'Treatment': True,\n"", ""    'Gender': True,\n"", ""    'Index of Multiple Deprivation Decile': False,\n"", ""    'Arrival Date': True,\n"", ""    'Arrival hour range': True,  \n"", ""    'Age bracket': True\n"", '}\n', '```\n', '\n', ""We'll be feeding these in to a `DataDescriber` instance.\n"", '\n', '```python\n', 'describer = DataDescriber()\n', '```\n', '\n', 'Using this `describer` instance, feeding in the attribute descriptions, we create a description file.\n', '\n', '```python\n', 'describer.describe_dataset_in_random_mode(\n', '    filepaths.hospital_ae_data_deidentify,\n', '    attribute_to_datatype=attribute_to_datatype,\n', '    attribute_to_is_categorical=attribute_is_categorical)\n', 'describer.save_dataset_description_to_file(\n', '    filepaths.hospital_ae_description_random)\n', '```\n', '\n', 'You can see an example description file in `data/hospital_ae_description_random.json`.\n', '\n', '#### Data Generation: Random\n', '\n', ""Next, generate the random data. We'll just generate the same amount of rows as was in the original data but, importantly, we could generate much more or less if we wanted to.\n"", '\n', '```python\n', 'num_rows = len(hospital_ae_df)\n', '```\n', '\n', 'Now generate the random data.\n', '\n', '```python\n', 'generator = DataGenerator()\n', 'generator.generate_dataset_in_random_mode(\n', '    num_rows, filepaths.hospital_ae_description_random)\n', 'generator.save_synthetic_data(filepaths.hospital_ae_data_synthetic_random)\n', '```\n', '\n', 'You can view this random synthetic data in the file `data/hospital_ae_data_synthetic_random.csv`.\n', '\n', '#### Attribute Comparison: Random\n', '\n', ""We'll compare each attribute in the original data to the synthetic data by generating plots of histograms using the `ModelInspector` class.\n"", '\n', ""`figure_filepath` is just a variable holding where we'll write the plot out to.\n"", '\n', '```python\n', 'synthetic_df = pd.read_csv(filepaths.hospital_ae_data_synthetic_random)\n', '\n', '# Read attribute description from the dataset description file.\n', 'attribute_description = read_json_file(\n', ""    filepaths.hospital_ae_description_random)['attribute_description']\n"", '\n', 'inspector = ModelInspector(hospital_ae_df, synthetic_df, attribute_description)\n', '\n', 'for attribute in synthetic_df.columns:\n', '    inspector.compare_histograms(attribute, figure_filepath)\n', '```\n', '\n', ""Let's look at the histogram plots now for a few of the attributes. We can see that the generated data is completely random and doesn't contain any information about averages or distributions.\n"", '\n', '*Comparison of ages in original data (left) and random synthetic data (right)*\n', '![Random mode age bracket histograms](plots/random_Age_bracket.png)\n', '\n', '*Comparison of hospital attendance in original data (left) and random synthetic data (right)*\n', '![Random mode age bracket histograms](plots/random_Hospital_ID.png)\n', '\n', '*Comparison of arrival date in original data (left) and random synthetic data (right)*\n', '![Random mode age bracket histograms](plots/random_Arrival_Date.png)\n', '\n', 'You can see more comparison examples in the `/plots` directory.\n', '\n', '#### Compare pairwise mutual information: Random\n', '\n', ""DataSynthesizer has a function to compare the _mutual information_ between each of the variables in the dataset and plot them. We'll avoid the mathematical definition of mutual information but [Scholarpedia notes](http://www.scholarpedia.org/article/Mutual_information) it:\n"", '\n', '> can be thought of as the reduction in uncertainty about one random variable given knowledge of another.\n', '\n', 'To create this plot we run.\n', '\n', '```python\n', 'synthetic_df = pd.read_csv(filepaths.hospital_ae_data_synthetic_random)\n', '\n', 'inspector = ModelInspector(hospital_ae_df, synthetic_df, attribute_description)\n', 'inspector.mutual_information_heatmap(figure_filepath)\n', '```\n', '\n', 'We can see the original, private data has a correlation between `Age bracket` and `Time in A&E (mins)`. Not surprisingly, this correlation is lost when we generate our random data.\n', '\n', '*Mutual Information Heatmap in original data (left) and random synthetic data (right)*\n', '![Random mode age mutual information](plots/mutual_information_heatmap_random.png)\n', '\n', '### Independent attribute mode\n', '\n', ""What if we had the use case where we wanted to build models to analyse the medians of ages, or hospital usage in the synthetic data? In this case we'd use independent attribute mode.\n"", '\n', '#### Data Description: Independent\n', '\n', '```python\n', 'describer.describe_dataset_in_independent_attribute_mode(\n', '    attribute_to_datatype=attribute_to_datatype,\n', '    attribute_to_is_categorical=attribute_is_categorical)\n', 'describer.save_dataset_description_to_file(\n', '    filepaths.hospital_ae_description_independent)\n', '```\n', '\n', '#### Data Generation: Independent\n', '\n', 'Next generate the data which keep the distributions of each column but not the data correlations.\n', '\n', '```python\n', 'generator = DataGenerator()\n', 'generator.generate_dataset_in_independent_mode(\n', '    num_rows, filepaths.hospital_ae_description_independent)\n', 'generator.save_synthetic_data(\n', '    filepaths.hospital_ae_data_synthetic_independent)\n', '```\n', '\n', '#### Attribute Comparison: Independent\n', '\n', 'Comparing the attribute histograms we see the independent mode captures the distributions pretty accurately. You can see the synthetic data is _mostly_ similar but not exactly.\n', '\n', '```python\n', 'synthetic_df = pd.read_csv(filepaths.hospital_ae_data_synthetic_independent)\n', 'attribute_description = read_json_file(\n', ""    filepaths.hospital_ae_description_random)['attribute_description']\n"", 'inspector = ModelInspector(hospital_ae_df, synthetic_df, attribute_description)\n', '\n', 'for attribute in synthetic_df.columns:\n', '    inspector.compare_histograms(attribute, figure_filepath)\n', '```\n', '\n', '*Comparison of ages in original data (left) and independent synthetic data (right)*\n', '![Random mode age bracket histograms](plots/independent_Age_bracket.png)\n', '\n', '*Comparison of hospital attendance in original data (left) and independent synthetic data (right)*\n', '![Random mode age bracket histograms](plots/independent_Hospital_ID.png)\n', '\n', '*Comparison of arrival date in original data (left) and independent synthetic data (right)*\n', '![Random mode age bracket histograms](plots/independent_Arrival_Date.png)\n', '\n', '#### Compare pairwise mutual information: Independent\n', '\n', '```python\n', 'synthetic_df = pd.read_csv(filepaths.hospital_ae_data_synthetic_independent)\n', '\n', 'inspector = ModelInspector(hospital_ae_df, synthetic_df, attribute_description)\n', 'inspector.mutual_information_heatmap(figure_filepath)\n', '```\n', '\n', 'We can see the independent data also does not contain any of the attribute correlations from the original data.\n', '\n', '*Mutual Information Heatmap in original data (left) and independent synthetic data (right)*\n', '![Independent mode mutual information](plots/mutual_information_heatmap_independent.png)\n', '\n', '### Correlated attribute mode - include correlations between columns in the data\n', '\n', ""If we want to capture correlated variables, for instance if patient is related to waiting times, we'll need correlated data. To do this we use *correlated mode*.\n"", '\n', '#### Data Description: Correlated\n', '\n', ""There's a couple of parameters that are different here so we'll explain them.\n"", '\n', ""`epsilon` is a value for DataSynthesizer's differential privacy which says the amount of noise to add to the data - the higher the value, the more noise and therefore more privacy. We're not using differential privacy so we can set it to zero.\n"", '\n', ""`k` is the maximum number of parents in a Bayesian network, i.e., the maximum number of incoming edges. For simplicity's sake, we're going to set this to 1, saying that for a variable only one other variable can influence it.\n"", '\n', '```python\n', 'describer.describe_dataset_in_correlated_attribute_mode(\n', '    dataset_file=filepaths.hospital_ae_data_deidentify,\n', '    epsilon=0,\n', '    k=1,\n', '    attribute_to_datatype=attribute_to_datatype,\n', '    attribute_to_is_categorical=attribute_is_categorical)\n', '\n', 'describer.save_dataset_description_to_file(filepaths.hospital_ae_description_correlated)\n', '```\n', '\n', '#### Data Generation: Correlated\n', '\n', '```python\n', 'generator.generate_dataset_in_correlated_attribute_mode(\n', '    num_rows, filepaths.hospital_ae_description_correlated)\n', 'generator.save_synthetic_data(filepaths.hospital_ae_data_synthetic_correlated)\n', '```\n', '\n', '#### Attribute Comparison: Correlated\n', '\n', 'We can see correlated mode keeps similar distributions also. It looks the exact same but if you look closely there are also small differences in the distributions.\n', '\n', '*Comparison of ages in original data (left) and correlated synthetic data (right)*\n', '![Random mode age bracket histograms](plots/correlated_Age_bracket.png)\n', '\n', '*Comparison of hospital attendance in original data (left) and independent synthetic data (right)*\n', '![Random mode age bracket histograms](plots/correlated_Hospital_ID.png)\n', '\n', '*Comparison of arrival date in original data (left) and independent synthetic data (right)*\n', '![Random mode age bracket histograms](plots/correlated_Arrival_Date.png)\n', '\n', '#### Compare pairwise mutual information: Correlated\n', '\n', 'Finally, we see in correlated mode, we manage to capture the correlation between `Age bracket` and `Time in A&E (mins)`.\n', '\n', '```python\n', 'synthetic_df = pd.read_csv(filepaths.hospital_ae_data_synthetic_correlated)\n', '\n', 'inspector = ModelInspector(hospital_ae_df, synthetic_df, attribute_description)\n', 'inspector.mutual_information_heatmap(figure_filepath)\n', '```\n', '\n', '*Mutual Information Heatmap in original data (left) and correlated synthetic data (right)*\n', '![Independent mode mutual information](plots/mutual_information_heatmap_correlated.png)\n', '\n', '---\n', '\n', '### Wrap-up\n', '\n', 'This is where our tutorial ends. But there is much, much more to the world of anonymisation and synthetic data. Please check out more in the references below.\n', '\n', 'If you have any queries, comments or improvements about this tutorial please do get in touch. You can send me a message through "
Synthetic+Data,sdv-dev/TGAN,sdv-dev,https://api.github.com/repos/sdv-dev/TGAN,240,82,6,"['https://api.github.com/users/ManuelAlvarezC', 'https://api.github.com/users/csala', 'https://api.github.com/users/leix28', 'https://api.github.com/users/JDTheRipperPC', 'https://api.github.com/users/pvk-developer', 'https://api.github.com/users/ppwwyyxx']",Python,2023-03-31T01:20:08Z,https://raw.githubusercontent.com/sdv-dev/TGAN/master/README.md,"['<p align=""left"">\n', '<img width=20% src=""https://dai.lids.mit.edu/wp-content/uploads/2018/06/Logo_DAI_highres.png"" alt=""sdv-dev"" />\n', '<i>An open source project from Data to AI Lab at MIT.</i>\n', '</p>\n', '\n', '[![Development Status](https://img.shields.io/badge/Development%20Status-2%20--%20Pre--Alpha-yellow)](https://pypi.org/search/?c=Development+Status+%3A%3A+2+-+Pre-Alpha)\n', '[![PyPi Shield](https://img.shields.io/pypi/v/TGAN.svg)](https://pypi.python.org/pypi/TGAN)\n', '[![Travis CI Shield](https://travis-ci.org/sdv-dev/TGAN.svg?branch=master)](https://travis-ci.org/sdv-dev/TGAN)\n', '[![CodeCov](https://codecov.io/gh/sdv-dev/TGAN/branch/master/graph/badge.svg)](https://codecov.io/gh/sdv-dev/TGAN)\n', '[![Downloads](https://pepy.tech/badge/tgan)](https://pepy.tech/project/tgan)\n', '\n', '__We are happy to announce that our new model for synthetic data called [CTGAN](https://github.com/sdv-dev/CTGAN) is open-sourced. Please check the new model in [this repo](https://github.com/sdv-dev/CTGAN). The new model is simpler and gives better performance on many datasets.__\n', '\n', '# TGAN\n', '\n', 'Generative adversarial training for synthesizing tabular data.\n', '\n', '* License: [MIT](https://github.com/sdv-dev/TGAN/blob/master/LICENSE)\n', '* Development Status: [Pre-Alpha](https://pypi.org/search/?c=Development+Status+%3A%3A+2+-+Pre-Alpha)\n', '* Homepage: https://github.com/sdv-dev/TGAN\n', '\n', '# Overview\n', '\n', 'TGAN is a tabular data synthesizer. It can generate fully synthetic data from real data. Currently, TGAN can\n', 'generate numerical columns and categorical columns.\n', '\n', '# Requirements\n', '\n', '## Python\n', '\n', '**TGAN** has been developed and runs on Python [3.5](https://www.python.org/downloads/release/python-356/),\n', '[3.6](https://www.python.org/downloads/release/python-360/) and\n', '[3.7](https://www.python.org/downloads/release/python-370/).\n', '\n', 'Also, although it is not strictly required, the usage of a [virtualenv](https://virtualenv.pypa.io/en/latest/)\n', 'is highly recommended in order to avoid interfering with other software installed in the system where **TGAN**\n', 'is run.\n', '\n', '# Installation\n', '\n', 'The simplest and recommended way to install TGAN is using `pip`:\n', '\n', '```\n', 'pip install tgan\n', '```\n', '\n', 'Alternatively, you can also clone the repository and install it from sources\n', '\n', '```\n', 'git clone git@github.com:sdv-dev/TGAN.git\n', 'cd TGAN\n', 'make install\n', '```\n', '\n', 'For development, you can use `make install-develop` instead in order to install all the required\n', 'dependencies for testing and code linting.\n', '\n', '# Data Format\n', '\n', '## Input Format\n', '\n', 'In order to be able to sample new synthetic data, **TGAN** first needs to be *fitted* to\n', 'existing data.\n', '\n', 'The input data for this *fitting* process has to be a single table that satisfies the following\n', 'rules:\n', '\n', '* Has no missing values.\n', '* Has columns of types `int`, `float`, `str` or `bool`.\n', '* Each column contains data of only one type.\n', '\n', 'An example of such a tables would be:\n', '\n', '| str_column | float_column | int_column | bool_column |\n', '|------------|--------------|------------|-------------|\n', ""|    'green' |         0.15 |         10 |        True |\n"", ""|     'blue' |         7.25 |         23 |       False |\n"", ""|      'red' |        10.00 |          1 |       False |\n"", ""|   'yellow' |         5.50 |         17 |        True |\n"", '\n', 'As you can see, this table contains 4 columns: `str_column`, `float_column`, `int_column` and\n', '`bool_column`, each one being an example of the supported value types. Notice aswell that there is\n', 'no missing values for any of the rows.\n', '\n', ""**NOTE**: It's important to have properly identifed which of the columns are numerical, which means\n"", 'that they represent a magnitude, and which ones are categorical, as during the preprocessing of\n', 'the data, numerical and categorical columns will be processed differently.\n', '\n', '## Output Format\n', '\n', 'The output of **TGAN** is a table of sampled data with the same columns as the input table and as\n', 'many rows as requested.\n', '\n', '## Demo Datasets\n', '\n', '**TGAN** includes a few datasets to use for development or demonstration purposes. These datasets\n', 'come from the [UCI Machine Learning repository](http://archive.ics.uci.edu/ml), and have been\n', 'preprocessed to be ready to use with **TGAN**, following the requirements specified in the\n', '[Input Format](#input-format) section.\n', '\n', 'These datasets can be browsed and directly downloaded from the\n', '[hdi-project-tgan AWS S3 Bucket](http://hdi-project-tgan.s3.amazonaws.com/index.html)\n', '\n', '### Census dataset\n', '\n', 'This dataset contains a single table, with information from the census, labeled with information of\n', ""wheter or not the income of is greater than 50.000 $/year. It's a single csv file, containing\n"", '199522 rows and 41 columns. From these 41 columns, only 7 are identified as continuous. In\n', '**TGAN** this dataset is called `census`.\n', '\n', '### Cover type\n', '\n', 'This dataset contains a single table with cartographic information labeled with the different\n', ""forrest cover types. It's a single csv file, containing 465588 rows and 55 columns. From these\n"", '55 columns, 10 are identified as continuous. In **TGAN** this dataset is called `covertype`.\n', '\n', '# Quickstart\n', '\n', 'In this short tutorial we will guide you through a series of steps that will help you getting\n', 'started with the most basic usage of **TGAN** in order to generate samples from a given dataset.\n', '\n', '**NOTE**: The following examples are also covered in a [Jupyter](https://jupyter.org/) notebook,\n', 'which you can execute by running the following commands inside your *virtualenv*:\n', '\n', '```\n', 'pip install jupyter\n', 'jupyter notebook examples/Usage_Example.ipynb\n', '```\n', '\n', '## 1. Load the data\n', '\n', 'The first step is to load the data wich we will use to fit TGAN. In order to do so, we will first\n', 'import the function `tgan.data.load_data` and call it with the name of the dataset that we want to\n', 'load.\n', '\n', 'In this case, we will load the `census` dataset, which we will use during the subsequent steps,\n', 'and obtain two objects:\n', '\n', '1. `data`, that will contain a `pandas.DataFrame` with the table of data from the `census`\n', 'dataset ready to be used to fit the model.\n', '\n', '2. `continuous_columns`, that will contain a `list` with the indices of continuous columns.\n', '\n', '```\n', '>>> from tgan.data import load_demo_data\n', "">>> data, continuous_columns = load_demo_data('census')\n"", '>>> data.head(3).T[:10]\n', '                              0                                     1                             2\n', '0                            73                                    58                            18\n', '1               Not in universe        Self-employed-not incorporated               Not in universe\n', '2                             0                                     4                             0\n', '3                             0                                    34                             0\n', '4          High school graduate            Some college but no degree                    10th grade\n', '5                             0                                     0                             0\n', '6               Not in universe                       Not in universe                   High school\n', '7                       Widowed                              Divorced                 Never married\n', '8   Not in universe or children                          Construction   Not in universe or children\n', '9               Not in universe   Precision production craft & repair               Not in universe\n', '\n', '>>> continuous_columns\n', '[0, 5, 16, 17, 18, 29, 38]\n', '\n', '```\n', '\n', '## 2. Create a TGAN instance\n', '\n', 'The next step is to import TGAN and create an instance of the model.\n', '\n', 'To do so, we need to import the `tgan.model.TGANModel` class and call it with the\n', '`continuous_columns` as unique argument.\n', '\n', 'This will create a TGAN instance with the default parameters:\n', '\n', '```\n', '>>> from tgan.model import TGANModel\n', '>>> tgan = TGANModel(continuous_columns)\n', '```\n', '\n', '## 3. Fit the model\n', '\n', ""Once you have a **TGAN** instance, you can proceed to call it's `fit` method passing the `data` that\n"", 'you loaded before in order to start the fitting process:\n', '\n', '```\n', '>>> tgan.fit(data)\n', '```\n', '\n', 'This process will not return anything, however, the progress of the fitting will be printed in the\n', 'screen.\n', '\n', '**NOTE** Depending on the performance of the system you are running, and the parameters selected\n', 'for the model, this step can take up to a few hours.\n', '\n', '## 4. Sample new data\n', '\n', 'After the model has been fitted, you are ready to generate new samples by calling the `sample`\n', 'method of the `TGAN` instance passing it the desired amount of samples:\n', '\n', '```\n', '>>> num_samples = 1000\n', '>>> samples = tgan.sample(num_samples)\n', '>>> samples.head(3).T[:10]\n', '                                         0                                     1                                   2\n', '0                                       12                                    27                                  56\n', '\n', '0                                       12                                    27                                  56\n', '1                          Not in universe        Self-employed-not incorporated                             Private\n', '2                                        0                                     4                                  35\n', '3                                        0                                    34                                  22\n', '4                                 Children            Some college but no degree          Some college but no degree\n', '5                                        0                                     0                                 500\n', '6                          Not in universe                       Not in universe                     Not in universe\n', '7                            Never married       Married-civilian spouse present     Married-civilian spouse present\n', '8              Not in universe or children                          Construction   Finance insurance and real estate\n', '9                          Not in universe   Precision production craft & repair      Adm support including clerical\n', '\n', '```\n', '\n', 'The returned object, `samples`, is a `pandas.DataFrame` containing a table of synthetic data with\n', 'the same format as the input data and 1000 rows as we requested.\n', '\n', '## 5. Save and Load a model\n', '\n', 'In the steps above we saw that the fitting process can take a lot of time, so we probably would\n', 'like to avoid having to fit every we want to generate samples. Instead we can fit a model once,\n', 'save it, and load it every time we want to sample new data.\n', '\n', ""If we have a fitted model, we can save it by calling it's `save` method, that only takes\n"", 'as argument the path where the model will be stored. Similarly, the `TGANModel.load` allows to load\n', 'a model stored on disk by passing as argument the path where the model is stored.\n', '\n', '```\n', "">>> model_path = 'models/mymodel.pkl'\n"", '>>> tgan.save(model_path)\n', 'Model saved successfully.\n', '```\n', '\n', 'Bear in mind that in case the file already exists, **TGAN** will avoid overwritting it unless the\n', '`force=True` argument is passed:\n', '\n', '```\n', '>>> tgan.save(model_path)\n', 'The indicated path already exists. Use `force=True` to overwrite.\n', '```\n', '\n', 'In order to do so:\n', '\n', '```\n', '>>> tgan.save(model_path, force=True)\n', 'Model saved successfully.\n', '```\n', '\n', 'Once the model is saved, it can be loaded back as a **TGAN** instance by using the `TGANModel.load`\n', 'method:\n', '\n', '```\n', '>>> new_tgan = TGANModel.load(model_path)\n', '>>> new_samples = new_tgan.sample(num_samples)\n', '>>> new_samples.head(3).T[:10]\n', '\n', '                                         0                                     1                                   2\n', '0                                       12                                    27                                  56\n', '\n', '0                                       12                                    27                                  56\n', '1                          Not in universe        Self-employed-not incorporated                             Private\n', '2                                        0                                     4                                  35\n', '3                                        0                                    34                                  22\n', '4                                 Children            Some college but no degree          Some college but no degree\n', '5                                        0                                     0                                 500\n', '6                          Not in universe                       Not in universe                     Not in universe\n', '7                            Never married       Married-civilian spouse present     Married-civilian spouse present\n', '8              Not in universe or children                          Construction   Finance insurance and real estate\n', '9                          Not in universe   Precision production craft & repair      Adm support including clerical\n', '```\n', '\n', 'At this point we could use this model instance to generate more samples.\n', '\n', '# Loading custom datasets\n', '\n', 'In the previous steps we used some demonstration data but we did not show you how to load your own\n', 'dataset.\n', '\n', 'In order to do so you will need to generate a `pandas.DataFrame` object from your dataset. If your\n', 'dataset is in a `csv` format you can do so by using `pandas.read_csv` and passing to it the path to\n', 'the CSV file that you want to load.\n', '\n', 'Additionally, you will need to create 0-indexed list of columns indices to be considered continuous.\n', '\n', 'For example, if we want to load a local CSV file, `path/to/my.csv`, that has as continuous columns\n', 'their first 4 columns, that is, indices `[0, 1, 2, 3]`, we would do it like this:\n', '\n', '```\n', '>>> import pandas as pd\n', "">>> data = pd.read_csv('data/census.csv')\n"", '>>> continuous_columns = [0, 1, 2, 3]\n', '```\n', '\n', 'Now you can use the `continuous_columns` to create a **TGAN** instance and use the `data` to `fit`\n', 'it, like we did before:\n', '\n', '```\n', '>>> from tgan.model import TGANModel\n', '>>> tgan = TGANModel(continuous_columns)\n', '>>> tgan.fit(data)\n', '```\n', '\n', '# Model Parameters\n', '\n', 'If you want to change the default behavior of `TGANModel`, such as as different `batch_size` or\n', '`num_epochs`, you can do so by passing different arguments when creating the instance.\n', '\n', '## Model general behavior\n', '\n', '* continous_columns (`list[int]`, required): List of columns indices to be considered continuous.\n', '* output (`str`, default=`output`): Path to store the model and its artifacts.\n', '\n', '## Neural network definition and fitting\n', '\n', '* max_epoch (`int`, default=`100`): Number of epochs to use during training.\n', '* steps_per_epoch (`int`, default=`10000`): Number of steps to run on each epoch.\n', '* save_checkpoints(`bool`, default=True): Whether or not to store checkpoints of the model after each training epoch.\n', '* restore_session(`bool`, default=True): Whether or not continue training from the last checkpoint.\n', '* batch_size (`int`, default=`200`): Size of the batch to feed the model at each step.\n', '* z_dim (`int`, default=`100`): Number of dimensions in the noise input for the generator.\n', '* noise (`float`, default=`0.2`): Upper bound to the gaussian noise added to categorical columns.\n', '* l2norm (`float`, default=`0.00001`): L2 reguralization coefficient when computing losses.\n', '* learning_rate (`float`, default=`0.001`): Learning rate for the optimizer.\n', '* num_gen_rnn (`int`, default=`400`): Number of units in rnn cell in generator.\n', '* num_gen_feature (`int`, default=`100`): Number of units in fully connected layer in generator.\n', '* num_dis_layers (`int`, default=`2`): Number of layers in discriminator.\n', '* num_dis_hidden (`int`, default=`200`): Number of units per layer in discriminator.\n', '* optimizer (`str`, default=`AdamOptimizer`): Name of the optimizer to use during `fit`, possible\n', '  values are: [`GradientDescentOptimizer`, `AdamOptimizer`, `AdadeltaOptimizer`].\n', '\n', 'If you wanted to create an identical instance to the one created on step 2, but passing the\n', 'arguments in a explicit way, this can be achieved with the following lines:\n', '\n', '```\n', '>>> from tgan.model import TGANModel\n', '>>> tgan = TGANModel(\n', '   ...:     continuous_columns,\n', ""   ...:     output='output',\n"", '   ...:     max_epoch=5,\n', '   ...:     steps_per_epoch=10000,\n', '   ...:     save_checkpoints=True,\n', '   ...:     restore_session=True,\n', '   ...:     batch_size=200,\n', '   ...:     z_dim=200,\n', '   ...:     noise=0.2,\n', '   ...:     l2norm=0.00001,\n', '   ...:     learning_rate=0.001,\n', '   ...:     num_gen_rnn=100,\n', '   ...:     num_gen_feature=100,\n', '   ...:     num_dis_layers=1,\n', '   ...:     num_dis_hidden=100,\n', ""   ...:     optimizer='AdamOptimizer'\n"", '   ...: )\n', '```\n', '\n', '# Command-line interface\n', '\n', 'We include a command-line interface that allows users to access TGAN functionality. Currently only\n', 'one action is supported.\n', '\n', '## Random hyperparameter search\n', '\n', '### Input\n', '\n', 'To run random searchs for the best model hyperparameters for a given dataset, we will need:\n', '\n', '* A dataset, in a csv file, without any missing value, only columns of type `bool`, `str`, `int` or\n', '  `float` and only one type for column, as specified in the [Input Format](#input-format).\n', '\n', '* A JSON file containing the configuration for the search. This configuration shall contain:\n', '\n', '  * `name`: Name of the experiment. A folder with this name will be created.\n', '  * `num_random_search`: Number of iterations in hyper parameter search.\n', '  * `train_csv`: Path to the csv file containing the dataset.\n', '  * `continuous_cols`: List of column indices, starting at 0, to be considered continuous.\n', '  * `epoch`: Number of epoches to train the model.\n', '  * `steps_per_epoch`: Number of optimization steps in each epoch.\n', '  * `sample_rows`: Number of rows to sample when evaluating the model.\n', '\n', 'You can see an example of such a json file in [examples/config.json](examples/config.json), which you\n', 'can download and use as a template.\n', '\n', '### Execution\n', '\n', 'Once we have prepared everything we can launch the random hyperparameter search with this command:\n', '\n', '``` bash\n', 'tgan experiments config.json results.json\n', '```\n', '\n', 'Where the first argument, `config.json`, is the path to your configuration JSON, and the second,\n', '`results.json`, is the path to store the summary of the execution.\n', '\n', 'This will run the random search, wich basically consist of the folling steps:\n', '\n', '1. We fetch and split our data between test and train.\n', '2. We randomly select the hyperparameters to test.\n', '3. Then, for each hyperparameter combination, we train a TGAN model using the real training data T\n', '   and generate a synthetic training dataset Tsynth.\n', '4. We then train machine learning models on both the real and synthetic datasets.\n', '5. We use these trained models on real test data and see how well they perform.\n', '\n', '### Output\n', '\n', 'After the experiment has finished, the following can be found:\n', '\n', '* A JSON file, in the example above called `results.json`, containing a summary of the experiments.\n', '  This JSON will contain a key for each experiment `name`, and on it, an array of length\n', '  `num_random_search`, with the selected parameters and its evaluation score. For a configuration\n', '  like the example, the summary will look like this:\n', '\n', '``` python\n', '{\n', ""    'census': [\n"", '        {\n', '            ""steps_per_epoch"" : 10000,\n', '            ""num_gen_feature"" : 300,\n', '            ""num_dis_hidden"" : 300,\n', '            ""batch_size"" : 100,\n', '            ""num_gen_rnn"" : 400,\n', '            ""score"" : 0.937802280415988,\n', '            ""max_epoch"" : 5,\n', '            ""num_dis_layers"" : 4,\n', '            ""learning_rate"" : 0.0002,\n', '            ""z_dim"" : 100,\n', '            ""noise"" : 0.2\n', '        },\n', '        ... # 9 more nodes\n', '    ]\n', '}\n', '```\n', '\n', '* A set of folders, each one names after the `name` specified in the JSON configuration, contained\n', 'in the `experiments` folder. In each folder, sampled data and the models can be found. For a configuration\n', 'like the example, this will look like this:\n', '\n', '```\n', 'experiments/\n', '  census/\n', '    data/       # Sampled data with each of the models in the random search.\n', '    model_0/\n', '      logs/     # Training logs\n', '      model/    # Tensorflow model checkpoints\n', '    model_1/    # 9 more folders, one for each model in the random search\n', '    ...\n', '```\n', '\n', '# Research\n', '\n', 'The first **TGAN** version was built as the supporting software for the [Synthesizing Tabular Data using Generative Adversarial Networks](https://arxiv.org/pdf/1811.11264.pdf) paper by Lei Xu and Kalyan Veeramachaneni.\n', '\n', 'The exact version of software mentioned in the paper can be found in the releases section as the [research pre-release](https://github.com/sdv-dev/TGAN/releases/tag/research)\n', '\n', '# Citing TGAN\n', '\n', 'If you use TGAN for yor research, please consider citing the following paper (https://arxiv.org/pdf/1811.11264.pdf):\n', '\n', 'If you use TGAN, please cite the following work:\n', '\n', '> Lei Xu, Kalyan Veeramachaneni. 2018. Synthesizing Tabular Data using Generative Adversarial Networks.\n', '\n', '```LaTeX\n', '@article{xu2018synthesizing,\n', '  title={Synthesizing Tabular Data using Generative Adversarial Networks},\n', '  author={Xu, Lei and Veeramachaneni, Kalyan},\n', '  journal={arXiv preprint arXiv:1811.11264},\n', '  year={2018}\n', '}\n', '```\n']"
Synthetic+Data,Unity-Technologies/datasetinsights,Unity-Technologies,https://api.github.com/repos/Unity-Technologies/datasetinsights,82,16,16,"['https://api.github.com/users/adason', 'https://api.github.com/users/Saurav-D', 'https://api.github.com/users/BlairLee', 'https://api.github.com/users/86sanj', 'https://api.github.com/users/AdamPalmarUnity', 'https://api.github.com/users/sanjayuconn', 'https://api.github.com/users/rutvij-unity', 'https://api.github.com/users/JonathanHUnity', 'https://api.github.com/users/masonrubenstein', 'https://api.github.com/users/StevenBorkman', 'https://api.github.com/users/kalyanijagdale', 'https://api.github.com/users/davidwang-unity', 'https://api.github.com/users/mkamalza', 'https://api.github.com/users/alextha-scale', 'https://api.github.com/users/salehe-ee', 'https://api.github.com/users/leopoldo-zugasti']",Python,2023-03-09T15:21:37Z,https://raw.githubusercontent.com/Unity-Technologies/datasetinsights/master/README.md,"['# Dataset Insights\n', '\n', '[![PyPI python](https://img.shields.io/pypi/pyversions/datasetinsights)](https://pypi.org/project/datasetinsights)\n', '[![PyPI version](https://badge.fury.io/py/datasetinsights.svg)](https://pypi.org/project/datasetinsights)\n', '[![Downloads](https://pepy.tech/badge/datasetinsights)](https://pepy.tech/project/datasetinsights)\n', '[![Tests](https://github.com/Unity-Technologies/datasetinsights/actions/workflows/linting-and-unittests.yaml/badge.svg?branch=master&event=push)](https://github.com/Unity-Technologies/datasetinsights/actions/workflows/linting-and-unittests.yaml?query=branch%3Amaster+event%3Apush)\n', '[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)\n', '\n', 'Unity Dataset Insights is a python package for downloading, parsing and analyzing synthetic datasets generated using the Unity [Perception package](https://github.com/Unity-Technologies/com.unity.perception).\n', '\n', '## Installation\n', '\n', 'Datasetinsights is published to PyPI. You can simply run `pip install datasetinsights` command under a supported python environments:\n', '\n', '## Getting Started\n', '\n', '### Dataset Statistics\n', '\n', 'We provide a sample [notebook](notebooks/Perception_Statistics.ipynb) to help you load synthetic datasets generated using [Perception package](https://github.com/Unity-Technologies/com.unity.perception) and visualize dataset statistics. We plan to support other sample Unity projects in the future.\n', '\n', '### Load Datasets\n', '\n', 'The [Unity Perception](https://datasetinsights.readthedocs.io/en/latest/datasetinsights.datasets.unity_perception.html#datasetinsights-datasets-unity-perception) package provides datasets under this [schema](https://datasetinsights.readthedocs.io/en/latest/Synthetic_Dataset_Schema.html#synthetic-dataset-schema). The datasetinsighs package also provide convenient python modules to parse datasets.\n', '\n', 'For example, you can load `AnnotationDefinitions` into a python dictionary by providing the corresponding annotation definition ID:\n', '\n', '```python\n', 'from datasetinsights.datasets.unity_perception import AnnotationDefinitions\n', '\n', 'annotation_def = AnnotationDefinitions(data_root=dest, version=""my_schema_version"")\n', 'definition_dict = annotation_def.get_definition(def_id=""my_definition_id"")\n', '```\n', '\n', 'Similarly, for `MetricDefinitions`:\n', '```python\n', 'from datasetinsights.datasets.unity_perception import MetricDefinitions\n', '\n', 'metric_def = MetricDefinitions(data_root=dest, version=""my_schema_version"")\n', 'definition_dict = metric_def.get_definition(def_id=""my_definition_id"")\n', '```\n', '\n', 'The `Captures` table provide the collection of simulation captures and annotations. You can load these records directly as a Pandas `DataFrame`:\n', '\n', '```python\n', 'from datasetinsights.datasets.unity_perception import Captures\n', '\n', 'captures = Captures(data_root=dest, version=""my_schema_version"")\n', 'captures_df = captures.filter(def_id=""my_definition_id"")\n', '```\n', '\n', '\n', 'The `Metrics` table can store simulation metrics for a capture or annotation. You can also load these records as a Pandas `DataFrame`:\n', '\n', '```python\n', 'from datasetinsights.datasets.unity_perception import Metrics\n', '\n', 'metrics = Metrics(data_root=dest, version=""my_schema_version"")\n', 'metrics_df = metrics.filter_metrics(def_id=""my_definition_id"")\n', '```\n', '\n', '### Download Datasets\n', '\n', 'You can download the datasets using the [download](https://datasetinsights.readthedocs.io/en/latest/datasetinsights.commands.html#datasetinsights-commands-download) command:\n', '\n', '```bash\n', 'datasetinsights download --source-uri=<xxx> --output=$HOME/data\n', '```\n', '\n', 'The download command supports HTTP(s), and GCS.\n', '\n', 'Alternatively, you can download dataset directly from python [interface](https://datasetinsights.readthedocs.io/en/latest/datasetinsights.io.downloader.html#module-datasetinsights.io.downloader).\n', '\n', '`GCSDatasetDownloader` can download a dataset from GCS locations.\n', '```python\n', 'from datasetinsights.io.downloader import GCSDatasetDownloader\n', '\n', 'source_uri=gs://url/to/file.zip # or gs://url/to/folder\n', 'dest = ""~/data""\n', 'downloader = GCSDatasetDownloader()\n', 'downloader.download(source_uri=source_uri, output=dest)\n', '```\n', '\n', '`HTTPDatasetDownloader` can a dataset from any HTTP(S) url.\n', '```python\n', 'from datasetinsights.io.downloader import HTTPDatasetDownloader\n', '\n', 'source_uri=http://url.to.file.zip\n', 'dest = ""~/data""\n', 'downloader = HTTPDatasetDownloader()\n', 'downloader.download(source_uri=source_uri, output=dest)\n', '```\n', '\n', '### Convert Datasets\n', '\n', 'If you are interested in converting the synthetic dataset to COCO format for\n', 'annotations that COCO supports, you can run the `convert` command:\n', '\n', '```bash\n', 'datasetinsights convert -i <input-directory> -o <output-directory> -f COCO-Instances\n', '```\n', 'or\n', '```bash\n', 'datasetinsights convert -i <input-directory> -o <output-directory> -f COCO-Keypoints\n', '```\n', '\n', 'You will need to provide 2D bounding box definition ID in the synthetic dataset. We currently only support 2D bounding box and human keypoint annotations for COCO format.\n', '\n', '## Docker\n', '\n', 'You can use the pre-build docker image [unitytechnologies/datasetinsights](https://hub.docker.com/r/unitytechnologies/datasetinsights) to interact with datasets.\n', '\n', '## Documentation\n', '\n', 'You can find the API documentation on [readthedocs](https://datasetinsights.readthedocs.io/en/latest/).\n', '\n', '## Contributing\n', '\n', 'Please let us know if you encounter a bug by filing an issue. To learn more about making a contribution to Dataset Insights, please see our Contribution [page](CONTRIBUTING.md).\n', '\n', '## License\n', '\n', 'Dataset Insights is licensed under the Apache License, Version 2.0. See [LICENSE](LICENCE) for the full license text.\n', '\n', '## Citation\n', 'If you find this package useful, consider citing it using:\n', '```\n', '@misc{datasetinsights2020,\n', '    title={Unity {D}ataset {I}nsights Package},\n', '    author={{Unity Technologies}},\n', '    howpublished={\\url{https://github.com/Unity-Technologies/datasetinsights}},\n', '    year={2020}\n', '}\n', '```\n']"
Synthetic+Data,yuliangguo/3D_Lane_Synthetic_Dataset,yuliangguo,https://api.github.com/repos/yuliangguo/3D_Lane_Synthetic_Dataset,117,21,1,['https://api.github.com/users/yuliangguo'],Python,2023-03-27T17:53:53Z,https://raw.githubusercontent.com/yuliangguo/3D_Lane_Synthetic_Dataset/master/README.md,"['# A Synthetic Dataset for 3D lane Detection\n', '\n', '## Introduction\n', '\n', 'This is a synthetic dataset constructed to stimulate the development and evaluation of 3D lane detection methods \n', '(download dataset from [[google drive](https://drive.google.com/open?id=1Kisxoj7mYl1YyA_4xBKTE8GGWiNZVain)] [[baidu netdisk](https://pan.baidu.com/s/1y_d73-SaNreesif5nVXIVg?pwd=a852)]). \n', 'This dataset is an extension to [Apollo Synthetic Dataset](http://apollo.auto/synthetic.html).\n', 'The detailed strategy of construction and the evaluation method refer to our ECCV 2020 paper:\n', '\n', '""Gen-LaneNet: a generalized and scalable approach for 3D lane detection"", Y. Guo, etal., ECCV, 2020 [[eccv](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660664.pdf)][[arxiv](https://arxiv.org/abs/2003.10656)] [[code](https://github.com/yuliangguo/Pytorch_Generalized_3D_Lane_Detection)]\n', '\n', '<p align=""center"">\n', '  <img src=""figs/00_0000045.jpg"" width=""280"" />\n', '  <img src=""figs/08_0000003.jpg"" width=""280"" /> \n', '  <img src=""figs/16_0000077.jpg"" width=""280"" />\n', '</p>\n', '\n', '\n', '\n', '## Requirements\n', '\n', '* python                    3.7.3\n', '* numpy                     1.16.2\n', '* scipy                     1.2.1 \n', '* matplotlib                3.0.3 \n', '* opencv-python             4.1.0.25\n', '* py3-ortools               5.1.4041\n', '\n', '\n', '\n', '## Data preparation\n', '\n', '\n', 'You are welcome to proceed to the development and evaluation directly using the splits of the training and testing sets we provide.\n', 'Feel free to skip this section if you use our data split directly.\n', '\n', '    ```\n', '    data_splits\n', '    ├── standard\n', '    │   ├── 3D_LaneNet\n', '    |   |       └──test_pred_file.json\n', '    │   ├── Gen_LaneNet\n', '    |   |       └──test_pred_file.json\n', '    │   ├── train.json\n', '    │   └── test.json\n', '    │── rare_subset\n', '    │   ├── 3D_LaneNet\n', '    |   |       └──test_pred_file.json\n', '    │   ├── Gen_LaneNet\n', '    |   |       └──test_pred_file.json\n', '    │   ├── train.json\n', '    │   └── test.json\n', '    |── illus_chg\n', '    │   ├── 3D_LaneNet\n', '    |   |       └──test_pred_file.json\n', '    │   ├── Gen_LaneNet\n', '    |   |       └──test_pred_file.json\n', '    │   ├── train.json\n', '    │   └── test.json\n', '    ```\n', '\n', 'Meanwhile, we provide the helper functions needed to build your own split from the raw datasets downloaded. The following codes \n', 'need to be right in order.\n', '\n', '    parse_apollo_sim_raw_data.py\n', '\n', 'This code extracts lane-lanes and center-lanes in an interested top-view area. The code reasons about the foreground and background\n', 'occlusion based on the provided ground-truth depth maps and semantic segmentation map. Those lane segments in the distance occluded\n', ""by background are discarded, because in general they are not expected to recover from a lane detection method. By setting 'vis=True',\n"", 'this code will draw ground-true lane-lines and center-lines on each image and save them.\n', '\n', '    prepare_data_split.py\n', '\n', ""This code randomly splits the whole data into training and testing sets following a 'standard' five-fold split. Specifically, \n"", ""a subset generated from a difficult urban map are further extracted to be the test set for 'rare subset' data split.\n"", '\n', '    prepare_data_subset\n', ' \n', ""Given the standard split of data, this code excludes images corresponding to a certain 'illumination' condition (before dawn)\n"", 'from the training set. On contrary, in the testing set, only images corresponding to that illumination condition are kept.\n', '\n', '\n', '\n', '## Evaluation\n', '\n', '\n', '    eval_3D_lane.py\n', '    \n', ""You need to modify 'method_name', 'data_split' to specify the method and the data split to conduct evaluation.\n"", ""For example, the default setting compares 'data_splits/illus_chg/Gen_LaneNet/test_pred_file.json' against ground-truth\n"", ""'data_splits/illus_chg/test.json'.\n"", ""Optionally, set 'args.dataset_dir' to the folder containing the original dataset. The original images are only required for visualizing lane results, when setting 'vis = True'.\n"", '\n', 'In this dataset, each image sample is associated with a set of ground-truth 3D lane-lines and center-lines, as well as \n', 'the camera height and pitch angle. \n', 'Per image, the optimal bipartite match between a set of predicted lane curves and a set of ground-truth lane curves is sought via\n', 'solving a min-cost flow.\n', 'Precision and recall are computed via varying lane confidence thresholds. Overall, evaluation metrics include:\n', ' * Average Precision (AP)\n', ' * max F-score\n', ' * x-error in close range (0-40 m)\n', ' * x-error in far range (40-100 m)\n', ' * z-error in close range (0-40 m)\n', ' * z-error in far range (40-100 m)\n', '\n', ""Before running the evaluation, you need to make sure the predicted lanes are saved in the 'test_pred_file.json' file following\n"", ""the format included in our example. Specifically, each lane needs to be associated with a 'prob' score to calculate the\n"", ""precision and recall in full-range. Otherwise, you can only keep 'evaluator.bench_one_submit' in the main code to \n"", 'evaluate your algorithm at a single operation point.\n', '\n', '## Baselines Results\n', '\n', 'We show the evaluation results comparing two baseline methods: \n', '* ""3d-lanenet:  end-to-end 3d multiple lane detection"", N. Garnet, etal., ICCV 2019\n', '* ""Gen-LaneNet: a generalized and scalable approach for 3D lane detection"", Y. Guo, etal., Arxiv, 2020\n', '\n', 'Comparisons are conducted under three distinguished splits of the dataset. For simplicity, only lane-line results are reported here.\n', 'The results from the code is slightly different from that reported in the paper due to different random splits.\n', '\n', '- **Standard**\n', '\n', '| Method                 | AP     | F-Score | x error near (m) | x error far (m) | z error near (m) | z error far (m) |\n', '|------------------------|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|\n', '| 3D-LaneNet             |   89.3    | 86.4      | 0.068     | 0.477     | 0.015     | 0.202\n', '| Gen-LaneNet            |   90.1    | 88.1      | 0.061     | 0.496     | 0.012     | 0.214\n', '\n', '- **Rare Subset**\n', '\n', '| Method                 | AP     | F-Score | x error near (m) | x error far (m) | z error near (m) | z error far (m) |\n', '|------------------------|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|\n', '| 3D-LaneNet             |  74.6     | 72.0      | 0.166     | 0.855     | 0.039     | 0.521\n', '| Gen-LaneNet            |  79.0     | 78.0      | 0.139     | 0.903     | 0.030     | 0.539\n', '\n', '- **Illumination Change**\n', '\n', '| Method                 | AP     | F-Score | x error near (m) | x error far (m) | z error near (m) | z error far (m) |\n', '|------------------------|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|\n', '| 3D-LaneNet             |   74.9    | 72.5      | 0.115     | 0.601     | 0.032     | 0.230\n', '| Gen-LaneNet            |   87.2    | 85.3      | 0.074     | 0.538     | 0.015     | 0.232\n', '\n', '\n', '\n', '## Visualization\n', '\n', ""Visual comparisons to the ground truth can be generated per image when setting 'vis = True'.\n"", 'We show two examples for each method under the data split involving illumination change.\n', '\n', '* 3D-LaneNet\n', '\n', '<img src=""figs/3D_LaneNet/images_00_0000148.jpg"" width=""400""> <img src=""figs/3D_LaneNet/images_00_0000171.jpg"" width=""400"">\n', '\n', '* Gen-LaneNet\n', '\n', '<img src=""figs/Gen_LaneNet/images_00_0000148.jpg"" width=""400""> <img src=""figs/Gen_LaneNet/images_00_0000171.jpg"" width=""400"">\n', '\n', '\n', '## Citation\n', 'Please cite the paper in your publications if it helps your research: \n', '\n', '    @article{guo2020gen,\n', '      title={Gen-LaneNet: A Generalized and Scalable Approach for 3D Lane Detection},\n', '      author={Yuliang Guo, Guang Chen, Peitao Zhao, Weide Zhang, Jinghao Miao, Jingao Wang, and Tae Eun Choe},\n', '      booktitle={Computer Vision - {ECCV} 2020 - 16th European Conference},\n', '      year={2020}\n', '    }\n']"
Synthetic+Data,InsulatorData/InsulatorDataSet,InsulatorData,https://api.github.com/repos/InsulatorData/InsulatorDataSet,181,86,2,"['https://api.github.com/users/InsulatorData', 'https://api.github.com/users/RegisWang']",,2023-04-07T07:16:09Z,https://raw.githubusercontent.com/InsulatorData/InsulatorDataSet/master/README.md,"['# Insulator Data Set - Chinese Power Line Insulator Dataset (CPLID)\n', 'Provide normal insulator images captured by UAVs and synthetic defective insulator images.\n', '\n', '    \n', '    @article{tao2018detection,\n', '      title={Detection of Power Line Insulator Defects Using Aerial Images Analyzed With Convolutional Neural Networks},\n', '      author={Tao, Xian and Zhang, Dapeng and Wang, Zihao and Liu, Xilong and Zhang, Hongyan and Xu, De},\n', '      journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems},\n', '      year={2018},\n', '      publisher={IEEE}\n', '    }\n', 'This dataset is divided into two part:\n', '\n', '- `Normal_Insulators` contains the normal insulators capture by UAVs. The number of the normal insulator images is **600**.\n', '\n', '\n', ""- `Defective_Insulators` contains the insulators with defect. The number of the defective insulator images is **248**. Since we don't have too much defective insulators, the data augmentation method is applied. These images are synthesized by following process:\n"", '    - Use the algorithm in [TVSeg](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.154.6237) to segment the defective insulator from a small part original images, the segment results are the mask images;\n', '    - Use affine transform to augment the original images and their mask, the augmentation results is a lot of original-mask image pairs;\n', '    - Use these image pairs to train the [U-Net](https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28);\n', '    - Use the trained U-Net to segment the rest part of images;\n', '    - Attach the insulators in different backgrounds.\n', '\n', 'Both these two directories contain two subdirectories, one called `images` contains the image files, the other called `labels` contains the VOC2007 format annotations.\n', '\n', '- The `labels` of `Normal_Insulators` contains **only** the annotations of insulators;\n', '- The `labels` of `Defective_Insulators` contains not only the annotations of insulators but also the annotations of defects which on the insulators.\n', '\n', 'The images is provided by the State Grid Corporation of China, and the dataset is made by WANG Zi-Hao.\n', 'If you have any question about this dataset, feel free to contact [zhwang0721@gmail.com](mailto:zhwang0721@gmail.com).\n']"
Synthetic+Data,namebrandon/Sparkov_Data_Generation,namebrandon,https://api.github.com/repos/namebrandon/Sparkov_Data_Generation,91,44,3,"['https://api.github.com/users/streamnsight', 'https://api.github.com/users/namebrandon', 'https://api.github.com/users/kartik2112']",Python,2023-04-05T06:07:46Z,https://raw.githubusercontent.com/namebrandon/Sparkov_Data_Generation/master/README.md,"['# Generate Fake Credit Card Transaction Data, Including Fraudulent Transactions\n', '\n', 'Note: Version v1.0 behavior has changed in such a way that it runs much faster, however transaction files are chunked, so that several files get generated per profile. If your downstream process expects 1 file per profile, please checkout the v0.5 release branch `release/v0.5`.\n', '\n', '## General Usage\n', '\n', 'In this version, the general usage has changed:\n', '\n', 'Please run the datagen script as follow:\n', '\n', '```bash\n', 'python datagen.py -n <NUMBER_OF_CUSTOMERS_TO_GENERATE> -o <OUTPUT_FOLDER> <START_DATE> <END_DATE>\n', '```\n', '\n', 'To see the full list of options, use:\n', '\n', '```bash\n', 'python datagen.py -h\n', '```\n', '\n', 'You can pass additional options with the following flags:\n', '\n', '- `-config <CONFIG_FILE>`: pass the name of the config file, defaults to `./profiles/main_config.json`\n', '- `-seed <INT>`: pass a seed to the Faker class\n', '- `-c <CUSTOMER_FILE>`: pass the path to an already generated customer file\n', '- `-o <OUTPUT_FOLDER>`: folder to save files into\n', '\n', 'This version is modified from the version v0.5 to parallelize the work using `multiprocessing`, so as to take advantage of all available CPUs and bring a huge speed improvement.\n', '\n', 'Because of the way it parallelize the work (chunking transaction generation by chunking the customer list), there will be multiple transaction files generated per profile. Also not that if the number of customers is small, there may be empty files (i.e. files where no customer in the chunk matched the profile). This is expected.\n', '\n', 'With standard profiles, it was benchmarked as generating ~95MB/thread/min. With a 64 cores/128 threads AMD E3, I was able to generate 1.4TB of data, 4.5B transactions, in just under 2h, as opposed to days when running the previous versions.\n', '\n', 'The generation code is originally based on code by [Josh Plotkin](https://github.com/joshplotkin/data_generation). Change log of modifications to original code are below.\n', '\n', '## Change Log\n', '\n', '### v1.0\n', '\n', '- Parallelized version, bringing orders of magnitude faster generation depending on the hardware used.\n', '\n', '### v0.5\n', '\n', '- 12x speed up thanks to some code refactoring.\n', '\n', '### v0.4\n', '\n', '- Only surface-level changes done in scripts so that simulation can be done using Python3\n', '- Corrected bat files to generate transactions files.\n', '\n', '### v0.3\n', '\n', '- Completely re-worked profiles / segmentation of customers\n', '- introduced fraudulent transactions\n', '- introduced fraudulent profiles\n', '- modification of transaction amount generation via Gamma distribution\n', '- added 150k_ shell scripts for multi-threaded data generation (one python process for each segment launched in the background)\n', '\n', '### v0.2\n', '\n', '- Added unix time stamp for transactions for easier programamtic evaluation.\n', '- Individual profiles modified so that there is more variation in the data.\n', '- Modified random generation of age/gender. Original code did not appear to work correctly?\n', '- Added batch files for windows users\n', '\n', '### v0.1\n', '\n', '- Transaction times are now included instead of just dates\n', '- Profile specific spending windows (AM/PM with weighting of transaction times)\n', '- Merchant names (specific to spending categories) are now included (along with code for generation)\n', '- Travel probability is added, with profile specific options\n', '- Travel max distances is added, per profile option\n', '- Merchant location is randomized based on home location and profile travel probabilities\n', '- Simulated transaction numbers via faker MD5 hash (replacing sequential 0..n numbering)\n', '- Includes credit card number via faker\n', '- improved cross-platform file path compatibility\n']"
Synthetic+Data,finos/datahub,finos,https://api.github.com/repos/finos/datahub,78,12,6,"['https://api.github.com/users/mcleo-d', 'https://api.github.com/users/grovesy', 'https://api.github.com/users/zheyu-wang-tony', 'https://api.github.com/users/maoo', 'https://api.github.com/users/finos-admin', 'https://api.github.com/users/pGrovesy']",Python,2023-03-27T17:32:06Z,https://raw.githubusercontent.com/finos/datahub/master/README.md,"['<H1>DataHub</H1> \n', '\n', '![DataHub logo](https://raw.githubusercontent.com/finos/datahub/master/docs/logo.png) \n', '\n', '_Synthetic data generation_\n', '\n', 'DataHub is a set of python libraries dedicated to the production of synthetic data to be used in tests, machine learning training, statistical analysis, and other use cases [wiki](https://en.wikipedia.org/wiki/Synthetic_data). DataHub uses existing datasets to generate synthetic models. If no existing data is available it will use user-provided scripts and data rules to generate synthetic data using out-of-the-box helper datasets.\n', '\n', 'Synthetic datasets are simply artificiality manufactured sets, produced to a desired degree of accuracy. Real Data does play a part in synthetic generation, all depending on the realism\xa0you require. The product roadmaps details out the functionality planned in this respect.\n', '\n', ""DataHub's core is predominantly based around pandas data frames and object generation.\n"", 'A common question: Now that I have a data frame of synthetic-data, what do I do with it? The Pandas library comes with an array of options\xa0here - so for the time being sinking to databases is out of the scope of the core library, however see that examples in the test folder for some common patterns.\n', '\n', '**note** As we build out a config based synthetic spec generator, we will bring this back into scope - please see our roadmap/issue list and get involved in the discussion.\n', '\n', '## Key documents\n', '\n', '1. For information on how to get started with DataHub see our [Getting Started Guide](https://github.com/finos/datahub/blob/master/docs/GettingStarted.md)\n', '2. For more technical information about DataHub and how to customize it, see the [Developer Guide](https://github.com/finos/datahub/blob/master/docs/DeveloperGuide.md)\n', '3. For high-level project direction see [Road Map](https://github.com/finos/datahub/blob/master/docs/synthetic-data-roadmap/roadmap.md), [Requirements Gathering Approach](https://github.com/finos/datahub/blob/master/docs/synthetic-data-roadmap/synthetic-data-requirements-gathering.md) and [Delegated Action Groups](https://github.com/finos/datahub/tree/master/docs/delegated-action-groups).\n', '4. For Feature Development, Good First Issues, Help Wanted and Bug Tracking see [DataHub GitHub Issues](https://github.com/finos/datahub/issues). \n', '5. This project uses [Gravizo](https://g.gravizo.com) for all diagrams and charts as highlighted in [DataHub Issue 41](https://github.com/finos/datahub/issues/41).   \n', '\n', '## Overview of Synthetic data\n', '\n', ""- Synthetic data is information that's is artificially manufactured rather than\xa0generated by *real-world events.\n"", '- Synthetic data is created algorithmically, and can be used as a stand-in for\xa0 test datasets of production data\n', '- **Real data** does play a part in synthetic data generation - depending on how\n', 'realistic you want the output\n', '\n', '## License\n', '\n', 'Copyright 2020 Citigroup\n', '\n', 'Distributed under the [Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0).\n', '\n', 'SPDX-License-Identifier: [Apache-2.0](https://spdx.org/licenses/Apache-2.0)\n']"
Synthetic+Data,lmoroney/synthetic_datasets,lmoroney,https://api.github.com/repos/lmoroney/synthetic_datasets,35,16,1,['https://api.github.com/users/lmoroney'],Python,2022-06-16T17:57:18Z,https://raw.githubusercontent.com/lmoroney/synthetic_datasets/master/README.md,"['# synthetic_datasets\n', ""Repository for Synthetic datasets I'm creating\n"", '\n', 'This will contain subfolders that have the details on the specific dataset within.\n']"
Synthetic+Data,LBNL-ETA/AlphaBuilding-SyntheticDataset,LBNL-ETA,https://api.github.com/repos/LBNL-ETA/AlphaBuilding-SyntheticDataset,34,11,1,['https://api.github.com/users/tsbyq'],Python,2022-09-24T01:19:13Z,https://raw.githubusercontent.com/LBNL-ETA/AlphaBuilding-SyntheticDataset/master/README.md,"['# AlphaBuilding-SyntheticDataset\n', '\n', 'This repository is created for the AlphaBuilding-SyntheticDataset. Details about this dataset could be found on its [GitHub page](https://lbnl-eta.github.io/AlphaBuilding-SyntheticDataset/).\n', '\n', '## Generate synthetic building operation data\n', 'The source code to reproduce the dataset could be found in the code directory. Follow the steps below to generate synthetic building operation data:\n', '1. Install [OpenStudio v2.9.1](https://github.com/NREL/OpenStudio/releases/tag/v2.9.1). \n', 'Set up the full path of openstudio.rb in the [create_workflow.rb](https://github.com/LBNL-ETA/AlphaBuilding-SyntheticDataset/blob/master/code/create_workflow.rb#L23) script. \n', 'The openstudio.rb file could be found in the installed OpenStudio folder: <paht_to_openstudio_installation>/openstudio-2.9.1/Ruby/openstudio.rb.\n', '\n', '2. Clone the [OpenStudio-Standards](https://github.com/NREL/openstudio-standards) repository to your local machine. Set up the full path of openstudio-standards.rb in the [create_workflow.rb](https://github.com/LBNL-ETA/AlphaBuilding-SyntheticDataset/blob/master/code/create_workflow.rb#L24) scipt. The openstudio-standards.rb file could be found in the cloned OpenStudio-Standards repository.\n', '\n', '3. Make sure [Ruby v2.2.4](https://www.ruby-lang.org/en/downloads/) is installed.\n', '\n', '4. Set up the arguments in the [create_workflow.rb](https://github.com/LBNL-ETA/AlphaBuilding-SyntheticDataset/blob/master/code/create_workflow.rb#L340-L362).\n', 'This allows you to create models and run simulations for different building types, vintages, climate zones\n', '    * Step 1. Select the [climate zone(s)](https://github.com/LBNL-ETA/AlphaBuilding-SyntheticDataset/blob/master/code/create_workflow.rb#L376-L393) for simulation. \n', '    The available climate zones are in the following array. \n', '    Uncomment the line(s) to specify the climate zone(s) you want to include:\n', '        \n', '        ```ruby\n', '        climate_zones = [\n', ""            'ASHRAE 169-2006-1A',     # Considered in the synthetic operatin dataset\n"", ""            # 'ASHRAE 169-2006-2A',\n"", ""            # 'ASHRAE 169-2006-2B',\n"", ""            # 'ASHRAE 169-2006-3A',\n"", ""            # 'ASHRAE 169-2006-3B',\n"", ""            'ASHRAE 169-2006-3C',     # Considered in the synthetic operatin dataset\n"", ""            # 'ASHRAE 169-2006-4A',\n"", ""            # 'ASHRAE 169-2006-4B',\n"", ""            # 'ASHRAE 169-2006-4C',\n"", ""            'ASHRAE 169-2006-5A',     # Considered in the synthetic operatin dataset\n"", ""            # 'ASHRAE 169-2006-5B',\n"", ""            # 'ASHRAE 169-2006-6A',\n"", ""            # 'ASHRAE 169-2006-6B',\n"", ""            # 'ASHRAE 169-2006-7A',\n"", ""            # 'ASHRAE 169-2006-8A',\n"", '        ]\n', '        ```\n', '    * Step 2. [Prepare the weather files (EPWs) and map the their folder to the climate zones.](https://github.com/LBNL-ETA/AlphaBuilding-SyntheticDataset/blob/master/code/create_workflow.rb#L397-L402)\n', ""    For example, this repository provides 30 years' historical and a TMY3 weather files for three U.S. cities - \n"", '    Chicago, Miami, and San Francicso. The weather files are saved in ```./EPWs/<city name>_AMY```. And the Hash below\n', '    maps the climate zones of the three cities and the weather file to be used in the simulations. \n', '        ```ruby\n', '        hash_climate_epw = {\n', ""            # 'climate zone option' => 'EPWs folder name', (example convention)\n"", ""            'ASHRAE 169-2006-1A' => 'Miami_AMY',\n"", ""            'ASHRAE 169-2006-3C' => 'SF_AMY',\n"", ""            'ASHRAE 169-2006-5A' => 'Chicago_AMY',\n"", '        }\n', '        ```\n', '        You need to provide weather files and mapping rule for buildings in other climate zones.\n', '    \n', '    * Step 3. [Select the vintages you want to consider.](https://github.com/LBNL-ETA/AlphaBuilding-SyntheticDataset/blob/master/code/create_workflow.rb#L406-L411)\n', '        ```ruby\n', '        vintages = [\n', ""            # '90.1-2004',\n"", ""            # '90.1-2007',\n"", ""            # '90.1-2010',\n"", ""            '90.1-2013'     # Considered in the synthetic operatin dataset\n"", '        ]\n', '        ```\n', '    \n', '    * [Step 4. Select the building type to consider.](https://github.com/LBNL-ETA/AlphaBuilding-SyntheticDataset/blob/master/code/create_workflow.rb#L416-L442)\n', '      Please note that occupancy_simulator only works for office buildings.\n', '      ```ruby\n', '        building_types = [\n', '            ###############################################################\n', '            ## building types that support stochastic occupancy simulation\n', '            ###############################################################\n', ""            # 'SmallOffice',\n"", ""            # 'MediumOffice',\n"", ""            # 'LargeOffice',\n"", ""            # 'SmallOfficeDetailed',\n"", ""            'MediumOfficeDetailed',     # Considered in the synthetic operatin dataset\n"", ""            # 'LargeOfficeDetailed',\n"", '            ###############################################################\n', '            ## building types that do not support stochastic occupancy simulation\n', '            ###############################################################\n', ""            # 'SecondarySchool',\n"", ""            # 'PrimarySchool',\n"", ""            # 'SmallHotel',\n"", ""            # 'LargeHotel',\n"", ""            # 'Warehouse',\n"", ""            # 'RetailStandalone',\n"", ""            # 'RetailStripmall',\n"", ""            # 'QuickServiceRestaurant',\n"", ""            # 'FullServiceRestaurant',\n"", ""            # 'MidriseApartment',\n"", ""            # 'HighriseApartment',\n"", ""            # 'Hospital',\n"", ""            # 'Outpatient',\n"", '        ]\n', '        ```\n', '\n', '    * Step 5. [Set the number of stochastic occupancy simulations for each building model.](https://github.com/LBNL-ETA/AlphaBuilding-SyntheticDataset/blob/master/code/create_workflow.rb#L445)\n', '        ```ruby\n', '        number_of_stochastic_occupancy_simulation = 5\n', '        ```\n', '    \n', '    * Step 6. [Set the energy efficiency level (1 - low, 2 - standard, 3 - high) to run.](https://github.com/LBNL-ETA/AlphaBuilding-SyntheticDataset/blob/master/code/create_workflow.rb#L448)\n', '        ```ruby\n', '        efficiency_level = 2\n', '        ```\n', '\n', '5. Run the create_workflow.rb script with ```<ruby 2.2.4 command> create_workflow.rb``` The script will generate and run OpenStudio workflows to output the synthetic building operation data.\n', '\n', '6. Post-processing. The above routine automatically generates OpenStudio models and runs the simulations.\n', 'This [Python script](https://github.com/LBNL-ETA/AlphaBuilding-SyntheticDataset/blob/master/code/results_extraction_demo.py) shows an example of extracting the raw CSV outputs and saving them in a structured way.\n', 'Depending on their purpose, readers may need develop custom routines to process the simulation results. \n', '\n', '\n', '## License\n', 'Refer to [License.txt](https://github.com/LBNL-ETA/AlphaBuilding-SyntheticDataset/blob/master/License.txt)\n']"
Synthetic+Data,milaan9/Clustering-Datasets,milaan9,https://api.github.com/repos/milaan9/Clustering-Datasets,218,202,1,['https://api.github.com/users/milaan9'],,2023-04-05T12:57:03Z,https://raw.githubusercontent.com/milaan9/Clustering-Datasets/master/README.md,"['<p align=""center""> \n', '<a href=""https://github.com/milaan9""><img src=""https://img.shields.io/static/v1?logo=github&label=maintainer&message=milaan9&color=ff3300"" alt=""Last Commit""/></a> \n', '<!--<img src=""https://badges.pufler.dev/created/milaan9/Clustering-Datasets"" alt=""Created""/>-->\n', '<!--<a href=""https://github.com/milaan9/Clustering-Datasets/graphs/commit-activity""><img src=""https://img.shields.io/github/last-commit/milaan9/Clustering-Datasets.svg?colorB=ff8000&style=flat"" alt=""Last Commit""/></a>-->\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/pulse"" alt=""Activity""><img src=""https://img.shields.io/github/commit-activity/m/milaan9/Clustering-Datasets.svg?colorB=teal&style=flat"" /></a> \n', '<a href=""https://hits.seeyoufarm.com""><img src=""https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2Fmilaan9%2FClustering-Datasets&count_bg=%231DC92C&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=views&edge_flat=false""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/stargazers""><img src=""https://img.shields.io/github/stars/milaan9/Clustering-Datasets.svg?colorB=1a53ff"" alt=""Stars Badge""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/network/members""><img src=""https://img.shields.io/github/forks/milaan9/Clustering-Datasets"" alt=""Forks Badge""/> </a>\n', '<img src=""https://img.shields.io/github/repo-size/milaan9/Clustering-Datasets.svg?colorB=CC66FF&style=flat"" alt=""Size""/>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/pulls""><img src=""https://img.shields.io/github/issues-pr/milaan9/Clustering-Datasets.svg?colorB=yellow&style=flat"" alt=""Pull Requests Badge""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/issues""><img src=""https://img.shields.io/github/issues/milaan9/Clustering-Datasets.svg?colorB=yellow&style=flat"" alt=""Issues Badge""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/blob/master/LICENSE""><img src=""https://img.shields.io/badge/License-MIT-blueviolet.svg"" alt=""MIT License""/></a> \n', '<a href=""https://github.com/milaan9/Clustering-Datasets""><img src=""https://img.shields.io/static/v1?label=%F0%9F%8C%9F&message=If%20Useful&style=style=flat&color=BC4E99"" alt=""Star Badge""/>\n', '</p> \n', '<!--<img src=""https://badges.pufler.dev/contributors/milaan9/01_Python_Introduction?size=50&padding=5&bots=true"" alt=""milaan9""/>-->\n', '\n', '\n', '\n', '# Clustering-Datasets\n', '\n', 'This repository contains the collection of UCI (real-life)datasets and Synthetic (artificial) datasets(with cluster labels).\n', '\n', '  * [UCI (real-world) datasets](https://github.com/milaan9/Clustering-Datasets/tree/master/01.%20UCI)\n', '  * [Synthetic (artificial) datasets](https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic)\n', '\n', '### Artificial data\n', '\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/2d-10c.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/2d-10c.png"" alt=""2d-10c"" title=""2d-10c"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/2d-20c.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/2d-20c.png"" alt=""2d-20c-no0"" title=""2d-20c"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/2d-3c.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/2d-3c.png"" alt=""2d-3c-no123"" title=""2d-3c"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/2d-4c-2.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/2d-4c-2.png"" alt=""2d-4c-no4"" title=""2d-4c-2"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/2d-4c-3.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/2d-4c-3.png"" alt=""2d-4c-no9"" title=""2d-4c-3"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/2d-4c-1.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/2d-4c-1.png"" alt=""2d-4c"" title=""2d-4c-1"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/2sp2glob.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/2sp2glob.png"" alt=""2sp2glob"" title=""2sp2glob"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/3-spiral.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/3-spiral.png"" alt=""3-spiral"" title=""3-spiral"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/3MC.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/3mc.png"" alt=""3MC"" title=""3MC"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/D31.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/d31.png"" alt=""D31"" title=""D31"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/DS577.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/ds577.png"" alt=""DS577"" title=""DS577"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/DS850.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/ds850.png"" alt=""DS850"" title=""DS850"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/R15.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/r15.png"" alt=""R15"" title=""R15"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/aggregation.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/aggregation.png"" alt=""aggregation"" title=""aggregation"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/atom.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/atom.png"" alt=""atom"" title=""atom"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/banana.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/banana.png"" alt=""banana"" title=""banana"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/birch-rg1.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/birch-rg1.png"" alt=""birch-rg1"" title=""birch-rg1"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/birch-rg2.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/birch-rg2.png"" alt=""birch-rg2"" title=""birch-rg2"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/birch-rg3.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/birch-rg3.png"" alt=""birch-rg3"" title=""birch-rg3"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/chainlink.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/chainlink.png"" alt=""chainlink"" title=""chainlink"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/cluto-t4.8k.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/cluto-t4.8k.png"" alt=""cluto-t4.8k"" title=""cluto-t4.8k"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/cluto-t5.8k.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/cluto-t5.8k.png"" alt=""cluto-t5.8k"" title=""cluto-t5.8k"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/cluto-t7.10k.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/cluto-t7.10k.png"" alt=""cluto-t7.10k"" title=""cluto-t7.10k"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/cluto-t8.8k.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/cluto-t8.8k.png"" alt=""cluto-t8.8k"" title=""cluto-t8.8k"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/complex8.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/complex8.png"" alt=""complex8"" title=""complex8"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/complex9.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/complex9.png"" alt=""complex9"" title=""complex9"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/compound.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/compound.png"" alt=""compound"" title=""compound"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/cure-t0-2000n-2D.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/cure-t0-2000n-2d.png"" alt=""cure-t0-2000n-2D"" title=""cure-t0-2000n-2D"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/cure-t1-2000n-2D.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/cure-t1-2000n-2d.png"" alt=""cure-t1-2000n-2D"" title=""cure-t1-2000n-2D"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/cure-t2-4k.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/cure-t2-4k.png"" alt=""cure-t2-4k"" title=""cure-t2-4k"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/curves1.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/curves1.png"" alt=""curves1"" title=""curves1"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/curves2.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/curves2.png"" alt=""curves2"" title=""curves2"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/dartboard1.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/dartboard1.png"" alt=""dartboard1"" title=""dartboard1"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/dartboard2.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/dartboard2.png"" alt=""dartboard2"" title=""dartboard2"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/dense-disk-3000.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/dense-disk-3000.png"" alt=""dense-disk-3000"" title=""dense-disk-3000"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/dense-disk-5000.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/dense-disk-5000.png"" alt=""dense-disk-5000"" title=""dense-disk-5000"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/diamond9.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/diamond9.png"" alt=""diamond9"" title=""diamond9"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/disk-1000n.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/disk-1000n.png"" alt=""disk-1000n"" title=""disk-1000n"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/disk-3000n.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/disk-3000n.png"" alt=""disk-3000n"" title=""disk-3000n"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/disk-4000n.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/disk-4000n.png"" alt=""disk-4000n"" title=""disk-4000n"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/disk-4500n.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/disk-4500n.png"" alt=""disk-4500n"" title=""disk-4500n"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/disk-4600n.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/disk-4600n.png"" alt=""disk-4600n"" title=""disk-4600n"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/disk-5000n.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/disk-5000n.png"" alt=""disk-5000n"" title=""disk-5000n"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/disk-6000n.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/disk-6000n.png"" alt=""disk-6000n"" title=""disk-6000n"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/donut1.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/donut1.png"" alt=""donut1"" title=""donut1"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/donut2.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/donut2.png"" alt=""donut2"" title=""donut2"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/donut3.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/donut3.png"" alt=""donut3"" title=""donut3"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/donutcurves.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/donutcurves.png"" alt=""donutcurves"" title=""donutcurves"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/ds2c2sc13.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/ds2c2sc13.png"" alt=""ds2c2sc13"" title=""ds2c2sc13"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/ds3c3sc6.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/ds3c3sc6.png"" alt=""ds3c3sc6"" title=""ds3c3sc6"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/ds4c2sc8.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/ds4c2sc8.png"" alt=""ds4c2sc8"" title=""ds4c2sc8"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/elliptical_10_2.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/elliptical_10_2.png"" alt=""elliptical_10_2"" title=""elliptical_10_2"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/elly-2d10c13s.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/elly-2d10c13s.png"" alt=""elly-2d10c13s"" title=""elly-2d10c13s"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/engytime.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/engytime.png"" alt=""engytime"" title=""engytime"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/flame.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/flame.png"" alt=""flame"" title=""flame"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/fourty.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/fourty.png"" alt=""fourty"" title=""fourty"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/golfball.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/golfball.png"" alt=""golfball"" title=""golfball"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/hepta.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/hepta.png"" alt=""hepta"" title=""hepta"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/insect.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/insect.png"" alt=""insect"" title=""insect"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/jain.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/jain.png"" alt=""jain"" title=""jain"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/long1.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/long1.png"" alt=""long1"" title=""long1"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/long2.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/long2.png"" alt=""long2"" title=""long2"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/long3.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/long3.png"" alt=""long3"" title=""long3"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/longsquare.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/longsquare.png"" alt=""longsquare"" title=""longsquare"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/lsun.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/lsun.png"" alt=""lsun"" title=""lsun"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/mopsi-finland.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/mopsi-finland.png"" alt=""mopsi-finland"" title=""mopsi-finland"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/mopsi-joensuu.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/mopsi-joensuu.png"" alt=""mopsi-joensuu"" title=""mopsi-joensuu"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/pathbased.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/pathbased.png"" alt=""pathbased"" title=""pathbased"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/rings.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/rings.png"" alt=""rings"" title=""rings"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/s-set1.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/s-set1.png"" alt=""s-set1"" title=""s-set1"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/s-set2.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/s-set2.png"" alt=""s-set2"" title=""s-set2"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/s-set3.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/s-set3.png"" alt=""s-set3"" title=""s-set3"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/s-set4.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/s-set4.png"" alt=""s-set4"" title=""s-set4"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/sizes1.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/sizes1.png"" alt=""sizes1"" title=""sizes1"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/sizes2.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/sizes2.png"" alt=""sizes2"" title=""sizes2"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/sizes3.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/sizes3.png"" alt=""sizes3"" title=""sizes3"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/sizes4.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/sizes4.png"" alt=""sizes4"" title=""sizes4"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/sizes5.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/sizes5.png"" alt=""sizes5"" title=""sizes5"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/smile1.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/smile1.png"" alt=""smile1"" title=""smile1"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/smile2.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/smile2.png"" alt=""smile2"" title=""smile2"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/smile3.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/smile3.png"" alt=""smile3"" title=""smile3"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/spherical_4_3.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/spherical_4_3.png"" alt=""spherical_4_3"" title=""spherical_4_3"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/spherical_5_2.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/spherical_5_2.png"" alt=""spherical_5_2"" title=""spherical_5_2"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/spherical_6_2.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/spherical_6_2.png"" alt=""spherical_6_2"" title=""spherical_6_2"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/spiral.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/spiral.png"" alt=""spiral"" title=""spiral"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/spiralsquare.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/spiralsquare.png"" alt=""spiralsquare"" title=""spiralsquare"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/square1.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/square1.png"" alt=""square1"" title=""square1"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/square2.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/square2.png"" alt=""square2"" title=""square2"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/square3.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/square3.png"" alt=""square3"" title=""square3"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/square4.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/square4.png"" alt=""square4"" title=""square4"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/square5.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/square5.png"" alt=""square5"" title=""square5"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/st900.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/st900.png"" alt=""st900"" title=""st900"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/target.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/target.png"" alt=""target"" title=""target"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/tetra.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/tetra.png"" alt=""tetra"" title=""tetra"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/triangle1.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/triangle1.png"" alt=""triangle1"" title=""triangle1"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/triangle2.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/triangle2.png"" alt=""triangle2"" title=""triangle2"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/twenty.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/twenty.png"" alt=""twenty"" title=""twenty"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/twodiamonds.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/twodiamonds.png"" alt=""twodiamonds"" title=""twodiamonds"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/wingnut.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/wingnut.png"" alt=""wingnut"" title=""wingnut"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/xclara.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/xclara.png"" alt=""xclara"" title=""xclara"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/zelnik1.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/zelnik1.png"" alt=""zelnik1"" title=""zelnik1"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synt"
Synthetic+Data,mrtzh/PrivateMultiplicativeWeights.jl,mrtzh,https://api.github.com/repos/mrtzh/PrivateMultiplicativeWeights.jl,44,16,5,"['https://api.github.com/users/mrtzh', 'https://api.github.com/users/giladroyz', 'https://api.github.com/users/KristofferC', 'https://api.github.com/users/Quantisan', 'https://api.github.com/users/tkelman']",Julia,2023-01-26T12:19:09Z,https://raw.githubusercontent.com/mrtzh/PrivateMultiplicativeWeights.jl/master/README.md,"['# PrivateMultiplicativeWeights.jl\n', '\n', 'This package implements `MWEM`, a simple and practical algorithm for differentially private data release.\n', '\n', 'MIT Licensed. See `LICENSE.md`.\n', '\n', '## Installation\n', '\n', 'Install required packages, then open a Julia prompt and call: \n', '```\n', 'using Pkg\n', 'Pkg.add(""PrivateMultiplicativeWeights"")\n', '```\n', '\n', '## Main Features\n', '\n', '* Differentially private synthetic data preserving lower order marginals of an input data set\n', '* Optimized in-memory implementation for small number of data attributes\n', '* Scalable heuristic for large number of data attributes\n', '* Easy-to-use interfaces for custom query sets and data representations\n', '\n', '## Examples\n', '\n', '### Histogram approximations\n', '\n', '![Histogram approximation](https://github.com/mrtzh/PrivateMultiplicativeWeights.jl/blob/master/examples/histograms.png?raw=true)\n', '\n', 'Check out [`histograms.ipynb`](/examples/histograms.ipynb) for details on how to\n', 'use the algorithm to compute differentially private histogram approximations. \n', '\n', '### Marginal approximations\n', '\n', 'The package can also be used to create synthetic data that approximates the\n', 'lower order marginals of a data set with binary features.  For the sake of\n', 'illustration, we create a random data set with hidden correlations. Columns\n', 'correspond to data points.  \n', '\n', '```\n', 'd, n = 20, 1000\n', 'data_matrix = rand(0:1, d ,n)\n', 'data_matrix[3, :] = data_matrix[1, :] .* data_matrix[2, :]\n', '```\n', '\n', 'We can run MWEM to produce synthetic data accurate for 1st, 2nd, 3rd order marginals of the source data.\n', '```\n', 'using PrivateMultiplicativeWeights\n', 'mw = mwem(Parities(d, 3), Tabular(data_matrix))\n', '```\n', 'This will convert the data to its explicit histogram representation of size 2^d\n', 'and may not be useful when d is large. See section on factored histograms\n', 'for an alternative when the dimension d is large.\n', '\n', '### Convert histograms to matrices\n', '\n', 'We can convert synthetic data in histogram representation to a tabular \n', '(matrix) representation.\n', '```\n', 'table = Tabular(mw.synthetic, n)\n', '```\n', '\n', '### Compute error of approximation\n', 'Compute error achieved by MWEM:\n', '```\n', 'maximum_error(mw), mean_squared_error(mw)\n', '```\n', 'Note that these statistics are *not* differentially private.\n', '\n', '## Parameters\n', '\n', 'Parameters can be set flexibly with the `MWParameters` constructor:\n', '```\n', 'mw = mwem(Parities(d, 3),\n', '          Tabular(data_matrix),\n', '          MWParameters(epsilon=1.0,\n', '                       iterations=10,\n', '                       repetitions=10,\n', '                       verbose=false,\n', '                       noisy_init=false,\n', '                       init_budget=0.05,\n', '                       noisy_max_budget=0.5))\n', '```\n', 'Available parameters:\n', '\n', '| Name | Default | Description |\n', '| ---- | ------- | ----------- |\n', '| `epsilon` | `1.0` | Privacy parameter for the algorithm. Each iteration of MWEM is `epsilon`-differentially private. Total privacy guarantees follow via composition theorems.|\n', '| `iterations` | `10` | Number of iterations of MWEM. Each iteration corresponds to selecting one query via the exponential mechanism, evaluating the query on the data, and updating the internal state. |\n', '| `repetitions`| `10` | Number of times MWEM cycles through previously measured queries per iteration. This has no additional privacy cost. |\n', '| `noisy_init` | `false` | This requires part of the `epsilon` privacy cost.  When `noisy_init` is set to false, the initialization is uniform.  |\n', '| `init_budget` | `0.05` | In case the `noisy_init` flag is set to true, this flag decide what fraction of the `epsilon` privacy cost will be given for the noisy initialization. When `noisy_init` is set to false, all the budget will be used by the iterations. |\n', '| `noisy_max_budget` | `0.5` | Decise what fraction from the `epsion` privacy badget of every iteration will go to the ""noisy max"" step. (the rest is for the Exponential Mechanism)  |\n', '| `verbose` | `false` | print timing and error statistics per iteration (information is not differentially private)\n', '\n', 'The function `MWParameters` accepts any subset of parameters, e.g.,\n', '`MWParameter(epsilon=0.5, iterations=5)`.\n', '\n', '## Data representations\n', '\n', '### Histogram representation\n', '\n', 'By default, MWEM works with the histogram representation of a data sets. This\n', 'means that the data is represented by a vector whose length is equal to the size\n', 'of domain. For example, data consisting of `d` binary attributes would be\n', 'converted to an array of length `2^d`. MWEM needs to store and array of this\n', 'length in main memory, which is often the computational bottleneck.\n', '\n', '## Factored histograms\n', '\n', 'When the histogram representation is too large, try using factored histograms.\n', 'Factored histograms maintain a product distribution over clusters of attributes\n', 'of the data. Each component is represented using a single histogram. Components\n', 'are merged as it becomes necessary. This often allows to scale up MWEM by orders\n', 'of magnitude.  \n', '```\n', 'd, n = 100, 1000\n', 'data_matrix = rand(0:1, d, n)\n', 'data_matrix[3, :] = data_matrix[1, :] .* data_matrix[2, :]\n', 'mw = mwem(FactorParities(d, 3), Tabular(data_matrix))\n', '```\n', '\n', 'Also see `examples.jl`.\n', '\n', '## Query representations\n', '\n', 'There are two ways to define custom query sets.\n', '\n', '### Histogram queries\n', '\n', 'Histogram queries are linear functions in the histogram representation of the\n', 'data.  You can define custom query workloads by using\n', '`HistogramQueries(query_matrix)` instead of `Parities(d, 3)`. Here `query\n', 'matrix` is an `N x k` matrix specifying the query set in its Histogram\n', 'representation, `N` is the histogram length and `k` is the `k` is the number of\n', 'queries.\n', '\n', '### Custom query types\n', '\n', 'To build query sets with your own implicit representations, sub-type\n', '`Query` and `Queries`. Implement the functions specified in `src/interface.jl`.\n', '\n', 'See `src/parities.jl` for an example.\n', '\n', '### Available query sets\n', '\n', '- **Parities**(d, k)\n', '\n', '  Parities of `k` out of `d` attributes. This corresponds to approximating\n', '  `k`-way marginals of the original data.\n', '\n', '- **FactorParities**(d, k)\n', '\n', '  Parities of `k` out of `d` attributes for factored histogram representation.\n', '\n', '- **SeriesRangeQueries**(N)\n', '\n', '  Range queries corresponding to all interval queries over a histogram of length `N`.\n', '  \n', '  - *SeriesRangeQueries**(Intervals)\n', '\n', '  Range queries over histogram with length N, corresponding to intervals = {Interval1, Interval2, ...}\n', '  where Interval = (i, j) so that 1 <= i <= j <= N.\n', '\n', '## Contributing to this package\n', '\n', 'There are many ways to contribute to this repository:\n', '\n', '* Experiments\n', '* Additional query sets (e.g., two-dimensional range queries)\n', '* Additional tests, debugging, optimization\n', '* Additional documentation\n', '\n', '## Citing this package\n', '\n', 'The MWEM algorithm was presented in the following paper:\n', '```\n', '@inproceedings{HLM12,\n', '  author = ""Moritz Hardt and Katrina Ligett and Frank McSherry"",\n', '  title = ""A simple and practical algorithm for differentially-private data release"",\n', '  booktitle = {Proc.\\ $26$th Neural Information Processing Systems (NIPS)},\n', '  year = {2012},\n', '}\n', '```\n', '\n', '## Status\n', '\n', '[![Build\n', 'Status](https://travis-ci.org/mrtzh/PrivateMultiplicativeWeights.jl.svg?branch=master)](https://travis-ci.org/mrtzh/PrivateMultiplicativeWeights.jl)\n']"
Responsible+AI,microsoft/responsible-ai-toolbox,microsoft,https://api.github.com/repos/microsoft/responsible-ai-toolbox,756,201,30,"['https://api.github.com/users/imatiach-msft', 'https://api.github.com/users/gaugup', 'https://api.github.com/users/xuke444', 'https://api.github.com/users/zhb000', 'https://api.github.com/users/romanlutz', 'https://api.github.com/users/vinuthakaranth', 'https://api.github.com/users/tongyu-microsoft', 'https://api.github.com/users/gregorybchris', 'https://api.github.com/users/dependabot%5Bbot%5D', 'https://api.github.com/users/RubyZ10', 'https://api.github.com/users/riedgar-ms', 'https://api.github.com/users/csigs', 'https://api.github.com/users/rihorn2', 'https://api.github.com/users/ms-kashyap', 'https://api.github.com/users/mesameki', 'https://api.github.com/users/jamesbchao', 'https://api.github.com/users/ilmarinen', 'https://api.github.com/users/microsoftopensource', 'https://api.github.com/users/JarvisG495', 'https://api.github.com/users/Advitya17', 'https://api.github.com/users/janjagusch', 'https://api.github.com/users/LeJit', 'https://api.github.com/users/yongjiaaaa', 'https://api.github.com/users/hawestra', 'https://api.github.com/users/natalie-isak', 'https://api.github.com/users/alexquach', 'https://api.github.com/users/aminadibi', 'https://api.github.com/users/zhb789', 'https://api.github.com/users/jlema', 'https://api.github.com/users/michaelamoako']",TypeScript,2023-04-09T11:04:16Z,https://raw.githubusercontent.com/microsoft/responsible-ai-toolbox/main/README.md,"['![MIT license](https://img.shields.io/badge/License-MIT-blue.svg)\n', '\n', '![Responsible AI Widgets Python Build](https://img.shields.io/github/actions/workflow/status/microsoft/responsible-ai-toolbox/CI-raiwidgets-pytest.yml?branch=main&label=Responsible%20AI%20Widgets%20Python%20Build)\n', '![UI deployment to test environment](https://img.shields.io/github/actions/workflow/status/microsoft/responsible-ai-toolbox/CD.yml?branch=main&label=UI%20deployment%20to%20test%20environment)\n', '\n', '![PyPI raiwidgets](https://img.shields.io/pypi/v/raiwidgets?label=PyPI%20raiwidgets)\n', '![PyPI responsibleai](https://img.shields.io/pypi/v/responsibleai?label=PyPI%20responsibleai)\n', '![PyPI erroranalysis](https://img.shields.io/pypi/v/erroranalysis?label=PyPI%20erroranalysis)\n', '![PyPI raiutils](https://img.shields.io/pypi/v/raiutils?label=PyPI%20raiutils)\n', '![PyPI rai_test_utils](https://img.shields.io/pypi/v/rai_test_utils?label=PyPI%20rai_test_utils)\n', '\n', '![npm model-assessment](https://img.shields.io/npm/v/@responsible-ai/model-assessment?label=npm%20%40responsible-ai%2Fmodel-assessment)\n', '\n', '# Responsible AI Toolbox\n', 'Responsible AI is an approach to assessing, developing, and deploying AI systems in a safe, trustworthy, and ethical manner, and take responsible decisions and actions.\n', '\n', 'Responsible AI Toolbox is a suite of tools providing a collection of model and data exploration and assessment user interfaces and libraries that enable a better understanding of AI systems. These interfaces and libraries empower developers and stakeholders of AI systems to develop and monitor AI more responsibly, and take better data-driven actions.\n', '\n', '\n', '<p align=""center"">\n', '<img src=""https://raw.githubusercontent.com/microsoft/responsible-ai-widgets/main/img/responsible-ai-toolbox.png"" alt=""ResponsibleAIToolboxOverview"" width=""750""/>\n', '\n', '\n', 'The Toolbox consists of three repositories: \n', '\n', '\xa0\n', '| Repository| Tools Covered  |\n', '|--|--|\n', '| [Responsible-AI-Toolbox Repository](https://github.com/microsoft/responsible-ai-toolbox) (Here) |This repository contains four visualization widgets for model assessment and decision making:<br>1. [Responsible AI dashboard](https://github.com/microsoft/responsible-ai-toolbox#introducing-responsible-ai-dashboard), a single pane of glass bringing together several mature Responsible AI tools from the toolbox for a holistic responsible assessment and debugging of models and making informed business decisions. With this dashboard, you can identify model errors, diagnose why those errors are happening, and mitigate them. Moreover, the causal decision-making capabilities provide actionable insights to your stakeholders and customers.<br>2. [Error Analysis dashboard](https://github.com/microsoft/responsible-ai-toolbox/blob/main/docs/erroranalysis-dashboard-README.md), for identifying model errors and discovering cohorts of data for which the model underperforms. \t<br>3. [Interpretability dashboard](https://github.com/microsoft/responsible-ai-toolbox/blob/main/docs/explanation-dashboard-README.md), for understanding model predictions. This dashboard is powered by InterpretML.<br>4. [Fairness dashboard](https://github.com/microsoft/responsible-ai-toolbox/blob/main/docs/fairness-dashboard-README.md), for understanding model’s fairness issues using various group-fairness metrics across sensitive features and cohorts. This dashboard is powered by Fairlearn. \n', '| [Responsible-AI-Toolbox-Mitigations Repository](https://github.com/microsoft/responsible-ai-toolbox-mitigations) | The Responsible AI Mitigations Library helps AI practitioners explore different measurements and mitigation steps that may be most appropriate when the model underperforms for a given data cohort. The library currently has two modules: <br>1. DataProcessing, which offers mitigation techniques for improving model performance for specific cohorts. <br>2. DataBalanceAnalysis, which provides metrics for diagnosing errors that originate from data imbalance either on class labels or feature values. <br> 3. Cohort: provides classes for handling and managing cohorts, which allows the creation of custom pipelines for each cohort in an easy and intuitive interface. The module also provides techniques for learning different decoupled estimators (models) for different cohorts and combining them in a way that optimizes different definitions of group fairness.|  \n', '[Responsible-AI-Tracker Repository](https://github.com/microsoft/responsible-ai-toolbox-tracker) |Responsible AI Toolbox Tracker is a JupyterLab extension for managing, tracking, and comparing results of machine learning experiments for model improvement. Using this extension, users can view models, code, and visualization artifacts within the same framework enabling therefore fast model iteration and evaluation processes. Main functionalities include: <br>1. Managing and linking model improvement artifacts<br> 2. Disaggregated model evaluation and comparisons<br>3. Integration with the Responsible AI Mitigations library<br>4. Integration with mlflow|\n', ' [Responsible-AI-Toolbox-GenBit Repository](https://github.com/microsoft/responsible-ai-toolbox-genbit) | The Responsible AI Gender Bias (GenBit) Library helps AI practitioners measure gender bias in Natural Language Processing (NLP) datasets. The main goal of GenBit is to analyze your text corpora and compute metrics that give insights into the gender bias present in a corpus.|\n', '\n', '  \n', '\n', '\n', '## Introducing Responsible AI dashboard\n', '\n', '[Responsible AI dashboard](https://github.com/microsoft/responsible-ai-toolbox/blob/main/notebooks/responsibleaidashboard/tour.ipynb) is a single pane of glass, enabling you to easily flow through different stages of model debugging and decision-making. This customizable experience can be taken in a multitude of directions, from analyzing the model or data holistically, to conducting a deep dive or comparison on cohorts of interest, to explaining and perturbing model predictions for individual instances, and to informing users on business decisions and actions.\n', '\n', '\n', '<p align=""center"">\n', '<img src=""https://raw.githubusercontent.com/microsoft/responsible-ai-widgets/main/img/responsible-ai-dashboard.png"" alt=""ResponsibleAIDashboard"" width=""750""/>\n', '\n', '\n', '\n', '\n', 'In order to achieve these capabilities, the dashboard integrates together ideas and technologies from several open-source toolkits in the areas of\n', '\n', '\n', '\n', '- <b>Error Analysis</b> powered by [Error Analysis](https://github.com/microsoft/responsible-ai-widgets/blob/main/docs/erroranalysis-dashboard-README.md), which identifies cohorts of data with higher error rate than the overall benchmark. These discrepancies might occur when the system or model underperforms for specific demographic groups or infrequently observed input conditions in the training data.\n', '- <b>Fairness Assessment</b> powered by [Fairlearn](https://github.com/fairlearn/fairlearn), which identifies which groups of people may be disproportionately negatively impacted by an AI system and in what ways.\n', '\n', ""- <b>Model Interpretability</b> powered by [InterpretML](https://github.com/interpretml/interpret-community), which explains blackbox models, helping users understand their model's global behavior, or the reasons behind individual predictions.\n"", '\n', ""- <b>Counterfactual Analysis</b> powered by [DiCE](https://github.com/interpretml/DiCE), which shows feature-perturbed versions of the same datapoint who would have received a different prediction outcome, e.g., Taylor's loan has been rejected by the model. But they would have received the loan if their income was higher by $10,000.\n"", '\n', '- <b>Causal Analysis</b> powered by [EconML](https://github.com/microsoft/EconML), which focuses on answering What If-style questions to apply data-driven decision-making – how would revenue be affected if a corporation pursues a new pricing strategy? Would a new medication improve a patient’s condition, all else equal?\n', '\n', '- <b>Data Balance</b> powered by [Responsible AI](https://github.com/microsoft/responsible-ai-toolbox/blob/main/docs/databalance-README.md), which helps users gain an overall understanding of their data, identify features receiving the positive outcome more than others, and visualize feature distributions.\n', '\n', 'Responsible AI dashboard is designed to achieve the following goals:\n', '\n', '- To help further accelerate engineering processes in machine learning by enabling practitioners to design customizable workflows and tailor Responsible AI dashboards that best fit with their model assessment and data-driven decision making scenarios.\n', '- To help model developers create end to end and fluid debugging experiences and navigate seamlessly through error identification and diagnosis by using interactive visualizations that identify errors, inspect the data, generate global and local explanations models, and potentially inspect problematic examples.\n', '- To help business stakeholders explore causal relationships in the data and take informed decisions in the real world.\n', '\n', 'This repository contains the Jupyter notebooks with examples to showcase how to use this widget. Get started [here](https://github.com/microsoft/responsible-ai-toolbox/blob/main/notebooks/responsibleaidashboard/getting-started.ipynb).\n', '\n', '\n', '### Installation\n', '\n', 'Use the following pip command to install the Responsible AI Toolbox.\n', '\n', 'If running in jupyter, please make sure to restart the jupyter kernel after installing.\n', '\n', '```\n', 'pip install raiwidgets\n', '```\n', '\n', '\n', '### Responsible AI dashboard Customization\n', '\n', 'The Responsible AI Toolbox’s strength lies in its customizability. It empowers users to design tailored, end-to-end model debugging and decision-making workflows that address their particular needs. Need some inspiration? Here are some examples of how Toolbox components can be put together to analyze scenarios in different ways:\n', '\n', 'Please note that model overview (including fairness analysis) and data explorer components are activated by default!\n', '\xa0\n', '| Responsible AI Dashboard Flow| Use Case  |\n', '|--|--|\n', '| Model Overview -> Error Analysis -> Data Explorer | To identify model errors and diagnose them by understanding the underlying data distribution\n', '| Model Overview -> Fairness Assessment -> Data Explorer | To identify model fairness issues and diagnose them by understanding the underlying data distribution\n', '| Model Overview -> Error Analysis -> Counterfactuals Analysis and What-If | To diagnose errors in individual instances with counterfactual analysis (minimum change to lead to a different model prediction)\n', '| Model Overview -> Data Explorer -> Data Balance | To understand the root cause of errors and fairness issues introduced via data imbalances or lack of representation of a particular data cohort\n', ' | Model Overview -> Interpretability | To diagnose model errors through understanding how the model has made its predictions\n', ' | Data Explorer -> Causal Inference | To distinguish between correlations and causations in the data or decide the best treatments to apply to see a positive outcome\n', '  | Interpretability -> Causal Inference | To learn whether the factors that model has used for decision making has any causal effect on the real-world outcome.\n', ' | Data Explorer -> Counterfactuals Analysis and What-If | To address customer questions about what they can do next time to get a different outcome from an AI.\n', '  | Data Explorer -> Data Balance | To gain an overall understanding of the data, identify features receiving the positive outcome more than others, and visualize feature distributions\n', '\n', '\n', '### Useful Links\n', '\n', '- [Take a tour of Responsible AI Dashboard](https://github.com/microsoft/responsible-ai-toolbox/blob/main/notebooks/responsibleaidashboard/tour.ipynb)\n', '- [Get started](https://github.com/microsoft/responsible-ai-toolbox/blob/main/notebooks/responsibleaidashboard/getting-started.ipynb)\n', '\n', 'Model Debugging Examples:\n', '- [Try the tool: model debugging of a census income prediction model (classification)](https://github.com/microsoft/responsible-ai-toolbox/tree/main/notebooks/responsibleaidashboard/responsibleaidashboard-census-classification-model-debugging.ipynb)\n', '- [Try the tool: model debugging of a housing price prediction model (classification)](https://github.com/microsoft/responsible-ai-toolbox/tree/main/notebooks/responsibleaidashboard/responsibleaidashboard-housing-classification-model-debugging.ipynb)\n', '- [Try the tool: model debugging of a diabetes progression prediction model (regression)](https://github.com/microsoft/responsible-ai-toolbox/tree/main/notebooks/responsibleaidashboard/responsibleaidashboard-diabetes-regression-model-debugging.ipynb)\n', '\n', ' Responsible Decision Making Examples:\n', '- [Try the tool: make decisions for house improvements](https://github.com/microsoft/responsible-ai-toolbox/tree/main/notebooks/responsibleaidashboard/responsibleaidashboard-housing-decision-making.ipynb)\n', '- [Try the tool: provide recommendations to patients using diabetes data](https://github.com/microsoft/responsible-ai-toolbox/tree/main/notebooks/responsibleaidashboard/responsibleaidashboard-diabetes-decision-making.ipynb)\n', '\n', '\n', '\n', '## Supported Models\n', '\n', 'This Responsible AI Toolbox API supports models that are trained on datasets in Python `numpy.ndarray`, `pandas.DataFrame`, `iml.datatypes.DenseData`, or `scipy.sparse.csr_matrix` format.\n', '\n', ""The explanation functions of [Interpret-Community](https://github.com/interpretml/interpret-community) accept both models and pipelines as input as long as the model or pipeline implements a `predict` or `predict_proba` function that conforms to the Scikit convention. If not compatible, you can wrap your model's prediction function into a wrapper function that transforms the output into the format that is supported (predict or predict_proba of Scikit), and pass that wrapper function to your selected interpretability techniques.\n"", '\n', 'If a pipeline script is provided, the explanation function assumes that the running pipeline script returns a prediction. The repository also supports models trained via **PyTorch**, **TensorFlow**, and **Keras** deep learning frameworks.\n', '\n', '## Other Use Cases\n', '\n', 'Tools within the Responsible AI Toolbox can also be used with AI models offered as APIs by providers such as [Azure Cognitive Services](https://azure.microsoft.com/en-us/services/cognitive-services/). To see example use cases, see the folders below:\n', '\n', '- [Cognitive Services Speech to Text Fairness testing](https://github.com/microsoft/responsible-ai-toolbox/tree/main/notebooks/cognitive-services-examples/speech-to-text)\n', '- [Cognitive Services Face Verification Fairness testing](https://github.com/microsoft/responsible-ai-toolbox/tree/main/notebooks/cognitive-services-examples/face-verification)\n', '\n', '## Maintainers\n', '\n', '- [Ke Xu](https://github.com/KeXu444)\n', '- [Roman Lutz](https://github.com/romanlutz)\n', '- [Ilya Matiach](https://github.com/imatiach-msft)\n', '- [Gaurav Gupta](https://github.com/gaugup)\n', '- [Vinutha Karanth](https://github.com/vinuthakaranth)\n', '- [Tong Yu](https://github.com/tongyu-microsoft)\n', '- [Ruby Zhu](https://github.com/RubyZ10)\n', '- [Mehrnoosh Sameki](https://github.com/mesameki)\n']"
Responsible+AI,alexandrainst/responsible-ai,alexandrainst,https://api.github.com/repos/alexandrainst/responsible-ai,57,14,6,"['https://api.github.com/users/nkasenburg', 'https://api.github.com/users/AmaliePauli', 'https://api.github.com/users/KrydenZ', 'https://api.github.com/users/kasperbaynoer', 'https://api.github.com/users/KatrineHJ', 'https://api.github.com/users/agnethe-gron']",,2023-04-04T16:30:50Z,https://raw.githubusercontent.com/alexandrainst/responsible-ai/main/README.md,"['# Responsible AI Knowledge-base\n', '\n', 'This repository is a knowledge-base of different areas of using and developing AI in a responsible way:heart:. Responsible AI includes both the field of explainable and interpretable machine learning, fairness and bias in machine learning, law regulations as well as the aspect of user experience and human centralized AI.  Hence, it is a cross-disciplinary field which includes both the field of computer science and social science. The aim is to achieve systems that are trustworthy, accountable and fair. Therefore, responsible AI should hopefully both interest researchers and practitioners, which includes both developers, system owners/buyers and users :family:.\n', '\n', 'This repo is a collection of links to **research papers, blog post, tools, tutorials, videos and books**. The references are divide into different areas as listed in the table of contents.\n', '\n', '#### Table of contents :open_file_folder:\n', '\n', '|        | | |\n', '| ------------- |:-------------:| -----:|\n', '| [Explainable AI](#explainable-ai)      | [Fairness](#fairness) | [Guidelines & principles](#guide-princip)\n', '| [People & Tech](#people-tech)  | [Policy & Regulation](#pol-reg)      | [User Experience](#ux) |\n', '\n', '<a name=""explainable-ai""></a>\n', '\n', '  ####  Contributions  :raising_hand:\n', '\n', 'We really welcome and appreciates :pray:contributions to make sure this knowledge-base stays relevant. So if you have a link or reference you think should be included then pleas create a pull request. You can also open an issue if you find it easier.\n', '\n', '\n', '\n', '#### Who is behind :construction_worker:\n', '\n', 'The Responsible AI repository is maintained by the [Alexandra Institute](https://alexandra.dk/uk) which is a Danish non-profit company with a mission to create value, growth and welfare in society. The Alexandra Institute is a member of [GTS](https://gts-net.dk/), a network of independent Danish research and technology organisations.\n', '\n', 'The initial work on this repository is conducted under a performance contract allocated to the Alexandra Insitute by the [Danish Ministry of Higher Education and Science](https://ufm.dk/en?set_language=en&cl=en). The project ran in the two years in 2019 and 2020.``\n', '\n', '\n', '\n', '# Explainable AI (XAI)\n', '## Frameworks and Github repos\n', '1. [InterpretML](https://interpret.ml/) - Open source Python framework that combines local and global explanation methods,\n', 'as well as, transparent models, like decision trees, rule based models, and GAMs (Generalized Additive Models), into\n', 'a common API and dashboard.\n', '2. [AI Explainability 360](http://aix360.mybluemix.net/) - Open source Python XAI framework devloped by IBM researchers\n', 'combining different data, local and global explanation methods. Also see there [github page](https://github.com/Trusted-AI/AIX360).\n', '3. [explainX.ai](https://github.com/explainX/explainx) - Open source Python framework that launches an\n', 'interactive dashboard for a model in a single line of code in which a model can be investigated using\n', 'different XAI methods.\n', '4. [Alibi Explain](https://github.com/SeldonIO/alibi) - Open source Pyton XAI framework combining different methods.\n', 'Main focus on counterfactual explanations and SHAP for classification tasks on tabular data or images.\n', '5. [SHAP](https://github.com/slundberg/shap) - THe open source Python framework for generating SHAP explanations. Focused\n', 'on tree based models, but contains the model agnostic KernelSHAP and an implementation for deep neural networks.\n', '6. [Lucid](https://github.com/tensorflow/lucid) - Open source Python framework to explain deep convolutional\n', 'neural networks used on image data (currently only supports Tensforflow 1). Focuses on understanding the\n', 'representations the network has learned.\n', '7. [DeepLIFT](https://github.com/kundajelab/deeplift) - Open source implementation of the DeepLIFT methods for generating\n', 'local feature attributions for deep neural networks.\n', '8. [iNNvestigate](https://github.com/albermax/innvestigate) - Github repository collecting implementations of different\n', 'feature attribution and gradient based explanation methods for deep neural networks.\n', '9. [Skope-rules](https://github.com/scikit-learn-contrib/skope-rules) - Open source Python framework for building rule\n', 'based models.\n', '10. [Yellowbrick](https://www.scikit-yb.org/en/latest/) - Open source Python framework to create different visualizations\n', 'of data and ML models.\n', '11. [Captum](https://captum.ai/) - Open source framework to explain deep learning models created with PyTorch. Includes\n', 'many known XAI algorithms for deep neural networks.\n', '12. [What-If Tool](https://pair-code.github.io/what-if-tool/) - Open source framework from Google to probe the behaviour\n', 'of a trained model.\n', '13. [AllenNLP Interpret](https://allennlp.org/interpret) - Python framework for explaining deep neural networks\n', 'for language processing developed by the Allen Institute for AI.\n', '14. [Dalex](http://dalex.drwhy.ai/) - Part of the DrWhy.AI universe of packages for interpretable and responsible ML.\n', '15. [RuleFit](https://github.com/christophM/rulefit) - Open source python implementation of an interpretable rule ensemble model.\n', '16. [SkopeRules](https://github.com/scikit-learn-contrib/skope-rules) - Open source python package for fitting a rule based model.\n', '17. [ELI5](https://eli5.readthedocs.io/en/latest/index.html) - Open source python package that implements LIME local explanations\n', '    and permutation explanations.\n', '18. [tf-explain](https://github.com/sicara/tf-explain) - Open source framework that implements interpretability methods as Tensorflow 2.x callbacks. Includes\n', 'several known XAI algorithms for deep neural networks.\n', '19. [PAIR - Saliency methods](https://github.com/PAIR-code/saliency) - Framework that collects different gradient based, saliency methods for deep learning model for Tensorflow created by the Google People+AI Research (PAIR) Initiative.\n', '20. [Quantus](https://github.com/understandable-machine-intelligence-lab/quantus) - Toolkit to evaluate XAI methods for neural networks.\n', '21. [Xplique](https://github.com/deel-ai/xplique) - Python library that gathers state of the art of XAI methods for deep neural networks (currently for Tensorflow).\n', '22. [PiML](https://github.com/SelfExplainML/PiML-Toolbox) - Python toolbox for developing interpretable models through low-code interfaces and high-code APIs.\n', '23. [VL-InterpreT](https://github.com/IntelLabs/VL-InterpreT) - Python toolbox for interactive visualizations of the attentions and hidden representations in vision-language transformers (**Note:** currently only link to the paper and live demo available, but no code)\n', '\n', '## Reading material\n', '1. [Ansvarlig AI](https://medium.com/ansvarlig-ai) - Cross-disciplinary medium blog about XAI,\n', 'fairness and responsible AI (in Danish)\n', '2. [Introducing the Model Card Toolkit](https://ai.googleblog.com/2020/07/introducing-model-card-toolkit-for.html) -\n', 'Google blogpost about the Model Card Toolkit that is a framework for reporting about a ML model.\n', '3. [Interpreting Decision Trees and Random Forests](https://engineering.pivotal.io/post/interpreting-decision-trees-and-random-forests/) -\n', 'Blog post about how to interpret and visualize tree based models.\n', '4. [Introducing PDPbox](https://towardsdatascience.com/introducing-pdpbox-2aa820afd312) - Blog post about a python\n', 'package for generating partial dependence plots.\n', '5. [Use SHAP loss values to debug/monitor your model](https://towardsdatascience.com/use-shap-loss-values-to-debug-monitor-your-model-83f7808af40f) -\n', ' Blog post about how to use SHAP explanations to debug and monitoring.\n', '6. [Be careful what you SHAP for…](https://medium.com/@pauldossantos/be-careful-what-you-shap-for-aeccabf3655c) - Blog\n', ' post about the assumption for how and when to use SHAP explanations.\n', '7. [Awesome Interpretable Machine Learning](https://github.com/lopusz/awesome-interpretable-machine-learning) - Collection\n', ' of resources (articles, conferences, frameworks, software, etc.) about interpretable ML.\n', '8. [http://heatmapping.org/](http://heatmapping.org/) - Homepage of the lab behind the LRP (layerwise propagation relevance)\n', ' method with links to tutorials and research articles.\n', '9. [Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/) - E-book by Christoph Molnar\n', 'describing and explaining different XAI methods and ways to build intepretable models or methods to interpret them, including\n', 'examples on open available datasets.\n', '10. [Can A.I. Be Taught to Explain Itself?](https://www.nytimes.com/2017/11/21/magazine/can-ai-be-taught-to-explain-itself.html) -\n', 'The New York Times Magazine article about the need of explainable models.\n', '11. [Deconstructing BERT, Part 2: Visualizing the Inner Workings of Attention](https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1) -\n', 'Blog post about how to interprete a BERT model.\n', '12. [AI Explanations Whitepaper](https://storage.googleapis.com/cloud-ai-whitepapers/AI%20Explainability%20Whitepaper.pdf) -\n', ""Google's whitepaper about Explainable AI.\n"", '13. [Robust-and-Explainable-machine-learning](https://github.com/dongyp13/Robust-and-Explainable-Machine-Learning) -\n', '    Collection of links and articles with respect to robust and explainable machine learning,\n', '    containing mostly deep learning related resources.\n', '14. [Explaining the decisions of XGBoost models using counterfactual examples](https://towardsdatascience.com/explaining-the-decisions-of-xgboost-models-using-counterfactual-examples-fd9c57c83062) - Blog post describing an algorithm of how to compute counterfactual explanations for decision tree ensemble models.\n', '15. [Interpretable K-Means: Clusters Feature Importances](https://towardsdatascience.com/interpretable-k-means-clusters-feature-importances-7e516eeb8d3c) - Blog post describing methods to compute feature importance for K-means clustering, i.e. which feature mostly contributes for a datapoint belonging to a cluster.\n', '16. [Explainable Graph Neural Networks](https://towardsdatascience.com/explainable-graph-neural-networks-cb009c2bc8ea) - Blog post that provides a brief overview of XAI methods for graph neural networks (GNNs).\n', '\n', '## Videos and presentations\n', '1. [ICML 2019 session - Robust statistics and interpretability](https://slideslive.com/38917641/robust-statistics-and-interpretability)\n', '\n', '## Courses\n', '1. [Kaggle - Machine Learning Explainability](https://www.kaggle.com/learn/machine-learning-explainability) -\n', 'Kaggle course about the basics of XAI with example notebooks and exercises.\n', '\n', '## Research articles\n', 'In this section we list research articles related to interpretable ML and explainable AI.\n', '\n', '### Definitions of interpretability\n', '1. A. Weller, ""Transparency: Motivations and Challenges"", [arXiv:1708.01870](https://arxiv.org/abs/1708.01870)\n', '[cs.CY]\n', '2. J. Chang et al., ""[Reading Tea Leaves: How Humans Interpret Topic Models](http://papers.neurips.cc/paper/3700-reading-tea-leaves-how-humans-interpret-topic-models.pdf)"",\n', 'NIPS 2009\n', '3. Z. C. Lipton, ""The Mythos of Model Interpretability"", [arXiv:1606.03490](https://arxiv.org/abs/1606.03490)\n', '[cs.LG]\n', '4. F. Doshi-Velez and B. Kim, ""Towards A Rigorous Science of Interpretable Machine Learning"",\n', '[arXiv:1702.08608](https://arxiv.org/abs/1702.08608) [stat.ML]\n', '\n', '### Review, survey and overview papers\n', '1. G. Vilone and L. Longo, ""Explainable Artificial Intelligence: a Systematic Review"",\n', '[arXiv:2006.00093](https://arxiv.org/abs/2006.00093) [cs.AI]\n', '2. U. Bhatt et al., ""[Explainable Machine Learning in Deployment](https://dl.acm.org/doi/abs/10.1145/3351095.3375624)"",\n', 'FAT*20 648-657, 2020 - Survey about how XAI is used in practice.  The key results are:\n', '    1. XAI methods are mainly used by ML engineers / designers for debugging.\n', '    2. Limitations of the methods are often unclear to those using it.\n', '    3. The goal og why XAI is used in the first place is often unclear or not well defined, which could potentially lead to using the wrong method.\n', '3. L. H. Gilpin, ""[Explaining Explanations: An Overview of Interpretability of Machine Learning](https://doi.org/10.1109/DSAA.2018.00018)"",\n', 'IEEE 5th DSAA 80-89, 2019\n', '4. S. T. Mueller,\n', '""Explanation in Human-AI Systems: A Literature Meta-Review, Synopsis of Key Ideas and Publications, and Bibliography for Explainable AI"",\n', '[arXiv:1902.01876](https://arxiv.org/abs/1902.01876) [cs.AI]\n', '5. R. Guidotti et al., ""[A Survey of Methods for Explaining Black Box Models](https://dl.acm.org/doi/abs/10.1145/3236009)"",\n', 'ACM Computing Surveys, 2018 - Overview of different interpretability methods grouping them after type of method,\n', 'model they explain and type of explanation.\n', '6. M. Du et al., ""[Techniques for interpretable machine learning](https://dl.acm.org/doi/10.1145/3359786)"",\n', 'Communications of the ACM, 2019\n', '7. I. C. Covert et al., Explaining by Removing:A Unified Framework for Model Explanation,\n', '[arXiv:2011.14878](https://arxiv.org/abs/2011.14878) [cs.LG] -\n', '(Mathematical) framework that summarizes 25 feature influence methods.\n', '8. A. Adadi and M. Berrada, ""[Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)](https://doi.org/10.1109/ACCESS.2018.2870052)"",\n', 'IEEE Access (6) 52138-52160, 2018\n', '9. A. Abdul et al.,\n', '""[Trends and Trajectories for Explainable, Accountable and Intelligible Systems: An HCI Research Agenda](https://dl.acm.org/doi/10.1145/3173574.3174156)"",\n', ""CHI'18 582 1-18, 2018\n"", '10. A. Preece, ""[Asking ‘Why’ in AI: Explainability of intelligent systems – perspectives and challenges](https://onlinelibrary.wiley.com/doi/abs/10.1002/isaf.1422)"",\n', 'Intell Sys Acc Fin Mgmt (25) 63-72, 2018\n', '11. Q. Zhang and S.-C. Zhu,\n', '    ""[Visual Interpretability for Deep Learning: a Survey](https://link.springer.com/article/10.1631/FITEE.1700808)"",\n', '    Technol. Electronic Eng. (19) 27–39, 2018\n', '12. B. Mittelstadt et al.,\n', '    ""[Explaining Explanations in AI](https://dl.acm.org/doi/10.1145/3287560.3287574)"",\n', ""    FAT*'19 279–288, 2019\n"", '13. T. Rojat et al., ""Explainable Artificial Intelligence (XAI) on TimeSeries Data: A Survey"",\n', '    [arXiv:2104.00950](https://arxiv.org/abs/2104.00950) [cs.LG] - Survey paper about XAI methods for models predicting on time series data. \n', '\n', '### Evaluation of XAI\n', 'This section contains articles that describe ways to evaluate explanations and explainable models.\n', '1. S. Mohseni et al., ""A Human-Grounded Evaluation Benchmark for Local Explanations of Machine Learning"",\n', '[arXiv:1801.05075](https://arxiv.org/abs/1801.05075) [cs.HC]\n', '2. J. Huysmans et al.,\n', '""[An empirical evaluation of the comprehensibility of decision table, tree and rule based predictive models](https://www.sciencedirect.com/science/article/abs/pii/S0167923610002368)"",\n', 'Decision Support Systems (51:1) 141-154, 2011\n', '3. F. Poursabzi-Sangdeh et al., ""Manipulating and Measuring Model Interpretability"",\n', '[arXiv:1802.07810](https://arxiv.org/abs/1802.07810) [cs.AI]\n', '4. C. J. Cai et al.,\n', '""[The Effects of Example-Based Explanations in a Machine Learning Interface](https://dl.acm.org/doi/abs/10.1145/3301275.3302289)"",\n', "" IUI'19 258-262, 2019\n"", '5. L. Sixt et al., ""When Explanations Lie: Why Many Modified BP Attributions Fail"",\n', '[arXiv:1912.09818](https://arxiv.org/abs/1912.09818) [cs.LG]\n', '6. Y. Zhang et al.,\n', '""[Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making](https://dl.acm.org/doi/abs/10.1145/3351095.3372852)"",\n', ""FAT*'20 295-305, 2020 - Analyses the effect of LIME explanation and confidence score as explanation on trust and human decision performance.\n"", '7. K. Sokol and P. Flach,\n', '""[Explainability fact sheets: a framework for systematic assessment of explainable approaches](https://dl.acm.org/doi/abs/10.1145/3351095.3372870)"",\n', ""FAT*'20 56-67, 2020 - Framework (essentially a list of questions or checklist) to evaluate and document XAI methods.\n"", 'Also includes question that are relevant to the context in which the XAI methods should be employed, i.e. changing the outcome of the assessment based on the context.\n', '8. E. S. Jo and T. Gebru,\n', '""[Lessons from archives: strategies for collecting sociocultural data in machine learning](https://dl.acm.org/doi/abs/10.1145/3351095.3372829)"",\n', ""FAT*'20 306-316, 2020 - Use archives as inspiration of how to collect, curate and annotate data.\n"", '9. J. Adebayo et al., ""Sanity Checks for Saliency Maps"", [arXiv:1810.03292](https://arxiv.org/abs/1810.03292) [cs.CV] - Comparing different saliency map XAI methods for their sensitivity to the input image and weights of the network.\n', '10. H. Kaur et al.,\n', '""[Interpreting Interpretability: Understanding Data Scientists’ Use of Interpretability Tools for Machine Learning](https://dl.acm.org/doi/fullHtml/10.1145/3313831.3376219)"",\n', ""CHI'20 1-14, 2020\n"", '11. P. Hase and M. Bansal, ""Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?"", [arXiv:2005.01831](https://arxiv.org/abs/2005.01831) [cs.CL]\n', '12. J. V. Jeyakumar et al.,\n', '    ""[How Can I Explain This to You? An Empirical Study of Deep Neural Network Explanation Methods](https://proceedings.neurips.cc/paper/2020/hash/2c29d89cc56cdb191c60db2f0bae796b-Abstract.html)"",\n', '    33rd NeurIPS, 2020 - The authors evaluate different methods for explaining deep neural networks for end-user preference. Code can be found on [github](https://github.com/nesl/Explainability-Study),\n', '    as well as, their [implementation of an example based explainer](https://github.com/nesl/ExMatchina).\n', '13. S. Jesus et al.,\n', '    ""How can I choose an explainer? An Application-grounded Evaluation of Post-hoc Explanations"", [arXiv:2101.08758](https://arxiv.org/abs/2101.08758) [cs.AI] - Evaluating XAI methods based on an application-grounded approach measuring decision time and accuracy of end-users.\n', '14. M. Nauta et al.,\n', '    ""From Anecdotal Evidence to Quantitative Evaluation Methods: A Systematic Review on Evaluating Explainable AI"", [arXiv:2201.08164](https://arxiv.org/abs/2201.08164) [cs.AI] - Lietrature survey of XAI methods and how they where evaluated in the presented paper.\n', '\n', '### Method to explain data\n', 'This section contains articles that explain datasets, for example by finding representative examples.\n', '1. B. Kim et al.,\n', '   ""[Examples are not Enough, Learn to Criticize! Criticism for Interpretability](https://papers.nips.cc/paper/2016/hash/5680522b8e2bb01943234bce7bf84534-Abstract.html)"",\n', '   NIPS, 2016 - Code can we found on [github](https://github.com/BeenKim/MMD-critic).\n', '\n', '### Explainable models\n', 'This section contains articles that describe models that are explainable or transparent by design.\n', '1. X. Zhang et al.,\n', '   ""[Axiomatic Interpretability for Multiclass Additive Models](https://dl.acm.org/doi/abs/10.1145/3292500.3330898)"",\n', ""   KDD'19 226–234, 2019\n"", '2. T. Kulesza et al.,\n', '   ""[Principles of Explanatory Debugging to Personalize Interactive Machine Learning](https://dl.acm.org/doi/10.1145/2678025.2701399)"",\n', ""   IUI'15 126–137, 2015 - Framework showing how a Naive Bayes method can be trained with user interaction and\n"", '   how to generate explanations for these kinds of models.\n', '3. M. Hind et al.,\n', '   ""[TED: Teaching AI to Explain its Decisions](https://dl.acm.org/doi/abs/10.1145/3306618.3314273)"",\n', ""   AIES'19 123–129, 2019\n"", '4. Y. Lou et al.,\n', '   ""[Accurate Intelligible Models with Pairwise Interactions](https://dl.acm.org/doi/10.1145/2487575.2487579)"",\n', ""   KDD'13 623–631, 2013\n"", '5. C. Chen et al., ""An Interpretable Model with Globally Consistent Explanations for Credit Risk"",\n', '   [arXiv:1811.12615](https://arxiv.org/abs/1811.12615) [cs.LG]\n', '6. C. Chen and C. Rudin,\n', '   ""[An Optimization Approach to Learning Falling Rule Lists](http://proceedings.mlr.press/v84/chen18a.html)"",\n', '   PMLR (84) 604-612, 2018\n', '7. F. Wang and C. Rudin,  ""Falling Rule Lists"",\n', '   [arXiv:1411.5899](https://arxiv.org/abs/1411.5899) [cs.AI]\n', '8. B. Ustun and C. Rudin, ""Supersparse Linear Integer Models for Optimized Medical Scoring Systems"",\n', '   [arXiv:1502.04269](https://arxiv.org/abs/1502.04269) [stat.ML]\n', '8. E. Angelino et al.,\n', '   ""[Learning Certifiably Optimal Rule Lists for Categorical Data](https://dl.acm.org/doi/abs/10.5555/3122009.3290419)"",\n', '   JMLR (18:234) 1-78, 2018\n', '9. H. Lakkaraju et al.,\n', '   ""[Interpretable Decision Sets: A Joint Framework for Description and Prediction](https://dl.acm.org/doi/10.1145/2939672.2939874)"",\n', ""   KDD'16 1675–1684, 2016\n"", '10. K. Shu et al.,\n', '      ""[dEFEND: Explainable Fake News Detection](https://dl.acm.org/doi/10.1145/3292500.3330935)"",\n', ""      KDD'19 395–405, 2019\n"", '11. J. Jung et al., ""Simple Rules for Complex Decisions"",\n', '    [arXiv:1702.04690](https://arxiv.org/abs/1702.04690) [stat.AP]\n', '\n', '### XAI methods to visualize / explain a model\n', 'This section contains articles that are describing methods to globally explain a model.\n', 'Typically, this is done by generating visualizations in one form or the other.\n', '1. B. Ustun et al.,\n', '   ""[Actionable Recourse in Linear Classification](https://dl.acm.org/doi/10.1145/3287560.3287566)"",\n', ""   FAT*'19 Pages 10–19, 2019 - Article describing a method to evaluate actionable variables,\n"", '   i.e. variables a person can impact to change the outcome af a model, of a linear\n', '   classification model.\n', '2. A Datta et al.,\n', '   ""[Algorithmic Transparency via Quantitative Input Influence: Theory and Experiments with Learning Systems](https://ieeexplore.ieee.org/document/7546525)"",\n', '    IEEE SP 598-617, 2016\n', '3. P.Adler et al.,\n', '   ""[Auditing black-box models for indirect influence](https://link.springer.com/article/10.1007/s10115-017-1116-3)"",\n', '   Knowl. Inf. Syst. (54) 95–122, 2018\n', '4. A. Lucic et al.,\n', '   ""[Why Does My Model Fail? Contrastive Local Explanations for Retail Forecasting](https://dl.acm.org/doi/abs/10.1145/3351095.3372824)"",\n', ""   FAT*'20 90–98, 2020 - Presents an explanation to explain failure cases of an ML/AI model.\n"", '   The explanation is presented in form of a feasible range of feature values in which the model works and a trend\n', '   for each feature. Code for the method is available on [github](https://github.com/a-lucic/mc-brp).\n', '5. J. Krause et al.,\n', '   ""[Interacting with Predictions: Visual Inspection of Black-box Machine Learning Models](https://dl.acm.org/doi/10.1145/2858036.2858529)"",\n', ""   CHI'16 5686–5697, 2016\n"", '6. B. Kim et al.,\n', '   ""[Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)](http://proceedings.mlr.press/v80/kim18d.html)"",\n', '   ICML, PMLR (80) 2668-2677, 2018 - Code for the method can be found on [github](https://github.com/tensorflow/tcav).\n', '7. A. Goldstein et al.,\n', '   ""[Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation](https://doi.org/10.1080/10618600.2014.907095)"",\n', '   Journal of Computational and Graphical Statistics (24:1) 44-65, 2015\n', '8. J. Wang et al., ""Shapley Flow: A Graph-based Approach to Interpreting Model Predictions"",\n', '   [arXiv:2010.14592](https://arxiv.org/abs/2010.14592) [cs.LG]\n', '\n', '### XAI methods that explain a model through construction of mimicking models\n', 'This section contains articles that are describing methods to explain a model by constructing an inherent\n', 'transparent model that mimics the behaviour of the black-box model.\n', '1. S. Tan et al.,  \n', '   ""[Distill-and-Compare: Auditing Black-Box Models Using Transparent Model Distillation](https://dl.acm.org/doi/abs/10.1145/3278721.3278725)"",\n', ""   AIES'18 303–310, 2018\n"", '2. L. Chu et al., ""Exact and Consistent Interpretation for Piecewise Linear Neural Networks: A Closed Form Solution"",\n', '   [arXiv:1802.06259](https://arxiv.org/abs/1802.06259) [cs.CV]\n', '3. C. Yang et al., ""Global Model Interpretation via Recursive Partitioning"",\n', '   [arXiv:1802.04253](https://arxiv.org/abs/1802.04253) [cs.LG]\n', '4. H. Lakkaraju et al., ""Interpretable & Explorable Approximations of Black Box Models"",\n', '   [arXiv:1707.01154](https://arxiv.org/abs/1707.01154) [cs.AI]\n', '5. Y. Hayashi,\n', '   ""[Synergy effects between grafting and subdivision in Re-RX with J48graft for the diagnosis of thyroid disease](https://www.sciencedirect.com/science/article/abs/pii/S095070511730285X)"",\n', '   Knowledge-Based Systems (131) 170-182, 2017\n', '6. H. F. Tan et al., ""Tree Space Prototypes: Another Look at Making Tree Ensembles Interpretable"",\n', '   [arXiv:1611.07115](https://arxiv.org/abs/1611.07115) [stat.ML]\n', '7. O. Sagi and L. Rokach, \n', '   ""[Approximating XGBoost with an interpretable decision tree](https://www.sciencedirect.com/science/article/abs/pii/S0020025521005272)"", Information Sciences (572) 522-542, 2021 \n', '\n', '### Local XAI methods\n', 'This section contains articles that describe local explanation methods, i.e. methods that generate an explanation\n', 'for a specific outcome of a model.\n', '1. M. T. Ribeiro et al.,\n', '   ""[Anchors: High-Precision Model-Agnostic Explanations](https://homes.cs.washington.edu/~marcotcr/aaai18.pdf)"",\n', '   AAAI Conference on Artificial Intelligence, 2018 -\n', '   The implementation of the method can be found on [github](https://github.com/marcotcr/anchor).\n', '2. A. Shrikumar et al.,\n', '   ""[Learning Important Features Through Propagating Activation Differences](https://dl.acm.org/doi/10.5555/3305890.3306006)"",\n', ""   ICML'17 3145–3153, 2017 - DeepLIFT method for local explanations of deep neural networks.\n"", '3. S. M. Lundberg et al., ""Explainable AI for Trees: From Local Explanations to Global Understanding"",\n', '   [arXiv:1905.04610](https://arxiv.org/abs/1905.04610) [stat.ML]\n', '4. S. M. Lundberg et al.,\n', '   ""[From local explanations to global understanding with explainable AI for trees](https://www.nature.com/articles/s42256-019-0138-9)"",\n', '   Nat. Mach. Intell. (2) 56–67, 2020\n', '5. M. T. Ribeiro et al.,\n', '   [“Why Should I Trust You?” Explaining the Predictions of Any Classifier](https://dl.acm.org/doi/10.1145/2939672.2939778),\n', ""   KDD'16 1135–1144, 2016\n"", '6. D. Slack et al., ""How Much Should I Trust You? Modeling Uncertainty of Black Box Explanations"",\n', '   [arXiv:2008.05030](https://arxiv.org/abs/2008.05030) [cs.LG]\n', '7. S. M. Lundberg and S.-I. Lee,\n', '   ""[A Unified Approach to Interpreting Model Predictions](https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html)"",\n', '   NIPS, 2017\n', '8. M. Sundararajan and A. Najmi,\n', '   ""[The Many Shapley Values for Model Explanation](http://proceedings.mlr.press/v119/sundararajan20b.html)"",\n', '   ICML (119) 9269-9278, 2020\n', '9. I. E. Kumar et al., ""Problems with Shapley-value-based explanations as feature importance measures"",\n', '   [arXiv:2002.11097](https://arxiv.org/abs/2002.11097) [cs.AI]\n', '10. P. W. Koh and P. Liang, ""Understanding Black-box Predictions via Influence Functions"",\n', '      [arXiv:1703.04730](https://arxiv.org/abs/1703.04730) [stat.ML]\n', '\n', '### Counterfactual explanations\n', 'This section contains articles that describe methods for counterfactual explanations.\n', '1. S. Sharma et al.,\n', '   ""[CERTIFAI: A Common Framework to Provide Explanations and Analyse the Fairness and Robustness of Black-box Models](https://dl.acm.org/doi/10.1145/3375627.3375812)"",\n', ""   AIES'20 166–172, 2020\n"", '2. C. Russell, ""[Efficient Search for Diverse Coherent Explanations](https://dl.acm.org/doi/10.1145/3287560.3287569)"",\n', ""   FAT*'19  20–28, 2019\n"", '3. R. K. Mothilal et al.,\n', '   ""[Explaining Machine Learning Classifiers through Diverse Counterfactual Explanations](https://dl.acm.org/doi/abs/10.1145/3351095.3372850)"",\n', ""   FAT*'20 607–617, 2020 - Code for the method is available on [github](https://github.com/interpretml/DiCE).\n"", '4. S. Barocas et al.,\n', '   ""[The Hidden Assumptions Behind Counterfactual Explanations and Principal Reasons](https://dl.acm.org/doi/abs/10.1145/3351095.3372830)"",\n', ""   FAT*'20  80–89, 2020 - Raises some questions with respect to the use of counterfactual examples as a form of explanation:\n"", '   * Are the changes proposed by the counterfactual example feasible (actionable) for a person to change their outcome?\n', '   * If the changes are performed, what do they affect otherwise, i.e. they might not be favorable in other contexts?\n', '   * Changing one factor might inherently change another factor that actually negatively affects the outcome\n', '     (counterfactual examples can not describe complex relationships between variables)?\n', '\n', '### XAI and user interaction\n', 'This section contains research articles that are looking at the interaction of users with explanations or\n', 'interpretable models.\n', '1. B. Y. Lim and A. K. Dey,\n', '   ""[Assessing Demand for Intelligibility in Context-Aware Applications](https://dl.acm.org/doi/10.1145/1620545.1620576)"",\n', ""   UbiComp'09 195–204, 2009\n"", '2. D. Wang et al.,\n', '   ""[Designing Theory-Driven User-Centric Explainable AI](https://dl.acm.org/doi/10.1145/3290605.3300831)"",\n', ""   CHI'19 (601)  1–15, 2019\n"", '3. M. Narayanan et al.,\n', '   ""How do Humans Understand Explanations from Machine Learning Systems? An Evaluation of the Human-Interpretability of Explanation"",\n', '   [arXiv:1802.00682](https://arxiv.org/abs/1802.00682) [cs.AI]\n', '4. U. Bhatt et al., ""Machine Learning Explainability for External Stakeholders"",\n', '   [arXiv:2007.05408](https://arxiv.org/abs/2007.05408) [cs.CY]\n', '5. V. Lai and C. Tan,\n', '   ""[On Human Predictions with Explanations and Predictions of Machine Learning Models: A Case Study on Deception Detection](https://dl.acm.org/doi/abs/10.1145/3287560.3287590)"",\n', ""   FAT*'19 29–38, 2019\n"", '6. C. Molnar et al., ""Pitfalls to Avoid when Interpreting Machine Learning Models"",\n', '   [arXiv:2007.04131](https://arxiv.org/abs/2007.04131) [stat.ML]\n', '7. A. Preece et al., ""Stakeholders in Explainable AI"",\n', '   [arXiv:1810.00184](https://arxiv.org/abs/1810.00184) [cs.AI]\n', '8. M. Katell et al.,\n', '   ""[Toward Situated Interventions for Algorithmic Equity: Lessons from the Field](https://dl.acm.org/doi/abs/10.1145/3351095.3372874)"",\n', ""   FAT*'20 45–55, 2020 - Presenting a framework for designing ML/AI solutions based on participatory design and co-design methods,\n"", '   which especially focuses on solutions that effect communities, i.e. models employed by municipalities. The framework is applied\n', '   to an example case in which a surveillance tool with an automatic decision system is designed.\n', '9. M. Eiband et al.,\n', '   ""[Bringing Transparency Design into Practice](https://dl.acm.org/doi/10.1145/3172944.3172961)"",\n', ""   IUI'18 211–223, 2018\n"", '\n', '### XAI used in practice\n', 'This section contains research articles where XAI was used as part of an application or used for validation on a system\n', 'deployed in practice.\n', '1. S. Coppers et al.,\n', '   ""[Intellingo: An Intelligible Translation Environment](https://dl.acm.org/doi/10.1145/3173574.3174098)"",\n', ""   CHI'18 (524) 1–13, 2018\n"", '2. H. Tang and P. Eratuuli,\n', '   ""[Package and Classify Wireless Product Features to Their Sales Items and Categories Automatically](https://link.springer.com/chapter/10.1007/978-3-030-29726-8_20)"",\n', '   Machine Learning and Knowledge Extraction. CD-MAKE 2019. LNCS (11713), 2019\n', '\n', '### XAI for deep neural networks\n', 'This section focuses on explainability with respect to deep neural networks (DNNs). This can be methods to explain\n', 'DNNs or methods to build DNNs that can explain themselves.\n', '1. Y. Goyal et al.,\n', '   ""[Counterfactual Visual Explanations](http://pr"
Responsible+AI,romanlutz/ResponsibleAI,romanlutz,https://api.github.com/repos/romanlutz/ResponsibleAI,49,9,3,"['https://api.github.com/users/romanlutz', 'https://api.github.com/users/riedgar-ms', 'https://api.github.com/users/benbyford']",,2023-04-08T18:10:32Z,https://raw.githubusercontent.com/romanlutz/ResponsibleAI/main/README.md,"['The following collection is meant to serve as a reference for engineers, data scientists, and others making decisions about building technological solutions for real-world problems. Hopefully, this will help us avoid repeating mistakes of the past by informing the design of new systems or the decision not to build a technological solution at all.\n', '\n', 'This is a living document, so please send suggestions for additions through ""Issues"" or feel free to send pull requests. If you find any other problems with links or the articles themselves, please also open an ""Issue"".\n', '\n', '# Fairness\n', '\n', '## Lending & Credit approval\n', '\n', '- [Gender Bias Complaints against Apple Card Signal a Dark Side to Fintech](https://hbswk.hbs.edu/item/gender-bias-complaints-against-apple-card-signal-a-dark-side-to-fintech)\n', '- [Exploring Racial Discrimination in Mortgage Lending: A Call for Greater Transparency](https://listwithclever.com/real-estate-blog/racial-discrimination-in-mortgage-lending/)\n', '- [DFS Issues Guidance to Life Insurers on Use of “External Data” in Underwriting Decisions](https://www.jdsupra.com/legalnews/dfs-issues-guidance-to-life-insurers-on-45997/)\n', '\n', '## Hiring\n', '\n', '- [Amazon scraps secret AI recruiting tool that showed bias against women](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G)\n', '- [Automated Employment Discrimination](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3437631)\n', '- [Help wanted: an examination of hiring algorithms, equity, and bias](https://apo.org.au/node/210071)\n', '- [All the Ways Hiring Algorithms Can Introduce Bias](https://hbr.org/2019/05/all-the-ways-hiring-algorithms-can-introduce-bias)\n', '- [Mitigating Bias in Algorithmic Hiring: Evaluating Claims and Practices](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3408010)\n', '- [Help Wanted - An Examination of Hiring Algorithms, Equity, and Bias](https://www.upturn.org/static/reports/2018/hiring-algorithms/files/Upturn%20--%20Help%20Wanted%20-%20An%20Exploration%20of%20Hiring%20Algorithms,%20Equity%20and%20Bias.pdf)\n', '- [Wanted: The ‘perfect babysitter.’ Must pass AI scan for respect and attitude.](https://www.washingtonpost.com/technology/2018/11/16/wanted-perfect-babysitter-must-pass-ai-scan-respect-attitude/)\n', '- [Job Screening Service Halts Facial Analysis of Applicants](https://www.wired.com/story/job-screening-service-halts-facial-analysis-applicants/)\n', '\n', '## Employee evaluation\n', '\n', '- [Houston Schools Must Face Teacher Evaluation Lawsuit](https://www.courthousenews.com/houston-schools-must-face-teacher-evaluation-lawsuit/)\n', '- [How Amazon automatically tracks and fires warehouse workers for ‘productivity’](https://www.theverge.com/2019/4/25/18516004/amazon-warehouse-fulfillment-centers-productivity-firing-terminations)\n', ""- [Court Rules Deliveroo Used 'Discriminatory' Algorithm](https://www.vice.com/en/article/7k9e4e/court-rules-deliveroo-used-discriminatory-algorithm)\n"", '\n', '## Pre-trial risk assessment and criminal sentencing\n', '\n', '- [Machine Bias](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)\n', '- [How We Analyzed the COMPAS Recidivism Algorithm](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm)\n', '- [GitHub repository for COMPAS analysis](https://github.com/propublica/compas-analysis)\n', '- [Can you make AI fairer than a judge? Play our courtroom algorithm game](https://www.technologyreview.com/s/613508/ai-fairer-than-judge-criminal-risk-assessment-algorithm/)\n', '    \n', '## Predictive Policing & Other Law Enforcement Use Cases\n', '\n', '- [Dirty Data, Bad Predictions: How Civil Rights Violations Impact Police Data, Predictive Policing Systems, and Justice](https://www.nyulawreview.org/online-features/dirty-data-bad-predictions-how-civil-rights-violations-impact-police-data-predictive-policing-systems-and-justice/)\n', '- [Amazon’s Face Recognition Falsely Matched 28 Members of Congress With Mugshots](https://www.aclu.org/blog/privacy-technology/surveillance-technologies/amazons-face-recognition-falsely-matched-28)\n', '- [The Perpetual Line-Up - Unregulated police face recognition in America](https://www.perpetuallineup.org/)\n', '- [Stuck in a Pattern: Early evidence on ""predictive policing"" and civil rights](https://www.upturn.org/reports/2016/stuck-in-a-pattern/)\n', '- [Crime-prediction tool PredPol amplifies racially biased policing, study shows](https://www.mic.com/articles/156286/crime-prediction-tool-pred-pol-only-amplifies-racially-biased-policing-study-shows#.DZeqQ4LYs)\n', '- [Criminal machine learning](https://callingbullshit.org/case_studies/case_study_criminal_machine_learning.html)\n', '- [The Liar’s Walk - Detecting Deception with Gait and Gesture](http://gamma.cs.unc.edu/GAIT/files/Deception_LSTM.pdf)\n', '- [Federal study confirms racial bias of many facial-recognition systems, casts doubt on their expanding use](https://www.washingtonpost.com/technology/2019/12/19/federal-study-confirms-racial-bias-many-facial-recognition-systems-casts-doubt-their-expanding-use/)\n', '- [Return of physiognomy? Facial recognition study says it can identify criminals from looks alone](https://www.rt.com/news/368307-facial-recognition-criminal-china/)\n', '- [Live facial recognition is tracking kids suspected of being criminals](https://www.technologyreview.com/2020/10/09/1009992/live-facial-recognition-is-tracking-kids-suspected-of-crime/)\n', '\n', '## Admissions\n', '\n', '- [British Medical Journal: A blot on the profession](https://www.bmj.com/content/296/6623/657)\n', '\n', '## School Choice\n', '\n', '- [Custom Software Helps Cities Manage School Choice](https://www.edweek.org/ew/articles/2013/12/04/13algorithm_ep.h33.html)\n', '\n', '## Speech Detection\n', '\n', '- [Oh dear... AI models used to flag hate speech online are, er, racist against black people](https://www.theregister.co.uk/2019/10/11/ai_black_people/)\n', '- [The Risk of Racial Bias in Hate Speech Detection](https://homes.cs.washington.edu/~msap/pdfs/sap2019risk.pdf)\n', '- [Toxicity and Tone Are Not The Same Thing: analyzing the new Google API on toxicity, PerspectiveAPI.](https://medium.com/@carolinesinders/toxicity-and-tone-are-not-the-same-thing-analyzing-the-new-google-api-on-toxicity-perspectiveapi-14abe4e728b3)\n', '- [Voice Is the Next Big Platform, Unless You Have an Accent](https://www.wired.com/2017/03/voice-is-the-next-big-platform-unless-you-have-an-accent/)\n', '- [Google’s speech recognition has a gender bias](https://makingnoiseandhearingthings.com/2016/07/12/googles-speech-recognition-has-a-gender-bias/)\n', '- [Fair Speech report by Stanford Computational Policy Lab](https://fairspeech.stanford.edu/), also covered in [Speech recognition algorithms may also have racial bias](https://arstechnica.com/science/2020/03/speech-recognition-algorithms-may-also-have-racial-bias/)\n', '- [Automated moderation tool from Google rates People of Color and gays as “toxic”](https://algorithmwatch.org/en/story/automated-moderation-perspective-bias/)\n', '- [Someone made an AI that predicted gender from email addresses, usernames. It went about as well as expected](https://www.theregister.com/2020/07/30/genderify_shuts_down/)\n', '\n', '## Image Labelling & Face Recognition\n', '\n', ""- [Google Photos identified two black people as 'gorillas'](https://mashable.com/2015/07/01/google-photos-black-people-gorillas/)\n"", '- [When It Comes to Gorillas, Google Photos Remains Blind](https://www.wired.com/story/when-it-comes-to-gorillas-google-photos-remains-blind/)\n', '- [The viral selfie app ImageNet Roulette seemed fun – until it called me a racist slur](https://www.theguardian.com/technology/2019/sep/17/imagenet-roulette-asian-racist-slur-selfie)\n', ""- [Google Is Investigating Why it Trained Facial Recognition on 'Dark Skinned' Homeless People](https://www.vice.com/en_uk/article/43k7yd/google-is-investigating-why-it-trained-facial-recognition-on-dark-skinned-homeless-people)\n"", '- [Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification](http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf)\n', '- [Machines Taught by Photos Learn a Sexist View of Women](https://www.wired.com/story/machines-taught-by-photos-learn-a-sexist-view-of-women/)\n', '- [Tenants sounded the alarm on facial recognition in their buildings. Lawmakers are listening.](https://www.msn.com/en-us/news/politics/tenants-sounded-the-alarm-on-facial-recognition-in-their-buildings-lawmakers-are-listening/ar-BBYnaqB)\n', '- [Google apologizes after its Vision AI produced racist results](https://algorithmwatch.org/en/story/google-vision-racism/)\n', ""- [When AI Sees a Man, It Thinks 'Official.' A Woman? 'Smile'](https://www.wired.com/story/ai-sees-man-thinks-official-woman-smile/)\n"", '\n', '## Public Benefits & Health\n', '\n', '- [A health care algorithm affecting millions is biased against black patients](https://www.theverge.com/2019/10/24/20929337/care-algorithm-study-race-bias-health)\n', '- [What happens when an algorithm cuts your health care](https://www.theverge.com/2018/3/21/17144260/healthcare-medicaid-algorithm-arkansas-cerebral-palsy)\n', '- [China Knows How to Take Away Your Health Insurance](https://www.bloomberg.com/opinion/articles/2019-06-14/china-knows-how-to-take-away-your-health-insurance)\n', '- [Foretelling the Future: A Critical Perspective on the Use of Predictive Analytics in Child Welfare](http://kirwaninstitute.osu.edu/wp-content/uploads/2017/05/ki-predictive-analytics.pdf)\n', '- [There’s no quick fix to find racial bias in health care algorithms](https://www.theverge.com/2019/12/4/20995178/racial-bias-health-care-algorithms-cory-booker-senator-wyden)\n', '- [Health algorithms discriminate against Black patients, also in Switzerland](https://algorithmwatch.ch/en/racial-health-bias/)\n', '\n', '## Ads\n', '\n', '- [Discrimination in Online Ad Delivery](https://arxiv.org/abs/1301.6822)\n', '- [Probing the Dark Side of Google’s Ad-Targeting System](https://www.technologyreview.com/s/539021/probing-the-dark-side-of-googles-ad-targeting-system/)\n', '- [Facebook Engages in Housing Discrimination With Its Ad Practices, U.S. Says](https://www.nytimes.com/2019/03/28/us/politics/facebook-housing-discrimination.html)\n', '- [Facebook Job Ads Raise Concerns About Age Discrimination](https://www.outtengolden.com/facebook-job-ads-raise-concerns-about-age-discrimination-nyt)\n', '- [Facebook Ads Can Still Discriminate Against Women and Older Workers, Despite a Civil Rights Settlement](https://www.propublica.org/article/facebook-ads-can-still-discriminate-against-women-and-older-workers-despite-a-civil-rights-settlement)\n', '- [Women less likely to be shown ads for high-paid jobs on Google, study shows](https://www.theguardian.com/technology/2015/jul/08/women-less-likely-ads-high-paid-jobs-google-study)\n', '- [Algorithms That “Don’t See Color”: Comparing Biases in Lookalike and Special Ad Audiences](https://sapiezynski.com/papers/sapiezynski2019algorithms.pdf)\n', '- [Facebook is letting job advertisers target only men](https://www.propublica.org/article/facebook-is-letting-job-advertisers-target-only-men)\n', '- [Facebook (Still) Letting Housing Advertisers Exclude Users by Race](https://www.propublica.org/article/facebook-advertising-discrimination-housing-race-sex-national-origin)\n', '\n', '## Search\n', '\n', '- [Algorithms of Oppression: How Search Engines reinforce racism](http://algorithmsofoppression.com/)\n', '- [Bias already exists in search engine results, and it’s only going to get worse](https://www.technologyreview.com/s/610275/meet-the-woman-who-searches-out-search-engines-bias-against-women-and-minorities/)\n', '- [Truth in pictures: What Google image searches tell us about inequality at work](https://www.diversityemployers.com/blog/2017/05/truth-in-pictures-what-google-image-searches-tell-us-about-inequality-at-work/)\n', '\n', '## Translations\n', '\n', '- [Google Translate might have a gender problem](https://mashable.com/2017/11/30/google-translate-sexism/)\n', '\n', '## Jury Selection\n', '\n', '- [The Big Data Jury](https://scholarship.law.nd.edu/ndlr/vol91/iss3/2/)\n', '\n', '## Dating\n', '\n', '- [Coffee Meets Bagel: The Online Dating Site That Helps You Weed Out The Creeps](https://www.laweekly.com/coffee-meets-bagel-the-online-dating-site-that-helps-you-weed-out-the-creeps/)\n', '- [The Biases we feed to Tinder algorithms](https://www.diggitmagazine.com/articles/biases-we-feed-tinder-algorithms)\n', '- [Redesign dating apps to lessen racial bias, study recommends](http://news.cornell.edu/stories/2018/09/redesign-dating-apps-lessen-racial-bias-study-recommends)\n', '\n', '## Word Embeddings\n', '\n', 'Word Embeddings may affect many of the categories above through applications that use them.\n', '- [Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings](https://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf)\n', '\n', '## Gerrymandering\n', '\n', '- [When drawing a line is hard](https://medium.com/equal-future/when-drawing-a-line-is-hard-8d92d30c9044)\n', '\n', '## Recommender systems\n', '\n', '- [Why is TikTok creating filter bubbles based on your race?](https://www.wired.co.uk/article/tiktok-filter-bubbles)\n', '\n', '## Picking areas for improved service\n', '\n', '- [Amazon Doesn’t Consider the Race of Its Customers. Should It?](https://www.bloomberg.com/graphics/2016-amazon-same-day/)\n', '\n', '# Safety\n', '\n', '## Self-driving cars\n', '\n', '- [Remember the Uber self-driving car that killed a woman crossing the street? The AI had no clue about jaywalkers](https://www.theregister.co.uk/2019/11/06/uber_self_driving_car_death/)\n', '- [Franken-algorithms: The Deadly Consequences of Unpredictable Code](https://getpocket.com/explore/item/franken-algorithms-the-deadly-consequences-of-unpredictable-code)\n', '\n', '## Weaponized AI\n', '\n', '- [Google employee protest: Now Google backs off Pentagon drone AI project](https://www.zdnet.com/article/google-employee-protests-now-google-backs-off-pentagon-drone-ai-project/)\n', '- [Google Wants to Do Business With the Military—Many of Its Employees Don’t](https://www.bloomberg.com/features/2019-google-military-contract-dilemma/)\n', '\n', '## Health\n', '\n', '- Model interpretability in Medicine\n', '  - [Intelligible Models for HealthCare: Predicting Pneumonia Risk and Hospital 30-day Readmission](http://people.dbmi.columbia.edu/noemie/papers/15kdd.pdf) shows importance of model interpretability for such critical decisions.\n', ""  - [Rich Caruana--Friends Don't Let Friends Release Black Box Models in Medicine](https://www.youtube.com/watch?v=iyGh46NA8tk)\n"", '- [IBM pitched its Watson supercomputer as a revolution in cancer care. It’s nowhere close](https://www.statnews.com/2017/09/05/watson-ibm-cancer/)\n', '- [International evaluation of an AI system for breast cancer screening](https://deepmind.com/research/publications/International-evaluation-of-an-artificial-intelligence-system-to-identify-breast-cancer-in-screening-mammography) - [This thread examines the issues with the problem setting.](https://twitter.com/VPrasadMDMPH/status/1212840987363442689?s=20)\n', '\n', '# Privacy\n', '\n', '## Machine Learning-based privacy attacks\n', '\n', '- [Privacy Attacks on Machine Learning Models](https://www.infoq.com/articles/privacy-attacks-machine-learning-models/)\n', '\n', '## Lending\n', '\n', '- [The new lending game, post-demonetisation](https://tech.economictimes.indiatimes.com/news/technology/the-new-lending-game-post-demonetisation/56367457)\n', '- [Perpetual Debt in the Silicon Savannah](http://bostonreview.net/class-inequality-global-justice/kevin-p-donovan-emma-park-perpetual-debt-silicon-savannah)\n', '\n', '## Work\n', '\n', '- [Woman fired after disabling work app that tracked her movements 24/7](https://www.theverge.com/2015/5/13/8597081/worker-gps-fired-myrna-arias-xora)\n', '- [Limitless Worker Surveillance](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2746211)\n', '\n', '## Prison tech\n', '\n', '- [Prison tech company is questioned for retaining ‘voice prints’ of people presumed innocent](https://theappeal.org/jails-across-the-u-s-are-extracting-the-voice-prints-of-people-presumed-innocent/)\n', '\n', '## Location data\n', '\n', '- [Twelve Million Phones, One Dataset, Zero Privacy](https://www.nytimes.com/interactive/2019/12/19/opinion/location-tracking-cell-phone.html) shines a light on data privacy (or the lack thereof). That same data may be used to for ML as well.\n', '- [Tenants sounded the alarm on facial recognition in their buildings. Lawmakers are listening.](https://www.msn.com/en-us/news/politics/tenants-sounded-the-alarm-on-facial-recognition-in-their-buildings-lawmakers-are-listening/ar-BBYnaqB)\n', '- [Face for sale: Leaks and lawsuits blight Russia facial recognition](https://www.reuters.com/article/us-russia-privacy-lawsuit-feature-trfn/face-for-sale-leaks-and-lawsuits-blight-russia-facial-recognition-idUSKBN27P10U)\n', '\n', '## Social media & dating\n', '\n', '- [OkCupid Study Reveals the Perils of Big-Data Science](https://www.wired.com/2016/05/okcupid-study-reveals-perils-big-data-science/)\n', '- [“We Are the Product”: Public Reactions to Online Data Sharing and Privacy Controversies in the Media](https://cmci.colorado.edu/~cafi5706/CHI2018_FieslerHallinan.pdf)\n', '\n', '## Basic anonymization as an insufficient measure\n', '\n', '- [“Anonymized” data really isn’t—and here’s why not](https://arstechnica.com/tech-policy/2009/09/your-secrets-live-online-in-databases-of-ruin/)\n', '\n', '## Health\n', '\n', '- [Health Insurers Are Vacuuming Up Details About You — And It Could Raise Your Rates](https://www.npr.org/sections/health-shots/2018/07/17/629441555/health-insurers-are-vacuuming-up-details-about-you-and-it-could-raise-your-rates)\n', '- [How Your Medical Data Fuels a Hidden Multi-Billion Dollar Industry](https://time.com/4588104/medical-data-industry/)\n', ""- [23andMe's Pharma Deals Have Been the Plan All Along](https://www.wired.com/story/23andme-glaxosmithkline-pharma-deal/)\n"", '- [If You Want Life Insurance, Think Twice Before Getting A Genetic Test](https://www.fastcompany.com/3055710/if-you-want-life-insurance-think-twice-before-getting-genetic-testing)\n', '- [Medical Start-up Invited Millions Of Patients To Write Reviews They May Not Realize Are Public. Some Are Explicit.](https://www.forbes.com/sites/kashmirhill/2013/10/21/practice-fusion-patient-privacy-explicit-reviews/#2918de354ae3)\n', '- [Help Desk: Can your medical records become marketing? We investigate a reader’s suspicious ‘patient portal.’](https://www.washingtonpost.com/technology/2019/10/22/help-desk-can-your-medical-records-become-marketing-we-investigate-readers-suspicious-patient-portal/)\n', '- [Is your pregnancy app sharing your intimate data with your boss?](https://www.washingtonpost.com/technology/2019/04/10/tracking-your-pregnancy-an-app-may-be-more-public-than-you-think/)\n', '- [Data Crisis: Who Owns Your Medical Records?](https://www.sandiegomagazine.com/San-Diego-Magazine/October-2016/Top-Doctors-2016-Innovation-in-Health-and-Medicine/Data-Crisis-Who-Owns-Your-Medical-Records/)\n', ""- [This Bluetooth Tampon Is the Smartest Thing You Can Put In Your Vagina](https://gizmodo.com/this-bluetooth-tampon-is-the-smartest-thing-you-can-put-1777044090) didn't mention the privacy concerns of such a device. [This Twitter comment adds the necessary comment.](https://twitter.com/DrRanjanaDas/status/1213940445245509671?s=20) \n"", '\n', '## Face Recognition\n', '\n', '- [Clearview AI: We Are ‘Working to Acquire All U.S. Mugshots’ From Past 15 Years](https://onezero.medium.com/clearview-ai-we-are-working-to-acquire-all-u-s-mugshots-from-past-15-years-645d92319f33)\n', '- [Face for sale: Leaks and lawsuits blight Russia facial recognition](https://www.reuters.com/article/us-russia-privacy-lawsuit-feature-trfn/face-for-sale-leaks-and-lawsuits-blight-russia-facial-recognition-idUSKBN27P10U)\n', '\n', '## Supply of goods\n', '\n', '- [Grocers Stopped Stockpiling Food. Then Came Coronavirus.](https://www.wsj.com/articles/grocers-stopped-stockpiling-food-then-came-coronavirus-11584982605)\n', '\n', '\n', '# Anti-Money Laundering\n', '\n', '- [Trusting Machine Learning in Anti-Money Laundering: A Risk-Based Approach](http://www.caspian.co.uk/rba/RBA.pdf)\n', '\n', '\n', '# General resources about Responsible AI\n', '\n', 'Many of the books and articles in this area cover a wide range of topics. Below is a list of a few of them, sorted alphabetically by title:\n', '\n', '- [A Hippocratic Oath for artificial intelligence practitioners](https://techcrunch.com/2018/03/14/a-hippocratic-oath-for-artificial-intelligence-practitioners/) by [Oren Etzioni](https://allenai.org/team/orene/)\n', '- [Algorithms, Correcting Biases](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3300171) by [Cass Sunstein](https://hls.harvard.edu/faculty/directory/10871/Sunstein)\n', '- [Algorithms of Oppression - How Search Engines Reinforce Racism](http://algorithmsofoppression.com/) by [Safiya Umoja Noble](https://safiyaunoble.com/)\n', '- [Artificial Unintelligence - How Computers Misunderstand the World](https://mitpress.mit.edu/books/artificial-unintelligence) by [Meredith Broussard](https://merbroussard.github.io/)\n', '- [Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor](https://us.macmillan.com/books/9781250074317) by [Virginia Eubanks](https://virginia-eubanks.com/)\n', ""- [Big Data's Disparate Impact](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2477899) by [Solon Barocas](http://solon.barocas.org/) and [Andrew D. Selbst](https://andrewselbst.com/)\n"", '- [Datasheets for Datasets](https://arxiv.org/abs/1803.09010) by [Timnit Gebru](http://ai.stanford.edu/~tgebru/) et al.\n', '- [Design Justice](https://design-justice.pubpub.org/) by [Sasha Costanza-Chock](https://www.schock.cc/)\n', '- [Fairness and Abstraction in Sociotechnical Systems](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3265913) by [Andrew D. Selbst](https://andrewselbst.com/), [danah boyd](https://www.danah.org/), [Sorelle Friedler](http://sorelle.friedler.net/), [Suresh Venkatasubramanian](http://www.cs.utah.edu/~suresh/), [Janet Vertesi](https://janet.vertesi.com/)\n', '- [Fairness and machine learning - Limitations and Opportunities](https://fairmlbook.org/) by [Solon Barocas](http://solon.barocas.org/), [Moritz Hardt](https://mrtz.org/), [Arvind Narayanan](http://randomwalker.info/)\n', ""- [How I'm fighting bias in algorithms](https://www.ted.com/talks/joy_buolamwini_how_i_m_fighting_bias_in_algorithms?language=en) by [Joy Buolamwini](https://www.poetofcode.com/)\n"", '- [Race after Technology](https://www.ruhabenjamin.com/race-after-technology) by [Ruha Benjamin](https://www.ruhabenjamin.com)\n', '- [Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/) by [Christoph Molnar](https://christophm.github.io/)\n', '- [Tech Ethics Curriculum](https://docs.google.com/spreadsheets/d/1jWIrA8jHz5fYAW4h9CkUD8gKS5V98PDJDymRf8d9vKI/edit#gid=1174187227) by [Casey Fiesler](https://caseyfiesler.com/)\n', '- [The Measure and Mismeasure of Fairness: A Critical Review of Machine Learning](https://5harad.com/papers/fair-ml.pdf) by [Sam Corbett-Davis](https://samcorbettdavies.com/) and [Sharad Goel](https://5harad.com/)\n', ""- [Weapons of Math Destruction](https://weaponsofmathdestructionbook.com/) by [Cathy O'Neil](https://mathbabe.org/)\n""]"
Responsible+AI,microsoft/robustlearn,microsoft,https://api.github.com/repos/microsoft/robustlearn,168,11,5,"['https://api.github.com/users/jindongwang', 'https://api.github.com/users/dependabot%5Bbot%5D', 'https://api.github.com/users/qianlanwyd', 'https://api.github.com/users/microsoftopensource', 'https://api.github.com/users/XixuHu']",Python,2023-04-09T19:14:55Z,https://raw.githubusercontent.com/microsoft/robustlearn/main/README.md,"['# robustlearn\n', '\n', 'Latest research in robust machine learning, including adversarial/backdoor attack and defense, out-of-distribution (OOD) generalization, and safe transfer learning.\n', '\n', 'Hosted projects:\n', '- **Diversify** (ICLR 2023, #OOD):\n', '  - [Code](./diversify/) | [Out-of-distribution Representation Learning for Time Series Classification](https://arxiv.org/abs/2209.07027)\n', '- **MARC** (ACML 2022, #Long-tail): \n', '  - [Code](./marc/) | [Margin Calibration for Long-Tailed Visual Recognition](https://arxiv.org/abs/2112.07225)\n', '- **ChatGPT robustness** (arXiv 2023, #OOD #Adversarial): \n', '  - [Code](./chatgpt-robust/) | [On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective](https://arxiv.org/abs/2302.12095)\n', '- Stay tuned for more upcoming projects!\n', '\n', 'You can clone or download this repo. Then, go to the project folder that you are interested to run and develop your research.\n', '\n', '## Contributing\n', '\n', 'This project welcomes contributions and suggestions.  Most contributions require you to agree to a\n', 'Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\n', 'the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n', '\n', 'When you submit a pull request, a CLA bot will automatically determine whether you need to provide\n', 'a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\n', 'provided by the bot. You will only need to do this once across all repos using our CLA.\n', '\n', 'This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\n', 'For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\n', 'contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n', '\n', '## Trademarks\n', '\n', 'This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \n', 'trademarks or logos is subject to and must follow \n', ""[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\n"", 'Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\n', ""Any use of third-party trademarks or logos are subject to those third-party's policies.\n""]"
Responsible+AI,Azure/Azureml-ResponsibleAI-Preview,Azure,https://api.github.com/repos/Azure/Azureml-ResponsibleAI-Preview,17,6,3,"['https://api.github.com/users/minthigpen', 'https://api.github.com/users/gaugup', 'https://api.github.com/users/microsoftopensource']",Python,2022-11-22T19:35:35Z,https://raw.githubusercontent.com/Azure/Azureml-ResponsibleAI-Preview/main/README.md,"['# [Deprecated] Azure Machine Learning Responsible AI Toolbox - Private Preview\n', '\n', '❗ **DO NOT USE THESE DOCS:* This repo has been deprecated, for the updated private preview docs please see here: https://github.com/Azure/RAI-vNext-Preview\n', '\n', 'Welcome to the private preview for the new Responsible AI toolbox in Azure Machine Learning (AzureML) SDK and studio. The following is a guide for you to onboard to the new capabilities. For questions, please contact mithigpe@microsoft.com.\n', '\n', '## What is this new feature?\n', '\n', 'AzureML currently supports both [model explanations](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-interpretability-aml) and [model fairness](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-fairness-aml) in public preview. As we expand our offerings under Responsible AI tools for AzureML users, this new feature brings pre-existing features and brand new offerings under one-stop-shop SDK package and studio UI dashboard:\n', '- Error Analysis (new): view and understand the error distributions of your model over your dataset via a decision tree map or heat map visualization.\n', '- Data Explorer: explore your dataset by feature sets and other metrics such as predicted Y or true Y\n', '- Model Statistics: explore the distribution of your model outcomes and performance metrics\n', '- Interpretability: view the aggregate and individual feature importances across your model and dataset\n', ""- Counterfactual Example What-If's (new): create automatically generated diverse sets of counterfactual examples for each datapoint that is minimally perturbed in order to switch its predicted class or output. Also create your own counterfactual datapoint by perturbing feature values manually to observe the new outcome of your model prediction.\n"", '- Causal Analysis (new): view the aggregate and individual causal effects of *treatment features* (features which you are interested in controlling to affect the outcome) on the outcome in order to make informed real-life business decisions. See recommended treatment policies for segmentations of your population for features in your dataset to see its effect on your real-life outcome. \n', '\n', 'This new feature offers users a new powerful and robust toolkit for understanding your model and data in order to develop your machine learning models responsibly, now all in one place and integrated with your AzureML workspace.\n', '\n', '❗ **Please note:** This initial version of the Responsible AI toolbox currently does not support the integration of fairness metrics. For fairness metrics, please refer to our existing offering [here.](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-fairness-aml)\n', '\n', '## Supported scenarios, models and datasets\n', '\n', ""`azureml-responsibleai` supports computation of Responsible AI insights for `scikit-learn` models that are trained on `pandas.DataFrame`. The `azureml-responsibleai` accept both models and pipelines as input as long as the model or pipeline implements a `predict` or `predict_proba` function that conforms to the `scikit-learn` convention. If not compatible, you can wrap your model's prediction function into a wrapper class that transforms the output into the format that is supported (`predict` or `predict_proba` of `scikit-learn`), and pass that wrapper class to modules in `azureml-responsibleai`.\n"", '\n', 'Currently, we support datasets having numerical and categorical features. The following table provides the scenarios supported for each of the four responsible AI insights:-\n', '\n', '| RAI insight | Binary classification | Multi-class classification | Multilabel classification | Regression | Timeseries forecasting | Categorical features | Text features | Image Features | Recommender Systems | Reinforcement Learning |\n', '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | -- |\n', '| Explainability | Yes | Yes | No | Yes | No | Yes | No | No | No | No |\n', '| Error Analysis | Yes | Yes | No | Yes | No | Yes | No | No | No | No |\n', '| Causal Analysis | Yes | No | No | Yes | No | Yes (max 5 features due to expensiveness) | No | No | No | No |\n', '| Counterfactual | Yes | Yes | No | Yes | No | Yes | No | No | No | No |\n', '\n', '\n', '\n', '## Set Up\n', 'In this section, we will go over the basic setup steps that you need in order to generate Responsible AI insights for your models from SDK and visualize the generated Responsible AI insights in [AML studio](https://ml.azure.com/).\n', '\n', '### Installing `azureml-responsibleai` SDK\n', 'In order to install `azureml-responsibleai` package you will need a python virtual environment. You can create a python virtual environment using `conda`.\n', '```c\n', 'conda create -n azureml_env python=3.7 nb_conda -y\n', '```\n', '\n', 'We support python versions `>= 3.6` and `< 3.9`. Once the `conda` environment `azureml_env` is created, you can install `azureml-responsibleai` using `pip`.\n', '\n', '```c\n', 'activate azureml_env\n', 'pip install azureml-responsibleai\n', 'pip install liac-arff\n', '```\n', '\n', '### Create an Azure subscription\n', 'Create an Azure workspace by using the [configuration notebook](https://github.com/Azure/MachineLearningNotebooks/blob/master/configuration.ipynb)\n', '\n', '### Generating Responsibleai AI Toolbox insights\n', 'Once you have installed `azureml-responsibleai` and created an Azure workspace, you can execute the responsibleai notebooks in the `notebooks` [folder](notebooks/model-analysis) in this repo.\n', '\n', '### Viewing your Responsible AI Toolbox Dashboard in the AzureML studio portal\n', 'After generating the Responsible AI insights via SDK, you can view them in your associated workspace in AzureML studio, under your model registry.\n', '\n', '![01](images/01_model_registry.png)\n', '1. Go to your model registry in your AzureML studio workspace\n', ""2. Click on the model you've uploaded your Responsible AI insights for\n"", '\n', '![02](images/02_model_details.png)\n', '3. Click on the tab for `Responsible AI toolbox (preview)` under your model details page\n', '\n', '![03](images/03_responsibleaitoolbox.png)\n', '4. Under the `Responsible AI toolbox (preview)` tab of your model details, you will see a list of your uploaded Responsible AI insights. You can upload more than one Responsible AI toolbox dashboards for each model. Each row represents one dashboard, with information on which components were uploaded to each dashboard (i.e. explanations, counterfactuals, etc).\n', '\n', '![04](images/04_dashboard.png)\n', '5. At anytime viewing the dashboard, if you wish to return to the model details page, click on `Back to model details`\n', '<ol type=""A"">\n', '  <li>You can view the dashboard insights for each component filtered down on a global cohort you specify. Hovering over the global cohort name will show the number of datapoints and filters in that cohort as a tooltip.</li>\n', '  <li>Switch which global cohort you are applying to the dashboard.</li>\n', '  <li>Create a new global cohort based on filters you can apply in a flyout panel.</li>\n', '  <li>View a list of all global cohorts created and duplicate, edit or delete them.</li>\n', ""  <li>View a list of all Responsible AI components you've uploaded to this dashboard as well as deleting components. The layout of the dashboard will reflect the order of the components in this list.</li>\n"", '</ol>\n', '\n', '❗ **Please note:** Error Analysis, if generated, will always be at the top of the component list in your dashboard. Selecting on the nodes of the error tree or tiles of the error heatmap will automatically generate a temporary cohort that will be populated in the components below so that you can easily experiment with looking at insights for different areas of your error distribution.\n', '\n', '![05](images/05_add_dashboard.png)\n', '6. In between each component you can add components by clicking the blue circular button with a plus sign. This will pop up a tooltip that will give you an option of adding whichever Responsible AI toolbox component you enabled with your SDK.\n', '\n', '#### Known limitations of viewing dashboard in AzureML studio\n', 'Due to current lack of active compute and backend storing and recomputing your Responsible AI toolbox insights in real time, the dashboard in AzureML studio is much less robust than the dashboard generated with the open source package. To generate the full dashboard in a Jupyter python notebook, please download and use our [open source Responsible AI Toolbox SDK](https://github.com/microsoft/responsible-ai-widgets). \n', '\n', 'Some limitations in AzureML studio include:\n', '- Retraining of the Error analysis tree on different features is disabled\n', '- Switching the Error analysis heat map to different features is disabled\n', '- Viewing the Error analysis tree or heatmap on a smaller subset of your full dataset passed into the dashboard (requires retraining of the tree) is disabled\n', '- ICE (Individual Conditional Expectation) plots in the feature importance tab for explanations is disabled\n', '- Manually creating a What-If datapoint is disabled; you can only view the counterfactual examples already pre-generated by the SDK\n', '- Causal analysis individual what-if is disabled; you can only view the individual causal effects of each individual datapoint\n', '\n', '## Responsible AI Toolbox walkthrough and sample notebooks\n', 'Please read through our [sample notebooks for both regression and classification](notebooks/model-analysis) to see if this feature supports your use case. For more details about each individual component, please read through our brief [tour guide of the new Responsible AI toolbox capabilities.](https://github.com/microsoft/responsible-ai-widgets/blob/main/notebooks/responsibleaitoolbox-dashboard/tour.ipynb) \n', '\n', '## What Next?: How to join Private Preview 👀\n', 'We are super excited for you to try this new feature in AzureML! \n', '- Reach out to mithigpe@microsoft.com to enable your Azure subscription for this Private Preview feature.\n', '- Fill out this form - Private Preview sign up for [Responsible AI Dashboard in AzureML](https://forms.office.com/r/R6PmBCkyWb)\n', '\n', '## Contributing\n', '\n', 'This project welcomes contributions and suggestions.  Most contributions require you to agree to a\n', 'Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\n', 'the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n', '\n', 'When you submit a pull request, a CLA bot will automatically determine whether you need to provide\n', 'a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\n', 'provided by the bot. You will only need to do this once across all repos using our CLA.\n', '\n', 'This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\n', 'For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\n', 'contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n', '\n', '## Trademarks\n', '\n', 'This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \n', 'trademarks or logos is subject to and must follow \n', ""[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\n"", 'Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\n', ""Any use of third-party trademarks or logos are subject to those third-party's policies.\n""]"
Responsible+AI,understandable-machine-intelligence-lab/Quantus,understandable-machine-intelligence-lab,https://api.github.com/repos/understandable-machine-intelligence-lab/Quantus,335,51,14,"['https://api.github.com/users/annahedstroem', 'https://api.github.com/users/dkrako', 'https://api.github.com/users/dilyabareeva', 'https://api.github.com/users/aaarrti', 'https://api.github.com/users/leanderweber', 'https://api.github.com/users/3-nan', 'https://api.github.com/users/FerranPares', 'https://api.github.com/users/annariasdu', 'https://api.github.com/users/Wickstrom', 'https://api.github.com/users/rodrigobdz', 'https://api.github.com/users/sebastian-lapuschkin', 'https://api.github.com/users/vedal', 'https://api.github.com/users/p-wojciechowski', 'https://api.github.com/users/sltzgs']",Python,2023-04-06T20:13:37Z,https://raw.githubusercontent.com/understandable-machine-intelligence-lab/Quantus/main/README.md,"['<p align=""center"">\n', '  <img width=""350"" src=""https://raw.githubusercontent.com/understandable-machine-intelligence-lab/Quantus/main/quantus_logo.png"">\n', '</p>\n', '<!--<h1 align=""center""><b>Quantus</b></h1>-->\n', '<h3 align=""center""><b>A toolkit to evaluate neural network explanations</b></h3>\n', '<p align=""center"">\n', '  PyTorch and TensorFlow\n', '\n', '[![Getting started!](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/understandable-machine-intelligence-lab/Quantus/blob/main/tutorials/Tutorial_ImageNet_Example_All_Metrics.ipynb)\n', '[![Launch Tutorials](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/understandable-machine-intelligence-lab/Quantus/HEAD?labpath=tutorials)\n', '[![Python package](https://github.com/understandable-machine-intelligence-lab/Quantus/actions/workflows/python-package.yml/badge.svg)](https://github.com/understandable-machine-intelligence-lab/Quantus/actions/workflows/python-package.yml)\n', '[![Code coverage](https://github.com/understandable-machine-intelligence-lab/Quantus/actions/workflows/codecov.yml/badge.svg)](https://github.com/understandable-machine-intelligence-lab/Quantus/actions/workflows/codecov.yml)\n', '![Python version](https://img.shields.io/badge/python-3.7%20%7C%203.8%20%7C%203.9-blue.svg)\n', '[![PyPI version](https://badge.fury.io/py/quantus.svg)](https://badge.fury.io/py/quantus)\n', '[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n', '[![Documentation Status](https://readthedocs.org/projects/quantus/badge/?version=latest)](https://quantus.readthedocs.io/en/latest/?badge=latest)\n', '[![codecov.io](https://codecov.io/github/understandable-machine-intelligence-lab/Quantus/coverage.svg?branch=master)](https://codecov.io/github/understandable-machine-intelligence-lab/Quantus?branch=master)\n', '[![Downloads](https://static.pepy.tech/badge/quantus)](https://pepy.tech/project/quantus)\n', '\n', '_Quantus is currently under active development so carefully note the Quantus release version to ensure reproducibility of your work._\n', '\n', '[📑 Shortcut to paper!](https://jmlr.org/papers/volume24/22-0142/22-0142.pdf)\n', '        \n', '## News and Highlights! :rocket:\n', '\n', '- Accepted to Journal of Machine Learning Research (MLOSS) ([paper](https://jmlr.org/papers/v24/22-0142.html))!\n', '- Offers more than **30+ metrics in 6 categories** for XAI evaluation\n', '- Supports different data types (image, time-series, tabular, NLP next up!) and models (PyTorch, TensorFlow)\n', '- Extended built-in support for explanation methods ([captum](https://captum.ai/) and [tf-explain](https://tf-explain.readthedocs.io/en/latest/))\n', '- New optimisations to help speed up computation, see API reference [here](https://quantus.readthedocs.io/en/latest/docs_api/quantus.metrics.base_batched.html)!\n', '\n', 'See [here](https://github.com/understandable-machine-intelligence-lab/Quantus/releases) for the latest release(s).\n', '\n', '## Citation\n', '\n', 'If you find this toolkit or its companion paper\n', '[**Quantus: An Explainable AI Toolkit for Responsible Evaluation of Neural Network Explanations and Beyond**](https://jmlr.org/papers/v24/22-0142.html)\n', 'interesting or useful in your research, use the following Bibtex annotation to cite us:\n', '\n', '```bibtex\n', '@article{hedstrom2023quantus,\n', '  author  = {Anna Hedstr{\\""{o}}m and Leander Weber and Daniel Krakowczyk and Dilyara Bareeva and Franz Motzkus and Wojciech Samek and Sebastian Lapuschkin and Marina Marina M.{-}C. H{\\""{o}}hne},\n', '  title   = {Quantus: An Explainable AI Toolkit for Responsible Evaluation of Neural Network Explanations and Beyond},\n', '  journal = {Journal of Machine Learning Research},\n', '  year    = {2023},\n', '  volume  = {24},\n', '  number  = {34},\n', '  pages   = {1--11},\n', '  url     = {http://jmlr.org/papers/v24/22-0142.html}\n', '}\n', '```\n', '\n', 'When applying the individual metrics of Quantus, please make sure to also properly cite the work of the original authors (as linked below).\n', '\n', '## Table of contents\n', '\n', '* [Library overview](#library-overview)\n', '* [Installation](#installation)\n', '* [Getting started](#getting-started)\n', '* [Tutorials](#tutorials)\n', '* [Contributing](#contributing)\n', '<!--* [Citation](#citation)-->\n', '\n', '## Library overview \n', '\n', 'A simple visual comparison of eXplainable Artificial Intelligence (XAI) methods is often not sufficient to decide which explanation method works best as shown exemplarily in Figure a) for four gradient-based methods — Saliency ([Mørch et al., 1995](https://ieeexplore.ieee.org/document/488997); [Baehrens et al., 2010](https://www.jmlr.org/papers/volume11/baehrens10a/baehrens10a.pdf)), Integrated Gradients ([Sundararajan et al., 2017](http://proceedings.mlr.press/v70/sundararajan17a/sundararajan17a.pdf)), GradientShap ([Lundberg and Lee, 2017](https://arxiv.org/abs/1705.07874)) or FusionGrad ([Bykov et al., 2021](https://arxiv.org/abs/2106.10185)), yet it is a common practice for evaluation XAI methods in absence of ground truth data. Therefore, we developed Quantus, an easy-to-use yet comprehensive toolbox for quantitative evaluation of explanations — including 30+ different metrics. \n', '\n', '</p>\n', '<p align=""center"">\n', '  <img width=""800"" src=""https://raw.githubusercontent.com/understandable-machine-intelligence-lab/Quantus/main/viz.png"">\n', '</p>\n', '\n', 'With Quantus, we can obtain richer insights on how the methods compare e.g., b) by holistic quantification on several evaluation criteria and c) by providing sensitivity analysis of how a single parameter e.g. the pixel replacement strategy of a faithfulness test influences the ranking of the XAI methods.\n', ' \n', '### Metrics\n', '\n', 'This project started with the goal of collecting existing evaluation metrics that have been introduced in the context of XAI research — to help automate the task of _XAI quantification_. Along the way of implementation, it became clear that XAI metrics most often belong to one out of six categories i.e., 1) faithfulness, 2) robustness, 3) localisation 4) complexity 5) randomisation or 6) axiomatic metrics. The library contains implementations of the following evaluation metrics:\n', '\n', '<details>\n', '  <summary><b>Faithfulness</b></summary>\n', 'quantifies to what extent explanations follow the predictive behaviour of the model (asserting that more important features play a larger role in model outcomes)\n', ' <br><br>\n', '  <ul>\n', '    <li><b>Faithfulness Correlation </b><a href=""https://www.ijcai.org/Proceedings/2020/0417.pdf"">(Bhatt et al., 2020)</a>: iteratively replaces a random subset of given attributions with a baseline value and then measuring the correlation between the sum of this attribution subset and the difference in function output \n', '    <li><b>Faithfulness Estimate </b><a href=""https://arxiv.org/pdf/1806.07538.pdf"">(Alvarez-Melis et al., 2018)</a>: computes the correlation between probability drops and attribution scores on various points\n', '    <li><b>Monotonicity Metric </b><a href=""https://arxiv.org/abs/1909.03012"">(Arya et al. 2019)</a>: starts from a reference baseline to then incrementally replace each feature in a sorted attribution vector, measuring the effect on model performance\n', '    <li><b>Monotonicity Metric </b><a href=""https://arxiv.org/pdf/2007.07584.pdf""> (Nguyen et al, 2020)</a>: measures the spearman rank correlation between the absolute values of the attribution and the uncertainty in the probability estimation\n', '    <li><b>Pixel Flipping </b><a href=""https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140"">(Bach et al., 2015)</a>: captures the impact of perturbing pixels in descending order according to the attributed value on the classification score\n', '    <li><b>Region Perturbation </b><a href=""https://arxiv.org/pdf/1509.06321.pdf"">(Samek et al., 2015)</a>: is an extension of Pixel-Flipping to flip an area rather than a single pixel\n', '    <li><b>Selectivity </b><a href=""https://arxiv.org/pdf/1706.07979.pdf"">(Montavon et al., 2018)</a>: measures how quickly an evaluated prediction function starts to drop when removing features with the highest attributed values\n', '    <li><b>SensitivityN </b><a href=""https://arxiv.org/pdf/1711.06104.pdf"">(Ancona et al., 2019)</a>: computes the correlation between the sum of the attributions and the variation in the target output while varying the fraction of the total number of features, averaged over several test samples\n', '    <li><b>IROF </b><a href=""https://arxiv.org/pdf/2003.08747.pdf"">(Rieger at el., 2020)</a>: computes the area over the curve per class for sorted mean importances of feature segments (superpixels) as they are iteratively removed (and prediction scores are collected), averaged over several test samples\n', '    <li><b>Infidelity </b><a href=""https://arxiv.org/pdf/1901.09392.pdf"">(Chih-Kuan, Yeh, et al., 2019)</a>: represents the expected mean square error between 1) a dot product of an attribution and input perturbation and 2) difference in model output after significant perturbation \n', '    <li><b>ROAD </b><a href=""https://arxiv.org/pdf/2202.00449.pdf"">(Rong, Leemann, et al., 2022)</a>: measures the accuracy of the model on the test set in an iterative process of removing k most important pixels, at each step k most relevant pixels (MoRF order) are replaced with noisy linear imputations\n', '    <li><b>Sufficiency </b><a href=""https://arxiv.org/abs/2202.00734"">(Dasgupta et al., 2022)</a>: measures the extent to which similar explanations have the same prediction label\n', '</ul>\n', '</details>\n', '\n', '<details>\n', '<summary><b>Robustness</b></summary>\n', 'measures to what extent explanations are stable when subject to slight perturbations of the input, assuming that model output approximately stayed the same\n', '     <br><br>\n', '<ul>\n', '    <li><b>Local Lipschitz Estimate </b><a href=""https://arxiv.org/pdf/1806.08049.pdf"">(Alvarez-Melis et al., 2018)</a>: tests the consistency in the explanation between adjacent examples\n', '    <li><b>Max-Sensitivity </b><a href=""https://arxiv.org/pdf/1901.09392.pdf"">(Yeh et al., 2019)</a>: measures the maximum sensitivity of an explanation using a Monte Carlo sampling-based approximation\n', '    <li><b>Avg-Sensitivity </b><a href=""https://arxiv.org/pdf/1901.09392.pdf"">(Yeh et al., 2019)</a>: measures the average sensitivity of an explanation using a Monte Carlo sampling-based approximation\n', '    <li><b>Continuity </b><a href=""https://arxiv.org/pdf/1706.07979.pdf"">(Montavon et al., 2018)</a>: captures the strongest variation in explanation of an input and its perturbed version\n', '    <li><b>Consistency </b><a href=""https://arxiv.org/abs/2202.00734"">(Dasgupta et al., 2022)</a>: measures the probability that the inputs with the same explanation have the same prediction label\n', '    <li><b>Relative Input Stability (RIS)</b><a href=""https://arxiv.org/pdf/2203.06877.pdf""> (Agarwal, et. al., 2022)</a>: measures the relative distance between explanations e_x and e_x\' with respect to the distance between the two inputs x and x\'\n', '    <li><b>Relative Representation Stability (RRS)</b><a href=""https://arxiv.org/pdf/2203.06877.pdf""> (Agarwal, et. al., 2022)</a>: measures the relative distance between explanations e_x and e_x\' with respect to the distance between internal models representations L_x and L_x\' for x and x\' respectively\n', '    <li><b>Relative Output Stability (ROS)</b><a href=""https://arxiv.org/pdf/2203.06877.pdf""> (Agarwal, et. al., 2022)</a>: measures the relative distance between explanations e_x and e_x\' with respect to the distance between output logits h(x) and h(x\') for x and x\' respectively\n', '</ul>\n', '</details>\n', '\n', '<details>\n', '<summary><b>Localisation</b></summary>\n', 'tests if the explainable evidence is centred around a region of interest (RoI) which may be defined around an object by a bounding box, a segmentation mask or, a cell within a grid\n', '     <br><br>\n', '<ul>\n', '    <li><b>Pointing Game </b><a href=""https://arxiv.org/abs/1608.00507"">(Zhang et al., 2018)</a>: checks whether attribution with the highest score is located within the targeted object\n', '    <li><b>Attribution Localization </b><a href=""https://arxiv.org/abs/1910.09840"">(Kohlbrenner et al., 2020)</a>: measures the ratio of positive attributions within the targeted object towards the total positive attributions\n', '    <li><b>Top-K Intersection </b><a href=""https://arxiv.org/abs/2104.14995"">(Theiner et al., 2021)</a>: computes the intersection between a ground truth mask and the binarized explanation at the top k feature locations\n', '    <li><b>Relevance Rank Accuracy </b><a href=""https://arxiv.org/abs/2003.07258"">(Arras et al., 2021)</a>: measures the ratio of highly attributed pixels within a ground-truth mask towards the size of the ground truth mask\n', '    <li><b>Relevance Mass Accuracy </b><a href=""https://arxiv.org/abs/2003.07258"">(Arras et al., 2021)</a>: measures the ratio of positively attributed attributions inside the ground-truth mask towards the overall positive attributions\n', '    <li><b>AUC </b><a href=""https://doi.org/10.1016/j.patrec.2005.10.010"">(Fawcett et al., 2006)</a>: compares the ranking between attributions and a given ground-truth mask\n', '    <li><b>Focus </b><a href=""https://arxiv.org/abs/2109.15035"">(Arias et al., 2022)</a>: quantifies the precision of the explanation by creating mosaics of data instances from different classes\n', '</ul>\n', '</details>\n', '\n', '<details>\n', '<summary><b>Complexity</b></summary>\n', 'captures to what extent explanations are concise i.e., that few features are used to explain a model prediction\n', '     <br><br>\n', '<ul>\n', '    <li><b>Sparseness </b><a href=""https://arxiv.org/abs/1810.06583"">(Chalasani et al., 2020)</a>: uses the Gini Index for measuring, if only highly attributed features are truly predictive of the model output\n', '    <li><b>Complexity </b><a href=""https://arxiv.org/abs/2005.00631"">(Bhatt et al., 2020)</a>: computes the entropy of the fractional contribution of all features to the total magnitude of the attribution individually\n', '    <li><b>Effective Complexity </b><a href=""https://arxiv.org/abs/2007.07584"">(Nguyen at el., 2020)</a>: measures how many attributions in absolute values are exceeding a certain threshold\n', '</ul>\n', '</details>\n', '\n', '<details>\n', '<summary><b>Randomisation</b></summary>\n', 'tests to what extent explanations deteriorate as inputs to the evaluation problem e.g., model parameters are increasingly randomised\n', '     <br><br>\n', '<ul>\n', '    <li><b>Model Parameter Randomisation </b><a href=""https://arxiv.org/abs/1810.03292"">(Adebayo et. al., 2018)</a>: randomises the parameters of single model layers in a cascading or independent way and measures the distance of the respective explanation to the original explanation\n', '    <li><b>Random Logit Test </b><a href=""https://arxiv.org/abs/1912.09818"">(Sixt et al., 2020)</a>: computes for the distance between the original explanation and the explanation for a random other class\n', '</ul>\n', '</details>\n', '\n', '<details>\n', '<summary><b>Axiomatic</b></summary>\n', '  assesses if explanations fulfil certain axiomatic properties\n', '     <br><br>\n', '<ul>\n', '    <li><b>Completeness </b><a href=""https://arxiv.org/abs/1703.01365"">(Sundararajan et al., 2017)</a>: evaluates whether the sum of attributions is equal to the difference between the function values at the input x and baseline x\' (and referred to as Summation to Delta (Shrikumar et al., 2017), Sensitivity-n (slight variation, Ancona et al., 2018) and Conservation (Montavon et al., 2018))\n', '    <li><b>Non-Sensitivity </b><a href=""https://arxiv.org/abs/2007.07584"">(Nguyen at el., 2020)</a>: measures whether the total attribution is proportional to the explainable evidence at the model output\n', '    <li><b>Input Invariance </b><a href=""https://arxiv.org/abs/1711.00867"">(Kindermans et al., 2017)</a>: adds a shift to input, asking that attributions should not change in response (assuming the model does not)\n', '</ul>\n', '</details>\n', '\n', 'Additional metrics will be included in future releases. Please [open an issue](https://github.com/understandable-machine-intelligence-lab/Quantus/issues/new/choose) if you have a metric you believe should be apart of Quantus.\n', '\n', '**Disclaimers.** It is worth noting that the implementations of the metrics in this library have not been verified by the original authors. Thus any metric implementation in this library may differ from the original authors. Further, bear in mind that evaluation metrics for XAI methods are often empirical interpretations (or translations) of qualities that some researcher(s) claimed were important for explanations to fulfil, so it may be a discrepancy between what the author claims to measure by the proposed metric and what is actually measured e.g., using entropy as an operationalisation of explanation complexity. Please read the [user guidelines](https://quantus.readthedocs.io/en/latest/guidelines/guidelines_and_disclaimers.html) for further guidance on how to best use the library. \n', '\n', '## Installation\n', '\n', 'If you already have [PyTorch](https://pytorch.org/) or [TensorFlow](https://www.TensorFlow.org) installed on your machine, \n', 'the most light-weight version of Quantus can be obtained from [PyPI](https://pypi.org/project/quantus/) as follows (no additional explainability functionality or deep learning framework will be included):\n', '\n', '```setup\n', 'pip install quantus\n', '```\n', 'Alternatively, you can simply add the desired deep learning framework (in brackets) to have the package installed together with Quantus.\n', 'To install Quantus with PyTorch, please run:\n', '```setup\n', 'pip install ""quantus[torch]""\n', '```\n', '\n', 'For TensorFlow, please run:\n', '\n', '```setup\n', 'pip install ""quantus[tensorflow]""\n', '```\n', '\n', 'Alternatively, you can simply install Quantus with [requirements.txt](https://github.com/understandable-machine-intelligence-lab/Quantus/blob/main/requirements.txt).\n', 'Note that this installation requires that either [PyTorch](https://pytorch.org/) or [TensorFlow](https://www.TensorFlow.org) are already installed on your machine.\n', '\n', '```setup\n', 'pip install -r requirements.txt\n', '```\n', '\n', 'For a more in-depth guide on how to install Quantus, please read more [here](https://quantus.readthedocs.io/en/latest/getting_started/installation.html). This includes instructions for how to install a desired deep learning framework such as PyTorch or TensorFlow together with Quantus.\n', '\n', '### Package requirements\n', '\n', 'The package requirements are as follows:\n', '```\n', 'python>=3.7.0\n', 'pytorch>=1.10.1\n', 'TensorFlow==2.6.2\n', '```\n', '\n', '## Getting started\n', '\n', 'The following will give a short introduction to how to get started with Quantus. Note that this example is based on the [PyTorch](https://pytorch.org/) framework, but we also support \n', '[TensorFlow](https://www.tensorflow.org), which would differ only in the loading of the model, data and explanations. To get started with Quantus, you need:\n', '* A model (`model`), inputs (`x_batch`) and labels (`y_batch`)\n', '* Some explanations you want to evaluate (`a_batch`)\n', '\n', '\n', '<details>\n', '<summary><b><big>Step 1. Load data and model</big></b></summary>\n', '\n', ""Let's first load the data and model. In this example, a pre-trained LeNet available from Quantus \n"", ""for the purpose of this tutorial is loaded, but generally, you might use any Pytorch (or TensorFlow) model instead. To follow this example, one needs to have quantus and torch installed, by e.g., `pip install 'quantus[torch]'`.\n"", '\n', '```python\n', 'import quantus\n', 'from quantus.helpers.model.models import LeNet\n', 'import torch\n', 'import torchvision\n', 'from torchvision import transforms\n', '  \n', '# Enable GPU.\n', 'device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n', '\n', '# Load a pre-trained LeNet classification model (architecture at quantus/helpers/models).\n', 'model = LeNet()\n', 'if device.type == ""cpu"":\n', '    model.load_state_dict(torch.load(""tests/assets/mnist"", map_location=torch.device(\'cpu\')))\n', 'else: \n', '    model.load_state_dict(torch.load(""tests/assets/mnist""))\n', '\n', '# Load datasets and make loaders.\n', ""test_set = torchvision.datasets.MNIST(root='./sample_data', download=True, transforms=transforms.Compose([transforms.ToTensor()]))\n"", 'test_loader = torch.utils.data.DataLoader(test_set, batch_size=24)\n', '\n', '# Load a batch of inputs and outputs to use for XAI evaluation.\n', 'x_batch, y_batch = iter(test_loader).next()\n', 'x_batch, y_batch = x_batch.cpu().numpy(), y_batch.cpu().numpy()\n', '```\n', '</details>\n', '\n', '<details>\n', '<summary><b><big>Step 2. Load explanations</big></b></summary>\n', '\n', 'We still need some explanations to evaluate. \n', 'For this, there are two possibilities in Quantus. You can provide either:\n', '1. a set of re-computed attributions (`np.ndarray`)\n', '2. any arbitrary explanation function (`callable`), e.g., the built-in method `quantus.explain` or your own customised function\n', '\n', 'We show the different options below.\n', '\n', '#### Using pre-computed explanations\n', '\n', 'Quantus allows you to evaluate explanations that you have pre-computed, \n', ""assuming that they match the data you provide in `x_batch`. Let's say you have explanations \n"", 'for [Saliency](https://arxiv.org/abs/1312.6034) and [Integrated Gradients](https://arxiv.org/abs/1703.01365)\n', 'already pre-computed.\n', '\n', 'In that case, you can simply load these into corresponding variables `a_batch_saliency` \n', 'and `a_batch_intgrad`:\n', '\n', '```python\n', 'a_batch_saliency = load(""path/to/precomputed/saliency/explanations"")\n', 'a_batch_saliency = load(""path/to/precomputed/intgrad/explanations"")\n', '```\n', '\n', 'Another option is to simply obtain the attributions using one of many XAI frameworks out there, \n', 'such as [Captum](https://captum.ai/), \n', '[Zennit](https://github.com/chr5tphr/zennit), \n', '[tf.explain](https://github.com/sicara/tf-explain),\n', 'or [iNNvestigate](https://github.com/albermax/innvestigate). The following code example shows how to obtain explanations ([Saliency](https://arxiv.org/abs/1312.6034) \n', 'and [Integrated Gradients](https://arxiv.org/abs/1703.01365), to be specific) \n', 'using [Captum](https://captum.ai/):\n', '\n', '```python\n', 'import captum\n', 'from captum.attr import Saliency, IntegratedGradients\n', '\n', '# Generate Integrated Gradients attributions of the first batch of the test set.\n', 'a_batch_saliency = Saliency(model).attribute(inputs=x_batch, target=y_batch, abs=True).sum(axis=1).cpu().numpy()\n', 'a_batch_intgrad = IntegratedGradients(model).attribute(inputs=x_batch, target=y_batch, baselines=torch.zeros_like(x_batch)).sum(axis=1).cpu().numpy()\n', '\n', '# Save x_batch and y_batch as numpy arrays that will be used to call metric instances.\n', 'x_batch, y_batch = x_batch.cpu().numpy(), y_batch.cpu().numpy()\n', '\n', '# Quick assert.\n', 'assert [isinstance(obj, np.ndarray) for obj in [x_batch, y_batch, a_batch_saliency, a_batch_intgrad]]\n', '```\n', '\n', '#### Passing an explanation function\n', '\n', ""If you don't have a pre-computed set of explanations but rather want to pass an arbitrary explanation function \n"", 'that you wish to evaluate with Quantus, this option exists. \n', '\n', 'For this, you can for example rely on the built-in `quantus.explain` function to get started, which includes some popular explanation methods \n', '(please run `quantus.available_methods()` to see which ones).  Examples of how to use `quantus.explain` \n', 'or your own customised explanation function are included in the next section.\n', '\n', '<img class=""center"" width=""500"" alt=""drawing""  src=""tutorials/assets/mnist_example.png""/>\n', '\n', 'As seen in the above image, the qualitative aspects of explanations \n', 'may look fairly uninterpretable --- since we lack ground truth of what the explanations\n', 'should be looking like, it is hard to draw conclusions about the explainable evidence. To gather quantitative evidence for the quality of the different explanation methods, we can apply Quantus.\n', '</details>\n', '\n', '<details>\n', '<summary><b><big>Step 3. Evaluate with Quantus</big></b></summary> \n', '\n', 'Quantus implements XAI evaluation metrics from different categories, \n', 'e.g., Faithfulness, Localisation and Robustness etc which all inherit from the base `quantus.Metric` class. \n', 'To apply a metric to your setting (e.g., [Max-Sensitivity](https://arxiv.org/abs/1901.09392)) \n', 'it first needs to be instantiated:\n', '\n', '```python\n', 'metric = quantus.MaxSensitivity(nr_samples=10,\n', '                                lower_bound=0.2,\n', '                                norm_numerator=quantus.fro_norm,\n', '                                norm_denominator=quantus.fro_norm,\n', '                                perturb_func=quantus.uniform_noise,\n', '                                similarity_func=quantus.difference)\n', '```\n', '\n', 'and then applied to your model, data, and (pre-computed) explanations:\n', '\n', '```python\n', 'scores = metric(\n', '    model=model,\n', '    x_batch=x_batch,\n', '    y_batch=y_batch,\n', '    a_batch=a_batch_saliency,\n', '    device=device\n', ')\n', '```\n', '\n', '#### Use quantus.explain\n', '\n', 'Alternatively, instead of providing pre-computed explanations, you can employ the `quantus.explain` function,\n', 'which can be specified through a dictionary passed to `explain_func_kwargs`.\n', '\n', '```python\n', 'scores = metric(\n', '    model=model,\n', '    x_batch=x_batch,\n', '    y_batch=y_batch,\n', '    device=device,\n', '    explain_func=quantus.explain,\n', '    explain_func_kwargs={""method"": ""Saliency""}\n', ')\n', '```\n', '\n', '#### Employ customised functions\n', '\n', 'You can alternatively use your own customised explanation function\n', '(assuming it returns an `np.ndarray` in a shape that matches the input `x_batch`). This is done as follows:\n', '\n', '```python\n', 'def your_own_callable(model, models, targets, **kwargs) -> np.ndarray\n', '  """"""Logic goes here to compute the attributions and return an \n', '  explanation  in the same shape as x_batch (np.array), \n', '  (flatten channels if necessary).""""""\n', '  return explanation(model, x_batch, y_batch)\n', '\n', 'scores = metric(\n', '    model=model,\n', '    x_batch=x_batch,\n', '    y_batch=y_batch,\n', '    device=device,\n', '    explain_func=your_own_callable\n', ')\n', '```\n', '#### Run large-scale evaluation\n', '\n', 'Quantus also provides high-level functionality to support large-scale evaluations,\n', 'e.g., multiple XAI methods, multifaceted evaluation through several metrics, or a combination thereof. To utilise `quantus.evaluate()`, you simply need to define two things:\n', '\n', '1. The **Metrics** you would like to use for evaluation (each `__init__` parameter configuration counts as its own metric):\n', '    ```python\n', '    metrics = {\n', '        ""max-sensitivity-10"": quantus.MaxSensitivity(nr_samples=10),\n', '        ""max-sensitivity-20"": quantus.MaxSensitivity(nr_samples=20),\n', '        ""region-perturbation"": quantus.RegionPerturbation(),\n', '    }\n', '    ```\n', '   \n', '2. The **XAI methods** you would like to evaluate, e.g., a `dict` with pre-computed attributions:\n', '    ```python\n', '    xai_methods = {\n', '        ""Saliency"": a_batch_saliency,\n', '        ""IntegratedGradients"": a_batch_intgrad\n', '    }\n', '    ```\n', '\n', 'You can then simply run a large-scale evaluation as follows (this aggregates the result by `np.mean` averaging):\n', '\n', '```python\n', 'import numpy as np\n', 'results = quantus.evaluate(\n', '      metrics=metrics,\n', '      xai_methods=xai_methods,\n', '      agg_func=np.mean,\n', '      model=model,\n', '      x_batch=x_batch,\n', '      y_batch=y_batch,\n', '      **{""softmax"": False,}\n', ')\n', '```\n', '</details>\n', '\n', 'Please see [\n', ""Getting started tutorial](https://github.com/understandable-machine-intelligence-lab/quantus/blob/main/tutorials/Tutorial_Getting_Started.ipynb) to run code similar to this example. For more information on how to customise metrics and extend Quantus' functionality, please see [Getting started guide](https://quantus.readthedocs.io/en/latest/getting_started/getting_started_example.html).\n"", '\n', '\n', '## Tutorials\n', '\n', 'Further tutorials are available that showcase the many types of analysis that can be done using Quantus.\n', 'For this purpose, please see notebooks in the [tutorials](https://github.com/understandable-machine-intelligence-lab/Quantus/blob/main/tutorials/) folder which includes examples such as:\n', '* [All Metrics ImageNet Example](https://github.com/understandable-machine-intelligence-lab/Quantus/blob/main/tutorials/Tutorial_ImageNet_Example_All_Metrics.ipynb): shows how to instantiate the different metrics for ImageNet dataset\n', '* [Metric Parameterisation Analysis](https://github.com/understandable-machine-intelligence-lab/Quantus/blob/main/tutorials/Tutorial_Metric_Parameterisation_Analysis.ipynb): explores how sensitive a metric could be to its hyperparameters\n', '* [Robustness Analysis Model Training](https://github.com/understandable-machine-intelligence-lab/Quantus/blob/main/tutorials/Tutorial_XAI_Sensitivity_Model_Training.ipynb): measures robustness of explanations as model accuracy increases \n', '* [Full Quantification with Quantus](https://github.com/understandable-machine-intelligence-lab/Quantus/blob/main/tutorials/Tutorial_ImageNet_Quantification_with_Quantus.ipynb): example of benchmarking explanation methods\n', '* [Tabular Data Example](https://github.com/understandable-machine-intelligence-lab/Quantus/blob/main/tutorials/Tutorial_Getting_Started_with_Tabular_Data.ipynb): example of how to use Quantus with tabular data\n', '* [Quantus and TensorFlow Data Example](https://github.com/understandable-machine-intelligence-lab/Quantus/blob/main/tutorials/Tutorial_Getting_Started_with_Tensorflow.ipynb): showcases how to use Quantus with TensorFlow\n', '\n', '... and more.\n', '\n', '## Contributing\n', '\n', 'We welcome any sort of contribution to Quantus! For a detailed contribution guide, please refer to [Contributing](https://github.com/understandable-machine-intelligence-lab/Quantus/blob/main/CONTRIBUTING.md) documentation first. \n', '\n', 'If you have any developer-related questions, please [open an issue](https://github.com/understandable-machine-intelligence-lab/Quantus/issues/new/choose)\n', 'or write us at [hedstroem.anna@gmail.com](mailto:hedstroem.anna@gmail.com).\n']"
Responsible+AI,mit-ll-responsible-ai/responsible-ai-toolbox,mit-ll-responsible-ai,https://api.github.com/repos/mit-ll-responsible-ai/responsible-ai-toolbox,29,5,6,"['https://api.github.com/users/rsokl', 'https://api.github.com/users/jgbos', 'https://api.github.com/users/dependabot%5Bbot%5D', 'https://api.github.com/users/oliviamb', 'https://api.github.com/users/miscpeeps', 'https://api.github.com/users/Jasha10']",Python,2023-03-27T09:39:13Z,https://raw.githubusercontent.com/mit-ll-responsible-ai/responsible-ai-toolbox/main/README.md,"['# Responsible AI Toolbox\n', '\n', '<p align=""center"">\n', '  <img width=""200"" height=""200"" src=""brand/logo_no_text_small.png"">\n', '</p>\n', '\n', '<p align=""center"">\n', '  <a href=""https://pypi.org/project/rai-toolbox/"">\n', '    <img src=""https://img.shields.io/pypi/v/rai-toolbox.svg"" alt=""PyPI"" />\n', '  </a>\n', '  <a>\n', '    <img src=""https://img.shields.io/badge/python-3.7%20&#8208;%203.10-blue.svg"" alt=""Python version support"" />\n', '  </a>\n', '  <a href=""https://github.com/mit-ll-responsible-ai/responsible-ai-toolbox/actions?query=workflow%3ATests+branch%3Amain"">\n', '    <img src=""https://github.com/mit-ll-responsible-ai/responsible-ai-toolbox/workflows/Tests/badge.svg"" alt=""GitHub Actions"" />\n', '  <a href=""https://hypothesis.readthedocs.io/"">\n', '    <img src=""https://img.shields.io/badge/hypothesis-tested-brightgreen.svg"" alt=""Tested with Hypothesis"" />\n', '  </a>\n', '\n', '  <p align=""center"">\n', '    A library that provides high-quality, PyTorch-centric tools for evaluating and enhancing both the robustness and the explainability of AI models.\n', '  </p>\n', '  <p align=""center"">\n', '    Check out our <a href=""https://mit-ll-responsible-ai.github.io/responsible-ai-toolbox/"">documentation</a> for more information.\n', '  </p>\n', '  <p align=""center"">\n', '    The rAI-toolbox works great with <a href=""https://www.pytorchlightning.ai/"">PyTorch Lightning</a> ⚡ and <a href=""https://hydra.cc/"">Hydra</a> 🐉. Check out <a href=""https://mit-ll-responsible-ai.github.io/responsible-ai-toolbox/ref_mushin.html"">rai_toolbox.mushin</a> to see how we use these frameworks to create efficient, configurable, and reproducible ML workflows with minimal boilerplate code.\n', '  </p>\n', '</p>\n', '\n', '\n', '\n', '## Citation\n', '\n', 'Using `rai_toolbox` for your research? Please cite the following publication:\n', '\n', '```\n', '@article{soklaski2022tools,\n', '  title={Tools and Practices for Responsible AI Engineering},\n', '  author={Soklaski, Ryan and Goodwin, Justin and Brown, Olivia and Yee, Michael and Matterer, Jason},\n', '  journal={arXiv preprint arXiv:2201.05647},\n', '  year={2022}\n', '}\n', '```\n', '\n', '\n', '## Contributing\n', '\n', 'If you would like to contribute to this repo, please refer to our `CONTRIBUTING.md` document.\n', '\n', '\n', '\n', '## Disclaimer\n', '\n', 'DISTRIBUTION STATEMENT A. Approved for public release. Distribution is unlimited.\n', '\n', '© 2023 MASSACHUSETTS INSTITUTE OF TECHNOLOGY\n', '\n', '- Subject to FAR 52.227-11 – Patent Rights – Ownership by the Contractor (May 2014)\n', '- SPDX-License-Identifier: MIT\n', '\n', 'This material is based upon work supported by the Under Secretary of Defense for Research and Engineering under Air Force Contract No. FA8702-15-D-0001. Any opinions, findings, conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Under Secretary of Defense for Research and Engineering.\n', '\n', 'A portion of this research was sponsored by the United States Air Force Research Laboratory and the United States Air Force Artificial Intelligence Accelerator and was accomplished under Cooperative Agreement Number FA8750-19-2-1000. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the United States Air Force or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein.\n', '\n', 'The software/firmware is provided to you on an As-Is basis.\n']"
Responsible+AI,microsoft/responsible-ai-workshop,microsoft,https://api.github.com/repos/microsoft/responsible-ai-workshop,13,6,5,"['https://api.github.com/users/philber', 'https://api.github.com/users/microsoftopensource', 'https://api.github.com/users/RiadEtm', 'https://api.github.com/users/alazraq', 'https://api.github.com/users/microsoft-github-operations%5Bbot%5D']",Python,2023-02-16T03:28:31Z,https://raw.githubusercontent.com/microsoft/responsible-ai-workshop/main/README.md,"['\n', '# Responsible AI Workshop\n', '![Workshop logo](https://github.com/microsoft/responsible-ai-workshop/blob/main/rai-ws-banner.png)\n', '\n', 'Responsible innovation is top of mind. As such, the tech industry as well as a growing number of organizations of all kinds in their digital transformation are being called upon to develop and deploy Artificial Intelligence (AI) technologies and Machine Learning (ML)-powered systems (products or services) and/or features (all referred as to AI systems below) more responsibly. And yet many organizations implementing such AI systems report being unprepared to address AI risks and failures, and struggle with new challenges in terms of governance, security and compliance.\n', '\n', 'Advancements in AI are indeed different than other technologies because of the pace of innovation. There has been hundreds of research papers published every year in the past few years -, but also because of its proximity to human intelligence, impacting us at a personal and societal level.\n', '\n', ""There are a number of challenges and questions raised through the use of AI technologies. We refer to these as socio-technical impacts. All of these have given rise to an industry debate about how the world should/shouldn't use these new capabilities. It isn't because you can do something that you should necessarily do it. \n"", '\n', 'This project is an attempt to introduce and illustrate the use of: \n', '* Resources designed to help you responsibly use AI at every stage of innovation - from concept to development, deployment, and beyond. \n', '* Available toolkits & frameworks that help you integrate relevant Responsible AI features into your AI environment by themes and through the lifecycle stages of your AI system.\n', '* Activities to strengthen gradually the confidence that we can have in this technology and therefore facilitate its adoption in contexts where it would have a great responsibility.\n', '\n', 'It is thus designed to help you or your ""customers"", whoever they are, to put Responsible AI into practice for your AI-powered solutions throughout their lifecycle.\n', '\n', '# Workshop Tutorials/Walkthroughs\n', '\n', '## Work in Progress\n', '\n', 'This project is a work in progress (WIP).\n', '\n', 'This project currently contains the following tutorials:\n', '* [Responsible AI Tooling Tutorials](https://github.com/microsoft/responsible-ai-workshop/tree/master/tooling-tutorials)\n', '* [End-to-End Responsible AI Lifecycle Walkthrough](https://github.com/microsoft/responsible-ai-workshop/tree/main/lifecycle-walkthrough)\n', '* [Towards a (more) trustworthy AI lifecycle](https://github.com/microsoft/responsible-ai-workshop/tree/main/trustworthy-ai-lifecycle)\n', '\n', 'Each of the above tutorials consists of a series of modules for data engineers, data scientists, ML developers, ML engineers, and other AI practitioners, as well as potentially anyone interested considering the wide range of socio-technical aspects involved in the subject.\n', '\n', '## Prerequisites\n', '\n', 'The workshop is meant to be hands-on. Therefore, basic knowledge of any version of Python is a prerequisite. It also assumes that you have prior experience training machine learning (ML) models with Python using open-source frameworks like Scikit-Learn, PyTorch, and TensorFlow.\n', '\n', 'One should also note that this workshop might also be introduced by the following [Microsoft Learn](https://docs.microsoft.com/en-us/learn/) learning paths:\n', '* [Discover ways to foster an AI-ready culture in your business](https://docs.microsoft.com/en-us/learn/paths/foster-ai-ready-culture/).\n', '* [Identify principles and practices for responsible AI](https://docs.microsoft.com/en-us/learn/paths/responsible-ai-business-principles/).\n', '* [Identify guiding principles for responsible AI in government](https://docs.microsoft.com/en-us/learn/paths/responsible-ai-government-principles/).\n', '\n', '## Additional resources\n', '\n', 'From holistically transforming industries to addressing critical issues facing humanity, AI is already solving some of our most complex challenges and redefining how humans and technology interact. \n', '\n', 'You can read the publicly shared [Microsoft Responsible AI Standard](https://blogs.microsoft.com/wp-content/uploads/prod/sites/5/2022/06/Microsoft-Responsible-AI-Standard-v2-General-Requirements-3.pdf), i.e., a framework to guide how Microsoft build AI systems. It is an important step in our journey to develop better, more trustworthy AI systems. We are releasing our latest Responsible AI Standard to share what we have learned, invite feedback from others, and contribute to the discussion about building better norms and practices around AI. \n', '\n', 'For those wanting to dig into our approach further, we have also made available some key resources that support the Responsible AI Standard: notably our [Impact Assessment template](https://blogs.microsoft.com/wp-content/uploads/prod/sites/5/2022/06/Microsoft-RAI-Impact-Assessment-Template.pdf) and [guide](https://blogs.microsoft.com/wp-content/uploads/prod/sites/5/2022/06/Microsoft-RAI-Impact-Assessment-Guide.pdf). Impact Assessments have proven valuable at Microsoft to ensure teams explore the impact of their AI system – including its stakeholders, intended benefits, and potential harms – in depth at the earliest design stages. \n', '\n', 'In addition, you can also visit our [Responsible AI resource center](https://www.microsoft.com/en-us/ai/responsible-ai) where you can find access to tools, guidelines, and additional resources that will help you create a (more) Responsible AI solution:\n', '* [Put Responsible AI into Practice webinar](https://info.microsoft.com/ww-put-responsible-ai-into-practice-On-Demand-Registration.html) (On Demand).\n', '* [Ten Guidelines for Product Leaders to implement AI Responsibly](https://aka.ms/RAITenGuidelines).\n', '* [Establish a responsible AI strategy](https://aka.ms/AIBS).\n', '* [Design, build, and manage your AI-powered solution](http://aka.ms/RAIresources).\n', '  \n', '\n', '# Contributing\n', '\n', 'This project welcomes contributions and suggestions.  Most contributions require you to agree to a\n', 'Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\n', 'the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n', '\n', 'When you submit a pull request, a CLA bot will automatically determine whether you need to provide\n', 'a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\n', 'provided by the bot. You will only need to do this once across all repos using our CLA.\n', '\n', 'This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\n', 'For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\n', 'contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n', '\n', '# Legal Notices\n', '\n', 'Microsoft and any contributors grant you a license to the Microsoft documentation and other content\n', 'in this repository under the [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/legalcode),\n', 'see the [LICENSE](LICENSE) file, and grant you a license to any code in the repository under the [MIT License](https://opensource.org/licenses/MIT), see the\n', '[LICENSE-CODE](LICENSE-CODE) file.\n', '\n', 'Microsoft, Windows, Microsoft Azure and/or other Microsoft products and services referenced in the documentation\n', 'may be either trademarks or registered trademarks of Microsoft in the United States and/or other countries.\n', 'The licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks.\n', ""Microsoft's general trademark guidelines can be found at http://go.microsoft.com/fwlink/?LinkID=254653.\n"", '\n', 'Privacy information can be found at https://privacy.microsoft.com/en-us/\n', '\n', 'Microsoft and any contributors reserve all other rights, whether under their respective copyrights, patents,\n', 'or trademarks, whether by implication, estoppel or otherwise.\n']"
Responsible+AI,jphall663/responsible_xai,jphall663,https://api.github.com/repos/jphall663/responsible_xai,17,6,1,['https://api.github.com/users/jphall663'],Python,2023-03-04T01:58:48Z,https://raw.githubusercontent.com/jphall663/responsible_xai/master/README.md,"['# responsible_xai\n', 'Guidelines for the responsible use of explainable AI and machine learning. See [responsible_xai.pdf](responsible_xai.pdf).\n', '\n', 'A static version of this document, that may not reflect recent changes, is available on [arXiv](https://arxiv.org/abs/1906.03533).\n', '\n', 'For associated slides see: https://github.com/jphall663/hc_ml.\n']"
Responsible+AI,AthenaCore/AwesomeResponsibleAI,AthenaCore,https://api.github.com/repos/AthenaCore/AwesomeResponsibleAI,11,5,2,"['https://api.github.com/users/josepcurto', 'https://api.github.com/users/mariolopezdeavila']",,2023-04-02T22:18:58Z,https://raw.githubusercontent.com/AthenaCore/AwesomeResponsibleAI/main/README.md,"['[![Awesome](awesome.svg)](https://github.com/AthenaCore/AwesomeResponsibleAI)\n', '[![Maintenance](https://img.shields.io/badge/Maintained%3F-YES-green.svg)](https://github.com/AthenaCore/AwesomeResponsibleAI/graphs/commit-activity)\n', '![GitHub](https://img.shields.io/badge/Release-PROD-yellow.svg)\n', '![GitHub](https://img.shields.io/badge/Languages-MULTI-blue.svg)\n', '![GitHub](https://img.shields.io/badge/License-MIT-lightgrey.svg)\n', '[![GitHub](https://img.shields.io/twitter/follow/athenacoreai.svg?label=Follow)](https://twitter.com/athenacoreai)\n', '\n', '# Awesome Responsible AI\n', 'A curated list of awesome academic research, books, code of ethics, newsletters, principles, podcast, reports, tools and regulations related to Responsible AI and Human-Centered AI.\n', '\n', '## Contents\n', '\n', '- [Academic Research](#academic-research)\n', '- [Books](#books)\n', '- [Code of Ethics](#code-of-ethics)\n', '- [Data Sets](#data-sets)\n', '- [Institutes](#institutes)\n', '- [Newsletters](#newsletters)\n', '- [Principles](#principles)\n', '- [Podcasts](#podcasts)\n', '- [Reports](#reports)\n', '- [Tools](#tools)\n', '- [Regulations](#regulations)\n', '\n', '## Academic Research\n', '\n', '### Bias\n', '\n', '- Towards a Standard for Identifying and Managing Bias in Artificial Intelligence ([Schwartz, Reva et al., 2022]()) `NIST`\n', '\n', '### Challenges\n', '\n', ""- Underspecification presents challenges for credibility in modern machine learning. ([D'AMOUR, Alexander, et al., 2020](https://arxiv.org/abs/2011.03395)) `Google`\n"", '\n', '### Drift\n', '\n', '- FreaAI: Automated extraction of data slices to test machine learning models ([Ackerman, S. et al. 2021](https://arxiv.org/pdf/2108.05620.pdf)) `IBM`\n', '- Machine Learning Model Drift Detection Via Weak Data Slices ([Ackerman, S. et al. 2021](https://arxiv.org/pdf/2108.05319.pdf)) `IBM`\n', '\n', '### Explainability\n', '\n', '- Efficient Data Representation by Selecting Prototypes with Importance Weights ([Gurumoorthy et al., 2019](https://arxiv.org/abs/1707.01212)) `Amazon Development Center` `IBM Research`\n', '- Explanations based on the Missing: Towards Contrastive Explanations with Pertinent Negatives ([Dhurandhar et al., 2018](https://papers.nips.cc/paper/7340-explanations-based-on-the-missing-towards-contrastive-explanations-with-pertinent-negatives)) `University of Michigan` `IBM Research`\n', '- Contrastive Explanations Method with Monotonic Attribute Functions ([Luss et al., 2019](https://arxiv.org/abs/1905.12698))\n', '- ""Why Should I Trust You?"": Explaining the Predictions of Any Classifier (LIME) ([Ribeiro et al. 2016](https://arxiv.org/abs/1602.04938),  [Github](https://github.com/marcotcr/lime)) `University of Washington`\n', '- A Unified Approach to Interpreting Model Predictions (SHAP) ([Lundberg, et al. 2017](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions),  [Github](https://github.com/slundberg/shap)) `University of Washington`\n', '- Teaching AI to Explain its Decisions ([Hind et al., 2019](https://doi.org/10.1145/3306618.3314273)) `IBM Research`\n', '- Boolean Decision Rules via Column Generation (Light Edition) ([Dash et al., 2018](https://papers.nips.cc/paper/7716-boolean-decision-rules-via-column-generation)) `IBM Research`\n', '- Generalized Linear Rule Models ([Wei et al., 2019](http://proceedings.mlr.press/v97/wei19a.html)) `IBM Research`\n', '- Improving Simple Models with Confidence Profiles ([Dhurandhar et al., 2018](https://papers.nips.cc/paper/8231-improving-simple-models-with-confidence-profiles)) `IBM Research`\n', '- Towards Robust Interpretability with Self-Explaining Neural Networks ([Alvarez-Melis et al., 2018](https://papers.nips.cc/paper/8003-towards-robust-interpretability-with-self-explaining-neural-networks)) `MIT`\n', '- Leveraging Latent Features for Local Explanations ([Luss et al., 2019](https://arxiv.org/abs/1905.12698)) `IBM Research` `University of Michigan`\n', '\n', '### Fairness\n', '\n', '- [LiFT: A Scalable Framework for Measuring Fairness in ML Applications](https://engineering.linkedin.com/blog/2020/lift-addressing-bias-in-large-scale-ai-applications) ([Vasuvedan et al., 2020](https://arxiv.org/abs/2008.07433)) `LinkedIn`\n', '\n', '### Ethical Data Products\n', '\n', '- [Building Inclusive Products Through A/B Testing](https://engineering.linkedin.com/blog/2020/building-inclusive-products-through-a-b-testing) ([Saint-Jacques et al, 2020](https://arxiv.org/pdf/2002.05819)) `LinkedIn`\n', '\n', '### Sustainability\n', '\n', '- Energy and policy considerations for deep learning in NLP ([Strubell, E. et al. 2019](https://arxiv.org/abs/1906.02243))\n', '- Quantifying the carbon emissions of machine learning. ([Lacoste, A. et al. 2019](https://arxiv.org/abs/1910.09700))\n', '- Carbon emissions and large neural network training. ([Patterson, D. et al. 2021](https://arxiv.org/abs/2104.10350)) \n', '- The Energy and Carbon Footprint of Training End-to-End Speech Recognizers. ([Parcollet, T., & Ravanelli, M. 2021](https://hal.archives-ouvertes.fr/hal-03190119/document))\n', '- Sustainable AI: AI for sustainability and the sustainability of AI ([van Wynsberghe, A. 2021](https://link.springer.com/article/10.1007/s43681-021-00043-6)). AI and Ethics, 1-6\n', '- Green Algorithms: Quantifying the carbon emissions of computation ([Lannelongue, L. et al. 2020](https://arxiv.org/abs/2007.07610))\n', '- Machine Learning: The High Interest Credit Card of Technical Debt ([Sculley, D. et al. 2014](https://research.google/pubs/pub43146/)) `Google`\n', '\n', '### Collections\n', '\n', '- Google Research on Responsible AI: https://research.google/pubs/?collection=responsible-ai `Google`\n', '\n', '## Books\n', '\n', '### Open Access\n', '\n', '- Interpretable Machine Learning ([Molnar, C., 2021](https://christophm.github.io/interpretable-ml-book/)) `Explainability` `Interpretability` `Transparency` `R`\n', '- Explanatory Model Analysis ([Biecek et al., 2020](https://ema.drwhy.ai)) `Explainability` `Interpretability` `Transparency` `R`\n', '\n', '### Commercial / Propietary / Closed Access\n', '\n', '- Trust in Machine Learning ([Varshney, K., 2022](https://www.manning.com/books/trust-in-machine-learning)) `Safety` `Privacy` `Drift` `Fairness` `Interpretability` `Explainability`\n', '- Interpretable AI ([Thampi, A., 2022](https://www.manning.com/books/interpretable-ai)) `Explainability` `Fairness` `Interpretability` \n', '- AI Fairness ([Mahoney, T., Varshney, K.R., Hind, M., 2020](https://learning.oreilly.com/library/view/ai-fairness/9781492077664/) `Report` `Fairness`\n', '- Practical Fairness ([Nielsen, A., 2021](https://learning.oreilly.com/library/view/practical-fairness/9781492075721/)) `Fairness`\n', '- Hands-On Explainable AI (XAI) with Python ([Rothman, D., 2020](https://www.packtpub.com/product/hands-on-explainable-ai-xai-with-python/9781800208131)) `Explainability`\n', '- AI and the Law ([Kilroy, K., 2021](https://learning.oreilly.com/library/view/ai-and-the/9781492091837/)) `Report` `Trust` `Law`\n', '- Responsible Machine Learning ([Hall, P., Gill, N., Cox, B., 2020](https://learning.oreilly.com/library/view/responsible-machine-learning/9781492090878/)) `Report` `Law`  `Compliance` `Safety` `Privacy` \n', '- [Privacy-Preserving Machine Learning](https://www.manning.com/books/privacy-preserving-machine-learning)\n', '- [Human-In-The-Loop Machine Learning: Active Learning and Annotation for Human-Centered AI](https://www.manning.com/books/human-in-the-loop-machine-learning)\n', '- [Interpretable Machine Learning With Python: Learn to Build Interpretable High-Performance Models With Hands-On Real-World Examples](https://www.packtpub.com/product/interpretable-machine-learning-with-python/9781800203907)\n', '- Responsible AI ([Hall, P., Chowdhury, R., 2023](https://learning.oreilly.com/library/view/responsible-ai/9781098102425/)) `Governance` `Safety` `Drift`\n', '\n', '## Code of Ethics\n', '\n', '- [ACS Code of Professional Conduct](https://www.acs.org.au/content/dam/acs/rules-and-regulations/Code-of-Professional-Conduct_v2.1.pdf) by Australian ICT (Information and Communication Technology)\n', '- [AI Standards Hub](https://aistandardshub.org)\n', ""- [Association for Computer Machinery's Code of Ethics and Professional Conduct](https://www.acm.org/code-of-ethics)\n"", '- [IEEE Global Initiative for Ethical Considerations in Artificial Intelligence (AI) and Autonomous Systems (AS)](https://ethicsinaction.ieee.org/)\n', ""- [ISO/IEC's Standards for Artificial Intelligence](https://www.iso.org/committee/6794475/x/catalogue/)\n"", '- [Ethics guidelines for trustworthy AI](https://op.europa.eu/en/publication-detail/-/publication/d3988569-0434-11ea-8c1f-01aa75ed71a1/language-en/format-PDF/source-229277158) - European Commission document prepared by the High-Level Expert Group on Artificial Intelligence (AI HLEG).\n', '- [Google AI Principles](https://ai.google/principles/)\n', '- [Microsoft AI Principles](https://www.microsoft.com/en-us/ai/responsible-ai)\n', '\n', '## Data Sets\n', '\n', '- [An ImageNet replacement for self-supervised pretraining without humans](https://www.robots.ox.ac.uk/~vgg/research/pass/)\n', '- [Huggingface Data Sets](https://huggingface.co/datasets)\n', '\n', '## Institutes\n', '\n', '- [Ada Lovelace Institute](https://www.adalovelaceinstitute.org/)\n', '- [European Centre for Algorithmic Transparency](https://algorithmic-transparency.ec.europa.eu/index_en)\n', '- [Center for Responsible AI](https://airesponsibly.com/)\n', '- [Montreal AI Ethics Institute](https://montrealethics.ai/)\n', '- [Munich Center for Technology in Society (IEAI)](https://ieai.mcts.tum.de/)\n', '- [Open Data Institute](https://theodi.org/)\n', '- [Stanford University Human-Centered Artificial Intelligence (HAI)](https://hai.stanford.edu)\n', '- [The Institute for Ethical AI & Machine Learning](https://ethical.institute/)\n', '- [University of Oxford Institute for Ethics in AI](https://www.oxford-aiethics.ox.ac.uk/)\n', '\n', '## Newsletters\n', '\n', '- [Import AI](https://jack-clark.net)\n', '- [The AI Ethics Brief](https://brief.montrealethics.ai)\n', '- [The Machine Learning Engineer](https://ethical.institute/mle.html) \n', '\n', '## Principles\n', '\n', ""- [European Commission's Guidelines for Trustworthy AI](https://ec.europa.eu/futurium/en/ai-alliance-consultation)\n"", ""- [IEEE's Ethically Aligned Design](https://ethicsinaction.ieee.org/)\n"", '- [The Institute for Ethical AI & Machine Learning: The Responsible Machine Learning Principles](https://ethical.institute/principles.html)\n', '\n', 'Additional:\n', '\n', '- [FAIR Principles](https://www.go-fair.org/fair-principles/) `Findability` `Accessibility` `Interoperability` `Reuse`\n', '\n', '## Podcasts\n', '\n', '- [The Human-Centered AI Podcast](https://podcasts.apple.com/us/podcast/the-human-centered-ai-podcast/id1499839858)\n', '- [Responsible AI Podcast](https://open.spotify.com/show/63Fx70r96P3ghWavisvPEQ)\n', '- [Trustworthy AI](https://marketing.truera.com/trustworthy-ai-podcast)\n', '\n', '## Reports\n', '\n', '- [Inferring Concept Drift Without Labeled Data, 2021](https://concept-drift.fastforwardlabs.com) `Drift`\n', '- [Interpretability, Fast Forward Labs, 2020](https://ff06-2020.fastforwardlabs.com) `Interpretability`\n', '- [State of AI](https://www.stateof.ai) - from 2018 up to now -\n', '\n', '## Tools\n', '\n', '### Bias\n', '\n', '- [balance](https://import-balance.org) `Python` `Facebook`\n', '\n', '### Causal Inference\n', '\n', '- [CausalAI](https://github.com/salesforce/causalai) `Python` `Salesforce`\n', '- [CausalNex](https://causalnex.readthedocs.io) `Python`\n', '- [CausalImpact](https://cran.r-project.org/web/packages/CausalImpact) `R`\n', '- [Causalinference](https://causalinferenceinpython.org) `Python`\n', '- [CIMTx: Causal Inference for Multiple Treatments with a Binary Outcome](https://cran.r-project.org/web/packages/CIMTx) `R`\n', '- [dagitty](https://cran.r-project.org/web/packages/dagitty) `R`\n', '- [DoWhy](https://github.com/Microsoft/dowhy) `Python` `Microsoft`\n', '- [mediation: Causal Mediation Analysis](https://cran.r-project.org/web/packages/mediation) `R`\n', '- [MRPC](https://cran.r-project.org/web/packages/MRPC) `R`\n', '\n', '### Differential Privacy\n', '\n', '- [BackPACK](https://toiaydcdyywlhzvlob.github.io/backpack) `Python`\n', '- [DataSynthesizer: Privacy-Preserving Synthetic Datasets](https://github.com/DataResponsibly/DataSynthesizer) `Python` `Drexel University` `University of Washington`\n', '- [diffpriv](https://github.com/brubinstein/diffpriv) `R`\n', '- [Diffprivlib](https://github.com/IBM/differential-privacy-library) `Python` `IBM`\n', '- [Discrete Gaussian for Differential Privacy](https://github.com/IBM/discrete-gaussian-differential-privacy) `Python` `IBM`\n', '- [Opacus](https://opacus.ai) `Python` `Facebook`\n', '- [PyVacy: Privacy Algorithms for PyTorch](https://github.com/ChrisWaites/pyvacy) `Python`\n', '- [SEAL](https://github.com/Microsoft/SEAL) `Python` `Microsoft`\n', '- [SmartNoise](https://github.com/opendp/smartnoise-core) `Python` `OpenDP`\n', '- [Tensorflow Privacy](https://github.com/tensorflow/privacy) `Python` `Google`\n', '\n', '### Drift\n', '\n', '- [Alibi Detect](https://github.com/SeldonIO/alibi-detect) `Python`\n', '- [Deepchecks](https://github.com/deepchecks/deepchecks) `Python`\n', '- [drifter](https://cran.r-project.org/web/packages/drifter/) `R`\n', '- [Evidently](https://github.com/evidentlyai/evidently) `Python`\n', '- [nannyML](https://github.com/NannyML/nannyml) `Python`\n', '\n', '### Fairness\n', '\n', ""- [Aequitas' Bias & Fairness Audit Toolkit](http://aequitas.dssg.io/) `Python`\n"", '- [AI360 Toolkit](https://github.com/Trusted-AI/AIF360) `Python` `R` `IBM`\n', '- [Fairlearn](https://fairlearn.org) `Python` `Microsoft`\n', '- [Fairmodels](https://fairmodels.drwhy.ai) `R`\n', '- [fairness](https://cran.r-project.org/web/packages/fairness/) `R`\n', '- [FairPAN - Fair Predictive Adversarial Network](https://modeloriented.github.io/FairPAN/) `R`\n', '- [Themis ML](https://github.com/cosmicBboy/themis-ml) `Python`\n', '- [What-If Tool](https://github.com/PAIR-code/what-if-tool) `Python` `Google`\n', '\n', '### Interpretability/Explicability\n', '\n', '- [AI360 Toolkit](https://github.com/Trusted-AI/AIF360) `Python` `R` `IBM`\n', '- [aorsf: Accelerated Oblique Random Survival Forests](https://cran.r-project.org/web/packages/aorsf/index.html) `R`\n', '- [breakDown: Model Agnostic Explainers for Individual Predictions](https://cran.r-project.org/web/packages/breakDown/index.html) `R`\n', '- [ceterisParibus: Ceteris Paribus Profiles](https://cran.r-project.org/web/packages/ceterisParibus/index.html) `R`\n', '- [DALEX: moDel Agnostic Language for Exploration and eXplanation](https://dalex.drwhy.ai) `Python` `R`\n', '- [DALEXtra: extension for DALEX](https://modeloriented.github.io/DALEXtra) `Python` `R`\n', '- [ecco](https://pypi.org/project/ecco/) [article](https://jalammar.github.io/explaining-transformers/) `Python`\n', '- [eli5](https://github.com/TeamHG-Memex/eli5) `Python`\n', '- [eXplainability Toolbox](https://ethical.institute/xai.html) `Python`\n', '- [ExplainerHub](https://explainerdashboard.readthedocs.io/en/latest/index.html) [in github](https://github.com/oegedijk/explainerdashboard) `Python` \n', '- [fasttreeshap](https://github.com/linkedin/fasttreeshap) `Python` `LinkedIn`\n', '- [FAT Forensics](https://fat-forensics.org/) `Python`\n', '- [intepretML](https://interpret.ml) `Python`\n', '- [interactions: Comprehensive, User-Friendly Toolkit for Probing Interactions](https://cran.r-project.org/web/packages/interactions/index.html) `R`\n', '- [kernelshap: Kernel SHAP](https://cran.r-project.org/web/packages/kernelshap/index.html) `R`\n', '- [lime: Local Interpretable Model-Agnostic Explanations](https://cran.r-project.org/web/packages/lime/index.html) `R`\n', '- [Shap](https://github.com/slundberg/shap) `Python`\n', '- [Shapash](https://github.com/maif/shapash) `Python`\n', '- [shapviz](https://cran.r-project.org/web/packages/shapviz/index.html) `R`\n', '- [Skater](https://github.com/oracle/Skater) `Python` `Oracle`\n', '- [survex](https://github.com/ModelOriented/survex) `R`\n', '- [pre: Prediction Rule Ensembles](https://cran.r-project.org/web/packages/pre/index.html) `R`\n', '- [XAI - An eXplainability toolbox for machine learning](https://github.com/EthicalML/xai) `Python` `The Institute for Ethical Machine Learning`\n', '- [Zennit](https://github.com/chr5tphr/zennit) `Python`\n', '\n', '### Performance (& Automated ML)\n', '\n', '- [automl: Deep Learning with Metaheuristic](https://cran.r-project.org/web/packages/automl/index.html) `R`\n', '- [AutoKeras](https://github.com/keras-team/autokeras) `Python`\n', '- [Auto-Sklearn](https://github.com/automl/auto-sklearn) `Python`\n', '- [DataPerf](https://sites.google.com/mlcommons.org/dataperf/) `Python` `Google`\n', '- [deepchecks](https://deepchecks.com) `Python`\n', '- [Featuretools](https://www.featuretools.com) `Python`\n', '- [forester](https://modeloriented.github.io/forester/) `R`\n', '- [metrica: Prediction performance metrics](https://adriancorrendo.github.io/metrica/) `R`\n', '- [NNI: Neural Network Intelligence](https://github.com/microsoft/nni) `Python` `Microsoft`\n', '- [TPOT](http://epistasislab.github.io/tpot/) `Python`\n', '- [Unleash](https://www.getunleash.io)\n', '\n', '### Responsible AI toolkit\n', '\n', '- [Dr. Why](https://github.com/ModelOriented/DrWhy) `R` `Warsaw University of Technology`\n', '- [Responsible AI Widgets](https://github.com/microsoft/responsible-ai-widgets) `R` `Microsoft`\n', '- [The Data Cards Playbook](https://pair-code.github.io/datacardsplaybook/)\n', '\n', '### Sustainability\n', '\n', '- [Code Carbon](https://github.com/mlco2/codecarbon) `Python`\n', '- [Azure Sustainability Calculator](https://appsource.microsoft.com/en-us/product/power-bi/coi-sustainability.sustainability_dashboard) `Microsoft`\n', '- [Computer Progress](https://www.computerprogress.com)\n', '\n', '## Reproducible Research\n', '\n', '- [Papers with Code](https://paperswithcode.com)\n', '- [Papers without Code](https://www.paperswithoutcode.com)\n', '\n', '## Regulations\n', '\n', '- [Data Protection Laws of the Word](https://www.dlapiperdataprotection.com)\n', '\n', '### European Union\n', '\n', '- [General Data Protection Regulation GDPR](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=celex%3A32016R0679) - Legal text for the EU GDPR regulation 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC\n', '- [GDPR.EU Guide](https://gdpr.eu/) - A project co-funded by the Horizon 2020 Framework programme of the EU which provides a resource for organisations and individuals researching GDPR, including a library of straightforward and up-to-date information to help organisations achieve GDPR compliance ([Legal Text](https://www.govinfo.gov/content/pkg/USCODE-2012-title5/pdf/USCODE-2012-title5-partI-chap5-subchapII-sec552a.pdf)).\n', '\n', '### United States\n', '\n', '- State consumer privacy laws: California ([CCPA](https://www.oag.ca.gov/privacy/ccpa) and its amendment, [CPRA](https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202120220AB1490)), Virginia ([VCDPA](https://lis.virginia.gov/cgi-bin/legp604.exe?212+sum+HB2307)), and Colorado ([ColoPA](https://leg.colorado.gov/sites/default/files/documents/2021A/bills/2021a_190_rer.pdf)).\n', '- Specific and limited privacy data laws: [HIPAA](https://www.cdc.gov/phlp/publications/topic/hipaa.html), [FCRA](https://www.ftc.gov/enforcement/statutes/fair-credit-reporting-act), [FERPA](https://www.cdc.gov/phlp/publications/topic/ferpa.html), [GLBA](https://www.ftc.gov/tips-advice/business-center/privacy-and-security/gramm-leach-bliley-act), [ECPA](https://bja.ojp.gov/program/it/privacy-civil-liberties/authorities/statutes/1285), [COPPA](https://www.ftc.gov/enforcement/rules/rulemaking-regulatory-reform-proceedings/childrens-online-privacy-protection-rule), [VPPA](https://www.law.cornell.edu/uscode/text/18/2710) and [FTC](https://www.ftc.gov/enforcement/statutes/federal-trade-commission-act).\n', '- [EU-U.S. and Swiss-U.S. Privacy Shield Frameworks](https://www.privacyshield.gov/welcome) - The EU-U.S. and Swiss-U.S. Privacy Shield Frameworks were designed by the U.S. Department of Commerce and the European Commission and Swiss Administration to provide companies on both sides of the Atlantic with a mechanism to comply with data protection requirements when transferring personal data from the European Union and Switzerland to the United States in support of transatlantic commerce.\n', '- [Executive Order on Maintaining American Leadership in AI](https://www.whitehouse.gov/presidential-actions/executive-order-maintaining-american-leadership-artificial-intelligence/) - Official mandate by the President of the US to \n', '[Privacy Act of 1974](https://www.justice.gov/opcl/privacy-act-1974) - The privacy act of 1974 which establishes a code of fair information practices that governs the collection, maintenance, use and dissemination of information about individuals that is maintained in systems of records by federal agencies.\n', '- [Privacy Protection Act of 1980](https://epic.org/privacy/ppa/) - The Privacy Protection Act of 1980 protects journalists from being required to turn over to law enforcement any work product and documentary materials, including sources, before it is disseminated to the public.\n', '- [AI Bill of Rights](https://www.whitehouse.gov/ostp/ai-bill-of-rights/) - The Blueprint for an AI Bill of Rights is a guide for a society that protects all people from IA threats based on five principles: Safe and Effective Systems, Algorithmic Discrimination Protections, Data Privacy, Notice and Explanation, and  Human Alternatives, Consideration, and Fallback.\n']"
Responsible+AI,aws-samples/aws-machine-learning-university-responsible-ai,aws-samples,https://api.github.com/repos/aws-samples/aws-machine-learning-university-responsible-ai,30,7,2,"['https://api.github.com/users/mimicarina', 'https://api.github.com/users/amazon-auto']",Python,2023-03-27T06:06:43Z,https://raw.githubusercontent.com/aws-samples/aws-machine-learning-university-responsible-ai/main/README.md,"['![logo](data/MLU_Logo.png)\n', '## Machine Learning University: Responsible AI\n', '\n', 'This repository contains __slides__, __notebooks__, and __data__ for the __Machine Learning University (MLU) Responsible AI__ class. Our mission is to make Machine Learning accessible to everyone. We have courses available across many topics of machine learning and believe knowledge of ML can be a key enabler for success. This class is designed to help you get started with Responsible AI, learn about widely used Machine Learning techniques, and apply them to real-world problems.\n', '\n', '## YouTube\n', 'Watch all Responsible AI video recordings in this [YouTube playlist](https://www.youtube.com/playlist?list=PL8P_Z6C4GcuVMxhwT9JO_nKuW0QMSJ-cZ) from our [YouTube channel](https://www.youtube.com/channel/UC12LqyqTQYbXatYS9AA7Nuw/playlists).\n', '\n', '## Course Overview\n', 'There are three lectures and one final project for this class.\n', '\n', '__Final Project:__ Practice working with a ""real-world"" dataset for the final project. Final project dataset is in the [data/final_project folder](https://github.com/aws-samples/aws-machine-learning-university-responsible-ai/tree/master/data/final_project). For more details on the final project, check out [this notebook](https://github.com/aws-samples/aws-machine-learning-university-responsible-ai/blob/main/notebooks/day_1/MLA-RESML-DAY1-FINAL-STUDENT-NB.ipynb).\n', '\n', '## Interactives/Visuals\n', 'Interested in visual, interactive explanations of core machine learning concepts? Check out our [MLU-Explain articles](https://mlu-explain.github.io/) to learn at your own pace! \n', '\n', '## Contribute\n', 'If you would like to contribute to the project, see [CONTRIBUTING](CONTRIBUTING.md) for more information.\n', '\n', '## License\n', ""The license for this repository depends on the section.  Data set for the course is being provided to you by permission of Amazon and is subject to the terms of the [Amazon License and Access](https://www.amazon.com/gp/help/customer/display.html?nodeId=201909000). You are expressly prohibited from copying, modifying, selling, exporting or using this data set in any way other than for the purpose of completing this course. The lecture slides are released under the CC-BY-SA-4.0 License.  This project is licensed under the Apache-2.0 License. See each section's LICENSE file for details.\n""]"
Responsible+AI,microsoft/responsible-ai-toolbox-tracker,microsoft,https://api.github.com/repos/microsoft/responsible-ai-toolbox-tracker,28,3,6,"['https://api.github.com/users/danyrouh', 'https://api.github.com/users/thuvanm', 'https://api.github.com/users/microsoftopensource', 'https://api.github.com/users/raghoshMSFT', 'https://api.github.com/users/mrfmendonca', 'https://api.github.com/users/microsoft-github-operations%5Bbot%5D']",TypeScript,2023-03-30T08:56:50Z,https://raw.githubusercontent.com/microsoft/responsible-ai-toolbox-tracker/main/README.md,"['![MIT license](https://img.shields.io/badge/License-MIT-blue.svg)\n', '\n', '# Responsible AI Tracker\n', '\n', 'Responsible AI Tracker is a JupyterLab Extension for managing, tracking, and comparing results of machine learning experiments for model improvement. Using this extension, users can view models, code, and visualization artifacts within the same framework enabling fast model iteration and evaluation processes. The extension is a work-in-progress research prototype to test and understand tooling functionalities and visualizations that can be helpful to data scientists. If you would like to propose new ideas for improvement feel free to contact the development team at [rai-toolbox@microsoft.com](mailto:rai-toolbox@microsoft.com) or create new issues in this repository.\n', '\n', 'This repo is a part of the [Responsible AI Toolbox](https://github.com/microsoft/responsible-ai-toolbox#responsible-ai-toolbox), a suite of tools providing a collection of model and data exploration and assessment user interfaces and libraries that enable a better understanding of AI systems. These interfaces and libraries empower developers and stakeholders of AI systems to develop and monitor AI more responsibly, and take better data-driven actions.\n', '\n', '### Main functionalities of the tracker include:\n', '\n', '- **Managing and linking model improvement artifacts**: the extension encourages clean and systematic data science practices by allowing users to associate the notebook used to create a model with the resulting model. These practices support careful model tracking and systematic experimentation.\n', '\n', '- **Disaggregated model evaluation and comparisons**: the model comparison table in the extension provides an in-depth comparison between the different models registered in the extension. This comparison contrasts performance results across different data cohorts and metrics, following a disaggregated approach, which goes beyond single-score performance numbers and highlights cohorts of data for which a model may perform worse than its older versions. Read more about disaggregated analysis [here](https://responsible-ai-toolbox-tracker.readthedocs.io/en/latest/basics_disaggregated.html).\n', '\n', '- **Integration with the Responsible AI Mitigations library**: as data scientists experiment and ideate different steps for model improvement, the [Responsible AI Mitigations Library](https://github.com/microsoft/responsible-ai-toolbox-mitigations) helps them implement different mitigation techniques in python that may improve model performance and can be targeted towards specified cohorts of interests.\n', '\n', '## Tour\n', '\n', 'Watch a [video tour](https://www.youtube.com/watch?v=jN6LWFzSLaU) of the Responsible AI Tracker and follow along using the notebooks and dataset [here](./tour).\n', '<p align=""center"">\n', '<img src=""./docs/imgs/RAI%20Tracker%20full%20view.png"" alt=""ResponsibleAITrackerOverview"" width=""750""/>\n', '\n', '\n', '\n', '## Installation\n', '\n', 'The Responsible AI Tracker can be deployed on Windows or Ubuntu, using anaconda or python.\n', '\n', '### The Responsible AI Tracker prerequisites:\n', '\n', '- [Nodejs](https://nodejs.org/)\n', '- [Python](https://www.python.org/downloads/) (versions supported 3.9 **to** 3.10.6)\n', '\n', '- JupyterLab\n', '  - If you use pip:\n', '  ```shell\n', '  pip install jupyterlab==3.6.1\n', '  ```\n', '  - If you use conda:\n', '  ```shell\n', '  conda install -c conda-forge jupyterlab==3.6.1\n', '  ```\n', '\n', '### The Responsible AI Tracker has two installation options:\n', '\n', '- The default installation only installs the essential packages.\n', '\n', '  ```shell\n', '  pip install raitracker\n', '  ```\n', '\n', '- The installation With the [all] flag installs the essential packages plus PyTorch, and Tensorflow.\n', '  ```shell\n', '  pip install raitracker[all]\n', '  ```\n', '\n', 'Installation through the JupyterLab Extension Manager coming soon. \n', '\n', '### Running\n', '\n', 'Start up JupyterLab using:\n', '\n', '```bash\n', 'jupyter lab\n', '```\n', '\n', 'The extension should be available in the left vertical bar. For ideas on getting started, watch the [video tour](https://www.youtube.com/watch?v=jN6LWFzSLaU) and follow along using the notebooks and dataset [here](./tour).\n', ' \n', '<details><summary>Dependencies</summary>\n', '<ul>\n', '\n', '<li>jupyterlab</li>\n', '<li>fluentui</li>\n', '<li>nodejs</li>\n', '<li>react</li>\n', '<li>redux</li>\n', '<li>lumino</li>\n', '<li>lodash</li>\n', '<li>babel</li>\n', '<li>codeMirror</li>\n', '<li>webpack</li>\n', '<li>mlflow</li>\n', '<li>numpy</li>\n', '<li>pandas</li>\n', '<li>scikit-learn</li>\n', '<li>pytorch</li>\n', '</ul>\n', '</details>\n', '\n', '---\n', '\n', '## Getting help\n', '\n', 'We encourage you to check the Responsible AI Tracker [documentation](https://responsible-ai-toolbox-tracker.readthedocs.io/en/latest/). \n', '\n', 'For Responsible AI Mitigations Library help see [Responsible AI Mitigations documentation](https://responsible-ai-toolbox-mitigations.readthedocs.io/en/latest/).  \n', '\n', 'See [here](https://github.com/microsoft/responsible-ai-toolbox-tracker/blob/main/SUPPORT.md) for further support information.\n', '\n', '\n', '### Bug reports\n', '\n', 'To report a bug please read the [guidelines](https://responsible-ai-toolbox-tracker.readthedocs.io/en/latest/) and then open a [Github issue](https://github.com/microsoft/responsible-ai-toolbox-tracker/issues/new). \n', '\n', '\n', '### Feature requests\n', '\n', 'We welcome suggestions for new features as they help make the project more useful for everyone. To request a feature please use the [feature request template](https://github.com/microsoft/responsible-ai-toolbox-tracker/labels/enhancement).\n', '\n', '### Contributing\n', '\n', 'To contribute code or documentation to the Responsible AI Tracker, please read the [contribution guidelines](https://github.com/microsoft/responsible-ai-toolbox-tracker/blob/main/CONTRIBUTING.md).\n', '\n', '---\n', '\n', '## Microsoft Open Source Code of conduct\n', '\n', 'The [Microsoft  Code of conduct](https://github.com/microsoft/responsible-ai-toolbox-tracker/blob/main/CODE_OF_CONDUCT.md) outlines expectations for participation in Microsoft-managed open source communities.\n', '\n', '\n', '## Trademarks\n', '\n', 'This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \n', 'trademarks or logos is subject to and must follow \n', ""[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\n"", 'Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\n', ""Any use of third-party trademarks or logos are subject to those third-party's policies.\n"", '\n', '## Research and Acknowledgements\n', '\n', '**Current Maintainers:** [ThuVan Pham](https://www.microsoft.com/en-us/research/people/thuvanp/), [Matheus Mendonça](https://github.com/mrfmendonca), [Besmira Nushi](https://github.com/nushib), [Rahee Ghosh Peshawaria](https://github.com/raghoshMSFT), [Marah Abdin](https://github.com/marah-abdin), [Mark Encarnación](https://github.com/markenc), [Dany Rouhana](https://github.com/danyrouh)\n', '\n', '**Past Maintainers:** [Irina Spiridonova](https://github.com/irinasp)\n', '\n', '**Research Contributors:** [Besmira Nushi](https://github.com/nushib), [Jingya Chen](https://www.jingyachen.net/), [Rahee Ghosh Peshawaria](https://github.com/raghoshMSFT), [ThuVan Pham](https://www.microsoft.com/en-us/research/people/thuvanp/), [Matheus Mendonça](https://github.com/mrfmendonca), [Ece Kamar](https://www.ecekamar.com/), [Dany Rouhana](https://github.com/danyrouh)']"
Responsible+AI,microsoft/responsible-ai-toolbox-genbit,microsoft,https://api.github.com/repos/microsoft/responsible-ai-toolbox-genbit,20,3,7,"['https://api.github.com/users/declangroves', 'https://api.github.com/users/mesameki', 'https://api.github.com/users/microsoftopensource', 'https://api.github.com/users/hal3', 'https://api.github.com/users/KinshukSengupta', 'https://api.github.com/users/dependabot%5Bbot%5D', 'https://api.github.com/users/microsoft-github-operations%5Bbot%5D']",Python,2023-04-05T06:50:33Z,https://raw.githubusercontent.com/microsoft/responsible-ai-toolbox-genbit/main/README.md,"['# GenBit: A Tool for Measuring Gender Bias in Text Corpora\n', '\n', 'This Responsible-AI-Toolbox-GenBit repo consists of a python library that aims to empower data scientists and ML developers to measure gender bias in their Natural Language Processing (NLP) datasets. \n', '\n', 'This repo is a part of the [Responsible AI Toolbox](https://github.com/microsoft/responsible-ai-toolbox#responsible-ai-toolbox), a suite of tools providing a collection of model and data exploration and assessment user interfaces and libraries that enable a better understanding of AI systems. These interfaces and libraries empower developers and stakeholders of AI systems to develop and monitor AI more responsibly, and take better data-driven actions.\n', '\n', ""With the increasing adoption of Natural Language Processing (NLP) models in real-world applications and products, it has become more critical than ever to understand these models' biases and potential harms they could cause to their end users. Many NLP systems suffer from various biases often inherited from the data on which these systems are trained. The prejudice is exhibited at multiple levels spilling from how individuals generate, collect, and label the information leveraged into datasets. Datasets, features, and rules in machine learning algorithms absorb and often magnify such biases present in datasets. Therefore, it becomes essential to measure preferences at the data level to prevent unfair model outcomes.\n"", '\n', 'This repository introduces the Gender Bias Tool (**G**en**B**i**t**), a tool to measure gender bias in NLP datasets. The main goal of GenBit is to analyze your corpora and compute metrics that give insights into the gender bias present in a corpus. The computations in this tool are based primarily on ideas from Shikha Bordia and Samuel R. Bowman, ""[Identifying and reducing gender bias in word-level language models](https://arxiv.org/abs/1904.03035)"" in the NAACL 2019 Student Research Workshop. \n', '\n', 'GenBit helps determine if gender is uniformly distributed across data by measuring the strength of association between a pre-defined list of gender definition words and other words in the corpus via co-occurrence statistics. The key metric it produces (the genbit_score) gives an estimate of the strength of association, on average, of any word in the corpus with a male, female, non-binary, transgender (trans), and cisgender (cis) gender definition words. The metrics that it provides can be used to identify gender bias in a data set to enable the production and use of more balanced datasets for training, tuning and evaluating machine learning models. It can also be used as a standalone corpus analysis tool.\n', '\n', '\n', 'GenBit supports 5 languages: English, German, Spanish, French, Italian and Russian. For English it provides metrics for both binary, non-binary, transgender, and cisgender bias; for the remaining four languages we currently only support binary gender bias. To deal with the challenges of grammatical gender in non-English languages, it leverages [stanza lemmatizers](https://stanfordnlp.github.io/stanza/lemma.html). It also uses the NLTK tokenization libraries. The full list of requirements are listed in [requirements.txt](requirements.txt)\n', '\n', '## Contents\n', '- [Install GenBit](#installation)\n', '- [Use GenBit](#use)\n', '- [Gendered Terms](#terms)\n', '- [Supported Metrics](#metrics)\n', '- [Metric Scores, Benchmarking and Interpretation](#interpret)\n', '- [Citation](#citation)\n', '- [Useful Links](#links)\n', '- [Contributing](#contributing)\n', '- [Trademarks](#trademarks)\n', '- [Authors and acknowledgment](#authors)\n', '\n', '# <a name=""installation""></a>\n', '## Install GenBit\n', '\n', 'The package can be installed from [pypi](https://pypi.org/project/genbit/) with:\n', '\n', '```\n', 'pip install genbit\n', '```\n', '\n', 'Tested and supported environments for the GenBit python package are:\n', '- Local usage on Windows\n', '- Local usage on Linux (Tested on Ubuntu 18.04 and Debian Buster 10)\n', '- As part of Azure functions installed using a [remote build](https://docs.microsoft.com/en-us/azure/azure-functions/functions-reference-python#remote-build-with-extra-index-url).\n', '\n', '\n', '\n', '# <a name=""use""></a>\n', '## Use GenBit\n', '\n', 'To use GenBit metrics, run the following code:\n', '\n', '```python\n', 'from genbit.genbit_metrics import GenBitMetrics\n', '\n', '# Create a GenBit object with the desired settings:\n', '\n', 'genbit_metrics_object = GenBitMetrics(language_code, context_window=5, distance_weight=0.95, percentile_cutoff=80)\n', '\n', '\n', ""# Let's say you want to use GenBit with a test sentence, you can add the sentence to GenBit:\n"", 'test_text = [""I think she does not like cats. I think he does not like cats."", ""He is a dog person.""]\n', '\n', 'genbit_metrics_object.add_data(test_text, tokenized=False)\n', '\n', '\n', '# To generate the gender bias metrics, we run `get_metrics` by setting `output_statistics` and `output_word_lists` to false, we can reduce the number of metrics created.\n', '\n', '\n', 'metrics = genbit_metrics_object.get_metrics(output_statistics=True, output_word_list=True)\n', '\n', '```\n', '\n', 'This process can be repeated as needed. Every separate string will be treated as an individual document, i.e. the context window will not reach beyond the limits of a single string. Therefore if a document is coherent, the content should be appended and added as a single string in the input list.\n', '\n', 'The metric works best on bigger corpora; therefore, we suggest analyzing at least 400 documents and 600 unique words (excluding stop words).\n', '\n', '\n', '# <a name=""terms""></a>\n', '## Gendered Terms\n', 'We have collected a [list of ""gendered"" terms for female, male, non-binary, binary, transgender, and cisgender groups](https://github.com/microsoft/responsible-ai-toolbox-genbit/tree/main/genbit/gendered-word-lists).\n', '\n', '- [Female words](https://github.com/microsoft/responsible-ai-toolbox-genbit/blob/main/genbit/gendered-word-lists/en/female.txt) contain terms such as ""her"" and ""waitress"".\n', '- [Male words](https://github.com/microsoft/responsible-ai-toolbox-genbit/blob/main/genbit/gendered-word-lists/en/male.txt) contain terms such as ""he"" and ""fireman"".\n', '- [Non-binary words](https://github.com/microsoft/responsible-ai-toolbox-genbit/blob/main/genbit/gendered-word-lists/en/non-binary.txt) contain terms such as ""sibling"" and ""parent"".\n', '- [Cisgender words](https://github.com/microsoft/responsible-ai-toolbox-genbit/blob/main/genbit/gendered-word-lists/en/cis.txt) contain terms such as ""cisgender"" and ""cissexual"".\n', '- [Transgender words](https://github.com/microsoft/responsible-ai-toolbox-genbit/blob/main/genbit/gendered-word-lists/en/trans.txt) contain terms such as ""trans woman"" and ""trans man"".\n', '\n', 'Please note that there is no explicit file for binary terms because the terms from male.txt and female.txt will be combined to form the binary word list.\n', '\n', 'The inclusion criterion for terms that are not inherently gendered (e.g., ""womb"" or ""homemaker"" in female.txt) should be: If the association of the term with a particular gender is only due to a social phenomenon that we want to approximate with our measurement (e.g., stereotyping, discrimination), then the term should be excluded. The categories are defined in terms of similar societal stereotypes/discrimination/bias/stigma because that is what GenBit would like to measure. \n', '\n', '### Creation of New Word Lists for Male/Female\n', 'The gender definition terms consist of pairs of corresponding entries – one for male gendered forms and one for female gendered forms. In general, for every male gendered form, there should be one or more female gendered forms, and for every female gendered form there should be one or more male gendered form. It should be noted, however, that there may be some rare cases, where there are entries that do not have a opposite gender equivalent form in some languages.\n', '\n', 'A gender definition entry is a concept that can only ever be attributed to a person of a specific gender; they represent words that are associated with gender by definition. They are unambiguous – they only ever have one possible semantic interpretation. They are primarily nouns or pronouns that refer to a person of a specific gender (e.g. he/she/zie, father/mother) but may occasionally include adjectives/adverbs/verbs (e.g. masculine / feminine, manly/womanly).\n', '\n', '\n', '### Creation of New Word Lists for Non-binary/Binary \n', 'The binary/non-binary dimension has 3 categories: \n', '- Binary: terms that typically refer to men or women, but not to non-binary people \n', '- Non-binary: terms that specifically refer to non-binary people and that are not typically used to refer to men or women - these terms require abandoning the idea of there being only two genders \n', '- All-gender: terms that can refer to people of any gender, including men, women, and non-binary people \n', '\n', '<b>Motivation for this split</b>: If we just put both non-binary and all-gender terms in the same txt file, we will not get any meaningful numbers out of GenBiT because the all-gender terms are so much more frequent and they\'re often used to refer to binary people, so we wouldn\'t really be measuring the anti-non-binary bias that we would like to measure.  On the other hand, if we only use the explicitly non-binary terms, we\'ll miss a lot of textual references to non-binary people that we would catch for binary people (e.g., we\'d catch ""brother"" and ""sister"", but not ""sibling""). \n', '\n', 'We combined the categories “non-binary” and “all-gender” into one word list. \n', '\n', '\n', '### Creation of New Word Lists for Transgender/Cisgender\n', '\n', 'The transgender category includes all terms that relate to transgressing gender boundaries or that are perceived as transgressing gender boundaries (for example, as evidenced by the type of stigma they receive). This is why the trans category includes terms for people that may not self-identify as trans, but that also transgress gender boundaries. \n', '\n', ""All terms in the non-binary category are also included in the trans category. Terms that are in the trans category, but not in the non-binary category, are terms that refer to transgressing gender boundaries, but that doesn't necessarily require abandoning the idea of there only being two genders. \n"", '\n', '\n', 'The collected terms could be found [here](https://github.com/microsoft/responsible-ai-toolbox-genbit/tree/main/genbit/gendered-word-lists), and you can explore [Lexicon Guidelines](https://github.com/microsoft/responsible-ai-toolbox-genbit/blob/main/genbit/gendered-word-lists/LEXICON_GUIDELINES.md) for creating lexicons to support new languages.\n', '\n', '\n', '\n', '# <a name=""metrics""></a>\n', '## Metrics\n', '\n', 'With the predefined ""gendered"" terms collected and available for you, GenBit computes a number of metrics which are functions of word co-occurrences with these predefined ""gendered"" words. As a reminder, the gendered words are divided into ""female"" words, ""male"" words, ""trans"" words (English only), ""cis"" words (English only), and ""non-binary"" words (English only). \n', '\n', '### Female vs Male Calculations\n', ""The main calculation is computing co-occurrences between words `w` in your provided text, and words `f` from the female list and words `m` from the male list. In all that follows, `c[w,F]` (respectively, `c[w,M]`) denotes the frequency that word `w` co-occurs with a word on the female (respectively, male) lists. These are naturally interpretable as probabilities by normalizing: `p(w|F) = c[w,F] / c[.,F]` where `c[.,F]` is the sum over all `w'` of `c[w',F]`.\n"", '\n', '### Non-binary vs Binary Calculations (English only)\n', ""The main calculation is computing co-occurrences between words `w` in your provided text, and words `nb` from the non-binary list and words `b` from the binary list. In all that follows, `c[w,nb]` (respectively, `c[w,b]`) denotes the frequency that word `w` co-occurs with a word on the binary (respectively, non-binary) lists. These are naturally interpretable as probabilities by normalizing: `p(w|nb) = c[w,nb] / c[.,nb]` where `c[.,nb]` is the sum over all `w'` of `c[w',nb]`.\n"", '\n', '### Transgender vs Cisgender Calculations (English only)\n', ""The main calculation is computing co-occurrences between words `w` in your provided text, and words `t` from the transgender list and words `c` from the cisgender list. In all that follows, `c[w,t]` (respectively, `c[w,t]`) denotes the frequency that word `w` co-occurs with a word on the transgender (respectively, cisgender) lists. These are naturally interpretable as probabilities by normalizing: `p(w|t) = c[w,t] / c[.,t]` where `c[.,t]` is the sum over all `w'` of `c[w',t]`.\n"", '\n', '### Supported Metrics\n', '\n', 'GenBit supports the following dataset metrics:\n', '- **avg_bias_ratio**: The average of the bias ratio scores per token. Formally, this is the average over all words `w` of `log( c[w,M] / c[w,F] )`.\n', '- **avg_bias_conditional**: The average of the bias conditional ratios per token. Similarly, this is the average over all words `w` of `log( p(w|M) / p(w|F) )`.\n', '- **avg_bias_ratio_absolute**. The average of the absolute bias ratio scores per token. Similar to `avg_bias_ratio`, this is the average over all `w` of `| log( c[w,M] / c[w,F] ) |`.\n', '- **avg_bias_conditional_absolute (genbit_score)**: The average of the absolute bias conditional ratios per token [note: this score is most commonly used as the key bias score in the literature]. Formally, this is the average over all `w` of `| log( p(w|M) / p(w|F) ) |`.\n', '- **avg_non_binary_bias_ratio**: the average of the token-level non-binary bias token ratios. Formally, this is the average over all words `w` of `log( (c[w,M] + c[w,F]) / c[w,NB] )`\n', '- **avg_non_binary_bias_conditional**: the average of the token-level non-binary bias conditional ratios. This is the average over all words `w` of `log( p(w|M) + p(w|F) / p(w|NB) )`\n', '- **avg_non_binary_bias_ratio_absolute**: the average of the token-level absolute non-binary bias ratio scores. Similar to `avg_non_binary_bias_ratio`, this is the average over all `w` of `| log( (c[w,M] + c[w,F]) / c[w,NB] ) |`\n', '- **avg_non_binary_bias_conditional_absolute**: the average of the token-level absolute non-binary bias conditional ratios.\n', '- **std_dev_bias_ratio**: Standard deviation of the bias ratio scores. This is the standard deviation that corresponds to `avg_bias_ratio`.\n', '- **std_dev_bias_conditional**: Standard deviation of the bias conditional scores. This is the standard deviation that corresponds to `avg_bias_conditional`.\n', '- **std_dev_non_binary_bias_ratio**: Standard deviation of non binary bias ratio scores. This is the standard deviation that corresponds to `avg_non_binary_bias_ratio`.\n', '- **std_dev_non_binary_bias_conditional**: Standard deviation of non binary bias conditional scores. This is the standard deviation that corresponds to `avg_non_binary_bias_conditional`.\n', '- **percentage_of_female_gender_definition_words**: The percentage of gendered (male, female and non-binary) words in the corpus that belong to the list of female gendered words\n', '- **percentage_of_male_gender_definition_words**: The percentage of gendered (male, female and non-binary) words in the corpus that belong to the list of male gendered words\n', '- **percentage_of_non_binary_gender_definition_words**: The percentage of gendered (male, female and non-binary words in the corpus that belong to the list of non-binary gendered words)\n', '\n', '### Metric Statistics\n', '\n', 'An optional set of `dict` containing statistics that can be included as part of the metrics dict object by the key `statistics`. These statistics will be returned if `output_statistics=True`.\n', '\n', '- **frequency_cutoff**: the percentile frequency cutoff value. Any co-occurrence counts that are above this frequency will be used in calculating the metrics\n', '- **num_words_considered**: the count of words that were included in calculating the metrics\n', '- **freq_of_female_gender_definition_words**: The number of times any of the female gendered words occur in the corpus\n', '- **freq_of_male_gender_definition_words**: The number of times any of the male gendered words occur in the corpus\n', '- **freq_of_non_binary_gender_definition_words**: The number of times any of the non-binary gendered words occur in the corpus\n', '- **jsd**: The Jensen-Shannon divergence between the word probabilities conditioned on male and female gendered words. This is `JSD( p(w|M) || p(w|F) )`, where `JSD(q||p)` is the average KL divergence between `q` and `m`, and between `p` and `m`, where m is the average distribution `(p+q)/2`.\n', '\n', '### Token Based Metrcs\n', '\n', 'A `dict` containing object containing per word bias statistics that can be included as part of the metrics dict object by the key `token_based_metrics`. These statistics will be returned if `output_word_list=True`.\n', 'By Token:\n', '\n', '- **frequency**: The frequency of the token (the number of times it appears in the document): `c[w,.]`\n', '- **male_count**: the number of times the word occurs within context of a male gendered word: `c[w,M]`\n', '- **female_count**: the number of times the word occurs within context of a female gendered word: `c[w,F]`\n', '- **non_binary_count**: the number of times the word occurs within context of a male gendered word: `c[w,NB]`\n', '- **female_conditional_prob**: the conditional probability of the token occurring in context with a female gendered word: `p(w|F)`\n', '- **male_conditional_prob**: the conditional probability of the token occurring in context with a male gendered word: `p(w|M)`\n', '- **bias_ratio**: log(male_count / female_count) the more positive the value, the more biased the word is towards being associated with male gendered word, the more negative the value the more biased the word is towards being associated with female gendered. Each value is `log( c[w,M] / c[w,F] )`. A value of zero means that this word is equally likely to appear co-occurring with words in the female list as the male list (positive indicates more co-occurrence with male words; negative indicates more co-occurrence with female words).\n', '- **non_binary_bias_ratio**: log((male_count + female_count) / non_binary_count) the more positive the value, the more biased the word is towards being associated with a binary gendered word. A value of zero means the word has non-binary gender bias associated with it\n', '- **bias_conditional_ratio**: log( male_cond_prob/female_cond_prob ) the more positive the value, the more biased the word is towards being associated with male gendered word, the more negative the value the more biased the word is towards being associated with female gendered. Each value is `log( p(w|M) / p(w|F) )`. A value of zero means that the probability of this word co-occurring with words in the female list is equal to the probability of co-occurring with words in the male list (positive indicates more likely co-occurrence with male words; negative indicates more likely co-occurrence with female words).\n', '- **non_binary_bias_conditional_ratio**: log( (male_cond_prob+female_cond_prob) / non_binary_cond_prob ) the more positive the value, the more biased the word is towards being associated with binary gendered words; the more negative the value the more biased the word is towards being associated with a non-binary gendered word. A value of zero means the word has no gender bias associated with it.\n', '\n', '# <a name=""interpret""></a>\n', '## Metric Scores, Benchmarking and Interpretation\n', '\n', ""A detailed benchmarking was conducted to evaluate Genbit's performance across different samples and quantities of gender bias in the corpora/datasets.\n"", '\n', 'The score interpretation depends on two key factors,\n', '\n', 'a) The percentage of male or female gendered definition words\n', '\n', 'b) Average bias condition absolute score (genbit_score)\n', '\n', 'It is observed that With the increase in the gendered definition word percentage the Genbit Score tends to surge, demonstrating the presence of gender bias.\n', '\n', 'A detailed benchmarking is conducted to study the correlation of score ranges across different datasets and to examine how genderbias could influence the overall machine learning task using multilingual parallel datasets[Winogender-schema, WinoMT, Curated-WinoMT, IMDB, TedTalks and few others].\n', '\n', '**Table1: GenBit V2 Reference Score Range for biased datasets.**\n', '| Language | Score Range | Data Size | Bias % Indicator<br>(moderate-high) |\n', '|:--------: |------------- |:------------: |:-----------------------------------: |\n', '| EN | 0.30-1.0+ | >400 Samples | > 0.30 |\n', '| IT | 0.50-1.5+ | >400 Samples | > 1.00 |\n', '| DE | 0.60-2.4+ | >200 Samples | > 0.60 |\n', '| ES | 0.60-2.5+ | >400 Samples | > 0.60 |\n', '| FR | 0.50-1.3+ | >200 Samples | > 0.60 |\n', '| RU | 0.80-2.3+ | >400 Samples | > 1.10+ |\n', '\n', ""The score ranges are derived from certain type of datasets and may vary with datasets. The bias indicator percentage can aid in understanding the degree of biased a dataset can be. A genbit score of greater than the value provided in the last column indicates observable gender bias in the data set that may impact any resulting model trained on the dataset negatively (we would dub this 'moderate' gender bias). The higher this value the great the gender bias in the dataset. \n"", '\n', ""It is recommended as a best practice to use both the **genbit_score** as well as observe the values given for **percentage_of_male/female/non-binary_gender_definition_words** to provide some indication of the reliability of the **genbit_score**. In a 'naturally' distributed dataset you would expect that the percentage values for the male/female/non-binary gender definition words not to be overly skewed e.g. if the value observed was 10% male_gender_definition_words, 90% female_gender_definition_words, 0% non-binary_gender_definition_words this would potentially indicate quality concerns with the dataset as such a extreme skew is unlikely (and definitely undesirable) in a dataset. \n"", '\n', '# <a name=""citation""></a>\n', '## Citation\n', '<a>\n', '<pre>\n', '@article{sengupta2021genbit,\n', '  title={GenBiT: measure and mitigate gender bias in language datasets},\n', '  author={Sengupta, Kinshuk and Maher, Rana and Groves, Declan and Olieman, Chantal},\n', '  journal={Microsoft Journal of Applied Research},\n', '  year={2021},\n', '  volume={16},\n', '  pages={63--71}}\n', '}\n', '</pre>\n', '</a>\n', '<a href=""https://www.microsoft.com/en-us/research/uploads/prod/2021/10/MSJAR_Genbit_Final_Version-616fd3a073758.pdf"">Paper link</a>\n', '  \n', '  \n', '# <a name=""links""></a>\n', '## Useful Links\n', '\n', '+ [Get started](notebooks/quickstart_sample_notebook.ipynb) using a sample Jupyter notebook.\n', '+ [Identifying and Reducing Gender Bias in Word-Level Language Models](https://arxiv.org/pdf/1904.03035.pdf): Bordia and Bowman paper that describes the approach that GenBit is based on.\n', '+ [Winogender](https://github.com/rudinger/winogender-schemas) Winogender data set; we use samples from these dataset as part of the GenBit tests and in our sample Jupyter notebook.\n', '\n', '# <a name=""contributing""></a>\n', '## Contributing\n', '\n', 'This project welcomes contributions and suggestions.  Most contributions require you to agree to a\n', 'Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\n', 'the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n', '\n', 'When you submit a pull request, a CLA bot will automatically determine whether you need to provide\n', 'a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\n', 'provided by the bot. You will only need to do this once across all repos using our CLA.\n', '\n', 'This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\n', 'For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\n', 'contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n', '\n', '\n', '# <a name=""trademarks""></a>\n', '## Trademarks\n', '\n', 'This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \n', 'trademarks or logos is subject to and must follow \n', ""[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\n"", 'Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\n', ""Any use of third-party trademarks or logos are subject to those third-party's policies.\n"", '\n', '# <a name=""authors""></a>\n', '## Authors and acknowledgment\n', '\n', 'The original GenBit tool was co-authored by (listed in alphabetical order) Declan Groves, Chantal Olieman, Kinshuk Sengupta, David Riff, Eshwar Stalin, Marion Zepf.\n', '\n', 'The team members behind the open source release of the tool are (listed in alphabetical order) Chad Atalla, Hal Daumé III, Declan Groves, Mehrnoosh Sameki, Kinshuk Sengupta, and Marion Zepf.\n']"
Responsible+AI,microsoft/responsible-ai-toolbox-mitigations,microsoft,https://api.github.com/repos/microsoft/responsible-ai-toolbox-mitigations,33,3,11,"['https://api.github.com/users/mrfmendonca', 'https://api.github.com/users/akshara-msft', 'https://api.github.com/users/marah-abdin', 'https://api.github.com/users/irinasp', 'https://api.github.com/users/morrissharp', 'https://api.github.com/users/mesameki', 'https://api.github.com/users/microsoftopensource', 'https://api.github.com/users/danyrouh', 'https://api.github.com/users/ms-kashyap', 'https://api.github.com/users/dependabot%5Bbot%5D', 'https://api.github.com/users/akshararama']",Python,2023-03-21T13:09:07Z,https://raw.githubusercontent.com/microsoft/responsible-ai-toolbox-mitigations/main/README.md,"['# Responsible AI Mitigations\n', '\n', '\n', 'This Responsible-AI-Toolbox-Mitigations repo consists of a python library that aims to empower data scientists and ML developers to measure their dataset balance and representation of different dataset cohorts, while having access to mitigation techniques they could incorporate to mitigate errors and fairness issues in their datasets. Together with the measurement and mitigation steps, ML professionals are empowered to build more accurate and fairer models.\n', '\n', 'This repo is a part of the [Responsible AI Toolbox](https://github.com/microsoft/responsible-ai-toolbox#responsible-ai-toolbox), a suite of tools providing a collection of model and data exploration and assessment user interfaces and libraries that enable a better understanding of AI systems. These interfaces and libraries empower developers and stakeholders of AI systems to develop and monitor AI more responsibly, and take better data-driven actions.\n', '\n', '\n', '<p align=""center"">\n', '<img src=""./docs/imgs/responsible-ai-toolbox-mitigations.png"" alt=""ResponsibleAIToolboxMitigationsOverview"" width=""750""/>\n', '\n', 'The Responsible AI Mitigations Library helps AI practitioners explore different measurements and mitigation steps that may be most appropriate when the model underperforms for a given data cohort. The library currently has three modules:\n', '\n', '- **DataProcessing** offers mitigation techniques for improving model performance for specific cohorts.\n', '- **DataBalanceAnalysis** provides metrics for diagnosing errors that originate from data imbalance either on class labels or feature values.\n', '- **Cohort** provides classes for handling and managing cohorts, which allows the creation of custom pipelines for each cohort in an easy and intuitive interface. The module also provides techniques for learning different decoupled estimators (models) for different cohorts and combining them in a way that optimizes different definitions of group fairness.\n', '\n', '\n', 'In this library, we take a **targeted approach to mitigating errors** in Machine Learning models. This is complementary and different from the traditional blanket approaches which aim at maximizing a single-score performance number, such as overall accuracy, by merely increasing the size of traning data or model architecture. Since blanket approaches are often costly but also ineffective for improving the model in areas of poorest performance, with targeted approaches to model improvement we focus the improvement efforts in areas previously identified to have more errors and their underlying diagnoses of error. For example, if a practitioner has identified that the model is underperforming for a cohort of interest by using Error Analysis in the Responsible AI Dashboard, they may also continue the debugging process by finding out through Data Balance Analysis and find out that there is class imbalance for this particular cohort. To mitigate the issue, they then focus on improving class imbalance for the cohort of interest by using the Responsible AI Mitigations library. This and several other examples in the documentation of each mitigation function illustrate how targeted approaches may help practitioner best at mitigation giving them more control in the model improvement process.\n', '\n', '\n', '## Installation\n', '\n', 'Use the following pip command to install the Responsible AI Toolbox. Make sure you are using Python 3.7, 3.8, 3.9 or 3.10. If running in jupyter, please make sure to restart the jupyter kernel after installing. There are three installation options for the ``raimitigations`` package:\n', '\n', '* To install the minimum dependencies, use:\n', '\n', '```\n', 'pip install raimitigations\n', '```\n', '\n', '* To install the minimum dependencies + the packages required to run all of the notebooks in the ``notebooks/`` folder:\n', '\n', '```\n', 'pip install raimitigations[all]\n', '```\n', '\n', '* To install all the dependencies used for development (such as ``pytest``, for example), use:\n', '\n', '```\n', 'pip install raimitigations[dev]\n', '```\n', '\n', '## Documentation\n', '\n', 'To learn more about the supported dataset measurements and mitigation techniques covered in the **raimitigations** package, [please check out this documentation.](https://responsible-ai-toolbox-mitigations.readthedocs.io/en/latest/)\n', '\n', '\n', '\n', '## Data Balance Analysis: Examples\n', '\n', '- [Data Balance Analysis Walk Through](notebooks/databalanceanalysis/data_balance_overall.ipynb)\n', '- [Data Balance Analysis Adult Census Example](notebooks/databalanceanalysis/data_balance_census.ipynb)\n', '- [End to End Notebook](notebooks/data_balance_e2e.ipynb)\n', '\n', '## Data Processing/Mitigations: Examples\n', '\n', 'Here is a set of tutorial notebooks that aim to explain how to use each one of the mitigation\n', 'methods offered in the **dataprocessing** module.\n', '\n', '- [Encoders](notebooks/dataprocessing/module_tests/encoding.ipynb)\n', '- [Scalers](notebooks/dataprocessing/module_tests/scaler.ipynb)\n', '- [Basic Imputer](notebooks/dataprocessing/module_tests/basic_imputation.ipynb)\n', '- [Iterative Imputer](notebooks/dataprocessing/module_tests/iterative_imputation.ipynb)\n', '- [KNN Imputer](notebooks/dataprocessing/module_tests/knn_imputation.ipynb)\n', '- [Sequential Feature Selection](notebooks/dataprocessing/module_tests/feat_sel_sequential.ipynb)\n', '- [Feature Selection using Catboost](notebooks/dataprocessing/module_tests/feat_sel_catboost.ipynb)\n', '- [Identifying correlated features: tutorial](notebooks/dataprocessing/module_tests/feat_sel_corr_tutorial.ipynb)\n', '- [Data Rebalance using imblearn](notebooks/dataprocessing/module_tests/rebalance_imbl.ipynb)\n', '- [Data Rebalance using SDV](notebooks/dataprocessing/module_tests/rebalance_sdv.ipynb)\n', ""- [Using scikit-learn's Pipeline](notebooks/dataprocessing/module_tests/pipeline_test.ipynb)\n"", '\n', 'Here is a set of case study scenarios where we use the transformations available in the **dataprocessing**\n', 'module in order to train a model for a real-world dataset.\n', '\n', '- [Simple Example](notebooks/dataprocessing/module_tests/model_test.ipynb)\n', '- [Case Study 1](notebooks/dataprocessing/case_study/case1.ipynb)\n', '- [Case Study 2](notebooks/dataprocessing/case_study/case2.ipynb)\n', '- [Case Study 3](notebooks/dataprocessing/case_study/case3.ipynb)\n', '\n', '## Handling Cohorts\n', '\n', 'Here is a set of tutorial notebooks that aim to explain how to manage cohorts.\n', '\n', '- [Creating Single Cohorts](notebooks/cohort/cohort_definition.ipynb)\n', '- [Creating Different Pipelines for each Cohort](notebooks/cohort/cohort_manager.ipynb)\n', '- [Different Pre-processing Scenarios using cohorts](notebooks/cohort/cohort_manager_scenarios.ipynb)\n', '- [Using Decoupled Classifiers](notebooks/cohort/decoupled.ipynb)\n', '\n', 'Here is a set of case study notebooks showing how creating customized dataprocessing pipelines for each\n', 'cohort can help in some scenarios.\n', '\n', '- [Cohort Case Study 1](notebooks/cohort/case_study/case_1.ipynb)\n', '- [Cohort Case Study 1 - Rebalancing only specific cohorts](notebooks/cohort/case_study/case_1_rebalance.ipynb)\n', '- [Cohort Case Study 1 - Using RAI Toolbox](notebooks/cohort/case_study/case_1_dashboard.ipynb)\n', '- [Cohort Case Study 2](notebooks/cohort/case_study/case_2.ipynb)\n', '- [Cohort Case Study 3](notebooks/cohort/case_study/case_3.ipynb)\n', '- [Decoupled Classifier Case 1](notebooks/cohort/case_study/decoupled_class/case_1.ipynb)\n', '- [Decoupled Classifier Case 2](notebooks/cohort/case_study/decoupled_class/case_2.ipynb)\n', '- [Decoupled Classifier Case 3](notebooks/cohort/case_study/decoupled_class/case_3.ipynb)\n', '\n', '\n', '\n', '## Dependencies\n', '\n', '**RAI Toolbox Mitigations** uses several libraries internally. The direct dependencies are the following:\n', '\n', '- [Numpy](https://numpy.org/)\n', '- [Pandas](https://pandas.pydata.org/)\n', '- [SciPy](https://scipy.org/)\n', '- [Scikit Learn](https://scikit-learn.org/stable/index.html)\n', '- [ResearchPY](https://pypi.org/project/researchpy/)\n', '- [Statsmodels](https://www.statsmodels.org/stable/index.html)\n', '- [Imbalanced Learn](https://imbalanced-learn.org/stable/)\n', '- [SDV](https://pypi.org/project/sdv/)\n', '- [CatBoost](https://catboost.ai/en/docs/)\n', '- [XGBoost](https://xgboost.readthedocs.io/en/stable/python/python_intro.html)\n', '- [MLxtend](https://pypi.org/project/mlxtend/)\n', '- [UCI Dataset](https://pypi.org/project/uci-dataset/)\n', '\n', '## Contributing\n', '\n', 'This project welcomes contributions and suggestions. Most contributions require you to agree to a\n', 'Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\n', 'the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n', '\n', 'When you submit a pull request, a CLA bot will automatically determine whether you need to provide\n', 'a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\n', 'provided by the bot. You will only need to do this once across all repos using our CLA.\n', '\n', 'This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\n', 'For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\n', 'contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n', '\n', '### Installing Using ``dev`` Mode\n', '\n', 'After cloning this repo and moving to its root folder, install the package in editable mode with the development dependencies using:\n', '\n', '```console\n', '> pip install -e .[dev]\n', '```\n', '\n', '### Pre-Commit\n', '\n', 'This repository uses pre-commit hooks to guarantee that the code format is kept consistent. For development, make sure to\n', 'activate pre-commit before creating a pull request. Any code pushed to this repository is checked for code consistency using\n', 'Github Actions, so if pre-commit is not used when doing a commit, there is a chance that it fails in the format check workflow.\n', 'Using pre-commit will avoid this.\n', '\n', 'To use pre-commit with this repository, first install pre-commit (**NOTE:** when installing the package with the ``[dev]`` tag, the\n', '``pre-commit`` package will already be installed):\n', '\n', '```console\n', '> pip install pre-commit\n', '```\n', '\n', 'After installed, navigate to the root directory of this repository and activate pre-commit through the following command:\n', '\n', '```console\n', '> pre-commit install\n', '```\n', '\n', 'With pre-commit installed and activated, whenever you do a new commit, pre-commit will check all new code using the pre-commit hooks configured in the *.pre-commit-config.yaml* file, located in the root of the repository. Some of the hooks might make formatting changes to some of the files commited. If any file is changed or if any other hook fails, the commit will fail. If that happens, make the necessary modifications, add the files to the commit and try commiting one more time. Do this until all hooks are successful. Note that these same checks will be done after pushing anything, so if your commit was successful while using pre-commit, it will pass in the format check workflow as well.\n', '\n', '### Updating the Docs\n', '\n', 'The documentation is built using [Sphinx](https://www.sphinx-doc.org/en/master/), [Pandoc](https://pandoc.org/installing.html), and [Graphviz](https://graphviz.org/) (to build the class diagrams). Graphviz and Pandoc must be installed separately ([detailed instructions here for Graphviz](https://graphviz.org/download/) and [here for Pandoc](https://pandoc.org/installing.html)). On Linux, this can be done with `apt` or `yum` (depending on your distribution):\n', '\n', '```console\n', '> sudo apt install graphviz pandoc\n', '```\n', '\n', '```console\n', '> sudo yum install graphviz pandoc\n', '```\n', '\n', 'Make sure Graphviz and Pandoc are installed before recompiling the docs. After that, update the documentation files, which are all located inside the ```docs/``` folder. Finally, use:\n', '\n', '```console\n', '> cd docs/\n', '> make html\n', '```\n', '\n', 'To view the documentation, open the file ```docs/_build/html/index.html``` in your browser.\n', '\n', '**Note for Windows users:** if you are trying to update the docs in a Windows environment, you might get an error regarding the *_sqlite3* module:\n', '\n', '```\n', 'ImportError: DLL load failed while importing _sqlite3: The specified module could not be found.\n', '```\n', '\n', 'To fix this, following the instructions found [in this link](https://www.dev2qa.com/how-to-fix-importerror-dll-load-failed-while-importing-_sqlite3-the-specified-module-could-not-be-found/).\n', '\n', '\n', '## Support\n', '### How to file issues and get help\n', '\n', 'This project uses GitHub Issues to track bugs and feature requests. Please search the existing\n', 'issues before filing new issues to avoid duplicates.  For new issues, file your bug or\n', 'feature request as a new Issue.\n', '\n', 'For help and questions about using this project, please post your question in Stack Overflow using the ``raimitigations`` tag.\n', '\n', '### Microsoft Support Policy\n', '\n', 'Support for this package is limited to the resources listed above.\n', '\n', '## Trademarks\n', '\n', 'This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\n', 'trademarks or logos is subject to and must follow\n', ""[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\n"", 'Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\n', ""Any use of third-party trademarks or logos are subject to those third-party's policies.\n"", '\n', '## Research and Acknowledgements\n', '\n', '**Current Maintainers:** [Marah Abdin](https://github.com/marah-abdin), [Matheus Mendonça](https://github.com/mrfmendonca), [Dany Rouhana](https://github.com/danyrouh), [Mark Encarnación](https://github.com/markenc)\n', '\n', '**Past Maintainers:** [Akshara Ramakrishnan](https://github.com/akshara-msft), [Irina Spiridonova](https://github.com/irinasp)\n', '\n', '**Research Contributors:** [Besmira Nushi](https://github.com/nushib), [Rahee Ghosh Peshawaria](https://github.com/raghoshMSFT), [Ece Kamar](https://www.ecekamar.com/)\n']"
Responsible+AI,leestott/ResponsibleAI,leestott,https://api.github.com/repos/leestott/ResponsibleAI,2,3,1,['https://api.github.com/users/leestott'],Python,2021-02-18T15:58:41Z,https://raw.githubusercontent.com/leestott/ResponsibleAI/master/README.md,"['---\n', 'languages:\n', '  - python\n', 'products:\n', '  - Azure Machine Learning Service\n', 'description: ""Responsible AI 2020""\n', '---\n', '\n', '![App Overview](/docs/rai.jpeg)\n', '\n', '# Responsible AI 2020 - Detect if a patient needs treatment based on heart-disease data\n', '\n', '# Introduction\n', '\n', 'This repository wishes to show how using the latest machine learning features in Azure Machine Learning and Microsoft open-source toolkits we can put the responsible AI principles into practice.\n', 'These tools empower data scientists and developers to understand ML models, protect people and their data, and control the end-to-end ML process. \n', '\n', 'For this, we will develop a solution that wishes to detect if a person is suitable for receiving treatment for heart disease or not. We will use a dataset that will help us classify patients that have heart disease from those that doesn’t. Using this example, we will show how to ensure ethical, transparent and accountable use of AI in a medical scenario.\n', '\n', 'This example ilustrates how to put the responsible AI principles into practice throughout the different stages of Machine Learning pipeline (Preprocessing, Training/Evaluation, Register Model).\n', '\n', 'Within this repository, you will find all the resources needed to create and simulate a medical scenario using Azure Machine Learning Service with Responsible AI techniques such as:\n', '\n', '1. [Differential Privacy](https://github.com/opendifferentialprivacy)\n', '2. [FairLearn](https://github.com/fairlearn/fairlearn)\n', '3. [InterpretML](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-interpretability)\n', '4. [DataDrift](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-monitor-data-drift)\n', '\n', '> **The goal of this project is to detect if a person is suitable for receiving a treatment for heart disease.**\n', '\n', '# Objectives\n', '\n', '- Understand how Responsible AI techniques work.\n', '- Use Azure Machine learning Service to create a Machine Learning Pipeline with these Responsible AI techniques.\n', '- Prevent data exposure with differential privacy.\n', '- Mitigate model unfairness.\n', '- Interpret and explain models.\n', '\n', '# Why use Azure Machine Learning Service?\n', '\n', 'Azure Machine Learning Service give to use the capability to use MLOps techniques, it empowers data scientists and app developers to help bring ML models to production.\n', '\n', 'This MLOps functionalities that Azure Machine Learning have, enables you to track / version / audit / certify / re-use every asset in your ML lifecycle and provides orchestration services to streamline managing this lifecycle.\n', '\n', '![mlops](docs/ml-lifecycle.png)\n', '\n', '## What are the key challenges we wish to solve with MLOps?\n', '\n', '![mlops_flow](docs/mlops.png)\n', '\n', '**Model reproducibility & versioning**\n', '\n', '- Track, snapshot & manage assets used to create the model\n', '- Enable collaboration and sharing of ML pipelines\n', '\n', '**Model packaging & validation**\n', '\n', '- Support model portability across a variety of platforms\n', '- Certify model performance meets functional and latency requirements\n', '\n', '**Model auditability & explainability**\n', '\n', '- Maintain asset integrity & persist access control logs\n', '- Certify model behavior meets regulatory & adversarial standards\n', '\n', '**Model deployment & monitoring**\n', '\n', '- Release models with confidence\n', '- Monitor & know when to retrain by analyzing signals such as data drift\n', '\n', 'if you want to know more about how we have implemented the machine learning workflow using Azure Machine Learning Studio, please, see the following [file](docs/ResponsibleAI_Presentation.pdf)\n', '\n', '# Understanding (InterpretML)\n', '\n', '![InterpretML](docs/interpretML.png)\n', '\n', 'As ML integrates deeply into our day-to-day business processes, transparency is critical. Azure Machine Learning helps not only to understand model behavior, but also to assess and mitigate bias.\n', '\n', 'Interpretation and model’s explanation in Azure Machine Learning is based on the InterpretMLtoolset. It helps developers and data scientists to understand the models’ behavior and provide explanations about the decisions made during the model’s inference. Thus, it provides transparency to customers and business stakeholders.\n', '\n', 'Use model interpretation capability to:\n', '\n', '1. **Create accurate ML models.**\n', '\n', '2. **Understand the behavior of a wide variety of models, including deep neural networks, during the learning and inference phases.**\n', '\n', '![InterpretML](docs/interpretML_2.png)\n', '\n', '3. **Perform conditional analysis to determine the impact on model predictions when characteristic values are changed.**\n', '\n', '![InterpretML](docs/interpretML_3.png)\n', '\n', '## Evaluation and mitigation of model bias (FairLearn)\n', '\n', '![FairLearn](docs/fairlearn.png)\n', '\n', 'Today, a challenge with the creation of artificial intelligence systems is the inability to prioritize impartiality. By using Fairlearn with Azure Machine Learning,developers and data scientists can leverage specialized algorithms to ensure more unbiased results for everyone.\n', '\n', 'Use impartiality capabilities to:\n', '\n', '1. **Evaluate the bias of models during learning and implementation.**\n', '\n', '2. **Mitigate bias while optimizing model performance.**\n', '\n', '3. **Use interactive visualizations to compare a set of recommended models that mitigate bias.**\n', '\n', '![FairLearn](docs/fairlearn_2.png)\n', '\n', '![FairLearn](docs/fairlearn_3.png)\n', '\n', '# Protection (Differential Privacy)\n', '\n', '|                                                     |                                                     |     |\n', '| :-------------------------------------------------: | :-------------------------------------------------: | :-: |\n', '| ![Grid_1](docs/differential-privacy.png)   | ![Grid_2](docs/differential-privacy_2.png)  |\n', '| ![Grid_3](docs/differential-privacy_3.png) | ![Grid_4](docs/differential-privacy_4.png) |\n', '| ![Grid_5](docs/differential-privacy_5.png) | ![Grid_6](docs/differential-privacy_6.png) |\n', '\n', 'ML is increasingly used in scenarios that encompass sensitive information, such as census and patient medical data. Current practices, such as writing or masking data, may be limiting ML. To address this issue, confidential machine learning and differential privacy techniques can be used to help organizations create solutions while maintaining data privacy and confidentiality.\n', '\n', '## Avoid data exposure with differential privacy\n', '\n', ""By using the new Differential Privacy Toolkit with Azure Machine Learning, data science teams can create ML solutions that preserve privacy and help prevent the re-identification of a person's data. These differential privacy techniques have been developed in collaboration with researchers from the Institute of Quantitative Social Sciences (IQSS) and the Harvard School of Engineering.\n"", '\n', 'Differential privacy protects sensitive information with:\n', '\n', '1. **Statistical noise injection into the data to help prevent the disclosure of private information, without a significant loss of accuracy.**\n', '\n', '2. **Exposure risk management with budget monitoring for information used in individual consultations and with greater limitation of consultations, as appropriate.**\n', '\n', '## Data protection with sensitive machine learning\n', '\n', 'In addition to data privacy, organizations seek to ensure the security and confidentiality of all ML resources.\n', '\n', 'To enable deployment and learning of secure models, Azure Machine Learning provides a robust set of network and data protection capabilities. This includes support for Azure virtual networks, private links to connect to machine learning workspaces, dedicated compute hosts, and client managed keys for encryption in transit and at rest.\n', '\n', 'On this secure basis, Azure Machine Learning also enables Microsoft data science teams to build models with sensitive data in a secure environment, without the ability to view data. The confidentiality of all machine learning resources is preserved during this process. This approach is fully compatible with open source machine learning frameworks and a wide range of hardware options. We are pleased to offer these confidential machine learning capabilities to all developers and data scientists later this year.\n', '\n', '# Control (Data Drift)\n', '\n', '![Differential Privacy](docs/drift-ui.png)\n', '\n', 'To build responsibly, the ML development process must be repeatable, reliable, and responsible to stakeholders. Azure Machine Learning enables decision makers, auditors, and all ML lifecycle members to support a responsible process.\n', '\n', '## Tracking ML resources through audit logs\n', '\n', 'Azure Machine Learning provides capabilities to automatically track lineage and maintain an audit trail of ML resources. Details such as execution history, learning environment, and explanations of data and models are captured in a central log, allowing organizations to meet various auditing requirements.\n', '\n', '![Differential Privacy](docs/data_drift.png)\n', '\n', '## Brief explanation of used Azure Machine Learning Services\n', '\n', '| Technology                     | Description                                                                                                                          |\n', '| ------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------ |\n', '| Azure Machine Learning Service | Cloud services to train, deploy and manage machine learning models                                                                   |\n', '| AutoML                         | Process of automating the time consuming, iterative tasks of machine learning model development                                      |\n', '| Differential Privacy           | Process of protecting personal information and users identity                                      |\n', '| Interpret ML                   | Interpret a model by using an explainer that quantifies the amount of influence each feature contribues to the predicted label       |\n', ""| FairLearn                      | Python package that empowers developers of AI systems to assess their system's fairness and mitigate any observed unfairness issues. |\n"", '| DataDrift                      | Data drift is the change in model input data that leads to model performance degradation                                             |\n', '\n', '## Final Result Azure Machine Learning Pipeline\n', '\n', '### Initial Azure Machine Learning Pipeline\n', '\n', '![pipeline](./docs/pipeline.png)\n', '\n', '### Re-train Azure Machine Learning Pipeline with new model metrics validation\n', '\n', '![pipeline_retrain](./docs/pipeline_retrain_check_model_metrics.png)\n', '\n', '## Understanding the Heart-Disease dataset\n', '\n', 'This database contains 76 attributes, but all published experiments refer to using a subset of 14 of them. In particular, the Cleveland database is the only one that has been used by ML researchers to this date. The ""goal"" field refers to the presence of heart disease in the patient. It is integer valued from 0 (no presence) to 1.\n', '\n', 'Download scratch dataset from: http://archive.ics.uci.edu/ml/datasets/Heart+Disease or https://www.kaggle.com/ronitf/heart-disease-uci\n', '\n', '#### Original Columns Dataset:\n', '\n', '  - **age:** age in years\n', '  - **sex:**\n', '    - 0: female\n', '    - 1: male\n', '  - **chest_pain_type:** chest pain type\n', '    - 1: typical angina\n', '    - 2: atypical angina\n', '    - 3: non-anginal pain\n', '    - 4: asymptomatic\n', '  - **resting_blood_pressure:** resting blood pressure (in mm Hg on admission to the hospital)\n', '  - **cholesterol:** serum cholestoral in mg/dl\n', '  - **fasting_blood_sugar:** (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n', '  - **rest_ecg:** resting electrocardiographic results\n', '    - 0: normal\n', '    - 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n', ""    - 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n"", '  - **max_heart_rate_achieved:** maximum heart rate achieved\n', '  - **exercise_induced_angina:** exercise induced angina (1 = yes; 0 = no)\n', '  - **st_depression:** ST depression induced by exercise relative to rest\n', '  - **st_slope:** the slope of the peak exercise ST segment\n', '    - 1: upsloping\n', '    - 2: flat\n', '    - 3: downsloping\n', '  - **num_major_vessels:** number of major vessels (0-3) colored by flourosopy\n', '  - **thalassemia:**\n', '    - 3 = normal;\n', '    - 6 = fixed defect;\n', '    - 7 = reversable defect\n', '  - **target:** diagnosis of heart disease (angiographic disease status)\n', '    - 0: < 50% diameter narrowing\n', '    - 1: > 50% diameter narrowing\n', '\n', '### Attribution:\n', '\n', 'The authors of the databases have requested that any publications resulting from the use of the data include the names of the principal investigator responsible for the data collection at each institution. They would be:\n', '\n', '1. Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.\n', '2. University Hospital, Zurich, Switzerland: William Steinbrunn, M.D.\n', '3. University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.\n', '4. V.A. Medical Center, Long Beach and Cleveland Clinic Foundation:Robert Detrano, M.D., Ph.D.\n', '\n', '### Responsible AI Dataset\n', '\n', ""The dataset we use in this repository is a customized one. In the Getting Started section, we explain how it is generated and which transformations were applied to create our new dataset. The dataset is based on UCI Heart Disease data. The original UCI dataset has by default 76 columns, but Kaggle provides a version containing 14 columns. We used Kaggle one. In this notebook, we'll explore the heart disease dataset.\n"", '\n', 'As part of the exploratory analysis and preprocessing of our data, we have applied several techniques that help us understand the data. The insights discovered (visualizations) were uploaded into an Azure ML Experiment. As part of the study, we took our target variable, and we analyzed it and checked its interaction with other variables.\n', '\n', ""The original dataset doesn't have any personal or sensible data that we can use it to identify a person or mitigate fairness, just Sex and Age. Therefore, for the purpose of this project and to show the capabilities of Differential Privacy and Detect Fairness techniques, we have created a notebook to generate a custom dataset with the following new columns and schema:\n"", '\n', '#### Custom columns base on original dataset:\n', '\n', '#### Original Columns Dataset:\n', '\n', '  - **age (Original)**\n', '  - **sex (Original)**\n', '  - **chest_pain_type: chest pain type (Original)**\n', '  - **resting_blood_pressure (Original)**\n', '  - **cholesterol (Original)**\n', '  - **fasting_blood_sugar (Original)**\n', '  - **rest_ecg (Original)**\n', '  - **max_heart_rate_achieved (Original)**\n', '  - **exercise_induced_angina (Original)**\n', '  - **st_depression (Original)**\n', '  - **st_slope (Original)**\n', '  - **num_major_vessels (Original)**\n', '  - **thalassemia (Original)**\n', '  - **target (Original)**\n', '\n', '#### Custom Columns Dataset:\n', '\n', '  - **state (custom)**\n', '  - **city (custom)**\n', '  - **address (custom)**\n', '  - **postal code (custom)**\n', '  - **ssn (social security card) (custom)**\n', '  - **diabetic (custom)**\n', '    - 0: not diabetic\n', '    - 1: diabetic\n', '  - **pregnant (custom)**\n', '    - 0: not pregnant\n', '    - 1: pregnant\n', '  - **ashtmatic (custom)**\n', '    - 0: not ashtmatic\n', '    - 1: ashtmatic\n', '  - **smoker (custom)**\n', '    - 0: not smoker\n', '    - 1: smoker\n', '  - **observations (custom)**\n', '\n', ""To generate the information related to state, address, city, postal code we have downloaded it from **https://github.com/EthanRBrown/rrad**. From this repository we are able to get a list of real, random addresses that geocode successfully (tested on Google's Geocoding API service). The address data comes from the OpenAddresses project, and all the addresses are in the public domain. The addresses are deliberately not linked to people or businesses; the only guarantee is that they are real addresses that geocode successfully.\n"", '\n', 'This *custom dataset* will be the dataset that the Azure ML steps will use. The name that we set to it was ""complete_patients_dataset.csv"". You can find it on *./dataset*\n', '\n', '# Getting started\n', '\n', 'The solution has this structure:\n', '\n', '```\n', '.\n', '├── dataset\n', '│   ├── complete_patients_dataset.csv\n', '│   ├── heart_disease_preprocessed_inference.csv\n', '│   ├── heart_disease_preprocessed_train.csv\n', '│   ├── uci_dataset.csv\n', '│   └── uci_dataset.yml\n', '├── docs (documentation images)\n', '├── infrastructure\n', '│   ├── Scripts\n', '│   │    └── Deploy-ARM.ps1\n', '│   ├── deployment.json\n', '│   ├── DeployUnified.ps1\n', '│   └── README.md\n', '├── src\n', '│   ├── automated-ml (notebook to run AutoML)\n', '│   ├── dataset-generator (notebook to generate dataset)\n', '│   ├── deployment\n', '│   ├── detect-fairness\n', '│   ├── differential-privacy\n', '│   ├── installation-libraries (notebook to install dependecies)\n', '│   ├── mlops-pipeline\n', '│   ├── monitoring\n', '│   ├── notebooks-settings\n', '│   ├── preprocessing\n', '│   ├── retrain\n', '│   └── utils\n', '├── .gitignore\n', '└── README.md\n', '```\n', '\n', 'To run this sample you have to do this steps:\n', '\n', '1. **Create infrastructure**\n', '2. **Run notebook to install dependencies**\n', '3. **Run Dataset Generator Notebook**\n', '4. **Publish the pipeline**\n', '5. **Submit pipeline using API**\n', '6. **Activate Data Drift Detector**\n', '7. **Activate re-train Step**\n', '\n', '## 1. Create infrastructure\n', '\n', 'We need a infrastructure in Azure to run the experiment. You can read more about the necessary infrastructure [here](./infrastructure/README.md).\n', '\n', 'To facilitate the task of creating the infrastructure, you can run the `infrastructure/DeployUnified.ps1` script. You have to indicate the **Resource Group**, the **Location** and the **Suscription Id**.\n', '\n', 'The final view of the Azure resource group will be like the following image:\n', '\n', '![resource-group](./docs/rg-view.png)\n', '\n', ""**Note:** The services you see marked with a red line will be created in the next steps. Don't worry about it!\n"", '\n', '## 2. Install Project dependencies\n', '\n', '## Install Responsible AI Requirements \n', '\n', 'Each notebook contains an environment.yml file listing all the necessary python libraries which are associated and required for the notebook execution.We recommend you use a conda environment.\n', '\n', '**Here is the basic recipe for using Conda to manage a project specific software stack.**\n', '\n', '`(base) $ cd project-dir`\n', '\n', '`(base) $ conda env create --prefix ./env --file environment.yml`\n', '\n', '`(base) $ conda activate ./env # activate the environment`\n', '\n', '`(/path/to/env) $ conda deactivate # done working on project (for now!)`\n', '\n', 'There are more details below on creating your conda environment\n', '\n', '### Libraries Required\n', '\n', 'The following libraries are required\n', '\n', '- pylint\n', '- numpy\n', '- pandas\n', '- ipykernel\n', '- joblib\n', '- sklearn\n', '- azureml-sdk\n', '- azureml-sdk[automl]\n', '- azureml-widgets\n', '- azureml-interpret\n', '- azureml-contrib-interpret\n', '- interpret-community\n', '- azureml-monitoring\n', '- opendp-whitenoise\n', '- opendp-whitenoise-core\n', '- matplotlib\n', '- seaborn\n', '- pandas-profiling\n', '- fairlearn\n', '- azureml-contrib-fairness\n', '- azureml-datadrift\n', '\n', '## Using Conda for Environments\n', '\n', 'The Notebook will automatically find all Jupyter kernels installed on the connected compute instance. To add a kernel to the compute instance:\n', '\n', 'Select Open terminal in the Notebook toolbar.\n', '\n', 'Use the Visual Studio Code terminal window to create a new environment. For example:\n', '\n', '- **Conda commands to create local env by environment.yml:** `conda env create -f environment.yml`\n', '- **Set conda env into jupyter notebook:** `python -m ipykernel install --user --name <environment_name> --display-name ""Python (<environment_name>)""`\n', '- **Activate the environment after creating newenv:** `conda activate <environment_name>`\n', '\n', '### Adding New Kernels (Optional)\n', '\n', '- **Install pip and ipykernel package to the new environment and create a kernel for that conda env**\n', '`conda install pip`\n', '`conda install ipykernel`\n', '`python -m ipykernel install --user --name newenv --display-name ""Python (newenv)""`\n', '\n', 'Any of the available Jupyter Kernels can be installed. https://github.com/jupyter/jupyter/wiki/Jupyter-kernels\n', '\n', '### Installation of Python Libraries (Optional)\n', '\n', '**Use the following to install the libraries:** `pip --disable-pip-version-check --no-cache-dir install pylint`\n', '\n', '**Or inline within a Juputer Notebook use:** `!pip install numpy`\n', '\n', 'You can now execute the notebooks successfully.\n', '\n', '### Virtual environments to execute Azure Machine Learning notebooks using Visual Studio Codespaces.(Optional)\n', '\n', 'This repository contains a labs to help you get started with Creating and deploying Azure machine learning module.\n', '\n', '[![Open in Visual Studio Online](https://img.shields.io/endpoint?style=social&url=https%3A%2F%2Faka.ms%2Fvso-badge)](https://online.visualstudio.com/environments/new?name=ResponsibleAI&repo=leestott/ResponsibleAI)\n', '\n', '## Manually creating a VS Online Container (Optional)\n', '\n', ""To complete the labs, you'll need the following:\n"", '\n', ""- A Microsoft Azure subscription. If you don't already have one, you can sign up for a free trial at <a href ='https://azure.microsoft.com' target='_blank'>https://azure.microsoft.com</a> or a Student Subscription at <a href ='https://aks.ms/azureforstudents' target='_blank'>https://aka.ms/azureforstudents</a>.\n"", '\n', ""- A Visual Studio Codespaces environment. This provides a hosted instance of Visual Studio Code, in which you'll be able to run the notebooks for the lab exercises. To set up this environment:\n"", '\n', ""    1. Browse to <a href ='https://online.visualstudio.com' target='_blank'>https://online.visualstudio.com</a>\n"", '    2. Click **Get Started**.\n', '    3. Sign in using the Microsoft account associated with your Azure subscription.\n', ""    4. Click **Create environment**. If you don't already have a Visual Studio Online plan, create one. This is used to track resource utlization by your Visual Studio Online environments. Then create an environment with the following settings:\n"", '        - **Environment Name**: *A name for your environment - for example, **MSLearn-create-deploy-azure-ml-module**.*\n', '        - **Git Repository**: leestott/create-deploy-azure-ml-module\n', '        - **Instance Type**: Standard (Linux) 4 cores, 8GB RAM\n', '        - **Suspend idle environment after**: 120 minutes\n', '    5. Wait for the environment to be created, and then click **Connect** to connect to it. This will open a browser-based instance of Visual Studio Code.\n', '\n', 'The current Visual Studio Codespaces Environment is based on Debian 10 there are some limitation to the Azure ML SDK with linux at present. Error on some notebooks may occur ensure the correct libraries and versions are installed using !pip install and please check library dependencies.\n', '\n', '### Using Azure Machine learning Notebooks (Optional)\n', '\n', '- Simply download the folder structure and upload the entire content to Azure Machine Learning Notebook \n', '\n', '![aml notebook](docs/aml_notebook_upload.PNG)\n', '\n', '- You now need to create a new compute instance for your notebook environment \n', '\n', '![aml compute](docs/aml_compute.PNG)\n', '\n', '- You now need to install the AML Prequestites to the Notebook Compute Host, to do this simply open a notebook and then select the open terminal. \n', '\n', '![aml compute terminal](docs/notebook_terminal.PNG)\n', '\n', '- select the terminal and install all the requirements using pip install \n', '\n', '## Jupyter Notebooks\n', '\n', 'In this project we have inside src folder many directories with jupyter notebook that you have to execute to obtain and complete the objective of this repository.\n', 'The folder src have:\n', '\n', '1. **automated-ml:** automated-ml.ipynb and environment.yml\n', '2. **dataset-generator:** dataset-generator.ipynb and environment.yml\n', '3. **detect-fairness:** fairlearn.ipynb and environment.yml\n', '4. **differential-privacy:** differential-privacy.ipynb and environment.yml\n', '5. **mlops pipelines:**\n', '   1. explain_automl_model_local.ipynb\n', '   2. mlops-publish-pipeline.ipynb\n', '   3. mlops-submit-pipeline.ipynb\n', '   4. environment.yml\n', '6. **monitoring:** datadrift-pipeline.ipynb and environment.yml\n', '7. **preprocessing:** exploratory_data_analysis.ipynb and environment.yml\n', '\n', 'Our recommendation is to use dedicated Conda environments for each of the Notebooks due to library and version dependencies if you are running this on a local machine non devcontainer you will need to create the conda enviroments via using conda navigator or execute the Conda installation before do anything inside these notebooks.\n', '\n', '## 3. Run Dataset Generator\n', '\n', 'Run `src/dataset-generator/dataset-generator.ipynb` to create the project dataset made from UCI Heart-Disease dataset specifically to Responsible AI steps.\n', 'See the dataset generated in the **./dataset** folder\n', '\n', '## 4. Publish the pipeline\n', '\n', 'Run `src/mlops-pipeline/mlops-publish-pipeline.ipynb` to create a machine learning service pipeline with Responsible AI steps and MLOps techniques that runs jobs unattended in different compute clusters.\n', '\n', 'You can see the run in the Azure Machine Learning Services Portal in the pipelines section of the portal.\n', '\n', '![Pipelines in portal](docs/pipelines.jpg)\n', '\n', '## 5. Submit pipeline using API Rest\n', '\n', 'Run `src/mlops-pipeline/mlops-submit-pipeline.ipynb` to execute/invoke this publishes the pipeline via REST endpoint.\n', '\n', 'You can see the run in the Azure Machine Learning Services Portal in the pipelines section of the portal.\n', '\n', '![Pipelines in portal](docs/pipelines_runs.jpg)\n', '\n', '## 6. Activate Data Drift Detector\n', '\n', 'Run `src/monitoring/datadrift-pipeline.ipynb` to create and execute data drift detector. At the end of this notebook, you will be able to make a request with new data in order to detect drift\n', '\n', 'Go to Azure Machine Learning portal models section. In the details tab now you can see a new section about Data Drift Detector status and configuration.\n', '\n', '![Data Drift in portal](./src/monitoring/images/drift_service.png)\n', '\n', '## 7. Execute pipeline with retrain configuration\n', '\n', 'If Data Drift coefficient is greater than the configured threshold a new alert will be sent to the final user. In that moment, the user will can execute the re-train pipeline in order to improve the performance of the model taking into account the new collected data.\n', '\n', 'Go to Azure ML Portal Pipelines section. Click on the last pipeline version. Then, you will have to click on submit button. Now, you should see something like the following image:\n', '\n', '![Retrain pipeline in portal](docs/retrain_pipeline.png)\n', '\n', 'First, select an existing experiment or create a new one for this new pipeline execution.\n', 'Finally, in the same view, to do the retrain process correctly some parameters have to change:\n', '\n', '1. **use_datadrift** = False\n', '2. **retrain_status_differential_privacy_step** = True\n', '3. **retrain_status_preprocessing_step** = True\n', '3. **update_deployment_deploy_step** = True\n', '\n', 'Once the parameters are set, we have everything ready to execute the retraining process!\n', '\n', '![Retrain parameters in portal](/docs/retrain_pipeline_parameters.png)\n', '\n', '# References\n', '\n', '- [Azure Machine Learning(Azure ML) Service Workspace](https://docs.microsoft.com/en-us/azure/machine-learning/service/overview-what-is-azure-ml)\n', '- [Azure ML CLI](https://docs.microsoft.com/en-us/azure/machine-learning/service/reference-azure-machine-learning-cli)\n', '- [Azure Responsible AI](https://azure.microsoft.com/es-es/blog/build-ai-you-can-trust-with-responsible-ml/)\n', '- [Azure ML Samples](https://docs.microsoft.com/en-us/azure/machine-learning/service/samples-notebooks)\n', '- [Azure ML Python SDK Quickstart](https://docs.microsoft.com/en-us/azure/machine-learning/service/quickstart-create-workspace-with-python)\n', '- [Azure ML MLOps Quickstart](https://github.com/Microsoft/MLOps)\n', '- [Azure Machine learning](https://azure.microsoft.com/services/machine-learning)\n', '- [Create development environment for Machine learning](https://docs.microsoft.com/azure/machine-learning/service/how-to-configure-environment)\n', '- [AML Python SDK](https://docs.microsoft.com/azure/machine-learning/service/how-to-configure-environment)\n', '- [AML Pipelines](https://docs.microsoft.com/azure/machine-learning/service/how-to-create-your-first-pipeline)\n', '- [Getting started with Auto ML](https://docs.microsoft.com/azure/machine-learning/service/concept-automated-ml)\n', '- [Intro to AML – MS Learn](https://docs.microsoft.com/en-us/learn/modules/intro-to-azure-machine-learning-service)\n', '- [Automate model select with AML - MS Learn](https://docs.microsoft.com/en-us/learn/modules/automate-model-selection-with-azure-automl)\n', '- [Train local model with AML - MS Learn](https://docs.microsoft.com/en-us/learn/modules/train-local-model-with-azure-mls)\n', '\n', '**Tags: Azure Machine Learning Service, Machine Learning, Differential-Privacy, Fairlearn, MLOPs, Data-Drift, InterpretML**\n']"
Responsible+AI,visenger/Awesome-ML-Model-Governance,visenger,https://api.github.com/repos/visenger/Awesome-ML-Model-Governance,67,18,5,"['https://api.github.com/users/visenger', 'https://api.github.com/users/ionicsolutions', 'https://api.github.com/users/floer32', 'https://api.github.com/users/mikeldking', 'https://api.github.com/users/aenyne']",,2023-04-07T06:38:49Z,https://raw.githubusercontent.com/visenger/Awesome-ML-Model-Governance/main/README.md,"['# Awesome ML Model Governance\n', '\n', '## Model Governance, Ethics, Responsible AI\n', '\n', '1. [Book: ""Responsible AI"". 2022. by Patrick Hall, Rumman Chowdhury. O\'Reilly Media, Inc.](https://learning.oreilly.com/library/view/responsible-ai/9781098102425/)\n', '1. [Book: ""Practical Fairness"". 2020. By Aileen Nielsen. O\'Reilly Media, Inc.](https://learning.oreilly.com/library/view/practical-fairness/9781492075721/)\n', '1. [Book: ""Fairness and machine learning: Limitations and Opportunities."" Barocas, S., Hardt, M. and Narayanan, A., 2018.](https://fairmlbook.org/)\n', '1. [Book: ""The Framework for ML Governance"" by Kyle Gallatin. 2021.  O\'Reilly Media](https://learning.oreilly.com/library/view/the-framework-for/9781098100483/)\n', '1. [What are model governance and model operations? A look at the landscape of tools for building and deploying robust, production-ready machine learning models](https://www.oreilly.com/radar/what-are-model-governance-and-model-operations/)\n', '2. [Specialized tools for machine learning development and model governance are becoming essential. Why companies are turning to specialized machine learning tools like MLflow.](https://www.oreilly.com/ideas/specialized-tools-for-machine-learning-development-and-model-governance-are-becoming-essential)\n', '1. [What are model governance and model operations? – O’Reilly](https://www.oreilly.com/radar/what-are-model-governance-and-model-operations/)\n', '1. [AI Fairness 360, A Step Towards Trusted AI - IBM Research](https://www.ibm.com/blogs/research/2018/09/ai-fairness-360/)\n', '1. [Responsible AI](https://www.microsoft.com/en-us/ai/responsible-ai-resources)\n', '1. [Learn how to integrate Responsible AI practices into your ML workflow using TensorFlow](https://www.tensorflow.org/resources/responsible-ai)\n', '1. [ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)](https://facctconference.org/index.html)\n', '1. [Programming Fairness in Algorithms. Understanding and combating issues of fairness in supervised learning.](https://towardsdatascience.com/programming-fairness-in-algorithms-4943a13dd9f8)\n', '1. [Secure, privacy-preserving and federated machine learning in medical imaging](https://www.nature.com/articles/s42256-020-0186-1)\n', '1. [Explainable AI (Gartner Prediction for 2023)](https://www.gartner.com/en/conferences/apac/data-analytics-india/gartner-insights/rn-top-10-data-analytics-trends/explainable-ai)\n', ""1. [What We've Learned to Control. By Ben Recht](https://www.argmin.net/2020/06/29/tour-revisited/)\n"", '1. [Practical Data Ethics](https://ethics.fast.ai/)\n', '1. Vasudevan, Sriram and Kenthapadi, Krishnaram. [""LiFT: A Scalable Framework for Measuring Fairness in ML Applications""](https://arxiv.org/abs/2008.07433) (2020) - Code: [The LinkedIn Fairness Toolkit (LiFT)](https://github.com/linkedin/LiFT)\n', '1. [Four Principles of Explainable Artificial Intelligence (NIST Draft). Phillips, P.J., Hahn, A.C., Fontana, P.C., Broniatowski, D.A. and Przybocki, M.A., 2020.](https://nvlpubs.nist.gov/nistpubs/ir/2020/NIST.IR.8312-draft.pdf)\n', '1. [Data Ethics Canvas](https://theodi.org/article/data-ethics-canvas/). Helps identify and manage ethical issues – at the start of a project that uses data, and throughout. Also see [Ethics Canvas](https://web.archive.org/web/20210528013717/https://www.ethicscanvas.org/) for broader scope.\n', '1. [The Open Ethics Canvas by the Open Ethics](https://github.com/OpenEthicsAI/Canvas)\n', '1. [ABOUT ML](https://www.partnershiponai.org/about-ml/) - Annotation and Benchmarking on Understanding and Transparency of Machine learning Lifecycles.\n', '1. [Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit. ""Model Cards for Model Reporting"" (2019)](https://arxiv.org/abs/1908.06165)  - Code: [Model Card Toolkit](https://github.com/tensorflow/model-card-toolkit)\n', '1. [Navigate the road to Responsible AI – Gradient Flow Blog](https://gradientflow.com/navigate-the-road-to-responsible-ai/)\n', '1. [😈 Awful AI is a curated list to track current scary usages of AI - hoping to raise awareness](https://github.com/daviddao/awful-ai)\n', '1. [Seven legal questions for data scientists](https://www.oreilly.com/radar/seven-legal-questions-for-data-scientists/)\n', '1. [2020 in Review: 8 New AI Regulatory Proposals from Governments](https://syncedreview.com/2020/12/31/2020-in-review-8-new-ai-regulatory-proposals-from-governments/)\n', '1. [Model Governance resources](https://github.com/bnh-ai/resources)\n', '1. [ML Cards for D/MLOps Governance (The combination of code, data, model, and service cards for D/MLOps, as an integrated solution.)](https://databaseline.tech/ml-cards/)\n', '1. [To regulate AI, try playing in a sandbox](https://www.morningbrew.com/emerging-tech/stories/2021/05/26/regulate-ai-just-play-sandbox)\n', '1. [Biases in AI Systems. A survey for practitioners](https://queue.acm.org/detail.cfm?id=3466134)\n', '1. [Artificial Intelligence Incident Database](https://incidentdatabase.ai/)\n', '1. [Data Ethics Considerations for more Responsible AI](https://arize.com/data-ethics-in-africa/)\n', '1. [Book: Interpretable Machine Learning with Python (by Serg Masis)](https://datatalks.club/books/20210719-interpretable-machine-learning-with-python.html)\n', '1. [Fairness in Machine Learning](https://fairlearn.org/main/user_guide/fairness_in_machine_learning.html)\n', '1. [Paper: Hendrycks, Dan, Nicholas Carlini, John Schulman, and Jacob Steinhardt. ""Unsolved problems in ml safety.""(2021)](https://arxiv.org/pdf/2109.13916.pdf)\n', '\n', '\n', '# Security for ML\n', '\n', '1. [Cybersecurity for Data Science](https://www.coursera.org/learn/cybersecurity-for-data-science)\n', '1. [Artifical intelligence and machine learning security (by Microsoft)](https://docs.microsoft.com/en-us/security/engineering/failure-modes-in-machine-learning) The references therein are useful.\n', '1. [Evtimov, Ivan, Weidong Cui, Ece Kamar, Emre Kiciman, Tadayoshi Kohno, and Jerry Li. ""Security and Machine Learning in the Real World."" arXiv (2020).](https://arxiv.org/pdf/2007.07205.pdf)\n', '1. [Machine Learning Systems: Security](https://sahbichaieb.com/mlsystems-security/)\n', '1. [Enterprise Security and Governance MLOps (by Diego Oppenheimer)](https://youtu.be/JNZk8diyIuE)\n', '1. [Adversarial Machine Learning 101](https://github.com/mitre/advmlthreatmatrix/blob/master/pages/adversarial-ml-101.md#adversarial-machine-learning-101)\n', '1. [ATLAS - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://github.com/mitre/advmlthreatmatrix)\n', '\n', '\n', '# Reports\n', '\n', '1. [State of AI Ethics June 2020 Report by the Montreal AI Ethics Institute](https://montrealethics.ai/wp-content/uploads/2020/06/State-of-AI-Ethics-June-2020-report.pdf)\n', '2. [State of AI Ethics October 2020 Report by the Montreal AI Ethics Institute](https://montrealethics.ai/wp-content/uploads/2020/10/State-of-AI-Ethics-Oct-2020.pdf)\n', '3. [State of AI Ethics January 2021 Report by the Montreal AI Ethics Institute](https://montrealethics.ai/wp-content/uploads/2021/01/State-of-AI-Ethics-Report-January-2021.pdf)\n', '\n', '# Organizations\n', '\n', '1. [AI Ethics Impact Group: From Principles to Practice](https://www.ai-ethics-impact.org/en)\n', '1. [Responsible AI Institute](https://www.responsible.ai/)\n']"
Responsible+AI,PacktPublishing/Designing-Models-for-Responsible-AI,PacktPublishing,https://api.github.com/repos/PacktPublishing/Designing-Models-for-Responsible-AI,3,4,4,"['https://api.github.com/users/sharmi1206', 'https://api.github.com/users/amita-kapoor', 'https://api.github.com/users/Packt-ITService', 'https://api.github.com/users/davids-packt']",Python,2023-03-09T00:04:00Z,https://raw.githubusercontent.com/PacktPublishing/Designing-Models-for-Responsible-AI/main/README.md,"['# Designing-Models-for-Responsible-AI\n', 'Designing Models for Responsible AI\n', '\n', 'Chapter 1\n', '•\tkeras-2.7.0, Tensorflow-2.7.0\n', '•\tpip install adversarial-robustness-toolbox\n', '•\tpip install git+https://github.com/Koukyosyumei/AIJack\n', '\n', 'Reference :https://github.com/Koukyosyumei/AIJack/tree/main/src/aijack,\n', 'https://github.com/Trusted-AI/adversarial-robustness-toolbox\n', '\n', 'Chapter 2\n', '\n', '•\tpip install adversarial-robustness-toolbox\n', '•\tpip install presidio_analyzer\n', '•\tpip install presidio_anonymizer\n', '•\tpip install syft==0.2.9\n', '•\tpip install Pyfhel\n', '•\tpip install secml\n', '•\tpip install crypten\n', '•\tgit clone https://github.com/privacytrustlab/ml_privacy_meter.git , pip install -r requirements.txt, pip install -e\n', '•\tpip install diffprivlib\n', '•\tpip install tensorflow-privacy\n', '•\tpip install mia\n', '•\tpip install foolbox\n', '\n', 'References :\n', 'https://github.com/OpenMined/PySyft\n', 'https://github.com/ibarrond/Pyfhel\n', 'https://github.com/pralab/secml\n', 'https://github.com/facebookresearch/CrypTen\n', 'https://github.com/IBM/differential-privacy-library\n', 'https://github.com/tensorflow/privacy\n', 'https://github.com/bethgelab/foolbox\n', 'https://github.com/privacytrustlab/ml_privacy_meter.git \n', '\n', '\n', '\n']"
Responsible+AI,microsoft/responsible-ai-toolbox-privacy,microsoft,https://api.github.com/repos/microsoft/responsible-ai-toolbox-privacy,12,2,3,"['https://api.github.com/users/shrutitople', 'https://api.github.com/users/microsoftopensource', 'https://api.github.com/users/microsoft-github-operations%5Bbot%5D']",Python,2023-03-19T16:06:17Z,https://raw.githubusercontent.com/microsoft/responsible-ai-toolbox-privacy/main/README.md,"['# Empirical Estimation of Differential Privacy\r\n', '\r\n', 'This repository provides utilities for estimating DP-$\\varepsilon$ from the confusion matrix of a membership inference attack based on the paper <a href=""https://arxiv.org/abs/2206.05199"">Bayesian Estimation of Differential Privacy</a>.\r\n', '\r\n', '## Installation\r\n', '\r\n', 'Simply run the following command to install the privacy-estimates python package. It should install all the relevant dependencies as well.\r\n', '\r\n', '``` bash\r\n', 'pip install privacy-estimates\r\n', '```\r\n', '\r\n', '\r\n', '## Example\r\n', '\r\n', 'The following command takes the output of a membership inference attack on a target model or multiples models in the form of true positives (TP), true negatives (TN), false positives (FP) and false negatives (FN). It also requires the value for  $\\alpha$ which states the significance level of the estimate for two sided intervals of the estimated $\\varepsilon$ value.\r\n', '\r\n', 'For example, we can post-proces the attack outputs of a CNN trained on CIFAR10 with $(\\varepsilon = 10, \\delta = 10^{-5})$ by running\r\n', '\r\n', '``` bash\r\n', 'python scripts/estimate-epsilon.py --alpha 0.1 --delta 1e-5 --TP 487 --TN 1 --FP 512 --FN 0 \r\n', '```\r\n', '\r\n', 'This should take approximately 5 minutes and produce the following output\r\n', '\r\n', '``` bash\r\n', 'Method             Interval                Significance level  eps_lo  eps_hi\r\n', 'Joint beta (ours)  two-sided equal-tailed  0.100               0.145   6.399\r\n', 'Joint beta (ours)  one-sided               0.050               0.145   inf\r\n', 'Clopper Pearson    two-sided equal-tailed  0.100               0.000   inf\r\n', 'Clopper Pearson    one-sided               0.050               0.000   inf\r\n', 'Jeffreys           two-sided equal-tailed  0.100               0.000   inf\r\n', 'Jeffreys           one-sided               0.050               0.000   inf\r\n', '```\r\n', '\r\n', '\r\n', '## Tests\r\n', '\r\n', 'We provide a few test cases which can be run by\r\n', '\r\n', '``` bash\r\n', 'pytest .\r\n', '```\r\n', '\r\n', '# Contributing\r\n', '\r\n', 'This project welcomes contributions and suggestions. Most contributions require you to\r\n', 'agree to a Contributor License Agreement (CLA) declaring that you have the right to,\r\n', 'and actually do, grant us the rights to use your contribution. For details, visit\r\n', 'https://cla.microsoft.com.\r\n', '\r\n', 'When you submit a pull request, a CLA-bot will automatically determine whether you need\r\n', 'to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the\r\n', 'instructions provided by the bot. You will only need to do this once across all repositories using our CLA.\r\n', '\r\n', 'This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\r\n', 'For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\r\n', 'or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\r\n']"
Responsible+AI,salilkanitkar/responsible_ai_hackathon,salilkanitkar,https://api.github.com/repos/salilkanitkar/responsible_ai_hackathon,2,3,2,"['https://api.github.com/users/Nithanaroy', 'https://api.github.com/users/salilkanitkar']",Python,2022-05-02T13:43:24Z,https://raw.githubusercontent.com/salilkanitkar/responsible_ai_hackathon/master/README.md,"['# responsible_ai_hackathon\n', 'Repository for Responsible AI Hackathon related code (May 2020)\n', '\n', 'Run the Neural Network model on [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/salilkanitkar/responsible_ai_hackathon/master?filepath=models%2Fbasic-model%2Fnn-model.ipynb)\n', '\n', '## Dataset Credits\n', '```\n', '@inproceedings{roffo2016personality,\n', '  title={Personality in computational advertising: A benchmark},\n', '  author={Roffo, Giorgio and Vinciarelli, Alessandro},\n', '  booktitle={4 th Workshop on Emotions and Personality in Personalized Systems (EMPIRE) 2016},\n', '  pages={18},\n', '  year={2016}\n', '}\n', '```\n', '\n', '## Live Dashboard\n', '\n', 'Please visit https://bit.ly/ads-rec-fairness-dashboard to see the dashboard live. It may take a few seconds to load as it is running on a free hosting service with basic hardware.\n', '\n', '<a href=""https://bit.ly/ads-rec-fairness-dashboard"" target=""_blank"" rel=""noopener noreferrer""><img src=""https://i.imgur.com/ZQ26GxH.png"" alt=""Ads fairness dashboard screenshot"" target=""_blank"" /></a>\n']"
Responsible+AI,PacktPublishing/Building-Responsible-AI-with-Python,PacktPublishing,https://api.github.com/repos/PacktPublishing/Building-Responsible-AI-with-Python,6,2,2,"['https://api.github.com/users/davids-packt', 'https://api.github.com/users/Packt-ITService']",Python,2023-02-23T02:23:03Z,https://raw.githubusercontent.com/PacktPublishing/Building-Responsible-AI-with-Python/main/README.md,"['# Building-Responsible-AI-with-Python\n', 'Building Responsible AI with Python\n']"
Responsible+AI,SEPIA-Framework/sepia-assist-server,SEPIA-Framework,https://api.github.com/repos/SEPIA-Framework/sepia-assist-server,87,14,1,['https://api.github.com/users/fquirin'],Java,2023-03-21T09:26:15Z,https://raw.githubusercontent.com/SEPIA-Framework/sepia-assist-server/master/README.md,"['# SEPIA Assist-Server\n', 'Part of the [SEPIA Framework](https://sepia-framework.github.io/)  \n', '\n', '<p align=""center"">\n', '  <img src=""https://sepia-framework.github.io/img/SEPIA_connected.png"" alt=""S.E.P.I.A. Framework"" width=350/>\n', '</p>\n', '\n', 'This is the core server of the SEPIA Framework and basically the ""brain"" of the assistant. It includes multiple modules and microservices exposed via the Assist-API, e.g.:\n', '* User-account management\n', '* Database integration (e.g. Elasticsearch)\n', '* Natural-Language-Understanding (NLU) and Named-Entity-Recognition (NER) (works out-of-the-box for German and English, but the modular NLU chain can use APIs and Python scripts as well)\n', '* Conversation flow (aka interview-module)\n', '* Answer-module\n', '* Smart-services (integration of local-services like a to-do lists or cloud-services like a weather API with the NLU, conversation and answer modules)\n', '* Remote-actions (e.g. receive data from IoT devices or wake-word tools)\n', '* Embedded open-source Text-to-Speech integration (eSpeak, MaryTTS, picoTTS - Note: TTS can be handled via this server or inside the SEPIA client)\n', '* ... and more\n', '\n', 'The [SEPIA cross-platform-clients](https://github.com/SEPIA-Framework/sepia-html-client-app) can access the [RESTful Assist-API](https://github.com/SEPIA-Framework/sepia-docs/blob/master/API/assist-server.md) directly and exchange data in JSON format (e.g. for user authentication) or connect to the SEPIA chat-server to send and receive messages.\n', 'SEPIAs running on this server can log-in to the WebSocket chat-server the same way a user does and communicated via channels with multiple users (or devices) at the same time.\n', '\n', 'The SEPIA Assist-Server operates as your own, self-hosted cloud-service and is designed to work the same way no matter if you run it on a Raspberry Pi for a small group of users in a private network \n', 'or when you host it on multiple servers for a larger company network.\n']"
Responsible+AI,malikamalik/RAI-vNext-Preview,malikamalik,https://api.github.com/repos/malikamalik/RAI-vNext-Preview,0,13,3,"['https://api.github.com/users/RachelKellam', 'https://api.github.com/users/riedgar-ms', 'https://api.github.com/users/microsoftopensource']",,2022-01-25T11:08:29Z,https://raw.githubusercontent.com/malikamalik/RAI-vNext-Preview/main/README.md,"['# Azure Machine Learning Responsible AI Dashboard - Private Preview\n', '\n', 'Welcome to the private preview for the new Responsible AI dashboard in Azure Machine Learning (AzureML) SDK and studio. The following is a guide for you to onboard to the new capabilities. For questions, please contact mithigpe@microsoft.com.\n', '\n', '## What is this new feature?\n', '\n', 'AzureML currently supports both [model explanations](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-interpretability-aml) and [model fairness](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-fairness-aml) in public preview. As we expand our offerings under Responsible AI tools for AzureML users, this new feature brings pre-existing features and brand new offerings under one-stop-shop SDK package and studio UI dashboard:\n', '- Error Analysis (new): view and understand the error distributions of your model over your dataset via a decision tree map or heat map visualization.\n', '- Data Explorer: explore your dataset by feature sets and other metrics such as predicted Y or true Y\n', '- Model Statistics: explore the distribution of your model outcomes and performance metrics\n', '- Interpretability: view the aggregate and individual feature importances across your model and dataset\n', ""- Counterfactual What-If's (new): create automatically generated diverse sets of counterfactual examples for each datapoint that is minimally perturbed in order to switch its predicted class or output. Also create your own counterfactual datapoint by perturbing feature values manually to observe the new outcome of your model prediction.\n"", '- Causal Analysis (new): view the aggregate and individual causal effects of *treatment features* (features which you are interested in controlling) on the outcome in order to make informed real-life business decisions. See recommended treatment policies for segmentations of your population for features in your dataset to see the effect on your real-life outcomes. \n', '\n', 'This new feature offers users a new powerful and robust toolkit for understanding your model and data in order to develop your machine learning models responsibly, now all in one place and integrated with your AzureML workspace.\n', '\n', '❗ **Please note:** This initial version of the Responsible AI dashboard currently does not support the integration of fairness metrics. For fairness metrics, please refer to our existing offering [here.](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-fairness-aml)\n', '\n', '## Supported scenarios, models and datasets\n', '\n', ""`azureml-responsibleai` supports computation of Responsible AI insights for `scikit-learn` models that are trained on `pandas.DataFrame`. The `azureml-responsibleai` package accepts both models and SciKit-Learn pipelines as input as long as the model or pipeline implements a `predict` or `predict_proba` function that conforms to the `scikit-learn` convention. If not compatible, you can wrap your model's prediction function into a wrapper class that transforms the output into the format that is supported (`predict` or `predict_proba` of `scikit-learn`), and pass that wrapper class to modules in `azureml-responsibleai`.\n"", '\n', 'Currently, we support datasets having numerical and categorical features. The following table provides the scenarios supported for each of the four responsible AI insights:-\n', '\n', '| RAI insight | Binary classification | Multi-class classification | Multilabel classification | Regression | Timeseries forecasting | Categorical features | Text features | Image Features | Recommender Systems | Reinforcement Learning |\n', '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | -- |\n', '| Explainability | Yes | Yes | No | Yes | No | Yes | No | No | No | No |\n', '| Error Analysis | Yes | Yes | No | Yes | No | Yes | No | No | No | No |\n', '| Causal Analysis | Yes | No | No | Yes | No | Yes (max 5 features due to computational cost) | No | No | No | No |\n', '| Counterfactual | Yes | Yes | No | Yes | No | Yes | No | No | No | No |\n', '\n', 'This is all available via Python SDK or CLI.\n', '\n', '## Set Up\n', 'In this section, we will go over the basic setup steps that you need in order to generate Responsible AI insights for your models from SDK v2, CLI v2 and visualize the generated Responsible AI insights in [AML studio](https://ml.azure.com/).\n', '\n', '### Create an AzureML workspace\n', 'Create an AzureML workspace by using the [configuration notebook](https://github.com/Azure/MachineLearningNotebooks/blob/master/configuration.ipynb)\n', '\n', '### Install the required packages\n', 'In order to install `azureml-responsibleai` package you will need a python virtual environment. You can create a python virtual environment using `conda`.\n', '```c\n', 'conda create -n azureml_env python=3.8\n', 'activate azureml_env\n', '```\n', '\n', 'After activating your environment, if this is your first time running the RAI Dashboard in private preview then continue to the [setup instructions](https://github.com/Azure/RAI-vNext-Preview/blob/main/docs/Setup.md) to do a one-time setup for your workspace.\n', '\n', '\n', '\n', '### Generating Responsibleai AI Dashboard insights\n', 'Once you have created an Azure workspace and registered your components in the one-time setup above, you can create a Responsible AI dashboard via the CLI or SDK. Start here for `examples` [folder](examples) to get started.\n', '\n', '### Viewing your Responsible AI Dashboard in the AzureML studio portal\n', 'After generating the Responsible AI insights, you can view them in your associated workspace in AzureML studio, under your model registry.\n', '\n', '![01](images/01_model_registry.png)\n', '1. Go to your model registry in your AzureML studio workspace\n', ""2. Click on the model for which you've uploaded your Responsible AI insights\n"", '\n', '![02](images/02_model_details.png)\n', '3. Click on the tab for `Responsible AI dashboard (preview)` under your model details page\n', '\n', '![03](images/03_responsibleaitoolbox.png)\n', '4. Under the `Responsible AI dashboard (preview)` tab of your model details, you will see a list of your uploaded Responsible AI insights. You can upload more than one Responsible AI dashboard for each model. Each row represents one dashboard, with information on which components were uploaded to each dashboard (i.e. explanations, counterfactuals, etc).\n', '\n', '![04](images/04_dashboard.png)\n', '5. At anytime while viewing the dashboard, if you wish to return to the model details page, click on `Back to model details`\n', '<ol type=""A"">\n', '  <li>You can view the dashboard insights for each component filtered down on a cohort you specify (or view all the data with the global cohort). Hovering over the cohort name will show the number of datapoints and filters in that cohort as a tooltip.</li>\n', '  <li>Switch which cohort you are applying to the dashboard.</li>\n', '  <li>Create a new cohort based on filters you can apply in a flyout panel.</li>\n', '  <li>View a list of all cohorts created and duplicate, edit or delete them.</li>\n', ""  <li>View a list of all Responsible AI components you've uploaded to this dashboard as well as hiding components. The layout of the dashboard will reflect the order of the components in this list.</li>\n"", '</ol>\n', '\n', '❗ **Please note:** Error Analysis, if generated, will always be at the top of the component list in your dashboard. Selecting on the nodes of the error tree or tiles of the error heatmap will automatically generate a temporary cohort that will be populated in the components below so that you can easily experiment with looking at insights for different areas of your error distribution.\n', '\n', '![05](images/05_add_dashboard.png)\n', '6. In between each component you can add components by clicking the blue circular button with a plus sign. This will pop up a tooltip that will give you an option of adding whichever Responsible AI component you enabled with your SDK.\n', '\n', '#### Known limitations of viewing dashboard in AzureML studio\n', 'Due to the (current) lack of active compute, the dashboard in AzureML studio has fewer features than the dashboard generated with the open source package. To generate the full dashboard in a Jupyter python notebook, please download and use our [open source Responsible AI Dashboard SDK](https://github.com/microsoft/responsible-ai-widgets). \n', '\n', 'Some limitations in AzureML studio include:\n', '- Retraining of the Error analysis tree on different features is disabled\n', '- Switching the Error analysis heat map to different features is disabled\n', '- Viewing the Error analysis tree or heatmap on a smaller subset of your full dataset passed into the dashboard (requires retraining of the tree) is disabled\n', '- ICE (Individual Conditional Expectation) plots in the feature importance tab for explanations are disabled\n', '- Manually creating a What-If datapoint is disabled; you can only view the counterfactual examples already pre-generated by the SDK\n', '- Causal analysis individual what-if is disabled; you can only view the individual causal effects of each individual datapoint\n', '\n', 'However, if you create a dashboard in AzureML, and then download it to a Jupyter notebook, it will be fully featured when running in that notebook.\n', '\n', '## Responsible AI Dashboard walkthrough and sample notebooks\n', 'Please read through our [examples folder](examples) to see if this feature supports your use case. For more details about each individual component, please read through our brief [tour guide of the new Responsible AI dashboard capabilities.](https://github.com/microsoft/responsible-ai-widgets/blob/main/notebooks/responsibleaitoolbox-dashboard/tour.ipynb) \n', '\n', '## What Next?: How to join Private Preview 👀\n', 'We are super excited for you to try this new feature in AzureML! \n', '- Reach out to mithigpe@microsoft.com to enable your Azure subscription for this Private Preview feature.\n', '- Fill out this form - Private Preview sign up for [Responsible AI Dashboard in AzureML](https://forms.office.com/r/R6PmBCkyWb)\n', '\n', '## Contributing\n', '\n', 'This project welcomes contributions and suggestions.  Most contributions require you to agree to a\n', 'Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\n', 'the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n', '\n', 'When you submit a pull request, a CLA bot will automatically determine whether you need to provide\n', 'a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\n', 'provided by the bot. You will only need to do this once across all repos using our CLA.\n', '\n', 'This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\n', 'For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\n', 'contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n', '\n', '## Trademarks\n', '\n', 'This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \n', 'trademarks or logos is subject to and must follow \n', ""[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\n"", 'Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\n', ""Any use of third-party trademarks or logos are subject to those third-party's policies.\n"", '\n']"
Responsible+AI,cylynx/verifyml,cylynx,https://api.github.com/repos/cylynx/verifyml,17,2,4,"['https://api.github.com/users/jasonho-lynx', 'https://api.github.com/users/timlrx', 'https://api.github.com/users/CodesAreHonest', 'https://api.github.com/users/swanhl']",Python,2023-03-15T16:10:39Z,https://raw.githubusercontent.com/cylynx/verifyml/main/README.md,"['# VerifyML\n', '\n', 'VerifyML is an opinionated, open-source toolkit and workflow to help companies implement human-centric AI practices. It is built on 3 principles:\n', '\n', '- A git and code first approach to model development and maintenance.\n', ""- Automatic generation of model cards - machine learning documents that provide context and transparency into a model's development and performance.\n"", '- Model tests for validating performance of models across protected groups of interest, during development and in production.\n', '\n', '## Components\n', '\n', '![VerifyML Dataflow](https://github.com/cylynx/verifyml/raw/main/verifyml-dataflow.png)\n', '\n', 'At the core of the VerifyML workflow is a model card that captures 6 aspects of a model:\n', '\n', '- Model details\n', '- Considerations\n', '- Model / data parameters\n', '- Quantitative analysis\n', '- Explainability analysis\n', '- Fairness analysis\n', '\n', ""It is adapted from Google's Model Card Toolkit and expanded to include broader considerations such as fairness and explainability.\n"", '\n', 'A [model card editor](https://report.verifyml.com), provides a web-based interface to gather input and align stakeholders across product, data science, compliance.\n', '\n', 'Our Python toolkit supports data science workflows, and allows a custom model to be built and logged within the model card framework. The package also contains perfomance and fairness tests for model diagnostics, fairness and reliability checks.\n', '\n', 'Being a standard protobuf format, the model card can be translated to various outputs including a model report, trade-off comparison and even tests results summary.\n', '\n', '## Installation\n', '\n', 'The Model Card Toolkit is hosted on [PyPI](https://pypi.org/project/verifyml/), and can be installed with `pip install verifyml`.\n', '\n', '## Getting Started\n', '\n', '### Generate a model card\n', '\n', '![VerifyML Model Card Editor](https://github.com/cylynx/verifyml/raw/main/model-card-editor.png)\n', '\n', 'The [VerifyML card creator](https://report.verifyml.com/create) provides an easy way for teams to create and edit model cards in a WYSIWYG editor. Use it to bootstrap your model card or edit text records through a web browser. It is a client side application and no data gets stored on a server.\n', '\n', 'Alternatively, generate a model card with the python toolkit:\n', '\n', '```py\n', 'import verifyml.model_card_toolkit as mctlib\n', '\n', '# Initialize the Model Card Toolkit with a path to store generate assets\n', 'mct = mctlib.ModelCardToolkit(output_dir=""model_card_output"", file_name=""breast_cancer_diagnostic_model_card"")\n', 'model_card = mct.scaffold_assets()\n', '```\n', '\n', '### Populate the model card with details\n', '\n', '```py\n', '# You can add free text fields\n', ""model_card.model_details.name = 'Breast Cancer Wisconsin (Diagnostic) Dataset'\n"", '\n', '# Or use helper classes\n', 'model_card.model_parameters.data.append(mctlib.Dataset())\n', 'model_card.model_parameters.data[0].graphics.description = (\n', ""  f'{len(X_train)} rows with {len(X_train.columns)} features')\n"", 'model_card.model_parameters.data[0].graphics.collection = [\n', '    mctlib.Graphic(image=mean_radius_train),\n', '    mctlib.Graphic(image=mean_texture_train)\n', ']\n', '```\n', '\n', '### Save and export to html\n', '\n', '```py\n', 'html = mct.export_format(output_file=""example.html"")\n', 'display.display(display.HTML(html))\n', '```\n', '\n', '## Model Tests\n', '\n', 'Model tests provides an out of the box way to conduct checks and analysis on performance, explainability and fairness. The tests included in VerifyML are atomic functions that can be imported and run without a model card. However, by using it with a model card, it provides a way to standardize objectives and check for intended or unintended model biases. It also automates documentation and renders the insights to a business friendly report.\n', '\n', 'Currently, VerifyML provides 5 classes of tests:\n', '\n', '1. **Subgroup Disparity Test** - For a given metric, assert that the difference between the best and worst performing group is less than a specified threshold\n', '2. **Min/Max Metric Threshold Test** - For a given metric, assert that all groups should be below / above a specified threshold\n', '3. **Perturbation Test** - Assert that a given metric does not change significantly after perturbing on a specified input variable\n', '4. **Feature Importance Test** - Assert that certain specified variables are not included as the top n most important features\n', '5. **Data Shift Test** - Assert that the distributions of specified attributes are similar across two given datasets of interest\n', '\n', 'The detailed [model tests readme](https://github.com/cylynx/verifyml/blob/main/verifyml/model_tests/README.md) contains more information on the tests.\n', '\n', 'You can also easily create your own model tests by inheriting from the base model test class. See [DEVELOPMENT](https://github.com/cylynx/verifyml/blob/main/DEVELOPMENT.md) for more details.\n', '\n', '### Example usage\n', '\n', '```py\n', 'from verifyml.model_tests.FEAT import SubgroupDisparity\n', '\n', '# Ratio of false positive rates between age subgroups should not be more than 1.5\n', ""sgd_test = SubgroupDisparity(metric='fpr', method='ratio', threshold=1.5)\n"", 'sgd_test.run(output) # test data with prediction results\n', 'sgd_test.plot(alpha=0.05)\n', '```\n', '\n', '### Adding the test to the model card\n', '\n', '```py\n', 'import verifyml.model_card_toolkit as mctlib\n', '\n', 'mc_sgd_test = mctlib.Test()\n', 'mc_sgd_test.read_model_test(sgd_test)\n', 'model_card.fairness_analysis.fairness_reports[0].tests = [mc_smt_test]\n', '```\n', '\n', '## Schema\n', '\n', 'Model cards are stored as a protobuf format. The reference model card protobuf schema can be found in the [proto directory](https://github.com/cylynx/verifyml/tree/main/verifyml/model_card_toolkit/proto). A translated copy in json schema format is also made available for convenience in the [schema folder](https://github.com/cylynx/verifyml/tree/main/verifyml/model_card_toolkit/schema)\n', '\n', '## Templates\n', '\n', 'Model cards can be rendered into various reports through the use of templates. The template folder contains two html templates - a default model report and a compare template, and a default markdown model report.\n', '\n', '## Contributions and Development\n', '\n', 'Contributions are always welcome - check out [CONTRIBUTING](https://github.com/cylynx/verifyml/blob/main/CONTRIBUTING.md)\n', '\n', ""The package and it's functionalities can be easily extended to meet the needs of a team. Check out [DEVELOPMENT](https://github.com/cylynx/verifyml/blob/main/DEVELOPMENT.md) for more info.\n"", '\n', '## Prior Art\n', '\n', ""The model card in VerifyML is adapted from Google's [Model Card Toolkit](https://github.com/tensorflow/model-card-toolkit). It is backward compatible with v0.0.2 and expands on it by adding sections on explainability and fairness. You can specify the desired rendering template by specifying the `template_path` argument when calling the `mct.export_format` function. For example:\n"", '\n', '```py\n', 'mct.export_format(output_file=""example.md"", template_path=""path_to_my_template"")\n', '```\n', '\n', ""View the [templates' README](https://github.com/cylynx/verifyml/blob/main/verifyml/model_card_toolkit/template/README.md) for more information on creating your own jinja templates.\n"", '\n', '## References\n', '\n', '[1] https://arxiv.org/abs/1810.03993\n', '\n', '## License\n', '\n', 'VerifyML is licensed under the Apache License, Version 2.0. See [LICENSE](https://github.com/cylynx/verifyml/blob/main/LICENSE) for the full license text.\n', '\n', '## Generating Docs\n', '\n', 'Docs are generated using [pydoc-markdown](https://github.com/NiklasRosenstein/pydoc-markdown), and our configuration is specified in `pydoc-markdown.yml`. The package reads the yml file, then converts the referenced READMEs and code files into corresponding [mkdocs](https://www.mkdocs.org/) markdown files, together with a `mkdocs.yml` config file. These converted files can be found in a `build/docs` directory, which will appear after the commands below are run.\n', '\n', '### Preview\n', '\n', 'To preview the docs locally, run\n', '\n', '```bash\n', './docs.sh serve\n', '```\n', '\n', 'This creates doc files in `build/docs/`, then serves them at `localhost:8000`.\n', '\n', '### Build\n', '\n', 'To build the HTML files, run\n', '\n', '```bash\n', './docs.sh build\n', '```\n', '\n', 'This creates doc files in `build/docs/`, then creates their HTML equivalents in `build/html/`.\n', '\n', '### Details\n', '\n', 'To render Jupyter Notebooks in the docs, we use the [`mkdocs-jupyter`](https://github.com/danielfrg/mkdocs-jupyter) plugin, and reference the notebooks in `pydoc-markdown.yml` (e.g. `source: example.ipynb` in one of the entries).\n', '\n', 'However, because `pydoc-markdown` converts everything to Markdown files by default, only the notebook text would show up by default. Thus, some intermediate steps (/ hacks) are required for the notebook to render correctly:\n', '\n', '1. Build the docs, converting the notebook text into a Markdown file (e.g. `build/docs/example.md`)\n', ""2. Rename the built file's extension from Markdown back into a notebook format (e.g. `mv example.md example.ipynb` in bash)\n"", ""3. Edit the built `mkdocs.yml` file such that the notebook's entry points to the renamed file in step 2 (this is done by `convert_md_to_ipynb.py`)\n"", '\n', '`./docs.sh` handles these steps.\n']"
Responsible+AI,h2oai/xai_guidelines,h2oai,https://api.github.com/repos/h2oai/xai_guidelines,5,6,2,"['https://api.github.com/users/jphall663', 'https://api.github.com/users/navdeep-G']",Python,2022-09-21T12:49:32Z,https://raw.githubusercontent.com/h2oai/xai_guidelines/master/README.md,"['# Responsible Use Guidelines for Explainable Machine Learning\n', 'A proposal for a 180-minute hands-on tutorial at ACM FAT* 2020, Barcelona, Spain.  \n', '\n', 'All tutorial code and materials are available here: https://github.com/h2oai/xai_guidelines. All materials may be re-used and re-purposed, even for commerical applications, with proper attribution of the authors.\n', '\n', '#### For the tutorial outline, please see: [responsible_xai.pdf](responsible_xai.pdf).\n', '\n', '#### To use the code examples for this tutorial: \n', '\n', '1. Navigate to [https://aquarium.h2o.ai](https://aquarium.h2o.ai). \n', '2. Click `Create a new account` below the login. Follow the Aquarium instructions to create a new account.\n', '3. Check the registered email inbox and use the temporary password sent there to login to Aquarium. \n', '4. Click `Browse Labs` in the upper left.\n', '5. Find `Open Source MLI Workshop` and click `View Details`.\n', '6. Click `Start Lab` and wait for several minutes as a cloud server is provisioned for you.  \n', '7. Once your server is ready, click on the `Jupyter URL` at the bottom of your screen. \n', '8. Enter the token `h2o` at the top Jupyter security `Password or Token` text box.\n', '9. Click the `xai_guidelines` folder. (For those interested, the `patrick_hall_mli` folder contains resources from a 2018 FAT* tutorial.)\n', '10. You now have access to the tutorial materials. You may browse them at your own pace or wait for instructions. You may also come back to them at anytime using your Aquarium login. \n', '\n', '#### To view preliminary example code:\n', '* Guideline 2.1: [An explainable, but untrustworthy, model](https://nbviewer.jupyter.org/github/h2oai/xai_guidelines/blob/master/global_shap_resid.ipynb)\n', '* Guideline 2.3: [Augmenting surrogate models with direct explanations](https://nbviewer.jupyter.org/github/h2oai/xai_guidelines/blob/master/dt_surrogate_pd_ice.ipynb)\n', '* Corollary 2.3.1: [Augmenting LIME with direct explanations](https://nbviewer.jupyter.org/github/h2oai/xai_guidelines/blob/master/dt_shap_lime.ipynb)\n', '* Corollary 2.4.1: [Combining interpretable models and explanations](https://nbviewer.jupyter.org/github/h2oai/xai_guidelines/blob/master/dt_shap_lime.ipynb)\n', '* Corollaries 2.4.2 - 2.4.2: [Combining constrained models, explanations, and bias testing](https://nbviewer.jupyter.org/github/h2oai/xai_guidelines/blob/master/dia.ipynb)\n', '\n', '#### Preliminary tutorial slides: [Guidelines for Responsible Explainable ML](https://github.com/jphall663/kdd_2019/blob/master/main.pdf)\n', '\n', '#### Tutorial Instructors:\n', '\n', '**Patrick Hall**: Patrick Hall is senior director for data science products at H2O.ai where he focuses on increasing trust and understanding in machine learning through interpretable models, post-hoc explanations, model debugging, and bias testing and remediation. Patrick is also currently an adjunct professor in the Department of Decision Sciences at George Washington University, where he teaches graduate classes in data mining and machine learning. Prior to joining H2O.ai, Patrick held global customer facing roles and research and development roles at SAS Institute. Find out more about Patrick on [GitHub](https://github.com/jphall663), [Linkedin](https://www.linkedin.com/in/jpatrickhall/), or [Twitter](https://twitter.com/jpatrickhall).\n', '\n', '**Navdeep Gill**: Navdeep Gill is a senior data scientist and engineer at H2O.ai. Navdeep is a founding member of the interpretability team at H2O.ai and has worked on various other projects at H2O.ai including the open source h2o, automl, and h2o4gpu machine learning libraries. Before joining H2O.ai, Navdeep worked at Cisco, focusing on data science and software development and previous to that he researched neuroscience. Find out more about Navdeep on [GitHub](https://github.com/navdeep-G), \n', '[Linkedin](https://www.linkedin.com/in/navdeep-gill-b1729456/), or [Twitter](https://twitter.com/Navdeep_Gill_).\n', '\n', '**Nick Schmidt**: Nick Schmidt is the director of the AI Practice at BLDS, a leading fair-lending advisory firm. At BLDS, Nick concentrates on creating real-world ethical AI systems for some of the largest financial institutions in the world. Prior to BLDS, Nick worked as an analyst and consultant at several well-respected economic and financial firms. Find out more about Nick on [Linkedin](https://www.linkedin.com/in/nickpschmidt/).\n']"
Responsible+AI,MarkMcKinney/responsible-ai-hot-takes,MarkMcKinney,https://api.github.com/repos/MarkMcKinney/responsible-ai-hot-takes,8,0,1,['https://api.github.com/users/MarkMcKinney'],JavaScript,2022-03-19T04:36:20Z,https://raw.githubusercontent.com/MarkMcKinney/responsible-ai-hot-takes/main/README.md,"['# Responsible AI Hot Takes\n', '\n', 'Generate and tweet awesome #techtwitter content with OpenAI via Telegram.\n', '\n', ""After following [FireShipIO's tutorial](https://github.com/fireship-io/gpt3-twitter-bot/) and hearing of OpenAI's new Twitter bot policy, I decided to create a slightly less automatic way to run a AI-backed Twitter bot.\n"", '\n', 'Instead of the bot automatically Tweeting, it will instead send the user potential options before you settle on the final result.\n', '\n', '![Screenshot of Telegram Chat](telegram_chat.JPG)\n', '\n', 'Check out my bot here: [@AIHotTakes](https://twitter.com/AIHotTakes)']"
Responsible+AI,crownpku/Responsible-AI,crownpku,https://api.github.com/repos/crownpku/Responsible-AI,6,0,1,['https://api.github.com/users/crownpku'],Python,2021-05-04T17:41:47Z,https://raw.githubusercontent.com/crownpku/Responsible-AI/master/README.md,"['# Responsible AI\n', '\n', '![](imgs/responsible-ai.png)\n', '\n', 'This is a demo project of using Responsible AI technology provided by Google to build responsible machine learning applications.\n', '\n', '## Content\n', '\n', '### Define Problem\n', '\n', '#### Who is my ML system for?\n', '\n', 'The way actual users experience your system is essential to assessing the true impact of its predictions, recommendations, and decisions. Make sure to get input from a diverse set of users early on in your development process.\n', '\n', '### Construct and prepare data\n', '\n', '#### Am I using a representative dataset?\n', '\n', 'Is your data sampled in a way that represents your users (e.g. will be used for all ages, but you only have training data from senior citizens) and the real-world setting (e.g. will be used year-round, but you only have training data from the summer)?\n', '\n', '#### Is there real-world/human bias in my data?\n', '\n', 'Underlying biases in data can contribute to complex feedback loops that reinforce existing stereotypes.\n', '\n', '### Build and train model\n', '\n', '#### What methods should I use to train my model?\n', '\n', 'Use training methods that build fairness, interpretability, privacy, and security into the model.\n', '\n', '### Evaluate model\n', '\n', '#### How is my model performing?\n', '\n', 'Evaluate user experience in real-world scenarios across a broad spectrum of users, use cases, and contexts of use. Test and iterate in dogfood first, followed by continued testing after launch.\n', '\n', '![](/imgs/equal_accuracy.png)\n', '\n', '### Deploy and monitor\n', '\n', '#### Are there complex feedback loops?\n', '\n', 'Even if everything in the overall system design is carefully crafted, ML-based models rarely operate with 100% perfection when applied to real, live data. When an issue occurs in a live product, consider whether it aligns with any existing societal disadvantages, and how it will be impacted by both short- and long-term solutions.\n', '\n', '# Reference\n', 'https://www.tensorflow.org/resources/responsible-ai\n']"
Responsible+AI,gulfaraz/responsible_ai,gulfaraz,https://api.github.com/repos/gulfaraz/responsible_ai,0,2,1,['https://api.github.com/users/gulfaraz'],JavaScript,2020-03-06T17:03:10Z,https://raw.githubusercontent.com/gulfaraz/responsible_ai/master/README.md,"['[![stable: 0.3.0](https://img.shields.io/badge/stable-0.3.0-ED2E26.svg?style=flat-square)](https://github.com/gulfaraz/responsible_ai)\n', '[![code style: prettier](https://img.shields.io/badge/code_style-prettier-ff69b4.svg?style=flat-square)](https://github.com/prettier/prettier)\n', '\n', '# Responsible A.I.\n', '\n', 'Estimate the responsibility of your A.I. project in terms of fairness,\n', 'accountability, confidentiality and transparency.\n', '\n', '\n', '## Contributing\n', '\n', 'We use [Prettier](https://github.com/prettier/prettier) to keep our code clean.\n']"
Responsible+AI,AI-Global/design-assistant,AI-Global,https://api.github.com/repos/AI-Global/design-assistant,0,2,12,"['https://api.github.com/users/dijonron', 'https://api.github.com/users/colinchoix', 'https://api.github.com/users/Micheal-Nguyen', 'https://api.github.com/users/aaronlugo', 'https://api.github.com/users/hmp31', 'https://api.github.com/users/geverit4', 'https://api.github.com/users/ErikLigai', 'https://api.github.com/users/ridwan888', 'https://api.github.com/users/colinphil', 'https://api.github.com/users/sshh12', 'https://api.github.com/users/lucindan', 'https://api.github.com/users/AIGlobalDev']",JavaScript,2022-06-28T20:16:28Z,https://raw.githubusercontent.com/AI-Global/design-assistant/main/README.md,"['# Responsible AI System Assessment\n', '\n', '#### [By AI Global](https://ai-global.org/)\n', '\n', '## Development\n', '\n', '[Original Documentation](https://github.com/AI-Global/design-assistant/tree/master/docs)\n', '\n', '> You will need to use [vscode](https://code.visualstudio.com/) in order to use our auto-formatting tools and linting.\n', '\n', '#### Getting Started\n', '\n', 'Set the following environment variables:\n', '\n', '```\n', 'MONGODB_URL=mongodb+srv...secret...\n', 'REACT_APP_API_BASE_URL=http://localhost:5000\n', '```\n', '\n', 'Then run:\n', '\n', '```\n', '$ git clone https://github.com/AI-Global/design-assistant && cd design-assistant\n', '$ yarn install\n', '$ yarn watch:react\n', '```\n', '\n', 'To start the backend, open another terminal and do:\n', '\n', '```\n', '$ yarn watch:api\n', '```\n', '\n', '#### Layout\n', '\n', '- `/src` - the react app\n', '- `/src/views` - each UI page\n', '- `/src/Components` - reusable react components\n', '- `/public` - static assets (images, compiled JS libraries, etc)\n', '- `/api` - the express backend\n', '- `/.vscode` - shared vscode settings for the project\n', '\n', '#### Helpful Docs\n', '\n', '- [ExpressJS](https://expressjs.com/en/5x/api.html)\n', '- [Mongoose](https://mongoosejs.com/docs/guide.html)\n']"
Responsible+AI,AI-Global/ai-portal,AI-Global,https://api.github.com/repos/AI-Global/ai-portal,0,2,6,"['https://api.github.com/users/sshh12', 'https://api.github.com/users/lucindan', 'https://api.github.com/users/colinphil', 'https://api.github.com/users/Marthacz', 'https://api.github.com/users/dependabot%5Bbot%5D', 'https://api.github.com/users/ameyand98']",JavaScript,2021-04-30T20:12:34Z,https://raw.githubusercontent.com/AI-Global/ai-portal/master/README.md,"['# Responsible AI Portal\n', '\n', '#### [By RAI](https://responsible.ai/)\n', '\n', '[[View Developer Docs]](https://github.com/AI-Global/ai-portal/blob/master/docs/development.md) | [[View All Docs]](https://github.com/AI-Global/ai-portal/blob/master/docs/general.md)\n', '\n', '![screenshot](https://user-images.githubusercontent.com/6625384/107436102-b2d70d00-6af2-11eb-8a48-05d9a963696e.png)\n']"
Synthetic+Data,Belval/TextRecognitionDataGenerator,Belval,https://api.github.com/repos/Belval/TextRecognitionDataGenerator,2635,842,30,"['https://api.github.com/users/Belval', 'https://api.github.com/users/FHainzl', 'https://api.github.com/users/Enzodtz', 'https://api.github.com/users/hendraet', 'https://api.github.com/users/nicolasmetallo', 'https://api.github.com/users/jtwsmeal', 'https://api.github.com/users/AghilesAzzoug', 'https://api.github.com/users/bakrianoo', 'https://api.github.com/users/astrocket', 'https://api.github.com/users/dc-chengchao', 'https://api.github.com/users/elahe-dastan', 'https://api.github.com/users/euihyun-lee', 'https://api.github.com/users/iknoorjobs', 'https://api.github.com/users/jinmingteo', 'https://api.github.com/users/junxnone', 'https://api.github.com/users/mohamadmansourX', 'https://api.github.com/users/JulienCoutault', 'https://api.github.com/users/PyaePhyoKhant', 'https://api.github.com/users/rkcosmos', 'https://api.github.com/users/Hrazhan', 'https://api.github.com/users/SunHaozhe', 'https://api.github.com/users/luangtatipsy', 'https://api.github.com/users/YacobBY', 'https://api.github.com/users/FLming', 'https://api.github.com/users/bact', 'https://api.github.com/users/edwardpwtsoi', 'https://api.github.com/users/gachiemchiep', 'https://api.github.com/users/wangershi', 'https://api.github.com/users/yifeitao', 'https://api.github.com/users/zhenglilei']",Python,2023-04-09T19:48:06Z,https://raw.githubusercontent.com/Belval/TextRecognitionDataGenerator/master/README.md,"['# TextRecognitionDataGenerator [![CircleCI](https://circleci.com/gh/Belval/TextRecognitionDataGenerator/tree/master.svg?style=svg)](https://circleci.com/gh/Belval/TextRecognitionDataGenerator/tree/master) [![PyPI version](https://badge.fury.io/py/trdg.svg)](https://badge.fury.io/py/trdg) [![codecov](https://codecov.io/gh/Belval/TextRecognitionDataGenerator/branch/master/graph/badge.svg)](https://codecov.io/gh/Belval/TextRecognitionDataGenerator) [![Documentation Status](https://readthedocs.org/projects/textrecognitiondatagenerator/badge/?version=latest)](https://textrecognitiondatagenerator.readthedocs.io/en/latest/?badge=latest)\n', '\n', 'A synthetic data generator for text recognition\n', '\n', '## What is it for?\n', '\n', 'Generating text image samples to train an OCR software. Now supporting non-latin text! For a more thorough tutorial see [the official documentation](https://textrecognitiondatagenerator.readthedocs.io/en/latest/index.html).\n', '\n', '## What do I need to make it work?\n', '\n', 'Install the pypi package\n', '\n', '```\n', 'pip install trdg\n', '```\n', '\n', 'Afterwards, you can use `trdg` from the CLI. I recommend using a virtualenv instead of installing with `sudo`.\n', '\n', 'If you want to add another language, you can clone the repository instead. Simply run `pip install -r requirements.txt`\n', '\n', '## Docker image\n', '\n', 'If you would rather not have to install anything to use TextRecognitionDataGenerator, you can pull the docker image.\n', '\n', '```\n', 'docker pull belval/trdg:latest\n', '\n', 'docker run -v /output/path/:/app/out/ -t belval/trdg:latest trdg [args]\n', '```\n', '\n', 'The path (`/output/path/`) must be absolute.\n', '\n', '## New\n', '- Add `--stroke_width` argument to set the width of the text stroke (Thank you [@SunHaozhe](https://github.com/SunHaozhe))\n', '- Add `--stroke_fill` argument to set the color of the text contour if stroke > 0 (Thank you [@SunHaozhe](https://github.com/SunHaozhe))\n', '- Add `--word_split` argument to split on word instead of per-character. This is useful for ligature-based languages\n', '- Add `--dict` argument to specify a custom dictionary (Thank you [@luh0907](https://github.com/luh0907))\n', '- Add `--font_dir` argument to specify the fonts to use\n', '- Add `--output_mask` to output character-level mask for each image\n', '- Add `--character_spacing` to control space between characters (in pixels)\n', '- Add python module\n', '- Add `--font` to use only one font for all the generated images (Thank you [@JulienCoutault](https://github.com/JulienCoutault)!)\n', '- Add `--fit` and `--margins` for finer layout control\n', '- Change the text orientation using the `-or` parameter\n', ""- Specify text color range using `-tc '#000000,#FFFFFF'`, please note that the quotes are **necessary**\n"", '- Add support for Simplified and Traditional Chinese\n', '\n', '## How does it work?\n', '\n', 'Words will be randomly chosen from a dictionary of a specific language. Then an image of those words will be generated by using font, background, and modifications (skewing, blurring, etc.) as specified.\n', '\n', '### Basic (Python module)\n', '\n', 'The usage as a Python module is very similar to the CLI, but it is more flexible if you want to include it directly in your training pipeline, and will consume less space and memory. There are 4 generators that can be used.\n', '\n', '```py\n', 'from trdg.generators import (\n', '    GeneratorFromDict,\n', '    GeneratorFromRandom,\n', '    GeneratorFromStrings,\n', '    GeneratorFromWikipedia,\n', ')\n', '\n', '# The generators use the same arguments as the CLI, only as parameters\n', 'generator = GeneratorFromStrings(\n', ""    ['Test1', 'Test2', 'Test3'],\n"", '    blur=2,\n', '    random_blur=True\n', ')\n', '\n', 'for img, lbl in generator:\n', '    # Do something with the pillow images here.\n', '```\n', '\n', 'You can see the full class definition here:\n', '\n', '- [`GeneratorFromDict`](trdg/generators/from_dict.py)\n', '- [`GeneratorFromRandom`](trdg/generators/from_random.py)\n', '- [`GeneratorFromStrings`](trdg/generators/from_strings.py)\n', '- [`GeneratorFromWikipedia`](trdg/generators/from_wikipedia.py)\n', '\n', '### Basic (CLI)\n', '\n', '`trdg -c 1000 -w 5 -f 64`\n', '\n', 'You get 1,000 randomly generated images with random text on them like:\n', '\n', '![1](samples/1.jpg ""1"")\n', '![2](samples/2.jpg ""2"")\n', '![3](samples/3.jpg ""3"")\n', '![4](samples/4.jpg ""4"")\n', '![5](samples/5.jpg ""5"")\n', '\n', 'By default, they will be generated to `out/` in the current working directory.\n', '\n', '### Text skewing\n', '\n', 'What if you want random skewing? Add `-k` and `-rk` (`trdg -c 1000 -w 5 -f 64 -k 5 -rk`)\n', '\n', '![6](samples/6.jpg ""6"")\n', '![7](samples/7.jpg ""7"")\n', '![8](samples/8.jpg ""8"")\n', '![9](samples/9.jpg ""9"")\n', '![10](samples/10.jpg ""10"")\n', '\n', '### Text distortion\n', 'You can also add distorsion to the generated text with `-d` and `-do`\n', '\n', '![23](samples/24.jpg ""0"")\n', '![24](samples/25.jpg ""1"")\n', '![25](samples/26.jpg ""2"")\n', '\n', '### Text blurring\n', '\n', ""But scanned document usually aren't that clear are they? Add `-bl` and `-rbl` to get gaussian blur on the generated image with user-defined radius (here 0, 1, 2, 4):\n"", '\n', '![11](samples/11.jpg ""0"")\n', '![12](samples/12.jpg ""1"")\n', '![13](samples/13.jpg ""2"")\n', '![14](samples/14.jpg ""4"")\n', '\n', '### Background\n', '\n', 'Maybe you want another background? Add `-b` to define one of the three available backgrounds: gaussian noise (0), plain white (1), quasicrystal (2) or image (3).\n', '\n', '![15](samples/15.jpg ""0"")\n', '![16](samples/16.jpg ""1"")\n', '![17](samples/17.jpg ""2"")\n', '![23](samples/23.jpg ""3"")\n', '\n', 'When using image background (3). A image from the images/ folder will be randomly selected and the text will be written on it.\n', '\n', '### Handwritten\n', '\n', 'Or maybe you are working on an OCR for handwritten text? Add `-hw`! (Experimental)\n', '\n', '![18](samples/18.jpg ""0"")\n', '![19](samples/19.jpg ""1"")\n', '![20](samples/20.jpg ""2"")\n', '![21](samples/21.jpg ""3"")\n', '![22](samples/22.jpg ""4"")\n', '\n', 'It uses a Tensorflow model trained using [this excellent project](https://github.com/Grzego/handwriting-generation) by Grzego.\n', '\n', ""**The project does not require TensorFlow to run if you aren't using this feature**\n"", '\n', '### Dictionary\n', '\n', 'The text is chosen at random in a dictionary file (that can be found in the *dicts* folder) and drawn on a white background made with Gaussian noise. The resulting image is saved as [text]\\_[index].jpg\n', '\n', 'There are a lot of parameters that you can tune to get the results you want, therefore I recommend checking out `trdg -h` for more information.\n', '\n', '## Create images with Chinese text\n', '\n', 'It is simple! Just do `trdg -l cn -c 1000 -w 5`!\n', '\n', 'Generated texts come both in simplified and traditional Chinese scripts.\n', '\n', 'Traditional:\n', '\n', '![27](samples/27.jpg ""0"")\n', '\n', 'Simplified:\n', '\n', '![28](samples/28.jpg ""1"")\n', '\n', '## Create images with Japanese text \n', '\n', 'It is simple! Just do `trdg -l ja -c 1000 -w 5`!\n', '\n', 'Output \n', '\n', '![29](samples/29.jpg ""2"")\n', '\n', '\n', '## Add new fonts\n', '\n', 'The script picks a font at random from the *fonts* directory.\n', '\n', '| Directory | Languages |\n', '|:----|:-----|\n', '| fonts/latin | English, French, Spanish, German |\n', '| fonts/cn | Chinese |\n', '| fonts/ko | Korean |\n', '| fonts/ja | Japanese |\n', '| fonts/th | Thai |\n', '\n', 'Simply add/remove fonts until you get the desired output.\n', '\n', 'If you want to add a new non-latin language, the amount of work is minimal.\n', '\n', '1. Create a new folder with your language [two-letters code](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes)\n', '2. Add a .ttf font in it\n', '3. Edit `run.py` to add an if statement in `load_fonts()`\n', '4. Add a text file in `dicts` with the same two-letters code\n', '5. Run the tool as you normally would but add `-l` with your two-letters code\n', '\n', 'It only supports .ttf for now.\n', '\n', '## Benchmarks\n', '\n', 'Number of images generated per second.\n', '\n', '- Intel Core i7-4710HQ @ 2.50Ghz + SSD (-c 1000 -w 1)\n', '    - `-t 1` : 363 img/s\n', '    - `-t 2` : 694 img/s\n', '    - `-t 4` : 1300 img/s\n', '    - `-t 8` : 1500 img/s\n', '- AMD Ryzen 7 1700 @ 4.0Ghz + SSD (-c 1000 -w 1)\n', '    - `-t 1` : 558 img/s\n', '    - `-t 2` : 1045 img/s\n', '    - `-t 4` : 2107 img/s\n', '    - `-t 8` : 3297 img/s\n', '\n', '## Contributing\n', '\n', ""1. Create an issue describing the feature you'll be working on\n"", '2. Code said feature\n', '3. Create a pull request\n', '\n', '## Feature request & issues\n', '\n', 'If anything is missing, unclear, or simply not working, open an issue on the repository.\n', '\n', '## What is left to do?\n', '- Better background generation\n', '- Better handwritten text generation\n', '- More customization parameters (mostly regarding background)\n']"
Synthetic+Data,ydataai/ydata-synthetic,ydataai,https://api.github.com/repos/ydataai/ydata-synthetic,916,198,19,"['https://api.github.com/users/fabclmnt', 'https://api.github.com/users/jfsantos-ds', 'https://api.github.com/users/dependabot%5Bbot%5D', 'https://api.github.com/users/renovate%5Bbot%5D', 'https://api.github.com/users/gmartinsribeiro', 'https://api.github.com/users/portellaa', 'https://api.github.com/users/aquemy', 'https://api.github.com/users/vascoalramos', 'https://api.github.com/users/miriamspsantos', 'https://api.github.com/users/ubabe53', 'https://api.github.com/users/arunnthevapalan', 'https://api.github.com/users/strickvl', 'https://api.github.com/users/archity', 'https://api.github.com/users/ceshine', 'https://api.github.com/users/fanconic', 'https://api.github.com/users/crownpku', 'https://api.github.com/users/ricardodcpereira', 'https://api.github.com/users/rajeshai', 'https://api.github.com/users/mglcampos']",Python,2023-04-09T17:09:54Z,https://raw.githubusercontent.com/ydataai/ydata-synthetic/dev/README.md,"['![](https://img.shields.io/github/workflow/status/ydataai/ydata-synthetic/prerelease)\n', '![](https://img.shields.io/pypi/status/ydata-synthetic)\n', '[![](https://pepy.tech/badge/ydata-synthetic)](https://pypi.org/project/ydata-synthetic/)\n', '![](https://img.shields.io/badge/python-3.9%20%7C%203.10-blue)\n', '[![](https://img.shields.io/pypi/v/ydata-synthetic)](https://pypi.org/project/ydata-synthetic/)\n', '![](https://img.shields.io/github/license/ydataai/ydata-synthetic)\n', '\n', '<p align=""center""><img width=""200"" src=""https://user-images.githubusercontent.com/3348134/177604157-11181f6c-57e5-44b1-8f6c-774edbba5512.png"" alt=""Synthetic Data Logo""></p>\n', '\n', 'Join us on [![Discord](https://img.shields.io/badge/Discord-7289DA?style=for-the-badge&logo=discord&logoColor=white)](https://discord.gg/mw7xjJ7b7s)\n', '\n', '# YData Synthetic\n', 'A package to generate synthetic tabular and time-series data leveraging the state of the art generative models.\n', '\n', '## 🎊 We have **big news**: v1.0.0 is here\n', '> We have exciting news for you. The new version of `ydata-synthetic` include new and exciting features:\n', '  > - A conditional architecture for tabular data: CTGAN, which will make the process of synthetic data generation easier and with higher quality!\n', '  > - A new streamlit app that delivers the synthetic data generation experience with a UI interface\n', '\n', '## Synthetic data\n', '### What is synthetic data?\n', ""Synthetic data is artificially generated data that is not collected from real world events. It replicates the statistical components of real data without containing any identifiable information, ensuring individuals' privacy.\n"", '\n', '### Why Synthetic Data?\n', 'Synthetic data can be used for many applications:\n', '  - Privacy\n', '  - Remove bias\n', '  - Balance datasets\n', '  - Augment datasets\n', '\n', '# ydata-synthetic\n', 'This repository contains material related with Generative Adversarial Networks for synthetic data generation, in particular regular tabular data and time-series.\n', 'It consists a set of different GANs architectures developed using Tensorflow 2.0. Several example Jupyter Notebooks and Python scripts are included, to show how to use the different architectures.\n', '\n', '## Quickstart\n', 'The source code is currently hosted on GitHub at: https://github.com/ydataai/ydata-synthetic\n', '\n', 'Binary installers for the latest released version are available at the [Python Package Index (PyPI).](https://pypi.org/project/ydata-synthetic/)\n', '```commandline\n', 'pip install ydata-synthetic\n', '```\n', '\n', '### The UI guide for synthetic data generation\n', '\n', 'YData synthetic has now a UI interface to guide you through the steps and inputs to generate structure tabular data.\n', 'The streamlit app is available form *v1.0.0* onwards, and supports the following flows:\n', '- Train a synthesizer model\n', '- Generate & profile synthetic data samples\n', '\n', '#### Installation\n', '\n', '```commandline\n', 'pip install ydata-syntehtic[streamlit]\n', '```\n', '#### Quickstart\n', 'Use the code snippet below in a python file (Jupyter Notebooks are not supported):\n', '```python\n', 'from ydata_synthetic import streamlit_app\n', '\n', 'streamlit_app.run()\n', '```\n', '\n', 'Or use the file streamlit_app.py that can be found in the [examples folder](https://github.com/ydataai/ydata-synthetic/tree/master/examples/streamlit_app.py).\n', '\n', '```commandline\n', 'python -m streamlit_app\n', '```\n', '\n', 'The below models are supported:\n', '  - CGAN\n', '  - WGAN\n', '  - WGANGP\n', '  - DRAGAN\n', '  - CRAMER\n', '  - CTGAN\n', '\n', '[![Watch the video](assets/streamlit_app.png)](https://youtu.be/ep0PhwsFx0A)\n', '\n', '### Examples\n', 'Here you can find usage examples of the package and models to synthesize tabular data.\n', '  \n', '  - Tabular synthetic data generation with CTGAN on adult census income dataset [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ydataai/ydata-synthetic/blob/master/examples/regular/models/CTGAN_Adult_Census_Income_Data.ipynb)\n', '  - Time Series synthetic data generation with TimeGAN on stock dataset [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ydataai/ydata-synthetic/blob/master/examples/timeseries/TimeGAN_Synthetic_stock_data.ipynb)\n', '  - More examples are continuously added and can be found in `/examples` directory.\n', '\n', '### Datasets for you to experiment\n', 'Here are some example datasets for you to try with the synthesizers:\n', '#### Tabular datasets\n', '- [Adult Census Income](https://www.kaggle.com/datasets/uciml/adult-census-income)\n', '- [Credit card fraud](https://www.kaggle.com/mlg-ulb/creditcardfraud)\n', '- [Cardiovascular Disease dataset](https://www.kaggle.com/datasets/sulianova/cardiovascular-disease-dataset)\n', '\n', '#### Sequential datasets\n', '- [Stock data](https://github.com/ydataai/ydata-synthetic/tree/master/data)\n', '\n', '## Project Resources\n', '\n', 'In this repository you can find the several GAN architectures that are used to create synthesizers:\n', '\n', '### Tabular data\n', '  - [GAN](https://arxiv.org/abs/1406.2661)\n', '  - [CGAN (Conditional GAN)](https://arxiv.org/abs/1411.1784)\n', '  - [WGAN (Wasserstein GAN)](https://arxiv.org/abs/1701.07875)\n', '  - [WGAN-GP (Wassertein GAN with Gradient Penalty)](https://arxiv.org/abs/1704.00028)\n', '  - [DRAGAN (On Convergence and stability of GANS)](https://arxiv.org/pdf/1705.07215.pdf)\n', '  - [Cramer GAN (The Cramer Distance as a Solution to Biased Wasserstein Gradients)](https://arxiv.org/abs/1705.10743)\n', '  - [CWGAN-GP (Conditional Wassertein GAN with Gradient Penalty)](https://cameronfabbri.github.io/papers/conditionalWGAN.pdf)\n', '  - [CTGAN (Conditional Tabular GAN)](https://arxiv.org/pdf/1907.00503.pdf)\n', '\n', '### Sequential data\n', '  - [TimeGAN](https://papers.nips.cc/paper/2019/file/c9efe5f26cd17ba6216bbe2a7d26d490-Paper.pdf)\n', '\n', '## Contributing\n', 'We are open to collaboration! If you want to start contributing you only need to:\n', '  1. Search for an issue in which you would like to work. Issues for newcomers are labeled with good first issue.\n', '  2. Create a PR solving the issue.\n', '  3. We would review every PRs and either accept or ask for revisions.\n', '\n', '## Support\n', 'For support in using this library, please join our Discord server. Our Discord community is very friendly and great about quickly answering questions about the use and development of the library. [Click here to join our Discord community!](https://discord.com/invite/mw7xjJ7b7s)\n', '\n', '## License\n', '[MIT License](https://github.com/ydataai/ydata-synthetic/blob/master/LICENSE)\n']"
Synthetic+Data,sdv-dev/CTGAN,sdv-dev,https://api.github.com/repos/sdv-dev/CTGAN,848,225,17,"['https://api.github.com/users/csala', 'https://api.github.com/users/fealho', 'https://api.github.com/users/pvk-developer', 'https://api.github.com/users/leix28', 'https://api.github.com/users/amontanez24', 'https://api.github.com/users/katxiao', 'https://api.github.com/users/kevinykuo', 'https://api.github.com/users/npatki', 'https://api.github.com/users/oregonpillow', 'https://api.github.com/users/JDTheRipperPC', 'https://api.github.com/users/tejuafonja', 'https://api.github.com/users/Baukebrenninkmeijer', 'https://api.github.com/users/lurosenb', 'https://api.github.com/users/matheusccouto', 'https://api.github.com/users/Deathn0t', 'https://api.github.com/users/timvink', 'https://api.github.com/users/mfhbree']",Python,2023-04-08T08:18:22Z,https://raw.githubusercontent.com/sdv-dev/CTGAN/master/README.md,"['<div align=""center"">\n', '<br/>\n', '<p align=""center"">\n', '    <i>This repository is part of <a href=""https://sdv.dev"">The Synthetic Data Vault Project</a>, a project from <a href=""https://datacebo.com"">DataCebo</a>.</i>\n', '</p>\n', '\n', '[![Development Status](https://img.shields.io/badge/Development%20Status-2%20--%20Pre--Alpha-yellow)](https://pypi.org/search/?c=Development+Status+%3A%3A+2+-+Pre-Alpha)\n', '[![PyPI Shield](https://img.shields.io/pypi/v/ctgan.svg)](https://pypi.python.org/pypi/ctgan)\n', '[![Unit Tests](https://github.com/sdv-dev/CTGAN/actions/workflows/unit.yml/badge.svg)](https://github.com/sdv-dev/CTGAN/actions/workflows/unit.yml)\n', '[![Downloads](https://pepy.tech/badge/ctgan)](https://pepy.tech/project/ctgan)\n', '[![Coverage Status](https://codecov.io/gh/sdv-dev/CTGAN/branch/master/graph/badge.svg)](https://codecov.io/gh/sdv-dev/CTGAN)\n', '\n', '<div align=""left"">\n', '<br/>\n', '<p align=""center"">\n', '<a href=""https://github.com/sdv-dev/CTGAN"">\n', '<img align=""center"" width=40% src=""https://github.com/sdv-dev/SDV/blob/master/docs/images/CTGAN-DataCebo.png""></img>\n', '</a>\n', '</p>\n', '</div>\n', '\n', '</div>\n', '\n', '# Overview\n', '\n', 'CTGAN\xa0is a collection of Deep Learning based\xa0synthetic data generators\xa0for\xa0single table\xa0data, which are able to learn from real data and generate synthetic data with high fidelity.\n', '\n', '| Important Links                               |                                                                      |\n', '| --------------------------------------------- | -------------------------------------------------------------------- |\n', '| :computer: **[Website]**                      | Check out the SDV Website for more information about our overall synthetic data ecosystem.|\n', '| :orange_book: **[Blog]**                      | A deeper look at open source, synthetic data creation and evaluation.|\n', '| :book: **[Documentation]**                    | Quickstarts, User and Development Guides, and API Reference.         |\n', '| :octocat: **[Repository]**                    | The link to the Github Repository of this library.                   |\n', '| :keyboard: **[Development Status]**           | This software is in its Pre-Alpha stage.                             |\n', '| [![][Slack Logo] **Community**][Community]    | Join our Slack Workspace for announcements and discussions.          |\n', '\n', '[Website]: https://sdv.dev\n', '[Blog]: https://datacebo.com/blog\n', '[Documentation]: https://bit.ly/sdv-docs\n', '[Repository]: https://github.com/sdv-dev/CTGAN\n', '[License]: https://github.com/sdv-dev/CTGAN/blob/master/LICENSE\n', '[Development Status]: https://pypi.org/search/?c=Development+Status+%3A%3A+2+-+Pre-Alpha\n', '[Slack Logo]: https://github.com/sdv-dev/SDV/blob/master/docs/images/slack.png\n', '[Community]: https://bit.ly/sdv-slack-invite\n', '\n', 'Currently, this library implements the **CTGAN** and **TVAE** models described in the [Modeling Tabular data using Conditional GAN](https://arxiv.org/abs/1907.00503) paper, presented at the 2019 NeurIPS conference.\n', '\n', '# Install\n', '\n', '## Use CTGAN through the SDV library\n', '\n', "":warning: If you're just getting started with synthetic data, we recommend installing the SDV library which provides user-friendly APIs for accessing CTGAN. :warning:\n"", '\n', 'The SDV library provides wrappers for preprocessing your data as well as additional usability features like constraints. See the [SDV documentation](https://bit.ly/sdv-docs) to get started.\n', '\n', '## Use the CTGAN standalone library\n', '\n', 'Alternatively, you can also install and use **CTGAN** directly, as a standalone library:\n', '\n', '**Using `pip`:**\n', '\n', '```bash\n', 'pip install ctgan\n', '```\n', '\n', '**Using `conda`:**\n', '\n', '```bash\n', 'conda install -c pytorch -c conda-forge ctgan\n', '```\n', '\n', 'When using the CTGAN library directly, you may need to manually preprocess your data into the correct format, for example:\n', '\n', '* Continuous data must be represented as floats\n', '* Discrete data must be represented as ints or strings\n', '* The data should not contain any missing values\n', '\n', '# Usage Example\n', '\n', 'In this example we load the [Adult Census Dataset](https://archive.ics.uci.edu/ml/datasets/adult)* which is a built-in demo dataset. We use CTGAN to learn from the real data and then generate some synthetic data.\n', '\n', '```python3\n', 'from ctgan import CTGAN\n', 'from ctgan import load_demo\n', '\n', 'real_data = load_demo()\n', '\n', '# Names of the columns that are discrete\n', 'discrete_columns = [\n', ""    'workclass',\n"", ""    'education',\n"", ""    'marital-status',\n"", ""    'occupation',\n"", ""    'relationship',\n"", ""    'race',\n"", ""    'sex',\n"", ""    'native-country',\n"", ""    'income'\n"", ']\n', '\n', 'ctgan = CTGAN(epochs=10)\n', 'ctgan.fit(real_data, discrete_columns)\n', '\n', '# Create synthetic data\n', 'synthetic_data = ctgan.sample(1000)\n', '```\n', '\n', '*For more information about the dataset see:\n', 'Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml].\n', 'Irvine, CA: University of California, School of Information and Computer Science.\n', '\n', '# Join our community\n', '\n', 'Join our [Slack channel](https://bit.ly/sdv-slack-invite) to discuss more about CTGAN and synthetic data. If you find a bug or have a feature request, you can also [open an issue](https://github.com/sdv-dev/CTGAN/issues) on our GitHub.\n', '\n', '**Interested in contributing to CTGAN?** Read our [Contribution Guide](CONTRIBUTING.rst) to get started.\n', '\n', '# Citing CTGAN\n', '\n', 'If you use CTGAN, please cite the following work:\n', '\n', '*Lei Xu, Maria Skoularidou, Alfredo Cuesta-Infante, Kalyan Veeramachaneni.* **Modeling Tabular data using Conditional GAN**. NeurIPS, 2019.\n', '\n', '```LaTeX\n', '@inproceedings{ctgan,\n', '  title={Modeling Tabular data using Conditional GAN},\n', '  author={Xu, Lei and Skoularidou, Maria and Cuesta-Infante, Alfredo and Veeramachaneni, Kalyan},\n', '  booktitle={Advances in Neural Information Processing Systems},\n', '  year={2019}\n', '}\n', '```\n', '\n', '# Related Projects\n', 'Please note that these projects are external to the SDV Ecosystem. They are not affiliated with or maintained by DataCebo.\n', '\n', '* **R Interface for CTGAN**: A wrapper around **CTGAN** that brings the functionalities to **R** users.\n', 'More details can be found in the corresponding repository: https://github.com/kasaai/ctgan\n', '* **CTGAN Server CLI**: A package to easily deploy CTGAN onto a remote server. Created by Timothy Pillow @oregonpillow at: https://github.com/oregonpillow/ctgan-server-cli\n', '\n', '---\n', '\n', '\n', '<div align=""center"">\n', '<a href=""https://datacebo.com""><img align=""center"" width=40% src=""https://github.com/sdv-dev/SDV/blob/master/docs/images/DataCebo.png""></img></a>\n', '</div>\n', '<br/>\n', '<br/>\n', '\n', ""[The Synthetic Data Vault Project](https://sdv.dev) was first created at MIT's [Data to AI Lab](\n"", 'https://dai.lids.mit.edu/) in 2016. After 4 years of research and traction with enterprise, we\n', 'created [DataCebo](https://datacebo.com) in 2020 with the goal of growing the project.\n', 'Today, DataCebo is the proud developer of SDV, the largest ecosystem for\n', 'synthetic data generation & evaluation. It is home to multiple libraries that support synthetic\n', 'data, including:\n', '\n', '* 🔄 Data discovery & transformation. Reverse the transforms to reproduce realistic data.\n', '* 🧠 Multiple machine learning models -- ranging from Copulas to Deep Learning -- to create tabular,\n', '  multi table and time series data.\n', '* 📊 Measuring quality and privacy of synthetic data, and comparing different synthetic data\n', '  generation models.\n', '\n', '[Get started using the SDV package](https://sdv.dev/SDV/getting_started/install.html) -- a fully\n', 'integrated solution and your one-stop shop for synthetic data. Or, use the standalone libraries\n', 'for specific needs.\n']"
Synthetic+Data,sdv-dev/SDV,sdv-dev,https://api.github.com/repos/sdv-dev/SDV,1349,214,19,"['https://api.github.com/users/csala', 'https://api.github.com/users/ManuelAlvarezC', 'https://api.github.com/users/amontanez24', 'https://api.github.com/users/JDTheRipperPC', 'https://api.github.com/users/katxiao', 'https://api.github.com/users/fealho', 'https://api.github.com/users/pvk-developer', 'https://api.github.com/users/frances-h', 'https://api.github.com/users/npatki', 'https://api.github.com/users/kveerama', 'https://api.github.com/users/xamm', 'https://api.github.com/users/sarahmish', 'https://api.github.com/users/Aylr', 'https://api.github.com/users/R-Palazzo', 'https://api.github.com/users/dyuliu', 'https://api.github.com/users/ludovicc', 'https://api.github.com/users/Deathn0t', 'https://api.github.com/users/rollervan', 'https://api.github.com/users/tssbas']",Python,2023-04-08T08:59:13Z,https://raw.githubusercontent.com/sdv-dev/SDV/master/README.md,"['<div align=""center"">\n', '<br/>\n', '<p align=""center"">\n', '    <i>This repository is part of <a href=""https://sdv.dev"">The Synthetic Data Vault Project</a>, a project from <a href=""https://datacebo.com"">DataCebo</a>.</i>\n', '</p>\n', '\n', '[![Dev Status](https://img.shields.io/badge/Dev%20Status-5%20--%20Production%2fStable-green)](https://pypi.org/search/?c=Development+Status+%3A%3A+5+-+Production%2FStable)\n', '[![PyPi Shield](https://img.shields.io/pypi/v/SDV.svg)](https://pypi.python.org/pypi/SDV)\n', '[![Unit Tests](https://github.com/sdv-dev/SDV/actions/workflows/unit.yml/badge.svg?branch=master)](https://github.com/sdv-dev/SDV/actions/workflows/unit.yml?query=branch%3Amaster)\n', '[![Integration Tests](https://github.com/sdv-dev/SDV/actions/workflows/integration.yml/badge.svg?branch=master)](https://github.com/sdv-dev/SDV/actions/workflows/integration.yml?query=branch%3Amaster)\n', '[![Coverage Status](https://codecov.io/gh/sdv-dev/SDV/branch/master/graph/badge.svg)](https://codecov.io/gh/sdv-dev/SDV)\n', '[![Downloads](https://static.pepy.tech/personalized-badge/sdv?period=total&units=international_system&left_color=grey&right_color=blue&left_text=Downloads)](https://pepy.tech/project/sdv)\n', '[![Colab](https://img.shields.io/badge/Tutorials-Try%20now!-orange?logo=googlecolab)](https://docs.sdv.dev/sdv/demos)\n', '[![Slack](https://img.shields.io/badge/Slack-Join%20now!-36C5F0?logo=slack)](https://bit.ly/sdv-slack-invite)\n', '\n', '<div align=""left"">\n', '<br/>\n', '<p align=""center"">\n', '<a href=""https://github.com/sdv-dev/SDV"">\n', '<img align=""center"" width=40% src=""https://github.com/sdv-dev/SDV/blob/master/docs/images/SDV-logo.png""></img>\n', '</a>\n', '</p>\n', '</div>\n', '\n', '</div>\n', '\n', '# Overview\n', '\n', 'The **Synthetic Data Vault** (SDV) is a Python library designed to be your one-stop shop for\n', 'creating tabular synthetic data. The SDV uses a variety of machine learning algorithms to learn\n', 'patterns from your real data and emulate them in synthetic data.\n', '\n', '## Features\n', ':brain: **Create synthetic data using machine learning.** The SDV offers multiple models, ranging\n', 'from classical statistical methods (GaussianCopula) to deep learning methods (CTGAN). Generate\n', 'data for single tables, multiple connected tables or sequential tables.\n', '\n', ':bar_chart: **Evaluate and visualize data.** Compare the synthetic data to the real data against a\n', 'variety of measures. Diagnose problems and generate a quality report to get more insights.\n', '\n', ':arrows_counterclockwise: **Preprocess, anonymize and define constraints.** Control data\n', 'processing to improve the quality of synthetic data, choose from different types of anonymization\n', 'and define business rules in the form of logical constraints.\n', '\n', '| Important Links                               |                                                                                                     |\n', '| --------------------------------------------- | ----------------------------------------------------------------------------------------------------|\n', '| [![][Colab Logo] **Tutorials**][Tutorials]    | Get some hands-on experience with the SDV. Launch the tutorial notebooks and run the code yourself. |\n', '| :book: **[Docs]**                             | Learn how to use the SDV library with user guides and API references.                               |\n', '| :orange_book: **[Blog]**                      | Get more insights about using the SDV, deploying models and our synthetic data community.          |\n', '| [![][Slack Logo] **Community**][Community]    | Join our Slack workspace for announcements and discussions.                                         |\n', '| :computer: **[Website]**                      | Check out the SDV website for more information about the project.                                   |\n', '\n', '[Website]: https://sdv.dev\n', '[Blog]: https://datacebo.com/blog\n', '[Docs]: https://bit.ly/sdv-docs\n', '[Repository]: https://github.com/sdv-dev/SDV\n', '[License]: https://github.com/sdv-dev/SDV/blob/master/LICENSE\n', '[Development Status]: https://pypi.org/search/?c=Development+Status+%3A%3A+5+-+Production%2FStable\n', '[Slack Logo]: https://github.com/sdv-dev/SDV/blob/master/docs/images/slack.png\n', '[Community]: https://bit.ly/sdv-slack-invite\n', '[Colab Logo]: https://github.com/sdv-dev/SDV/blob/master/docs/images/google_colab.png\n', '[Tutorials]: https://docs.sdv.dev/sdv/demos\n', '\n', '# Install\n', 'The SDV is publicly available under the [Business Source License](https://github.com/sdv-dev/SDV/blob/master/LICENSE).\n', 'Install SDV using pip or conda. We recommend using a virtual environment to avoid conflicts with\n', 'other software on your device.\n', '\n', '```bash\n', 'pip install sdv\n', '```\n', '\n', '```bash\n', 'conda install -c pytorch -c conda-forge sdv\n', '```\n', '\n', '# Getting Started\n', 'Load a demo dataset to get started. This dataset is a single table describing guests staying at a\n', 'fictional hotel.\n', '\n', '```python\n', 'from sdv.datasets.demo import download_demo\n', '\n', 'real_data, metadata = download_demo(\n', ""    modality='single_table',\n"", ""    dataset_name='fake_hotel_guests')\n"", '```\n', '\n', '![Single Table Metadata Example](https://github.com/sdv-dev/SDV/blob/master/docs/images/Single-Table-Metadata-Example.png)\n', '\n', 'The demo also includes **metadata**, a description of the dataset, including the data types in each\n', 'column and the primary key (`guest_email`).\n', '\n', '## Synthesizing Data\n', 'Next, we can create an **SDV synthesizer**,  an object that you can use to create synthetic data.\n', ""It learns patterns from the real data and replicates them to generate synthetic data. Let's use\n"", 'the `FAST_ML` preset synthesizer, which is optimized for performance.\n', '\n', '```python\n', 'from sdv.lite import SingleTablePreset\n', '\n', ""synthesizer = SingleTablePreset(metadata, name='FAST_ML')\n"", 'synthesizer.fit(data=real_data)\n', '```\n', '\n', 'And now the synthesizer is ready to create synthetic data!\n', '\n', '```python\n', 'synthetic_data = synthesizer.sample(num_rows=500)\n', '```\n', '\n', 'The synthetic data will have the following properties:\n', '- **Sensitive columns are fully anonymized.** The email, billing address and credit card number\n', ""columns contain new data so you don't expose the real values.\n"", '- **Other columns follow statistical patterns.** For example, the proportion of room types, the\n', 'distribution of check in dates and the correlations between room rate and room type are preserved.\n', '- **Keys and other relationships are intact.** The primary key (guest email) is unique for each row.\n', 'If you have multiple tables, the connection between a primary and foreign keys makes sense.\n', '\n', '## Evaluating Synthetic Data\n', 'The SDV library allows you to evaluate the synthetic data by comparing it to the real data. Get\n', 'started by generating a quality report.\n', '\n', '```python\n', 'from sdv.evaluation.single_table import evaluate_quality\n', '\n', 'quality_report = evaluate_quality(\n', '    real_data,\n', '    synthetic_data,\n', '    metadata)\n', '```\n', '\n', '```\n', 'Creating report: 100%|██████████| 4/4 [00:00<00:00, 19.30it/s]\n', 'Overall Quality Score: 89.12%\n', 'Properties:\n', 'Column Shapes: 90.27%\n', 'Column Pair Trends: 87.97%\n', '```\n', '\n', 'This object computes an overall quality score on a scale of 0 to 100% (100 being the best) as well\n', 'as detailed breakdowns. For more insights, you can also visualize the synthetic vs. real data.\n', '\n', '```python\n', 'from sdv.evaluation.single_table import get_column_plot\n', '\n', 'fig = get_column_plot(\n', '    real_data=real_data,\n', '    synthetic_data=synthetic_data,\n', ""    column_name='amenities_fee',\n"", '    metadata=metadata\n', ')\n', '    \n', 'fig.show()\n', '```\n', '\n', '![Real vs. Synthetic Data](https://github.com/sdv-dev/SDV/blob/master/docs/images/Real-vs-Synthetic-Evaluation.png)\n', '\n', ""# What's Next?\n"", 'Using the SDV library, you can synthesize single table, multi table and sequential data. You can\n', 'also customize the full synthetic data workflow, including preprocessing, anonymization and adding\n', 'constraints.\n', '\n', 'To learn more, visit the [SDV Demo page](https://docs.sdv.dev/sdv/demos).\n', '\n', '# Credits\n', 'Thank you to our team of contributors who have built and maintained the SDV ecosystem over the\n', 'years!\n', '\n', '[View Contributors](https://github.com/sdv-dev/SDV/graphs/contributors)\n', '\n', '## Citation\n', 'If you use SDV for your research, please cite the following paper:\n', '\n', '*Neha Patki, Roy Wedge, Kalyan Veeramachaneni*. [The Synthetic Data Vault](https://dai.lids.mit.edu/wp-content/uploads/2018/03/SDV.pdf). [IEEE DSAA 2016](https://ieeexplore.ieee.org/document/7796926).\n', '\n', '```\n', '@inproceedings{\n', '    SDV,\n', '    title={The Synthetic data vault},\n', '    author={Patki, Neha and Wedge, Roy and Veeramachaneni, Kalyan},\n', '    booktitle={IEEE International Conference on Data Science and Advanced Analytics (DSAA)},\n', '    year={2016},\n', '    pages={399-410},\n', '    doi={10.1109/DSAA.2016.49},\n', '    month={Oct}\n', '}\n', '```\n', '\n', '---\n', '\n', '\n', '<div align=""center"">\n', '  <a href=""https://datacebo.com""><picture>\n', '      <source media=""(prefers-color-scheme: dark)"" srcset=""https://github.com/sdv-dev/SDV/blob/master/docs/images/datacebo-logo-dark-mode.png"">\n', '      <img align=""center"" width=40% src=""https://github.com/sdv-dev/SDV/blob/master/docs/images/datacebo-logo.png""></img>\n', '  </picture></a>\n', '</div>\n', '<br/>\n', '<br/>\n', '\n', ""[The Synthetic Data Vault Project](https://sdv.dev) was first created at MIT's [Data to AI Lab](\n"", 'https://dai.lids.mit.edu/) in 2016. After 4 years of research and traction with enterprise, we\n', 'created [DataCebo](https://datacebo.com) in 2020 with the goal of growing the project.\n', 'Today, DataCebo is the proud developer of SDV, the largest ecosystem for\n', 'synthetic data generation & evaluation. It is home to multiple libraries that support synthetic\n', 'data, including:\n', '\n', '* 🔄 Data discovery & transformation. Reverse the transforms to reproduce realistic data.\n', '* 🧠 Multiple machine learning models -- ranging from Copulas to Deep Learning -- to create tabular,\n', '  multi table and time series data.\n', '* 📊 Measuring quality and privacy of synthetic data, and comparing different synthetic data\n', '  generation models.\n', '\n', '[Get started using the SDV package](https://bit.ly/sdv-docs) -- a fully\n', 'integrated solution and your one-stop shop for synthetic data. Or, use the standalone libraries\n', 'for specific needs.\n']"
Synthetic+Data,wang-tf/Chinese_OCR_synthetic_data,wang-tf,https://api.github.com/repos/wang-tf/Chinese_OCR_synthetic_data,256,86,0,[],Python,2023-04-06T09:36:31Z,https://raw.githubusercontent.com/wang-tf/Chinese_OCR_synthetic_data/master/README.md,"['# Chinese_OCR_synthetic_data\n', '---\n', '## The progress was used to generate synthetic dataset for Chinese OCR.\n', 'Here we used [Augmenter](https://github.com/mdbloice/Augmentor) to augment out output characters in images, including rotate, skew, shear and distort.\n', 'And you can change characters.txt file to use other characters.\n', 'The main function can be found in the synthetic_data.py file.\n', '\n', 'The python package you may need:\n', '- tqdm\n', '- PIL(pillow)\n', '- pathlib\n', '- cv2(opencv)\n', '- numpy\n', '- codecs\n', '- glob\n', '\n', '---\n', '## 本程序用于合成中文OCR数据库。\n', '本程序使用了[Augmenter](https://github.com/mdbloice/Augmentor)库，以对输出的图像进行增强图片中的文本，其中包括旋转、倾斜、剪切和扭曲。这些形变的参数可以在utils.py中找到并修改。\n', '在characters.txt中存放着所有的中文字符，如果想更换训练的字符请替换该文件。\n', 'main函数在synthetic_data.py中，可以按需要做修改。\n', '\n', '使用之前可能需要安装一下的包：\n', '- tqdm\n', '- PIL(pillow)\n', '- pathlib\n', '- cv2(opencv)\n', '- numpy\n', '- codecs\n', '- glob\n', '\n', '\n', '![test](https://github.com/wang-tf/Chinese_OCR_synthetic_data/blob/master/test_ocrdataset/train_part_image/0_0.jpg)\n']"
Synthetic+Data,GeostatsGuy/GeoDataSets,GeostatsGuy,https://api.github.com/repos/GeostatsGuy/GeoDataSets,46,108,1,['https://api.github.com/users/GeostatsGuy'],,2023-03-24T08:25:41Z,https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/README.md,"['<p>\n', '    <img src=""https://github.com/GeostatsGuy/GeostatsPy/blob/master/TCG_color_logo.png"" width=""220"" height=""200"" />\n', '</p>\n', '\n', '# GeoDataSets: Synthetic Subsurface Data Repository (1.0.0) \n', '\n', '## Prof. Michael J. Pyrcz, Associate Professor, The University of Texas at Austin\n', '\n', 'A collection of synthetic subsurface datasets to support education, publications, and prototyping.\n', '\n', 'Please cite as:\n', '\n', 'Pyrcz, Michael J. (2021). GeoDataSets: Synthetic Subsurface Data Repository (1.0.0). Zenodo. https://doi.org/10.5281/zenodo.5564874\n', '\n', 'This repository includes a wide variety of synthetic, subsurface datasets with a variety of:\n', '\n', '#### Data Dimensionality\n', '\n', 'To support education with easy visualization and interactivity the datasets are 1D and 2D.\n', '\n', '* 1D cores from wells and 2D seismic maps. \n', '\n', '#### Number of Features\n', '\n', 'For multivariate analysis some of the datasets include up to 6 features with a variety of structures.\n', '\n', '* linear and nonlinear, homoscedastic and heteroscedastic, and multivariate constraints \n', '\n', '#### Data Issues\n', '\n', 'The datasets attempt to include typical issues such as non-physical values, random and structured noise\n', '\n', '#### Use and Attribution\n', '\n', 'You are welcome to use these datasets for any purpose. Please cite the repository as:\n', '\n', 'Pyrcz, M.J., 2021, GeoDataSets Repository, GitHub Respository, https://github.com/GeostatsGuy/GeoDataSets/.\n', '\n', 'I hope this is helpful,\n', '\n', 'Michael\n']"
Synthetic+Data,gretelai/gretel-synthetics,gretelai,https://api.github.com/repos/gretelai/gretel-synthetics,383,61,21,"['https://api.github.com/users/zredlined', 'https://api.github.com/users/johntmyers', 'https://api.github.com/users/drew', 'https://api.github.com/users/kboyd', 'https://api.github.com/users/misberner', 'https://api.github.com/users/tylersbray', 'https://api.github.com/users/pimlock', 'https://api.github.com/users/lipikaramaswamy', 'https://api.github.com/users/santhosh97', 'https://api.github.com/users/anthager', 'https://api.github.com/users/lememta', 'https://api.github.com/users/andrewnc', 'https://api.github.com/users/arronhunt', 'https://api.github.com/users/Marjan-emd', 'https://api.github.com/users/mckornfield', 'https://api.github.com/users/anastasia-nesterenko', 'https://api.github.com/users/dni138', 'https://api.github.com/users/hgascon', 'https://api.github.com/users/Jeesh96', 'https://api.github.com/users/csbailey5t', 'https://api.github.com/users/theonlyrob']",Python,2023-04-07T07:17:55Z,https://raw.githubusercontent.com/gretelai/gretel-synthetics/master/README.md,"['# Gretel Synthetics\n', '\n', '<p align=""center"">\n', '    <a href=""https://gretel.ai""><img width=""128px"" src=""https://gretel-public-website.s3.amazonaws.com/assets/gobs_the_cat_@1x.png"" alt=""Gobs the Gretel.ai cat"" /></a><br />\n', '    <i>A permissive synthetic data library from Gretel.ai</i>\n', '</p>\n', '\n', '[![Documentation Status](https://readthedocs.org/projects/gretel-synthetics/badge/?version=stable)](https://gretel-synthetics.readthedocs.io/en/stable/?badge=stable)\n', '[![CLA assistant](https://cla-assistant.io/readme/badge/gretelai/gretel-synthetics)](https://cla-assistant.io/gretelai/gretel-synthetics)\n', '[![PyPI](https://badge.fury.io/py/gretel-synthetics.svg)](https://badge.fury.io/py/gretel-synthetics)\n', '[![Python](https://img.shields.io/pypi/pyversions/gretel-synthetics.svg)](https://github.com/gretelai/gretel-synthetics)\n', '[![Downloads](https://pepy.tech/badge/gretel-synthetics)](https://pepy.tech/project/gretel-synthetics)\n', '[![GitHub stars](https://img.shields.io/github/stars/gretelai/gretel-synthetics?style=social)](https://github.com/gretelai/gretel-synthetics)\n', '[![Discord](https://img.shields.io/discord/1007817822614847500?label=Discord&logo=Discord)](https://gretel.ai/discord)\n', '\n', '## Documentation\n', '\n', '- [Get started with gretel-synthetics](https://gretel-synthetics.readthedocs.io/en/stable/)\n', '- [Configuration](https://gretel-synthetics.readthedocs.io/en/stable/api/config.html)\n', '- [Train your model](https://gretel-synthetics.readthedocs.io/en/stable/api/train.html)\n', '- [Generate synthetic records](https://gretel-synthetics.readthedocs.io/en/stable/api/generate.html)\n', '\n', '## Try it out now!\n', '\n', 'If you want to quickly discover gretel-synthetics, simply click the button below and follow the tutorials!\n', '\n', '[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gretelai/gretel-synthetics/blob/master/examples/synthetic_records.ipynb)\n', '\n', 'Check out additional examples [here](https://github.com/gretelai/gretel-synthetics/tree/master/examples).\n', '\n', '## Getting Started\n', '\n', 'This section will guide you through installation of `gretel-synthetics` and dependencies that are not directly installed by the Python package manager.\n', '\n', '### Dependency Requirements\n', '\n', 'By default, we do not install certain core requirements, the following dependencies should be installed _external to the installation_\n', 'of `gretel-synthetics`, depending on which model(s) you plan to use.\n', '\n', '- Tensorflow: Used by the LSTM model, we recommend version 2.8.x\n', '- Torch: Used by Timeseries DGAN and ACTGAN (for ACTGAN, Torch is installed by SDV)\n', '- SDV (Synthetic Data Vault): Used by ACTGAN, we recommned version 0.17.x\n', '\n', 'These dependencies can be installed by doing the following:\n', '\n', '```\n', 'pip install tensorflow==2.8 # for LSTM\n', 'pip install sdv<0.18 # for ACTGAN\n', 'pip install torch==1.13.1 # for Timeseries DGAN\n', '```\n', '\n', 'To install the actual `gretel-synthetics` package, first clone the repo and then...\n', '\n', '```\n', 'pip install -U .\n', '```\n', '\n', '_or_\n', '\n', '```\n', 'pip install gretel-synthetics\n', '```\n', '\n', '_then..._\n', '\n', '```\n', '$ pip install jupyter\n', '$ jupyter notebook\n', '```\n', '\n', 'When the UI launches in your browser, navigate to `examples/synthetic_records.ipynb` and get generating!\n', '\n', 'If you want to install `gretel-synthetics` locally and use a GPU (recommended):\n', '\n', '1. Create a virtual environment (e.g. using `conda`)\n', '\n', '```\n', '$ conda create --name tf python=3.9\n', '```\n', '\n', '2. Activate the virtual environment\n', '\n', '```\n', '$ conda activate tf\n', '```\n', '\n', '3. Run the setup script `./setup-utils/setup-gretel-synthetics-tensorflow24-with-gpu.sh`\n', '\n', 'The last step will install all the necessary software packages for GPU usage, `tensorflow=2.8` and `gretel-synthetics`.\n', 'Note that this script works only for Ubuntu 18.04. You might need to modify it for other OS versions.\n', '\n', '## Timeseries DGAN Overview\n', '\n', 'The [timeseries DGAN module](https://synthetics.docs.gretel.ai/en/stable/models/timeseries_dgan.html#timeseries-dgan) contains a PyTorch implementation of a DoppelGANger model that is optimized for timeseries data. Similar to tensorflow, you will need to manually install pytorch:\n', '\n', '```\n', 'pip install torch==1.13.1\n', '```\n', '\n', '[This notebook](https://github.com/gretelai/gretel-synthetics/blob/master/examples/timeseries_dgan.ipynb) shows basic usage on a small data set of smart home sensor readings.\n', '\n', '## ACTGAN Overview\n', '\n', 'ACTGAN (Anyway CTGAN) is an extension of the popular [CTGAN implementation](https://sdv.dev/SDV/user_guides/single_table/ctgan.html) that provides\n', 'some additiona functionality to improve memory usage, autodetection and transformation of columns, and more.\n', '\n', 'To use this model, you will need to manually install SDV:\n', '\n', '```\n', 'pip install sdv<0.18\n', '```\n', '\n', 'Keep in mind that this will also install several dependencies like PyTorch that SDV relies on, which may conflict with PyTorch\n', 'versions installed for use with other models like Timeseries DGAN.\n', '\n', 'The ACTGAN interface is a superset of the CTGAN interface. To see the additional features, please take a look at the ACTGAN demo notebook in the `examples` directory of this repo.\n', '\n', '## LSTM Overview\n', '\n', 'This package allows developers to quickly get immersed with synthetic data generation through the use of neural networks. The more complex pieces of working with libraries like Tensorflow and differential privacy are bundled into friendly Python classes and functions. There are two high level modes that can be utilized.\n', '\n', '### Simple Mode\n', '\n', 'The simple mode will train line-per-line on an input file of text. When generating data, the generator will yield a custom object that can be used a variety of different ways based on your use case. [This notebook](https://github.com/gretelai/gretel-synthetics/blob/master/examples/tensorflow/simple-character-model.ipynb) demonstrates this mode.\n', '\n', '### DataFrame Mode\n', '\n', 'This library supports CSV / DataFrames natively using the DataFrame ""batch"" mode. This module provided a wrapper around our simple mode that is geared for working with tabular data. Additionally, it is capabable of handling a high number of columns by breaking the input DataFrame up into ""batches"" of columns and training a model on each batch. [This notebook](https://github.com/gretelai/gretel-synthetics/blob/master/examples/dataframe_batch.ipynb) shows an overview of using this library with DataFrames natively.\n', '\n', '### Components\n', '\n', 'There are four primary components to be aware of when using this library.\n', '\n', '1. Configurations. Configurations are classes that are specific to an underlying ML engine used to train and generate data. An example would be using `TensorFlowConfig` to create all the necessary parameters to train a model based on TF. `LocalConfig` is aliased to `TensorFlowConfig` for backwards compatability with older versions of the library. A model is saved to a designated directory, which can optionally be archived and utilized later.\n', '\n', '2. Tokenizers. Tokenizers convert input text into integer based IDs that are used by the underlying ML engine. These tokenizers can be created and sent to the training input. This is optional, and if no specific tokenizer is specified then a default one will be used. You can find [an example](https://github.com/gretelai/gretel-synthetics/blob/master/examples/tensorflow/batch-df-char-tokenizer.ipynb) here that uses a simple char-by-char tokenizer to build a model from an input CSV. When training in a non-differentially private mode, we suggest using the default `SentencePiece` tokenizer, an unsupervised tokenizer that learns subword units (e.g., **byte-pair-encoding (BPE)** [[Sennrich et al.](http://www.aclweb.org/anthology/P16-1162)]) and **unigram language model** [[Kudo.](https://arxiv.org/abs/1804.10959)]) for faster training and increased accuracy of the synthetic model.\n', '\n', '3. Training. Training a model combines the configuration and tokenizer and builds a model, which is stored in the designated directory, that can be used to generate new records.\n', '\n', '4. Generation. Once a model is trained, any number of new lines or records can be generated. Optionally, a record validator can be provided to ensure that the generated data meets any constraints that are necessary. See our notebooks for examples on validators.\n', '\n', '### Utilities\n', '\n', 'In addition to the four primary components, the `gretel-synthetics` package also ships with a set of utilities that are helpful for training advanced synthetics models and evaluating synthetic datasets.\n', '\n', 'Some of this functionality carries large dependencies, so they are shipped as an extra called `utils`. To install these dependencies, you may run\n', '\n', '```\n', 'pip install gretel-synthetics[utils]\n', '```\n', '\n', 'For additional details, please refer to the [Utility module API docs](https://synthetics.docs.gretel.ai/en/latest/utils/index.html).\n', '\n', '### Differential Privacy\n', '\n', 'Differential privacy support for our TensorFlow mode is built on the great work being done by the Google TF team and their [TensorFlow Privacy library](https://github.com/tensorflow/privacy).\n', '\n', 'When utilizing DP, we currently recommend using the character tokenizer as it will only create a vocabulary of single tokens and removes the risk of sensitive data being memorized as actual tokens that can be replayed during generation.\n', '\n', 'There are also a few configuration options that are notable such as:\n', '\n', '- `predict_batch_size` should be set to 1\n', '- `dp` should be enabled\n', '- `learning_rate`, `dp_noise_multiplier`, `dp_l2_norm_clip`, and `dp_microbatches` can be adjusted to achieve various epsilon values.\n', '- `reset_states` should be disabled\n', '\n', 'Please see our [example Notebook](https://github.com/gretelai/gretel-synthetics/blob/master/examples/tensorflow/diff_privacy.ipynb) for training a DP model based on the [Netflix Prize](https://en.wikipedia.org/wiki/Netflix_Prize) dataset.\n']"
Synthetic+Data,sdv-dev/SDGym,sdv-dev,https://api.github.com/repos/sdv-dev/SDGym,204,55,13,"['https://api.github.com/users/csala', 'https://api.github.com/users/katxiao', 'https://api.github.com/users/leix28', 'https://api.github.com/users/fealho', 'https://api.github.com/users/pvk-developer', 'https://api.github.com/users/ManuelAlvarezC', 'https://api.github.com/users/amontanez24', 'https://api.github.com/users/Elesa', 'https://api.github.com/users/tejuafonja', 'https://api.github.com/users/Baukebrenninkmeijer', 'https://api.github.com/users/JDTheRipperPC', 'https://api.github.com/users/k15z', 'https://api.github.com/users/sbrugman']",Python,2023-04-09T16:37:22Z,https://raw.githubusercontent.com/sdv-dev/SDGym/master/README.md,"['<div align=""center"">\n', '<br/>\n', '<p align=""center"">\n', '    <i>This repository is part of <a href=""https://sdv.dev"">The Synthetic Data Vault Project</a>, a project from <a href=""https://datacebo.com"">DataCebo</a>.</i>\n', '</p>\n', '\n', '[![Development Status](https://img.shields.io/badge/Development%20Status-2%20--%20Pre--Alpha-yellow)](https://pypi.org/search/?c=Development+Status+%3A%3A+2+-+Pre-Alpha)\n', '[![Travis](https://travis-ci.org/sdv-dev/SDGym.svg?branch=master)](https://travis-ci.org/sdv-dev/SDGym)\n', '[![PyPi Shield](https://img.shields.io/pypi/v/sdgym.svg)](https://pypi.python.org/pypi/sdgym)\n', '[![Downloads](https://pepy.tech/badge/sdgym)](https://pepy.tech/project/sdgym)\n', '[![Slack](https://img.shields.io/badge/Community-Slack-blue?style=plastic&logo=slack)](https://bit.ly/sdv-slack-invite)\n', '\n', '<div align=""left"">\n', '<br/>\n', '<p align=""center"">\n', '<a href=""https://github.com/sdv-dev/SDGym"">\n', '<img align=""center"" width=40% src=""https://github.com/sdv-dev/SDV/blob/master/docs/images/SDGym-DataCebo.png""></img>\n', '</a>\n', '</p>\n', '</div>\n', '\n', '</div>\n', '\n', '# Overview\n', '\n', 'The Synthetic Data Gym (SDGym) is a benchmarking framework for modeling and generating\n', 'synthetic data. Measure performance and memory usage across different synthetic data modeling\n', 'techniques – classical statistics, deep learning and more!\n', '\n', '<img align=""center"" src=""docs/images/SDGym_Results.png""></img>\n', '\n', 'The SDGym library integrates with the Synthetic Data Vault ecosystem. You can use any of its\n', 'synthesizers, datasets or metrics for benchmarking. You also customize the process to include\n', 'your own work.\n', '\n', '* **Datasets**: Select any of the publicly available datasets from the SDV project, or input your own data.\n', '* **Synthesizers**: Choose from any of the SDV synthesizers and baselines. Or write your own custom\n', 'machine learning model.\n', '* **Evaluation**: In addition to performance and memory usage, you can also measure synthetic data\n', 'quality and privacy through a variety of metrics\n', '\n', '# Install\n', '\n', 'Install SDGym using pip or conda. We recommend using a virtual environment to avoid conflicts with other software on your device.\n', '\n', '```bash\n', 'pip install sdgym\n', '```\n', '\n', '```bash\n', 'conda install -c pytorch -c conda-forge sdgym\n', '```\n', '\n', 'For more information about using SDGym, visit the [SDGym Documentation](https://docs.sdv.dev/sdgym).\n', '\n', '# Usage\n', '\n', ""Let's benchmark synthetic data generation for single tables. First, let's define which modeling\n"", ""techniques we want to use. Let's choose a few synthesizers from the SDV library and a few others\n"", 'to use as baselines.\n', '\n', '```python\n', '# these synthesizers come from the SDV library\n', '# each one uses different modeling techniques\n', ""sdv_synthesizers = ['GaussianCopulaSynthesizer', 'CTGANSynthesizer']\n"", '\n', '# these basic synthesizers are available in SDGym\n', '# as baselines\n', ""baseline_synthesizers = ['UniformSynthesizer']\n"", '```\n', '\n', 'Now, we can benchmark the different techniques:\n', '```python\n', 'import sdgym\n', '\n', 'sdgym.benchmark_single_table(\n', '    synthesizers=(sdv_synthesizers + baseline_synthesizers)\n', ')\n', '```\n', '\n', 'The result is a detailed performance, memory and quality evaluation across the synthesizers\n', 'on a variety of publicly available datasets.\n', '\n', '## Supplying a custom synthesizer\n', '\n', 'Benchmark your own synthetic data generation techniques. Define your synthesizer by\n', 'specifying the training logic (using machine learning) and the sampling logic.\n', '\n', '```python\n', 'def my_training_logic(data, metadata):\n', '    # create an object to represent your synthesizer\n', '    # train it using the data\n', '    return synthesizer\n', '\n', 'def my_sampling_logic(trained_synthesizer, num_rows):\n', '    # use the trained synthesizer to create\n', '    # num_rows of synthetic data\n', '    return synthetic_data\n', '```\n', '\n', 'Learn more in the [Custom Synthesizers Guide](https://docs.sdv.dev/sdgym/customization/synthesizers/custom-synthesizers).\n', '\n', '## Customizing your datasets\n', '\n', 'The SDGym library includes many publicly available datasets that you can include right away.\n', 'List these using the ``get_available_datasets`` feature.\n', '\n', '```python\n', 'sdgym.get_available_datasets()\n', '```\n', '\n', '```\n', 'dataset_name   size_MB     num_tables\n', 'KRK_v1         0.072128    1\n', 'adult          3.907448    1\n', 'alarm          4.520128    1\n', 'asia           1.280128    1\n', '...\n', '```\n', '\n', 'You can also include any custom, private datasets that are stored on your computer on an\n', 'Amazon S3 bucket.\n', '\n', '```\n', ""my_datasets_folder = 's3://my-datasets-bucket'\n"", '```\n', '\n', 'For more information, see the docs for [Customized Datasets](https://docs.sdv.dev/sdgym/customization/datasets).\n', '\n', ""# What's next?\n"", '\n', 'Visit the [SDGym Documentation](https://docs.sdv.dev/sdgym) to learn more!\n', '\n', '---\n', '\n', '\n', '<div align=""center"">\n', '<a href=""https://datacebo.com""><img align=""center"" width=40% src=""https://github.com/sdv-dev/SDV/blob/master/docs/images/DataCebo.png""></img></a>\n', '</div>\n', '<br/>\n', '<br/>\n', '\n', ""[The Synthetic Data Vault Project](https://sdv.dev) was first created at MIT's [Data to AI Lab](\n"", 'https://dai.lids.mit.edu/) in 2016. After 4 years of research and traction with enterprise, we\n', 'created [DataCebo](https://datacebo.com) in 2020 with the goal of growing the project.\n', 'Today, DataCebo is the proud developer of SDV, the largest ecosystem for\n', 'synthetic data generation & evaluation. It is home to multiple libraries that support synthetic\n', 'data, including:\n', '\n', '* 🔄 Data discovery & transformation. Reverse the transforms to reproduce realistic data.\n', '* 🧠 Multiple machine learning models -- ranging from Copulas to Deep Learning -- to create tabular,\n', '  multi table and time series data.\n', '* 📊 Measuring quality and privacy of synthetic data, and comparing different synthetic data\n', '  generation models.\n', '\n', '[Get started using the SDV package](https://sdv.dev/SDV/getting_started/install.html) -- a fully\n', 'integrated solution and your one-stop shop for synthetic data. Or, use the standalone libraries\n', 'for specific needs.\n']"
Synthetic+Data,ankush-me/SynthText,ankush-me,https://api.github.com/repos/ankush-me/SynthText,1877,608,3,"['https://api.github.com/users/ankush-me', 'https://api.github.com/users/carandraug', 'https://api.github.com/users/codeVerySlow']",Python,2023-04-05T14:11:34Z,https://raw.githubusercontent.com/ankush-me/SynthText/master/README.md,"['# SynthText\n', 'Code for generating synthetic text images as described in [""Synthetic Data for Text Localisation in Natural Images"", Ankush Gupta, Andrea Vedaldi, Andrew Zisserman, CVPR 2016](https://www.robots.ox.ac.uk/~vgg/data/scenetext/).\n', '\n', '\n', '**Synthetic Scene-Text Image Samples**\n', '![Synthetic Scene-Text Samples](samples.png ""Synthetic Samples"")\n', '\n', 'The code in the `master` branch is for Python2. Python3 is supported in the `python3` branch.\n', '\n', 'The main dependencies are:\n', '\n', '```\n', 'pygame==2.0.0, opencv (cv2), PIL (Image), numpy, matplotlib, h5py, scipy\n', '```\n', '\n', '### Generating samples\n', '\n', '```\n', 'python gen.py --viz [--datadir <path-to-dowloaded-renderer-data>]\n', '```\n', 'where, `--datadir` points to the `renderer_data` directory included in the\n', '[data torrent](https://academictorrents.com/details/2dba9518166cbd141534cbf381aa3e99a087e83c).\n', 'Specifying this `datadir` is optional, and if not specified, the script will\n', 'automatically download and extract the same `renderer.tar.gz` data file (~24 M).\n', 'This data file includes:\n', '\n', '  - **sample.h5**: This is a sample h5 file which contains a set of 5 images along with their depth and segmentation information. Note, this is just given as an example; you are encouraged to add more images (along with their depth and segmentation information) to this database for your own use.\n', '  - **fonts**: three sample fonts (add more fonts to this folder and then update `fonts/fontlist.txt` with their paths).\n', '  - **newsgroup**: Text-source (from the News Group dataset). This can be subsituted with any text file. Look inside `text_utils.py` to see how the text inside this file is used by the renderer.\n', '  - **models/colors_new.cp**: Color-model (foreground/background text color model), learnt from the IIIT-5K word dataset.\n', '  - **models**: Other cPickle files (**char\\_freq.cp**: frequency of each character in the text dataset; **font\\_px2pt.cp**: conversion from pt to px for various fonts: If you add a new font, make sure that the corresponding model is present in this file, if not you can add it by adapting `invert_font_size.py`).\n', '\n', 'This script will generate random scene-text image samples and store them in an h5 file in `results/SynthText.h5`. If the `--viz` option is specified, the generated output will be visualized as the script is being run; omit the `--viz` option to turn-off the visualizations. If you want to visualize the results stored in  `results/SynthText.h5` later, run:\n', '\n', '```\n', 'python visualize_results.py\n', '```\n', '### Pre-generated Dataset\n', 'A dataset with approximately 800000 synthetic scene-text images generated with this code can be found [here](https://www.robots.ox.ac.uk/~vgg/data/scenetext/).\n', '\n', '### Adding New Images\n', 'Segmentation and depth-maps are required to use new images as background. Sample scripts for obtaining these are available [here](https://github.com/ankush-me/SynthText/tree/master/prep_scripts).\n', '\n', '* `predict_depth.m` MATLAB script to regress a depth mask for a given RGB image; uses the network of [Liu etal.](https://bitbucket.org/fayao/dcnf-fcsp/) However, more recent works (e.g., [this](https://github.com/iro-cp/FCRN-DepthPrediction)) might give better results.\n', '* `run_ucm.m` and `floodFill.py` for getting segmentation masks using [gPb-UCM](https://github.com/jponttuset/mcg).\n', '\n', 'For an explanation of the fields in `sample.h5` (e.g.: `seg`,`area`,`label`), please check this [comment](https://github.com/ankush-me/SynthText/issues/5#issuecomment-274490044).\n', '\n', '### Pre-processed Background Images\n', '\n', 'The 8,000 background images used in the paper, along with their\n', 'segmentation and depth masks, are included in the [same\n', 'torrent](https://academictorrents.com/details/2dba9518166cbd141534cbf381aa3e99a087e83c)\n', 'as the pre-generated dataset under the `bg_data` directory.  The files are:\n', '\n', '|    filenames    |                      description                     |\n', '|:--------------- |:---------------------------------------------------- |\n', '| `imnames.cp`    | names of images which do not contain background text |\n', '| `bg_img.tar.gz` | images (filter these using `imnames.cp`)             |\n', '| `depth.h5`      | depth maps                                           |\n', '| `seg.h5`        | segmentation maps                                    |\n', '\n', '#### Downloading without BitTorrent\n', '\n', 'Downloading with BitTorrent is strongly recommended.  If that is not\n', 'possible, the files are also available to download over http from\n', '`https://thor.robots.ox.ac.uk/~vgg/data/scenetext/preproc/<filename>`,\n', 'where, `<filename>` can be:\n', '\n', '|    filenames    | size |             md5 hash             |\n', '|:--------------- | ----:|:-------------------------------- |\n', '| `imnames.cp`    | 180K |                                  |\n', '| `bg_img.tar.gz` | 8.9G | 3eac26af5f731792c9d95838a23b5047 |\n', '| `depth.h5`      |  15G | af97f6e6c9651af4efb7b1ff12a5dc1b |\n', '| `seg.h5`        | 6.9G | 1605f6e629b2524a3902a5ea729e86b2 |\n', '\n', 'Note: due to large size, `depth.h5` is also available for download as 3-part split-files of 5G each.\n', 'These part files are named: `depth.h5-00, depth.h5-01, depth.h5-02`. Download using the path above, and put them together using `cat depth.h5-0* > depth.h5`.\n', 'To download, use the something like the following:\n', '```\n', 'wget --continue https://thor.robots.ox.ac.uk/~vgg/data/scenetext/preproc/<filename>\n', '```\n', '[`use_preproc_bg.py`](https://github.com/ankush-me/SynthText/blob/master/use_preproc_bg.py) provides sample code for reading this data.\n', '\n', 'Note: I do not own the copyright to these images.\n', '\n', '### Generating Samples with Text in non-Latin (English) Scripts\n', '- @JarveeLee has modified the pipeline for generating samples with Chinese text [here](https://github.com/JarveeLee/SynthText_Chinese_version).\n', '- @adavoudi has modified it for arabic/persian script, which flows from right-to-left [here](https://github.com/adavoudi/SynthText).\n', '- @MichalBusta has adapted it for a number of languages (e.g. Bangla, Arabic, Chinese, Japanese, Korean) [here](https://github.com/MichalBusta/E2E-MLT).\n', '- @gachiemchiep has adapted for Japanese [here](https://github.com/gachiemchiep/SynthText).\n', '- @gungui98 has adapted for Vietnamese [here](https://github.com/gungui98/SynthText).\n', '- @youngkyung has adapted for Korean [here](https://github.com/youngkyung/SynthText_kr).\n', '- @kotomiDu has developed an interactive UI for generating images with text [here](https://github.com/kotomiDu/GameSynthText).\n', '- @LaJoKoch has adapted for German [here](https://github.com/LaJoKoch/SynthTextGerman).\n', '\n', '### Further Information\n', 'Please refer to the paper for more information, or contact me (email address in the paper).\n']"
Synthetic+Data,tirthajyoti/Synthetic-data-gen,tirthajyoti,https://api.github.com/repos/tirthajyoti/Synthetic-data-gen,68,37,1,['https://api.github.com/users/tirthajyoti'],Python,2022-11-08T22:55:34Z,https://raw.githubusercontent.com/tirthajyoti/Synthetic-data-gen/master/README.md,"['# Synthetic-data-gen\n', 'Various methods for generating synthetic data for data science and ML.\n', '\n', 'Read my article on Medium **""[Synthetic data generation — a must-have skill for new data scientists](https://towardsdatascience.com/synthetic-data-generation-a-must-have-skill-for-new-data-scientists-915896c0c1ae)""**\n', '\n', 'Also, a related article on generating random variables from scratch: **""[How to generate random variables from scratch (no library used)](https://towardsdatascience.com/how-to-generate-random-variables-from-scratch-no-library-used-4b71eb3c8dc7)""**\n', '\n', '---\n', '## Notebooks\n', '\n', '* [Scikit-learn data generation (regression/classification/clustering) methods](https://github.com/tirthajyoti/Synthetic-data-gen/blob/master/Notebooks/Scikit-learn-data-generation.ipynb)\n', '* [Random regression and classification problem generation from symbolic expressions (using `SymPy`)](https://github.com/tirthajyoti/Synthetic-data-gen/blob/master/Notebooks/Symbolic%20regression%20classification%20generator.ipynb)\n', '* [Synthesizing time series](https://github.com/tirthajyoti/Synthetic-data-gen/blob/master/Notebooks/Synth_Time_series.ipynb)\n', '* [Generating Gaussian mixture model data](https://github.com/tirthajyoti/Synthetic-data-gen/blob/master/Notebooks/GMM_generator.ipynb)\n', '\n', '## Why do you need the skill of synthetic data generation?\n', '\n', 'Imagine you are tinkering with a cool machine learning algorithm like SVM or a deep neural net. What kind of dataset you should practice them on? If you are learning from scratch, the advice is to start with simple, small-scale datasets which you can plot in two dimensions to understand the patterns visually and see for yourself the working of the ML algorithm in an intuitive fashion. For example, [here is an excellent article](https://www.analyticsvidhya.com/blog/2018/05/24-ultimate-data-science-projects-to-boost-your-knowledge-and-skills/) on various datasets you can try at various level of learning.\n', '\n', 'This is a great start. But it is not all.\n', '\n', 'Sure, you can go up a level and find yourself a real-life large dataset to practice the algorithm on. But that is still a fixed dataset, with a fixed number of samples, a fixed pattern, and a fixed degree of class separation between positive and negative samples (if we assume it to be a classification problem). Are you learning all the intricacies of the algorithm in terms of\n', '- sample complexity,\n', '- computational efficiency,\n', '- ability to handle class imbalance,\n', '- robustness of the metrics in the face of varying degree of class separation\n', '- bias-variance trade-off as a function of data complexity\n', '\n', 'Probably not. **Perhaps, no single dataset can lend all these deep insights for a given ML algorithm**. But, these are extremely important insights to master for you to become a true expert practitioner of machine learning. So, you will need an **extremely rich and sufficiently large dataset, which is amenable enough for all these experimentation**.\n', '\n', 'So, what can you do in this situation? Scour the internet for more datasets and just hope that some of them will bring out the limitations and challenges, associated with a particular algorithm, and help you learn?\n', '\n', 'Yes, it is a possible approach but may not be the most viable or optimal one in terms of time and effort. Good datasets may not be clean or easily obtainable. You may spend much more time looking for, extracting, and wrangling with a suitable dataset than putting that effort to understand the ML algorithm.\n', '\n', 'Make no mistake. **The experience of searching for a real life dataset, extracting it, running exploratory data analysis, and wrangling with it to make it suitably prepared for a machine learning based modeling is invaluable**. I know because I wrote a book about it :-)\n', '\n', 'But that can be taught and practiced separately. In many situations, however, **you may just want to have access to a flexible dataset (or several of them) to ‘teach’ you the ML algorithm in all its gory details**.\n', '\n', 'Surprisingly enough, in many cases, such teaching can be done with **synthetic datasets**.\n', '\n', '## What is a synthetic dataset?\n', 'As the name suggests, quite obviously, a synthetic dataset is a repository of data that is generated programmatically. So, it is not collected by any real-life survey or experiment. Its main purpose, therefore, is **to be flexible and rich enough to help an ML practitioner conduct fascinating experiments with various classification, regression, and clustering algorithms**. Desired properties are,\n', '\n', '* It can be numerical, binary, or categorical (ordinal or non-ordinal),\n', '* The number of features and length of the dataset should be arbitrary\n', '* It should preferably be random and the user should be able to choose a wide variety of statistical distribution to base this data upon i.e. the underlying random process can be precisely controlled and tuned,\n', '* If it is used for classification algorithms, then the degree of class separation should be controllable to make the learning problem easy or hard,\n', '* Random noise can be interjected in a controllable manner\n', '* For a regression problem, a complex, non-linear generative process can be used for sourcing the data\n']"
Synthetic+Data,stefan-jansen/synthetic-data-for-finance,stefan-jansen,https://api.github.com/repos/stefan-jansen/synthetic-data-for-finance,79,29,1,['https://api.github.com/users/stefan-jansen'],Python,2023-04-05T12:46:11Z,https://raw.githubusercontent.com/stefan-jansen/synthetic-data-for-finance/main/README.md,"['# Generative Adversarial Nets for Synthetic Time Series Data\n', '\n', 'This repo shows how to create synthetic time-series data using generative adversarial networks (GAN). GANs train a generator and a discriminator network in a competitive setting so that the generator learns to produce samples that the discriminator cannot distinguish from a given class of training data. The goal is to yield a generative model capable of producing synthetic samples representative of this class.\n', 'While most popular with image data, GANs have also been used to generate synthetic time-series data in the medical domain. Subsequent experiments with financial data explored whether GANs can produce alternative price trajectories useful for ML training or strategy backtests. \n', '\n', 'We replicate the 2019 NeurIPS [Time-Series GAN](https://proceedings.neurips.cc/paper/2019/file/c9efe5f26cd17ba6216bbe2a7d26d490-Paper.pdf) paper by Jinsung Yoon, et al., to illustrate the approach and demonstrate the results. The material is based on the 2<sup>nd</sup> edition of my book on [Machine Learning for Trading]((https://www.amazon.com/Machine-Learning-Algorithmic-Trading-alternative/dp/1839217715?pf_rd_r=GZH2XZ35GB3BET09PCCA&pf_rd_p=c5b6893a-24f2-4a59-9d4b-aff5065c90ec&pd_rd_r=91a679c7-f069-4a6e-bdbb-a2b3f548f0c8&pd_rd_w=2B0Q0&pd_rd_wg=GMY5S&ref_=pd_gw_ci_mcx_mr_hp_d)) (see [GitHub repo](https://github.com/stefan-jansen/machine-learning-for-trading)).  \n', '\n', '<p align=""center"">\n', '<img src=""https://i.imgur.com/W1Rp89K.png"" width=""60%"">\n', '</p>\n', '\n', '## Content\n', '\n', '1. [Generative adversarial networks for synthetic data](#generative-adversarial-networks-for-synthetic-data)\n', '    * [Comparing generative and discriminative models](#comparing-generative-and-discriminative-models)\n', '    * [Adversarial training: a zero-sum game of trickery](#adversarial-training-a-zero-sum-game-of-trickery)\n', '2. [Code example: TimeGAN: Adversarial Training for Synthetic Financial Data](#code-example-timegan-adversarial-training-for-synthetic-financial-data)\n', '    * [Learning the data generation process across features and time](#learning-the-data-generation-process-across-features-and-time)\n', '    * [Combining adversarial and supervised training with time-series embedding](#combining-adversarial-and-supervised-training-with-time-series-embedding)\n', '    * [The four components of the TimeGAN architecture](#the-four-components-of-the-timegan-architecture)\n', '    * [Implementing TimeGAN using TensorFlow 2](#implementing-timegan-using-tensorflow-2)\n', '    * [Evaluating the quality of synthetic time-series data](#evaluating-the-quality-of-synthetic-time-series-data)\n', '3. [Resources](#resources)\n', ""    * [How GAN's work](#how-gans-work)\n"", '    * [Implementation](#implementation)\n', '    * [The rapid evolution of the GAN architecture zoo](#the-rapid-evolution-of-the-gan-architecture-zoo)\n', '    * [Applications](#applications)\n', '\n', '## Generative adversarial networks for synthetic data\n', '\n', 'The [book](https://www.amazon.com/Machine-Learning-Algorithmic-Trading-alternative/dp/1839217715?pf_rd_r=GZH2XZ35GB3BET09PCCA&pf_rd_p=c5b6893a-24f2-4a59-9d4b-aff5065c90ec&pd_rd_r=91a679c7-f069-4a6e-bdbb-a2b3f548f0c8&pd_rd_w=2B0Q0&pd_rd_wg=GMY5S&ref_=pd_gw_ci_mcx_mr_hp_d) mostly focuses on supervised learning algorithms that receive input data and predict an outcome, which we can compare to the ground truth to evaluate their performance. Such algorithms are also called discriminative models because they learn to differentiate between different output values.\n', 'Generative adversarial networks (GANs) are an instance of generative models like the variational autoencoder covered in [Chapter 20](https://github.com/stefan-jansen/machine-learning-for-trading/tree/master/20_autoencoders_for_conditional_risk_factors).\n', '\n', '### Comparing generative and discriminative models\n', '\n', 'Discriminative models learn how to differentiate among outcomes y, given input data X. In other words, they learn the probability of the outcome given the data: p(y | X). Generative models, on the other hand, learn the joint distribution of inputs and outcome p(y, X). \n', '\n', 'While generative models can be used as discriminative models using Bayes Rule to compute which class is most likely (see [Chapter 10](https://github.com/stefan-jansen/machine-learning-for-trading/tree/master/10_bayesian_machine_learning)), it appears often preferable to solve the prediction problem directly rather than by solving the more general generative challenge first.\n', '\n', '### Adversarial training: a zero-sum game of trickery\n', '\n', 'The key innovation of GANs is a new way of learning the data-generating probability distribution. The algorithm sets up a competitive, or adversarial game between two neural networks called the generator and the discriminator.\n', '\n', '<p align=""center"">\n', '<img src=""https://i.imgur.com/0vuUsY0.png"" width=""80%"">\n', '</p>\n', '\n', '## Code example: How to build a GAN using TensorFlow 2\n', '\n', 'To illustrate the implementation of a generative adversarial network using Python, we use the deep convolutional GAN (DCGAN) example discussed earlier in this section to synthesize images from the fashion MNIST dataset that we first encountered in Chapter 13. \n', '\n', 'The notebook [deep_convolutional_generative_adversarial_network](https://github.com/stefan-jansen/machine-learning-for-trading/blob/master/21_gans_for_synthetic_time_series/01_deep_convolutional_generative_adversarial_network.ipynb) illustrates the implementation of a GAN using Python. It uses the Deep Convolutional GAN (DCGAN) example to synthesize images from the fashion MNIST dataset\n', '\n', '## Code example: TimeGAN: Adversarial Training for Synthetic Financial Data\n', '\n', 'Generating synthetic time-series data poses specific challenges above and beyond those encountered when designing GANs for images. \n', 'In addition to the distribution over variables at any given point, such as pixel values or the prices of numerous stocks, a generative model for time-series data should also learn the temporal dynamics that shapes how one sequence of observations follows another (see also discussion in Chapter 9: [Time Series Models for Volatility Forecasts and Statistical Arbitrage](../09_time_series_models)).\n', '\n', 'Very recent and promising research by Yoon, Jarrett, and van der Schaar, presented at NeurIPS in December 2019, introduces a novel [Time-Series Generative Adversarial Network](https://papers.nips.cc/paper/8789-time-series-generative-adversarial-networks.pdf) (TimeGAN) framework that aims to account for temporal correlations by combining supervised and unsupervised training. \n', 'The model learns a time-series embedding space while optimizing both supervised and adversarial objectives that encourage it to adhere to the dynamics observed while sampling from historical data during training. \n', 'The authors test the model on various time series, including historical stock prices, and find that the quality of the synthetic data significantly outperforms that of available alternatives.\n', '\n', '### Learning the data generation process across features and time\n', '\n', 'A successful generative model for time-series data needs to capture both the cross-sectional distribution of features at each point in time and the longitudinal relationships among these features over time. \n', 'Expressed in the image context we just discussed, the model needs to learn not only what a realistic image looks like, but also how one image evolves from the next as in a video.\n', '\n', '### Combining adversarial and supervised training with time-series embedding\n', '\n', 'Prior attempts at generating time-series data like the recurrent (conditional) GAN relied on recurrent neural networks (RNN, see Chapter 19, [RNN for Multivariate Time Series and Sentiment Analysis](../19_recurrent_neural_nets)) in the roles of generator and discriminator. \n', '\n', 'TimeGAN explicitly incorporates the autoregressive nature of time series by combining the unsupervised adversarial loss on both real and synthetic sequences familiar from the DCGAN example with a stepwise supervised loss with respect to the original data. \n', 'The goal is to reward the model for learning the distribution over transitions from one point in time to the next present in the historical data.\n', '\n', '### The four components of the TimeGAN architecture\n', '\n', 'The TimeGAN architecture combines an adversarial network with an autoencoder and has thus four network components as depicted in Figure 21.4:\n', 'Autoencoder: embedding and recovery networks\n', 'Adversarial Network: sequence generator and sequence discriminator components\n', '<p align=""center"">\n', '<img src=""https://i.imgur.com/WqoXbr8.png"" width=""80%"">\n', '</p>\n', '\n', '### Implementing TimeGAN using TensorFlow 2\n', '\n', 'In this section, we implement the TimeGAN architecture just described. The authors provide sample code using TensorFlow 1 that we port to TensorFlow 2. Building and training TimeGAN requires several steps:\n', '1. Selecting and preparing real and random time series inputs\n', '2. Creating the key TimeGAN model components\n', '3. Defining the various loss functions and train steps used during the three training phases\n', '4. Running the training loops and logging the results\n', '5. Generating synthetic time series and evaluating the results\n', '\n', 'The notebook [TimeGAN_TF2](02_TimeGAN_TF2.ipynb) shows how to implement these steps.\n', '\n', '### Installation\n', '\n', 'Using a GPU is recommended to speed up training. There are several options to run the notebook:\n', '1) Use a [Docker](https://docs.docker.com/get-started/overview/) image provided by TensorFlow with either CPU or GPU support. See [instructions](https://www.tensorflow.org/install/docker). \n', ""    - To start the container configured with TensorFlow and mount the project directory in the `/home` directory, run the following command in this repo's root folder on your machine:\n"", '        - With GPU support (using [nvidia-docker](https://github.com/NVIDIA/nvidia-docker) as describe in the linked [instructions](https://www.tensorflow.org/install/docker)):\n', '            ```bash\n', '            docker run --gpus all -it -v $(pwd):/home -p 8888:8888 --name ml4t tensorflow/tensorflow:latest-gpu-jupyter bash\n', '          ```\n', '      - With CPU support:\n', '          ```bash\n', '          docker run -it -v $(pwd):/home -p 8888:8888 --name ml4t tensorflow/tensorflow:latest-gpu-jupyter bash\n', '          ```\n', '    - Change into the `/home` folder of your container using `cd /home`\n', '    - Run the install script to get some requisite packages: `./install.sh`\n', '    - Then, launch the jupyter server to work with the notebooks as usual:\n', '        ```bash\n', '        jupyter notebook --ip 0.0.0.0 --no-browser --allow-root\n', '        ```\n', '2) Create a virtual environment using the `requirements.txt` file (Ubuntu only; other OS requires modifying the content).\n', '\n', '### Evaluating the quality of synthetic time-series data\n', '\n', 'The TimeGAN authors assess the quality of the generated data with respect to three practical criteria:\n', '1. **Diversity**: the distribution of the synthetic samples should roughly match that of the real data\n', '2. **Fidelity**: the sample series should be indistinguishable from the real data, and \n', '3. **Usefulness**: the synthetic data should be as useful as their real counterparts for solving a predictive task\n', '\n', 'The authors apply three methods to evaluate whether the synthetic data actually exhibits these characteristics:\n', '1. **Visualization**: for a qualitative diversity assessment of diversity, we use dimensionality reduction (principal components analysis (PCA) and t-SNE, see Chapter 13) to visually inspect how closely the distribution of the synthetic samples resembles that of the original data\n', '2. **Discriminative Score**: for a quantitative assessment of fidelity, the test error of a time-series classifier such as a 2-layer LSTM (see Chapter 18) let’s us evaluate whether real and synthetic time series can be differentiated or are, in fact, indistinguishable.\n', '3. **Predictive Score**: for a quantitative measure of usefulness, we can compare the test errors of a sequence prediction model trained on, alternatively, real or synthetic data to predict the next time step for the real data.\n', '\n', 'The notebook [evaluating_synthetic_data](03_evaluating_synthetic_data.ipynb) contains the relevant code samples.\n', '\n', '## Resources\n', '\n', ""### How GAN's work\n"", '\n', '- [NIPS 2016 Tutorial: Generative Adversarial Networks](https://arxiv.org/pdf/1701.00160.pdf), Ian Goodfellow, 2017\n', '- [Why is unsupervised learning important?](https://www.quora.com/Why-is-unsupervised-learning-important), Yoshua Bengio on Quora, 2018\n', '- [GAN Lab: Understanding Complex Deep Generative Models using Interactive Visual Experimentation](https://www.groundai.com/project/gan-lab-understanding-complex-deep-generative-models-using-interactive-visual-experimentation/), Minsuk Kahng, Nikhil Thorat, Duen Horng (Polo) Chau, Fernanda B. Viégas, and Martin Wattenberg, IEEE Transactions on Visualization and Computer Graphics, 25(1) (VAST 2018), Jan. 2019\n', '    - [GitHub](https://poloclub.github.io/ganlab/)\n', '- [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661), Ian Goodfellow, et al, 2014\n', '- [Generative Adversarial Networks: an Overview](https://arxiv.org/pdf/1710.07035.pdf), Antonia Creswell, et al, 2017\n', '- [Generative Models](https://blog.openai.com/generative-models/), OpenAI Blog\n', '\n', '### Implementation\n', '\n', '- [Deep Convolutional Generative Adversarial Network](https://www.tensorflow.org/tutorials/generative/dcgan)\n', '- [CycleGAN](https://www.tensorflow.org/tutorials/generative/cyclegan)\n', '- [Keras-GAN](https://github.com/eriklindernoren/Keras-GAN), numerous Keras GAN implementations\n', '- [PyTorch-GAN](https://github.com/eriklindernoren/PyTorch-GAN), numerous PyTorch GAN implementations\n', '\n', '\n', '### The rapid evolution of the GAN architecture zoo\n', '\n', '- [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (DCGAN)](https://arxiv.org/pdf/1511.06434.pdf), Luke Metz et al, 2016\n', '- [Conditional Generative Adversarial Net](https://arxiv.org/pdf/1411.1784.pdf), Medhi Mirza and Simon Osindero, 2014\n', '- [Infogan: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets](https://arxiv.org/pdf/1606.03657.pdf), Xi Chen et al, 2016\n', '- [Stackgan: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks](https://arxiv.org/pdf/1612.03242.pdf), Shaoting Zhang et al, 2016\n', '- [Photo-realistic Single Image Super-resolution Using a Generative Adversarial Network](https://arxiv.org/pdf/1609.04802.pdf), Alejando Acosta et al, 2016\n', '- [Unpaired Image-to-image Translation Using Cycle-consistent Adversarial Networks](https://arxiv.org/pdf/1703.10593.pdf), Juan-Yan Zhu et al, 2018\n', '- [Learning What and Where to Draw](https://arxiv.org/abs/1610.02454), Scott Reed, et al 2016\n', '- [Fantastic GANs and where to find them](http://guimperarnau.com/blog/2017/03/Fantastic-GANs-and-where-to-find-them)\n', '\n', '### Applications\n', '\n', '- [Real-valued (Medical) Time Series Generation with Recurrent Conditional GANs](https://arxiv.org/abs/1706.02633), Cristóbal Esteban, Stephanie L. Hyland, Gunnar Rätsch, 2016\n', '    - [GitHub Repo](https://github.com/ratschlab/RGAN)\n', '- [MAD-GAN: Multivariate Anomaly Detection for Time Series Data with Generative Adversarial Networks](https://arxiv.org/pdf/1901.04997.pdf), Dan Li, Dacheng Chen, Jonathan Goh, and See-Kiong Ng, 2019\n', '    - [GitHub Repo](https://github.com/LiDan456/MAD-GANs)\n', '- [GAN\u200a—\u200aSome cool applications](https://medium.com/@jonathan_hui/gan-some-cool-applications-of-gans-4c9ecca35900), Jonathan Hui, 2018\n', '- [gans-awesome-applications](https://github.com/nashory/gans-awesome-applications), curated list of awesome GAN applications\n', '\n', '\n', '\n']"
Synthetic+Data,microsoft/synthetic-data-showcase,microsoft,https://api.github.com/repos/microsoft/synthetic-data-showcase,76,25,8,"['https://api.github.com/users/rracanicci', 'https://api.github.com/users/dworthen', 'https://api.github.com/users/natoverse', 'https://api.github.com/users/darrenedge', 'https://api.github.com/users/katua', 'https://api.github.com/users/andresmor-ms', 'https://api.github.com/users/microsoftopensource', 'https://api.github.com/users/microsoft-github-operations%5Bbot%5D']",Rust,2023-04-09T06:50:54Z,https://raw.githubusercontent.com/microsoft/synthetic-data-showcase/main/README.md,"['[![Rust CI](https://github.com/microsoft/synthetic-data-showcase/actions/workflows/rust-ci.yml/badge.svg?branch=main&event=push)](https://github.com/microsoft/synthetic-data-showcase/actions/workflows/rust-ci.yml)\n', '[![Javascript CI](https://github.com/microsoft/synthetic-data-showcase/actions/workflows/javascript-ci.yml/badge.svg?branch=main&event=push)](https://github.com/microsoft/synthetic-data-showcase/actions/workflows/javascript-ci.yml)\n', '[![Python CI](https://github.com/microsoft/synthetic-data-showcase/actions/workflows/python-ci.yml/badge.svg?branch=main&event=push)](https://github.com/microsoft/synthetic-data-showcase/actions/workflows/python-ci.yml)\n', '\n', '# Synthetic data showcase\n', '\n', '> Generates synthetic data and user interfaces for privacy-preserving data sharing and analysis.\n', '\n', '> Free-to-use web application for private data release: https://microsoft.github.io/synthetic-data-showcase/\n', '\n', '# Overview\n', '\n', 'In many cases, the best way to share sensitive datasets is not to share the actual sensitive datasets, but user interfaces to derived datasets that are inherently anonymous. Our name for such an interface is a _data showcase_. In this project, we provide an automated set of tools for generating the three elements of a _synthetic data showcase_:\n', '\n', '1. _Synthetic data_ representing the overall structure and statistics of the input data, without describing actual identifiable individuals.\n', '2. _Aggregate data_ reporting the number of individuals with different combinations of attributes, without disclosing exact counts.\n', '3. _Data dashboards_ enabling exploratory visual analysis of both datasets, without the need for custom data science or interface development.\n', '\n', 'To generate these elements, our tool provides two approaches to create anonymous datasets that are safe to release: (i) differential privacy and (ii) k-anonymity.\n', '\n', '# Differential privacy\n', '\n', '## Privacy guarantees\n', '\n', 'The paradigm of differential privacy (DP) offers ""safety in noise"" &ndash; just enough calibrated noise is added to the data to control the maximum possible privacy loss, $\\varepsilon$ (epsilon). When applied in the context of private data release, $\\varepsilon$ bounds the ratio of probabilities of getting an arbitrary result to an arbitrary computation when using two synthetic datasets &ndash; one generated from the sensitive dataset itself and the other from a neighboring dataset missing a single arbitrary record.\n', '\n', 'Our approach to synthesizing data with differential privacy first protects attribute combination counts in the aggregate data using our [DP Marginals](./docs/dp/dp_marginals.pdf) algorithm and then uses the resulting DP aggregate counts to derive synthetic records that retain differential privacy under the post-processing property.\n', '\n', '> For a detailed explanation of how SDS uses differential privacy, please check our [DP documentation](./docs/dp/README.md).\n', '\n', '## Usage\n', '\n', 'Use of our differential privacy synthesizer is recommended for **repeated data releases** where cumulative privacy loss must be quantified and controlled and where provable guarantees against all possible privacy attacks are desired.\n', '\n', 'Any differentially-private dataset should be evaluated for potential risks in situations where missing, fabricated, or inaccurate counts of attribute combinations could trigger inappropriate downstream decisions or actions. Our DP synthesizer prioritises the release of accurate combination counts (with minimal noise) of actual combinations (with minimal fabrication).\n', '\n', '# K-anonymity\n', '\n', '## Privacy guarantees\n', '\n', 'The paradigm of k-anonymity offers ""safety in numbers"" &ndash; combinations of attributes are only released when they occur at least k times in the sensitive dataset. When applied in the context of private data release, we interpret k as a privacy resolution determining the minimum group size that will be (a) reported explicitly in the aggregate dataset and (b) represented implicitly by the records of the synthetic dataset. This makes it possible to offer privacy guarantees in clearly understandable terms, e.g.:\n', '\n', '""All attribute combinations in this synthetic dataset describe groups of 10 or more individuals in the original sensitive dataset, therefore may never be used to infer the presence of individuals or groups smaller than 10.""\n', '\n', 'Our approach to synthesizing data with k-anonymity overcomes many of the limitations of standard [k-anonymization](https://en.wikipedia.org/wiki/K-anonymity), in which attributes of sensitive data records are generalized and suppressed until k-anonymity is reached, and only for those attributes determined in advance to be potentially identifying when used in combination (so-called quasi-identifiers). In this standard approach, all remaining sensitive attributes are released so long as k-anonymity holds for the designated quasi-identifiers. This makes the records (and thus subjects) of k-anonymized datasets susceptible to linking attacks based on auxiliary data or background knowledge.\n', '\n', 'In contrast, our k-anonymity synthesizers generate synthetic records that do not represent actual individuals, yet are composed exclusively from common combinations of attributes in the sensitive dataset. The k-anonymity guarantee therefore holds for all data columns and all combinations of attributes.\n', '\n', '## Usage\n', '\n', 'Use of our k-anonymity synthesizers is recommended only for **one-off data releases** where there is a need for precise counts of attribute combinations (at a given privacy resolution).\n', '\n', 'These synthesizers are designed to offer strong group-level protection against membership inference, i.e., preventing an adversary from inferring whether a known individual or small group of individuals is present in the sensitive dataset.\n', '\n', 'They should not be used in situations where attribute inference from homogeneity attacks are a concern, i.e., when an adversary knows that a certain individual is present in the sensitive dataset, identifies them as part of a group sharing known attributes, and then infers previously unknown attributes of the individual because those attributes are common to the group.\n', '\n', '# Quick setup\n', '\n', 'The easiest way to start is to [run the web application locally with docker](./packages/webapp/README.md#locally-run-the-web-application-with-docker). You will be able to experiment with your data and see the result in real time using the UI.\n', '\n', 'If you are looking for faster alternatives to process bigger datasets, please refer to our [python pipeline tool](./packages/python-pipeline/README.md), [CLI application tool](./packages/cli/README.md) or [python synthesizer library](./packages/lib-pacsynth/README.md).\n', '\n', '# All available tools\n', '\n', 'We provide a set of tools to synthesize, aggregate and evaluate your data, which can be used according to your use case/preference. The available tools are described below:\n', '\n', '- **Python pipeline**: if you want to synthesize, aggregate your data and also generate the dashboards for visual analysis with a single command line command in python, please check the [python pipeline tool](./packages/python-pipeline/README.md).\n', '- **Web application**: if you want to locally run a web application capable of synthesize, aggregate and evaluate your data directly on your browser using Javascript and Web Assembly, this is the tool for you. The data is processed locally and never leaves your machine. Please check the [web application tool](./packages/webapp/README.md).\n', '- **Raw CLI application**: if you only want a command line interface (CLI) around our [core Rust library](./packages/core/README.md) for data synthesis and aggregation, please check the [CLI application tool](./packages/cli/README.md).\n', '- **pac-synth library**: if want to aggregate and synthesize data locally with python, please check the [python synthesizer library](./packages/lib-pacsynth/README.md).\n', '\n', '# Quick references\n', '\n', '- [python-pipeline](./packages/python-pipeline/README.md)\n', '- [webapp](./packages/webapp/README.md)\n', '- [cli](./packages/cli/README.md)\n', '- [core](./packages/core/README.md)\n', '- [lib-wasm](./packages/lib-wasm/README.md)\n', '- [lib-python](./packages/lib-python/README.md)\n', '- [lib-pacsynth](./packages/lib-pacsynth/README.md)\n', '\n', '# License\n', '\n', 'Synthetic data showcase\n', '\n', 'MIT License\n', '\n', 'Copyright (c) Microsoft Corporation.\n', '\n', 'Permission is hereby granted, free of charge, to any person obtaining a copy\n', 'of this software and associated documentation files (the ""Software""), to deal\n', 'in the Software without restriction, including without limitation the rights\n', 'to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n', 'copies of the Software, and to permit persons to whom the Software is\n', 'furnished to do so, subject to the following conditions:\n', '\n', 'The above copyright notice and this permission notice shall be included in all\n', 'copies or substantial portions of the Software.\n', '\n', 'THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n', 'IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n', 'FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n', 'AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n', 'LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n', 'OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n', 'SOFTWARE\n', '\n', '# Contributing\n', '\n', 'This project welcomes contributions and suggestions. Most contributions require you to agree to a\n', 'Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\n', 'the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n', '\n', 'When you submit a pull request, a CLA bot will automatically determine whether you need to provide\n', 'a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\n', 'provided by the bot. You will only need to do this once across all repos using our CLA.\n', '\n', 'This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\n', 'For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\n', 'contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n', '\n', '# Acknowledgements\n', '\n', 'This project resulted from a [Tech Against Trafficking (TAT)](https://techagainsttrafficking.org/) accelerator program with the [Counter Trafficking Data Collaborative (CTDC)](https://www.ctdatacollaborative.org/) and the [International Organization for Migration (IOM)](https://www.iom.int/) on how to safely share data on identified victims of human trafficking. Read more in this [TAT blog post](https://techagainsttrafficking.org/accelerating-toward-data-insights-tech-against-trafficking-successfully-concludes-its-pilot-accelerator/).\n', '\n', '# Contact\n', '\n', 'Feedback and suggestions are welcome via email to sds-team@microsoft.com.\n']"
Synthetic+Data,theodi/synthetic-data-tutorial,theodi,https://api.github.com/repos/theodi/synthetic-data-tutorial,71,26,2,"['https://api.github.com/users/fionntan', 'https://api.github.com/users/olivierthereaux']",Python,2023-02-03T09:27:35Z,https://raw.githubusercontent.com/theodi/synthetic-data-tutorial/master/README.md,"['_Last tested: 2022-04-14. Updated the requirements and ran in Python 3.10 (although a few warnings from Pandas)._\n', '\n', '# Anonymisation with Synthetic Data Tutorial\n', '\n', '## Some questions\n', '\n', '**What is this?**\n', '\n', 'A hands-on tutorial showing how to use Python to create synthetic data.\n', '\n', '**Wait, what is this ""synthetic data"" you speak of?**\n', '\n', ""It's data that is created by an automated process which contains many of the statistical patterns of an original dataset. It is also sometimes used as a way to release data that has no personal information in it, even if the original did contain lots of data that could identify people. This means programmers and data scientists can crack on with building software and algorithms that they know will work similarly on the real data.\n"", '\n', '**Who is this tutorial for?**\n', '\n', 'For any person who programs who wants to learn about data anonymisation in general or more specifically about synthetic data.\n', '\n', '**What is it not for?**\n', '\n', ""Non-programmers. Although we think this tutorial is still worth a browse to get some of the main ideas in what goes in to anonymising a dataset. However, if you're looking for info on how to create synthetic data using the latest and greatest deep learning techniques, this is not the tutorial for you.\n"", '\n', '**Who are you?**\n', '\n', ""We're the Open Data Institute. We work with companies and governments to build an open, trustworthy data ecosystem. Anonymisation and synthetic data are some of the many, many ways we can responsibly increase access to data. If you want to learn more, [check out our site](http://theodi.org).\n"", '\n', '**Why did you make this?**\n', '\n', ""We have an [R&D program](https://theodi.org/project/data-innovation-for-uk-research-and-development/) that has a number of projects looking in to how to support innovation, improve data infrastructure and encourage ethical data sharing. One of our projects is about [managing the risks of re-identification](https://theodi.org/project/rd-broaden-access-to-personal-data-while-protecting-privacy-and-creating-a-fair-market/) in shared and open data. As you can see in the *Key outputs* section, we have other material from the project, but we thought it'd be good to have something specifically aimed at programmers who are interested in learning by doing.\n"", '\n', '**Speaking of which, can I just get to the tutorial now?**\n', '\n', ""Sure! Let's go.\n"", '\n', '## Overview\n', '\n', ""In this tutorial you are aiming to create a safe version of accident and emergency (A&E) admissions data, collected from multiple hospitals. This data contains some sensitive personal information about people's health and can't be openly shared. By removing and altering certain identifying information in the data we can greatly reduce the risk that patients can be re-identified and therefore hope to release the data.\n"", '\n', ""Just to be clear, we're not using actual A&E data but are creating our own simple, mock, version of it.\n"", '\n', 'The practical steps involve:\n', '\n', '1. Create an A&E admissions dataset which will contain (pretend) personal information.\n', '2. Run some anonymisation steps over this dataset to generate a new dataset with much less re-identification risk.\n', '3. Take this de-identified dataset and generate multiple synthetic datasets from it to reduce the re-identification risk even further.\n', '4. Analyse the synthetic datasets to see how similar they are to the original data.\n', '\n', ""You may be wondering, why can't we just do synthetic data step? If it's synthetic surely it won't contain any personal information?\n"", '\n', 'Not exactly. Patterns picked up in the original data can be transferred to the synthetic data. This is especially true for outliers. For instance if there is only one person from an certain area over 85 and this shows up in the synthetic data, we would be able to re-identify them.\n', '\n', '## Credit to others\n', '\n', ""This tutorial is inspired by the [NHS England and ODI Leeds' research](https://odileeds.org/events/synae/) in creating a synthetic dataset from NHS England's accident and emergency admissions. Please do read about their project, as it's really interesting and great for learning about the benefits and risks in creating synthetic data.\n"", '\n', ""Also, the synthetic data generating library we use is [DataSynthetizer](https://homes.cs.washington.edu/~billhowe//projects/2017/07/20/Data-Synthesizer.html) and comes as part of this codebase. Coming from researchers in Drexel University and University of Washington, it's an excellent piece of software and their research and papers are well worth checking out. It's available as a [repo on Github](https://github.com/DataResponsibly/DataSynthesizer) which includes some short tutorials on how to use the toolkit and an accompanying research paper describing the theory behind it.\n"", '\n', '---\n', '\n', '## Setup\n', '\n', 'First, make sure you have [Python3 installed](https://www.python.org/downloads/). Minimum Python 3.6.\n', '\n', 'Download this repository either as a zip or clone using Git.\n', '\n', 'Install required dependent libraries. You can do that, for example, with a _virtualenv_.\n', '\n', '```bash\n', 'cd /path/to/repo/synthetic_data_tutorial/\n', 'pip install -r requirements.txt\n', '```\n', '\n', ""Next we'll go through how to create, de-identify and synthesise the code. We'll show this using code snippets but the full code is contained within the `/tutorial` directory.\n"", '\n', ""There's small differences between the code presented here and what's in the Python scripts but it's mostly down to variable naming. I'd encourage you to run, edit and play with the code locally.\n"", '\n', '## Generate mock NHS A&E dataset\n', '\n', 'The data already exists in `data/nhs_ae_mock.csv` so feel free to browse that. But you should generate your own fresh dataset using the `tutorial/generate.py` script.\n', '\n', ""To do this, you'll need to download one dataset first. It's a list of all postcodes in London. You can find it at this page on [doogal.co.uk](https://www.doogal.co.uk/PostcodeDownloads.php), at the _London_ link under the _By English region_ section. Or just download it directly at [this link](https://www.doogal.co.uk/UKPostcodesCSV.ashx?region=E12000007) (just take note, it's 133MB in size), then place the `London postcodes.csv` file in to the `data/` directory.\n"", '\n', 'Or you can just do it using `curl`.\n', '\n', '```bash\n', 'curl -o ""./data/London postcodes.csv"" https://www.doogal.co.uk/UKPostcodesCSV.ashx?region=E12000007\n', '```\n', '\n', 'Then, to generate the data, from the project root directory run the `generate.py` script.\n', '\n', '```bash\n', 'python tutorial/generate.py\n', '```\n', '\n', ""Voila! You'll now see a new `hospital_ae_data.csv` file in the `/data` directory. Open it up and have a browse. It's contains the following columns:\n"", '\n', '- **Health Service ID**: NHS number of the admitted patient  \n', '- **Age**: age of patient\n', '- **Time in A&E (mins)**: time in minutes of how long the patient spent in A&E. This is generated to correlate with the age of the patient.\n', '- **Hospital**: which hospital admitted the patient - with some hospitals being more prevalent in the data than others\n', '- **Arrival Time**: what time and date the patient was admitted - with weekends as busier and and a different peak time for each day\n', '- **Treatment**: what the person was treated for - with certain treatments being more common than others\n', '- **Gender**: patient gender - based on [NHS patient gender codes](https://www.datadictionary.nhs.uk/data_dictionary/attributes/p/person/person_gender_code_de.asp?shownav=1)\n', '- **Postcode**: postcode of patient - random, in use, London postcodes extracted from the `London postcodes.csv` file.\n', '\n', ""We can see this dataset obviously contains some personal information. For instance, if we knew roughly the time a neighbour went to A&E we could use their postcode to figure out exactly what ailment they went in with. Or, if a list of people's Health Service ID's were to be leaked in future, lots of people could be re-identified.\n"", '\n', ""Because of this, we'll need to take some de-identification steps.\n"", '\n', '---\n', '\n', '## De-identification\n', '\n', ""For this stage, we're going to be loosely following the de-identification techniques used by Jonathan Pearson of NHS England, and described in a blog post about [creating its own synthetic data](https://odileeds.org/blog/2019-01-24-exploring-methods-for-creating-synthetic-a-e-data).\n"", '\n', ""If you look in `tutorial/deidentify.py` you'll see the full code of all de-identification steps. You can run this code easily.\n"", '\n', '```bash\n', 'python tutorial/deidentify.py\n', '```\n', '\n', 'It takes the `data/hospital_ae_data.csv` file, run the steps, and saves the new dataset to `data/hospital_ae_data_deidentify.csv`.\n', '\n', 'Breaking down each of these steps. It first loads the `data/nhs_ae_data.csv` file in to the Pandas DataFrame as `hospital_ae_df`.\n', '\n', '```python\n', '# _df is a common way to refer to a Pandas DataFrame object\n', 'hospital_ae_df = pd.read_csv(filepaths.hospital_ae_data)\n', '```\n', '\n', '(`filepaths.py` is, surprise, surprise, where all the filepaths are listed)\n', '\n', '### Remove Health Service ID numbers\n', '\n', ""Health Service ID numbers are direct identifiers and should be removed. So we'll simply drop the entire column.\n"", '\n', '```python\n', ""hospital_ae_df = hospital_ae_df.drop('Health Service ID', 1)\n"", '```\n', '\n', '### Where a patient lives\n', '\n', ""Pseudo-identifiers, also known as [quasi-identifiers](https://en.wikipedia.org/wiki/Quasi-identifier), are pieces of information that don't directly identify people but can used with other information to identify a person. If we were to take the age, postcode and gender of a person we could combine these and check the dataset to see what that person was treated for in A&E.\n"", '\n', 'The data scientist from NHS England, Jonathan Pearson, describes this in the blog post:\n', '\n', '> I started with the postcode of the patients resident lower super output area (LSOA). This is a geographical definition with an average of 1500 residents created to make reporting in England and Wales easier. I wanted to keep some basic information about the area where the patient lives whilst completely removing any information regarding any actual postcode. A key variable in health care inequalities is the patients Index of Multiple deprivation (IMD) decile (broad measure of relative deprivation) which gives an average ranked value for each LSOA. By replacing the patients resident postcode with an IMD decile I have kept a key bit of information whilst making this field non-identifiable.\n', '\n', ""We'll do just the same with our dataset.\n"", '\n', ""First we'll map the rows' postcodes to their LSOA and then drop the postcodes column.\n"", '\n', '```python\n', 'postcodes_df = pd.read_csv(filepaths.postcodes_london)\n', 'hospital_ae_df = pd.merge(\n', '    hospital_ae_df,\n', ""    postcodes_df[['Postcode', 'Lower layer super output area']],\n"", ""    on='Postcode'\n"", ')\n', ""hospital_ae_df = hospital_ae_df.drop('Postcode', 1)\n"", '```\n', '\n', 'Then we\'ll add a mapped column of ""Index of Multiple Deprivation"" column for each entry\'s LSOA.\n', '\n', '```python\n', 'hospital_ae_df = pd.merge(\n', '    hospital_ae_df,\n', ""    postcodes_df[['Lower layer super output area', 'Index of Multiple Deprivation']].drop_duplicates(),\n"", ""    on='Lower layer super output area'\n"", ')\n', '```\n', '\n', ""Next calculate the decile bins for the IMDs by taking all the IMDs from large list of London. We'll use the Pandas `qcut` (quantile cut), function for this.\n"", '\n', '```python\n', '_, bins = pd.qcut(\n', ""    postcodes_df['Index of Multiple Deprivation'],\n"", '    10,\n', '    retbins=True,\n', '    labels=False\n', ')\n', '```\n', '\n', ""Then we'll use those decile `bins` to map each row's IMD to its IMD decile.\n"", '\n', '```python\n', '# add +1 to get deciles from 1 to 10 (not 0 to 9)\n', ""hospital_ae_df['Index of Multiple Deprivation Decile'] = pd.cut(\n"", ""    hospital_ae_df['Index of Multiple Deprivation'],\n"", '    bins=bins,\n', '    labels=False,\n', '    include_lowest=True) + 1\n', '```\n', '\n', 'And finally drop the columns we no longer need.\n', '\n', '```python\n', ""hospital_ae_df = hospital_ae_df.drop('Index of Multiple Deprivation', 1)\n"", ""hospital_ae_df = hospital_ae_df.drop('Lower layer super output area', 1)\n"", '```\n', '\n', '### Individual hospitals\n', '\n', 'The data scientist at NHS England masked individual hospitals giving the following reason.\n', '\n', '> As each hospital has its own complex case mix and health system, using these data to identify poor performance or possible improvements would be invalid and un-helpful. Therefore, I decided to replace the hospital code with a random number.\n', '\n', ""So we'll do as they did, replacing hospitals with a random six-digit ID.\n"", '\n', '```python\n', ""hospitals = hospital_ae_df['Hospital'].unique().tolist()\n"", 'random.shuffle(hospitals)\n', 'hospitals_map = {\n', ""    hospital : ''.join(random.choices(string.digits, k=6))\n"", '    for hospital in hospitals\n', '}\n', ""hospital_ae_df['Hospital ID'] = hospital_ae_df['Hospital'].map(hospitals_map)\n"", '```\n', '\n', 'And remove the `Hospital` column.\n', '\n', '```python\n', ""hospital_ae_df = hospital_ae_df.drop('Hospital', 1)\n"", '```\n', '\n', '### Time in the data\n', '\n', ""> The next obvious step was to simplify some of the time information I have available as health care system analysis doesn't need to be responsive enough to work on a second and minute basis. Thus, I removed the time information from the 'arrival date', mapped the 'arrival time' into 4-hour chunks\n"", '\n', ""First we'll split the `Arrival Time` column in to `Arrival Date` and `Arrival Hour`.\n"", '\n', '```python\n', ""arrival_times = pd.to_datetime(hospital_ae_df['Arrival Time'])\n"", ""hospital_ae_df['Arrival Date'] = arrival_times.dt.strftime('%Y-%m-%d')\n"", ""hospital_ae_df['Arrival Hour'] = arrival_times.dt.hour\n"", ""hospital_ae_df = hospital_ae_df.drop('Arrival Time', 1)\n"", '```\n', '\n', ""Then we'll map the hours to 4-hour chunks and drop the `Arrival Hour` column.\n"", '\n', '```python\n', ""hospital_ae_df['Arrival hour range'] = pd.cut(\n"", ""    hospital_ae_df['Arrival Hour'],\n"", '    bins=[0, 4, 8, 12, 16, 20, 24],\n', ""    labels=['00-03', '04-07', '08-11', '12-15', '16-19', '20-23'],\n"", '    include_lowest=True\n', ')\n', ""hospital_ae_df = hospital_ae_df.drop('Arrival Hour', 1)\n"", '```\n', '\n', '### Patient demographics\n', '\n', '> I decided to only include records with a sex of male or female in order to reduce risk of re identification through low numbers.\n', '\n', '```python\n', ""hospital_ae_df = hospital_ae_df[hospital_ae_df['Gender'].isin(['Male', 'Female'])]\n"", '```\n', '\n', ""> For the patients age it is common practice to group these into bands and so I've used a standard set - 1-17, 18-24, 25-44, 45-64, 65-84, and 85+ - which although are non-uniform are well used segments defining different average health care usage.\n"", '\n', '```python\n', ""hospital_ae_df['Age bracket'] = pd.cut(\n"", ""    hospital_ae_df['Age'],\n"", '    bins=[0, 18, 25, 45, 65, 85, 150],\n', ""    labels=['0-17', '18-24', '25-44', '45-64', '65-84', '85-'],\n"", '    include_lowest=True\n', ')\n', ""hospital_ae_df = hospital_ae_df.drop('Age', 1)\n"", '```\n', '\n', ""That's all the steps we'll take. We'll finally save our new de-identified dataset.\n"", '\n', '```python\n', 'hospital_ae_df.to_csv(filepaths.hospital_ae_data_deidentify, index=False)\n', '```\n', '\n', '---\n', '\n', '## Synthesise\n', '\n', 'Synthetic data exists on a spectrum from merely the same columns and datatypes as the original data all the way to carrying nearly all of the statistical patterns of the original dataset.\n', '\n', ""The UK's Office of National Statistics has a great report on synthetic data and the [_Synthetic Data Spectrum_](https://www.ons.gov.uk/methodology/methodologicalpublications/generalmethodology/onsworkingpaperseries/onsmethodologyworkingpaperseriesnumber16syntheticdatapilot?utm_campaign=201903_UK_DataPolicyNetwork&utm_source=hs_email&utm_medium=email&utm_content=70377606&_hsenc=p2ANqtz-9W6ByBext_HsgkTPG1lw2JJ_utRoJSTIeVC5Z2lz3QkzwFQpZ0dp2ns9SZLPqxLJrgWzsjC_zt7FQcBvtIGoeSjZtwNg&_hsmi=70377606#synthetic-dataset-spectrum) section is very good in explaining the nuances in more detail.\n"", '\n', ""In this tutorial we'll create not one, not two, but *three* synthetic datasets, that are on a range across the synthetic data spectrum: *Random*, *Independent* and *Correlated*.\n"", '\n', '> In **correlated attribute mode**, we learn a differentially private Bayesian network capturing the correlation structure between attributes, then draw samples from this model to construct the result dataset.\n', '>\n', '> In cases where the correlated attribute mode is too computationally expensive or when there is insufficient data to derive a reasonable model, one can use **independent attribute mode**. In this mode, a histogram is derived for each attribute, noise is added to the histogram to achieve differential privacy, and then samples are drawn for each attribute.\n', '>\n', '> Finally, for cases of extremely sensitive data, one can use **random mode** that simply generates type-consistent random values for each attribute.\n', '\n', ""We'll go through each of these now, moving along the synthetic data spectrum, in the order of random to independent to correlated.\n"", '\n', 'The toolkit we will be using to generate the three synthetic datasets is DataSynthetizer.\n', '\n', '### DataSynthesizer\n', '\n', ""As described in the introduction, this is an open-source toolkit for generating synthetic data. And I'd like to lavish much praise on the researchers who made it as it's excellent.\n"", '\n', ""Instead of explaining it myself, I'll use the researchers' own words from their paper:\n"", '\n', '> DataSynthesizer infers the domain of each attribute and derives a description of the distribution of attribute values in the private dataset. This information is saved in a dataset description file, to which we refer as data summary. Then DataSynthesizer is able to generate synthetic datasets of arbitrary size by sampling from the probabilistic model in the dataset description file.\n', '\n', ""We'll create and inspect our synthetic datasets using three modules within it.\n"", '\n', '> DataSynthesizer consists of three high-level modules:\n', '>\n', '> 1. **DataDescriber**: investigates the data types, correlations and distributions of the attributes in the private dataset, and produces a data summary.\n', '> 2. **DataGenerator**: samples from the summary computed by DataDescriber and outputs synthetic data\n', '> 3. **ModelInspector**: shows an intuitive description of the data summary that was computed by DataDescriber, allowing the data owner to evaluate the accuracy of the summarization process and adjust any parameters, if desired.\n', '\n', 'If you want to browse the code for each of these modules, you can find the Python classes for in the `DataSynthetizer` directory (all code in here from the [original repo](https://github.com/DataResponsibly/DataSynthesizer)).\n', '\n', '\n', '### An aside about differential privacy and Bayesian networks\n', '\n', 'You might have seen the phrase ""differentially private Bayesian network"" in the *correlated mode* description earlier, and got slightly panicked. But fear not! You don\'t need to worry *too* much about these to get DataSynthesizer working.\n', '\n', ""First off, while DataSynthesizer has the option of using differential privacy for anonymisation, we are turning it off and won't be using it in this tutorial. So you can ignore that part. However, if you care about anonymisation you really should read up on differential privacy. I've read a lot of explainers on it and the best I found was [this article from Access Now](https://www.accessnow.org/understanding-differential-privacy-matters-digital-rights/).\n"", '\n', 'Now the next term, Bayesian networks. These are graphs with directions which model the statistical relationship between a dataset\'s variables. It does this by saying certain variables are ""parents"" of others, that is, their value influences their ""children"" variables. Parent variables can influence children but children can\'t influence parents. In our case, if patient age is a parent of waiting time, it means the age of patient influences how long they wait, but how long they doesn\'t influence their age. So by using Bayesian Networks, DataSynthesizer can model these influences and use this model in generating the synthetic data.\n', '\n', 'It can be a slightly tricky topic to grasp but a nice, introductory tutorial on them is at the [Probabilistic World site](https://www.probabilisticworld.com/bayesian-belief-networks-part-1/). Give it a read.\n', '\n', '### Random mode\n', '\n', ""If we were just to generate A&E data for testing our software, we wouldn't care too much about the statistical patterns within the data. Just that it was roughly a similar size and that the datatypes and columns aligned.\n"", '\n', 'In this case, we can just generate the data at random using the `generate_dataset_in_random_mode` function within the `DataGenerator` class.\n', '\n', '#### Data Description: Random\n', '\n', 'The first step is to create a description of the data, defining the datatypes and which are the categorical variables.\n', '\n', '```python\n', 'attribute_to_datatype = {\n', ""    'Time in A&E (mins)': 'Integer',\n"", ""    'Treatment': 'String',\n"", ""    'Gender': 'String',\n"", ""    'Index of Multiple Deprivation Decile': 'Integer',\n"", ""    'Hospital ID': 'String',\n"", ""    'Arrival Date': 'String',\n"", ""    'Arrival hour range': 'String',  \n"", ""    'Age bracket': 'String'\n"", '}\n', '\n', 'attribute_is_categorical = {\n', ""    'Hospital ID': True,\n"", ""    'Time in A&E (mins)': False,\n"", ""    'Treatment': True,\n"", ""    'Gender': True,\n"", ""    'Index of Multiple Deprivation Decile': False,\n"", ""    'Arrival Date': True,\n"", ""    'Arrival hour range': True,  \n"", ""    'Age bracket': True\n"", '}\n', '```\n', '\n', ""We'll be feeding these in to a `DataDescriber` instance.\n"", '\n', '```python\n', 'describer = DataDescriber()\n', '```\n', '\n', 'Using this `describer` instance, feeding in the attribute descriptions, we create a description file.\n', '\n', '```python\n', 'describer.describe_dataset_in_random_mode(\n', '    filepaths.hospital_ae_data_deidentify,\n', '    attribute_to_datatype=attribute_to_datatype,\n', '    attribute_to_is_categorical=attribute_is_categorical)\n', 'describer.save_dataset_description_to_file(\n', '    filepaths.hospital_ae_description_random)\n', '```\n', '\n', 'You can see an example description file in `data/hospital_ae_description_random.json`.\n', '\n', '#### Data Generation: Random\n', '\n', ""Next, generate the random data. We'll just generate the same amount of rows as was in the original data but, importantly, we could generate much more or less if we wanted to.\n"", '\n', '```python\n', 'num_rows = len(hospital_ae_df)\n', '```\n', '\n', 'Now generate the random data.\n', '\n', '```python\n', 'generator = DataGenerator()\n', 'generator.generate_dataset_in_random_mode(\n', '    num_rows, filepaths.hospital_ae_description_random)\n', 'generator.save_synthetic_data(filepaths.hospital_ae_data_synthetic_random)\n', '```\n', '\n', 'You can view this random synthetic data in the file `data/hospital_ae_data_synthetic_random.csv`.\n', '\n', '#### Attribute Comparison: Random\n', '\n', ""We'll compare each attribute in the original data to the synthetic data by generating plots of histograms using the `ModelInspector` class.\n"", '\n', ""`figure_filepath` is just a variable holding where we'll write the plot out to.\n"", '\n', '```python\n', 'synthetic_df = pd.read_csv(filepaths.hospital_ae_data_synthetic_random)\n', '\n', '# Read attribute description from the dataset description file.\n', 'attribute_description = read_json_file(\n', ""    filepaths.hospital_ae_description_random)['attribute_description']\n"", '\n', 'inspector = ModelInspector(hospital_ae_df, synthetic_df, attribute_description)\n', '\n', 'for attribute in synthetic_df.columns:\n', '    inspector.compare_histograms(attribute, figure_filepath)\n', '```\n', '\n', ""Let's look at the histogram plots now for a few of the attributes. We can see that the generated data is completely random and doesn't contain any information about averages or distributions.\n"", '\n', '*Comparison of ages in original data (left) and random synthetic data (right)*\n', '![Random mode age bracket histograms](plots/random_Age_bracket.png)\n', '\n', '*Comparison of hospital attendance in original data (left) and random synthetic data (right)*\n', '![Random mode age bracket histograms](plots/random_Hospital_ID.png)\n', '\n', '*Comparison of arrival date in original data (left) and random synthetic data (right)*\n', '![Random mode age bracket histograms](plots/random_Arrival_Date.png)\n', '\n', 'You can see more comparison examples in the `/plots` directory.\n', '\n', '#### Compare pairwise mutual information: Random\n', '\n', ""DataSynthesizer has a function to compare the _mutual information_ between each of the variables in the dataset and plot them. We'll avoid the mathematical definition of mutual information but [Scholarpedia notes](http://www.scholarpedia.org/article/Mutual_information) it:\n"", '\n', '> can be thought of as the reduction in uncertainty about one random variable given knowledge of another.\n', '\n', 'To create this plot we run.\n', '\n', '```python\n', 'synthetic_df = pd.read_csv(filepaths.hospital_ae_data_synthetic_random)\n', '\n', 'inspector = ModelInspector(hospital_ae_df, synthetic_df, attribute_description)\n', 'inspector.mutual_information_heatmap(figure_filepath)\n', '```\n', '\n', 'We can see the original, private data has a correlation between `Age bracket` and `Time in A&E (mins)`. Not surprisingly, this correlation is lost when we generate our random data.\n', '\n', '*Mutual Information Heatmap in original data (left) and random synthetic data (right)*\n', '![Random mode age mutual information](plots/mutual_information_heatmap_random.png)\n', '\n', '### Independent attribute mode\n', '\n', ""What if we had the use case where we wanted to build models to analyse the medians of ages, or hospital usage in the synthetic data? In this case we'd use independent attribute mode.\n"", '\n', '#### Data Description: Independent\n', '\n', '```python\n', 'describer.describe_dataset_in_independent_attribute_mode(\n', '    attribute_to_datatype=attribute_to_datatype,\n', '    attribute_to_is_categorical=attribute_is_categorical)\n', 'describer.save_dataset_description_to_file(\n', '    filepaths.hospital_ae_description_independent)\n', '```\n', '\n', '#### Data Generation: Independent\n', '\n', 'Next generate the data which keep the distributions of each column but not the data correlations.\n', '\n', '```python\n', 'generator = DataGenerator()\n', 'generator.generate_dataset_in_independent_mode(\n', '    num_rows, filepaths.hospital_ae_description_independent)\n', 'generator.save_synthetic_data(\n', '    filepaths.hospital_ae_data_synthetic_independent)\n', '```\n', '\n', '#### Attribute Comparison: Independent\n', '\n', 'Comparing the attribute histograms we see the independent mode captures the distributions pretty accurately. You can see the synthetic data is _mostly_ similar but not exactly.\n', '\n', '```python\n', 'synthetic_df = pd.read_csv(filepaths.hospital_ae_data_synthetic_independent)\n', 'attribute_description = read_json_file(\n', ""    filepaths.hospital_ae_description_random)['attribute_description']\n"", 'inspector = ModelInspector(hospital_ae_df, synthetic_df, attribute_description)\n', '\n', 'for attribute in synthetic_df.columns:\n', '    inspector.compare_histograms(attribute, figure_filepath)\n', '```\n', '\n', '*Comparison of ages in original data (left) and independent synthetic data (right)*\n', '![Random mode age bracket histograms](plots/independent_Age_bracket.png)\n', '\n', '*Comparison of hospital attendance in original data (left) and independent synthetic data (right)*\n', '![Random mode age bracket histograms](plots/independent_Hospital_ID.png)\n', '\n', '*Comparison of arrival date in original data (left) and independent synthetic data (right)*\n', '![Random mode age bracket histograms](plots/independent_Arrival_Date.png)\n', '\n', '#### Compare pairwise mutual information: Independent\n', '\n', '```python\n', 'synthetic_df = pd.read_csv(filepaths.hospital_ae_data_synthetic_independent)\n', '\n', 'inspector = ModelInspector(hospital_ae_df, synthetic_df, attribute_description)\n', 'inspector.mutual_information_heatmap(figure_filepath)\n', '```\n', '\n', 'We can see the independent data also does not contain any of the attribute correlations from the original data.\n', '\n', '*Mutual Information Heatmap in original data (left) and independent synthetic data (right)*\n', '![Independent mode mutual information](plots/mutual_information_heatmap_independent.png)\n', '\n', '### Correlated attribute mode - include correlations between columns in the data\n', '\n', ""If we want to capture correlated variables, for instance if patient is related to waiting times, we'll need correlated data. To do this we use *correlated mode*.\n"", '\n', '#### Data Description: Correlated\n', '\n', ""There's a couple of parameters that are different here so we'll explain them.\n"", '\n', ""`epsilon` is a value for DataSynthesizer's differential privacy which says the amount of noise to add to the data - the higher the value, the more noise and therefore more privacy. We're not using differential privacy so we can set it to zero.\n"", '\n', ""`k` is the maximum number of parents in a Bayesian network, i.e., the maximum number of incoming edges. For simplicity's sake, we're going to set this to 1, saying that for a variable only one other variable can influence it.\n"", '\n', '```python\n', 'describer.describe_dataset_in_correlated_attribute_mode(\n', '    dataset_file=filepaths.hospital_ae_data_deidentify,\n', '    epsilon=0,\n', '    k=1,\n', '    attribute_to_datatype=attribute_to_datatype,\n', '    attribute_to_is_categorical=attribute_is_categorical)\n', '\n', 'describer.save_dataset_description_to_file(filepaths.hospital_ae_description_correlated)\n', '```\n', '\n', '#### Data Generation: Correlated\n', '\n', '```python\n', 'generator.generate_dataset_in_correlated_attribute_mode(\n', '    num_rows, filepaths.hospital_ae_description_correlated)\n', 'generator.save_synthetic_data(filepaths.hospital_ae_data_synthetic_correlated)\n', '```\n', '\n', '#### Attribute Comparison: Correlated\n', '\n', 'We can see correlated mode keeps similar distributions also. It looks the exact same but if you look closely there are also small differences in the distributions.\n', '\n', '*Comparison of ages in original data (left) and correlated synthetic data (right)*\n', '![Random mode age bracket histograms](plots/correlated_Age_bracket.png)\n', '\n', '*Comparison of hospital attendance in original data (left) and independent synthetic data (right)*\n', '![Random mode age bracket histograms](plots/correlated_Hospital_ID.png)\n', '\n', '*Comparison of arrival date in original data (left) and independent synthetic data (right)*\n', '![Random mode age bracket histograms](plots/correlated_Arrival_Date.png)\n', '\n', '#### Compare pairwise mutual information: Correlated\n', '\n', 'Finally, we see in correlated mode, we manage to capture the correlation between `Age bracket` and `Time in A&E (mins)`.\n', '\n', '```python\n', 'synthetic_df = pd.read_csv(filepaths.hospital_ae_data_synthetic_correlated)\n', '\n', 'inspector = ModelInspector(hospital_ae_df, synthetic_df, attribute_description)\n', 'inspector.mutual_information_heatmap(figure_filepath)\n', '```\n', '\n', '*Mutual Information Heatmap in original data (left) and correlated synthetic data (right)*\n', '![Independent mode mutual information](plots/mutual_information_heatmap_correlated.png)\n', '\n', '---\n', '\n', '### Wrap-up\n', '\n', 'This is where our tutorial ends. But there is much, much more to the world of anonymisation and synthetic data. Please check out more in the references below.\n', '\n', 'If you have any queries, comments or improvements about this tutorial please do get in touch. You can send me a message through "
Synthetic+Data,sdv-dev/TGAN,sdv-dev,https://api.github.com/repos/sdv-dev/TGAN,240,82,6,"['https://api.github.com/users/ManuelAlvarezC', 'https://api.github.com/users/csala', 'https://api.github.com/users/leix28', 'https://api.github.com/users/JDTheRipperPC', 'https://api.github.com/users/pvk-developer', 'https://api.github.com/users/ppwwyyxx']",Python,2023-03-31T01:20:08Z,https://raw.githubusercontent.com/sdv-dev/TGAN/master/README.md,"['<p align=""left"">\n', '<img width=20% src=""https://dai.lids.mit.edu/wp-content/uploads/2018/06/Logo_DAI_highres.png"" alt=""sdv-dev"" />\n', '<i>An open source project from Data to AI Lab at MIT.</i>\n', '</p>\n', '\n', '[![Development Status](https://img.shields.io/badge/Development%20Status-2%20--%20Pre--Alpha-yellow)](https://pypi.org/search/?c=Development+Status+%3A%3A+2+-+Pre-Alpha)\n', '[![PyPi Shield](https://img.shields.io/pypi/v/TGAN.svg)](https://pypi.python.org/pypi/TGAN)\n', '[![Travis CI Shield](https://travis-ci.org/sdv-dev/TGAN.svg?branch=master)](https://travis-ci.org/sdv-dev/TGAN)\n', '[![CodeCov](https://codecov.io/gh/sdv-dev/TGAN/branch/master/graph/badge.svg)](https://codecov.io/gh/sdv-dev/TGAN)\n', '[![Downloads](https://pepy.tech/badge/tgan)](https://pepy.tech/project/tgan)\n', '\n', '__We are happy to announce that our new model for synthetic data called [CTGAN](https://github.com/sdv-dev/CTGAN) is open-sourced. Please check the new model in [this repo](https://github.com/sdv-dev/CTGAN). The new model is simpler and gives better performance on many datasets.__\n', '\n', '# TGAN\n', '\n', 'Generative adversarial training for synthesizing tabular data.\n', '\n', '* License: [MIT](https://github.com/sdv-dev/TGAN/blob/master/LICENSE)\n', '* Development Status: [Pre-Alpha](https://pypi.org/search/?c=Development+Status+%3A%3A+2+-+Pre-Alpha)\n', '* Homepage: https://github.com/sdv-dev/TGAN\n', '\n', '# Overview\n', '\n', 'TGAN is a tabular data synthesizer. It can generate fully synthetic data from real data. Currently, TGAN can\n', 'generate numerical columns and categorical columns.\n', '\n', '# Requirements\n', '\n', '## Python\n', '\n', '**TGAN** has been developed and runs on Python [3.5](https://www.python.org/downloads/release/python-356/),\n', '[3.6](https://www.python.org/downloads/release/python-360/) and\n', '[3.7](https://www.python.org/downloads/release/python-370/).\n', '\n', 'Also, although it is not strictly required, the usage of a [virtualenv](https://virtualenv.pypa.io/en/latest/)\n', 'is highly recommended in order to avoid interfering with other software installed in the system where **TGAN**\n', 'is run.\n', '\n', '# Installation\n', '\n', 'The simplest and recommended way to install TGAN is using `pip`:\n', '\n', '```\n', 'pip install tgan\n', '```\n', '\n', 'Alternatively, you can also clone the repository and install it from sources\n', '\n', '```\n', 'git clone git@github.com:sdv-dev/TGAN.git\n', 'cd TGAN\n', 'make install\n', '```\n', '\n', 'For development, you can use `make install-develop` instead in order to install all the required\n', 'dependencies for testing and code linting.\n', '\n', '# Data Format\n', '\n', '## Input Format\n', '\n', 'In order to be able to sample new synthetic data, **TGAN** first needs to be *fitted* to\n', 'existing data.\n', '\n', 'The input data for this *fitting* process has to be a single table that satisfies the following\n', 'rules:\n', '\n', '* Has no missing values.\n', '* Has columns of types `int`, `float`, `str` or `bool`.\n', '* Each column contains data of only one type.\n', '\n', 'An example of such a tables would be:\n', '\n', '| str_column | float_column | int_column | bool_column |\n', '|------------|--------------|------------|-------------|\n', ""|    'green' |         0.15 |         10 |        True |\n"", ""|     'blue' |         7.25 |         23 |       False |\n"", ""|      'red' |        10.00 |          1 |       False |\n"", ""|   'yellow' |         5.50 |         17 |        True |\n"", '\n', 'As you can see, this table contains 4 columns: `str_column`, `float_column`, `int_column` and\n', '`bool_column`, each one being an example of the supported value types. Notice aswell that there is\n', 'no missing values for any of the rows.\n', '\n', ""**NOTE**: It's important to have properly identifed which of the columns are numerical, which means\n"", 'that they represent a magnitude, and which ones are categorical, as during the preprocessing of\n', 'the data, numerical and categorical columns will be processed differently.\n', '\n', '## Output Format\n', '\n', 'The output of **TGAN** is a table of sampled data with the same columns as the input table and as\n', 'many rows as requested.\n', '\n', '## Demo Datasets\n', '\n', '**TGAN** includes a few datasets to use for development or demonstration purposes. These datasets\n', 'come from the [UCI Machine Learning repository](http://archive.ics.uci.edu/ml), and have been\n', 'preprocessed to be ready to use with **TGAN**, following the requirements specified in the\n', '[Input Format](#input-format) section.\n', '\n', 'These datasets can be browsed and directly downloaded from the\n', '[hdi-project-tgan AWS S3 Bucket](http://hdi-project-tgan.s3.amazonaws.com/index.html)\n', '\n', '### Census dataset\n', '\n', 'This dataset contains a single table, with information from the census, labeled with information of\n', ""wheter or not the income of is greater than 50.000 $/year. It's a single csv file, containing\n"", '199522 rows and 41 columns. From these 41 columns, only 7 are identified as continuous. In\n', '**TGAN** this dataset is called `census`.\n', '\n', '### Cover type\n', '\n', 'This dataset contains a single table with cartographic information labeled with the different\n', ""forrest cover types. It's a single csv file, containing 465588 rows and 55 columns. From these\n"", '55 columns, 10 are identified as continuous. In **TGAN** this dataset is called `covertype`.\n', '\n', '# Quickstart\n', '\n', 'In this short tutorial we will guide you through a series of steps that will help you getting\n', 'started with the most basic usage of **TGAN** in order to generate samples from a given dataset.\n', '\n', '**NOTE**: The following examples are also covered in a [Jupyter](https://jupyter.org/) notebook,\n', 'which you can execute by running the following commands inside your *virtualenv*:\n', '\n', '```\n', 'pip install jupyter\n', 'jupyter notebook examples/Usage_Example.ipynb\n', '```\n', '\n', '## 1. Load the data\n', '\n', 'The first step is to load the data wich we will use to fit TGAN. In order to do so, we will first\n', 'import the function `tgan.data.load_data` and call it with the name of the dataset that we want to\n', 'load.\n', '\n', 'In this case, we will load the `census` dataset, which we will use during the subsequent steps,\n', 'and obtain two objects:\n', '\n', '1. `data`, that will contain a `pandas.DataFrame` with the table of data from the `census`\n', 'dataset ready to be used to fit the model.\n', '\n', '2. `continuous_columns`, that will contain a `list` with the indices of continuous columns.\n', '\n', '```\n', '>>> from tgan.data import load_demo_data\n', "">>> data, continuous_columns = load_demo_data('census')\n"", '>>> data.head(3).T[:10]\n', '                              0                                     1                             2\n', '0                            73                                    58                            18\n', '1               Not in universe        Self-employed-not incorporated               Not in universe\n', '2                             0                                     4                             0\n', '3                             0                                    34                             0\n', '4          High school graduate            Some college but no degree                    10th grade\n', '5                             0                                     0                             0\n', '6               Not in universe                       Not in universe                   High school\n', '7                       Widowed                              Divorced                 Never married\n', '8   Not in universe or children                          Construction   Not in universe or children\n', '9               Not in universe   Precision production craft & repair               Not in universe\n', '\n', '>>> continuous_columns\n', '[0, 5, 16, 17, 18, 29, 38]\n', '\n', '```\n', '\n', '## 2. Create a TGAN instance\n', '\n', 'The next step is to import TGAN and create an instance of the model.\n', '\n', 'To do so, we need to import the `tgan.model.TGANModel` class and call it with the\n', '`continuous_columns` as unique argument.\n', '\n', 'This will create a TGAN instance with the default parameters:\n', '\n', '```\n', '>>> from tgan.model import TGANModel\n', '>>> tgan = TGANModel(continuous_columns)\n', '```\n', '\n', '## 3. Fit the model\n', '\n', ""Once you have a **TGAN** instance, you can proceed to call it's `fit` method passing the `data` that\n"", 'you loaded before in order to start the fitting process:\n', '\n', '```\n', '>>> tgan.fit(data)\n', '```\n', '\n', 'This process will not return anything, however, the progress of the fitting will be printed in the\n', 'screen.\n', '\n', '**NOTE** Depending on the performance of the system you are running, and the parameters selected\n', 'for the model, this step can take up to a few hours.\n', '\n', '## 4. Sample new data\n', '\n', 'After the model has been fitted, you are ready to generate new samples by calling the `sample`\n', 'method of the `TGAN` instance passing it the desired amount of samples:\n', '\n', '```\n', '>>> num_samples = 1000\n', '>>> samples = tgan.sample(num_samples)\n', '>>> samples.head(3).T[:10]\n', '                                         0                                     1                                   2\n', '0                                       12                                    27                                  56\n', '\n', '0                                       12                                    27                                  56\n', '1                          Not in universe        Self-employed-not incorporated                             Private\n', '2                                        0                                     4                                  35\n', '3                                        0                                    34                                  22\n', '4                                 Children            Some college but no degree          Some college but no degree\n', '5                                        0                                     0                                 500\n', '6                          Not in universe                       Not in universe                     Not in universe\n', '7                            Never married       Married-civilian spouse present     Married-civilian spouse present\n', '8              Not in universe or children                          Construction   Finance insurance and real estate\n', '9                          Not in universe   Precision production craft & repair      Adm support including clerical\n', '\n', '```\n', '\n', 'The returned object, `samples`, is a `pandas.DataFrame` containing a table of synthetic data with\n', 'the same format as the input data and 1000 rows as we requested.\n', '\n', '## 5. Save and Load a model\n', '\n', 'In the steps above we saw that the fitting process can take a lot of time, so we probably would\n', 'like to avoid having to fit every we want to generate samples. Instead we can fit a model once,\n', 'save it, and load it every time we want to sample new data.\n', '\n', ""If we have a fitted model, we can save it by calling it's `save` method, that only takes\n"", 'as argument the path where the model will be stored. Similarly, the `TGANModel.load` allows to load\n', 'a model stored on disk by passing as argument the path where the model is stored.\n', '\n', '```\n', "">>> model_path = 'models/mymodel.pkl'\n"", '>>> tgan.save(model_path)\n', 'Model saved successfully.\n', '```\n', '\n', 'Bear in mind that in case the file already exists, **TGAN** will avoid overwritting it unless the\n', '`force=True` argument is passed:\n', '\n', '```\n', '>>> tgan.save(model_path)\n', 'The indicated path already exists. Use `force=True` to overwrite.\n', '```\n', '\n', 'In order to do so:\n', '\n', '```\n', '>>> tgan.save(model_path, force=True)\n', 'Model saved successfully.\n', '```\n', '\n', 'Once the model is saved, it can be loaded back as a **TGAN** instance by using the `TGANModel.load`\n', 'method:\n', '\n', '```\n', '>>> new_tgan = TGANModel.load(model_path)\n', '>>> new_samples = new_tgan.sample(num_samples)\n', '>>> new_samples.head(3).T[:10]\n', '\n', '                                         0                                     1                                   2\n', '0                                       12                                    27                                  56\n', '\n', '0                                       12                                    27                                  56\n', '1                          Not in universe        Self-employed-not incorporated                             Private\n', '2                                        0                                     4                                  35\n', '3                                        0                                    34                                  22\n', '4                                 Children            Some college but no degree          Some college but no degree\n', '5                                        0                                     0                                 500\n', '6                          Not in universe                       Not in universe                     Not in universe\n', '7                            Never married       Married-civilian spouse present     Married-civilian spouse present\n', '8              Not in universe or children                          Construction   Finance insurance and real estate\n', '9                          Not in universe   Precision production craft & repair      Adm support including clerical\n', '```\n', '\n', 'At this point we could use this model instance to generate more samples.\n', '\n', '# Loading custom datasets\n', '\n', 'In the previous steps we used some demonstration data but we did not show you how to load your own\n', 'dataset.\n', '\n', 'In order to do so you will need to generate a `pandas.DataFrame` object from your dataset. If your\n', 'dataset is in a `csv` format you can do so by using `pandas.read_csv` and passing to it the path to\n', 'the CSV file that you want to load.\n', '\n', 'Additionally, you will need to create 0-indexed list of columns indices to be considered continuous.\n', '\n', 'For example, if we want to load a local CSV file, `path/to/my.csv`, that has as continuous columns\n', 'their first 4 columns, that is, indices `[0, 1, 2, 3]`, we would do it like this:\n', '\n', '```\n', '>>> import pandas as pd\n', "">>> data = pd.read_csv('data/census.csv')\n"", '>>> continuous_columns = [0, 1, 2, 3]\n', '```\n', '\n', 'Now you can use the `continuous_columns` to create a **TGAN** instance and use the `data` to `fit`\n', 'it, like we did before:\n', '\n', '```\n', '>>> from tgan.model import TGANModel\n', '>>> tgan = TGANModel(continuous_columns)\n', '>>> tgan.fit(data)\n', '```\n', '\n', '# Model Parameters\n', '\n', 'If you want to change the default behavior of `TGANModel`, such as as different `batch_size` or\n', '`num_epochs`, you can do so by passing different arguments when creating the instance.\n', '\n', '## Model general behavior\n', '\n', '* continous_columns (`list[int]`, required): List of columns indices to be considered continuous.\n', '* output (`str`, default=`output`): Path to store the model and its artifacts.\n', '\n', '## Neural network definition and fitting\n', '\n', '* max_epoch (`int`, default=`100`): Number of epochs to use during training.\n', '* steps_per_epoch (`int`, default=`10000`): Number of steps to run on each epoch.\n', '* save_checkpoints(`bool`, default=True): Whether or not to store checkpoints of the model after each training epoch.\n', '* restore_session(`bool`, default=True): Whether or not continue training from the last checkpoint.\n', '* batch_size (`int`, default=`200`): Size of the batch to feed the model at each step.\n', '* z_dim (`int`, default=`100`): Number of dimensions in the noise input for the generator.\n', '* noise (`float`, default=`0.2`): Upper bound to the gaussian noise added to categorical columns.\n', '* l2norm (`float`, default=`0.00001`): L2 reguralization coefficient when computing losses.\n', '* learning_rate (`float`, default=`0.001`): Learning rate for the optimizer.\n', '* num_gen_rnn (`int`, default=`400`): Number of units in rnn cell in generator.\n', '* num_gen_feature (`int`, default=`100`): Number of units in fully connected layer in generator.\n', '* num_dis_layers (`int`, default=`2`): Number of layers in discriminator.\n', '* num_dis_hidden (`int`, default=`200`): Number of units per layer in discriminator.\n', '* optimizer (`str`, default=`AdamOptimizer`): Name of the optimizer to use during `fit`, possible\n', '  values are: [`GradientDescentOptimizer`, `AdamOptimizer`, `AdadeltaOptimizer`].\n', '\n', 'If you wanted to create an identical instance to the one created on step 2, but passing the\n', 'arguments in a explicit way, this can be achieved with the following lines:\n', '\n', '```\n', '>>> from tgan.model import TGANModel\n', '>>> tgan = TGANModel(\n', '   ...:     continuous_columns,\n', ""   ...:     output='output',\n"", '   ...:     max_epoch=5,\n', '   ...:     steps_per_epoch=10000,\n', '   ...:     save_checkpoints=True,\n', '   ...:     restore_session=True,\n', '   ...:     batch_size=200,\n', '   ...:     z_dim=200,\n', '   ...:     noise=0.2,\n', '   ...:     l2norm=0.00001,\n', '   ...:     learning_rate=0.001,\n', '   ...:     num_gen_rnn=100,\n', '   ...:     num_gen_feature=100,\n', '   ...:     num_dis_layers=1,\n', '   ...:     num_dis_hidden=100,\n', ""   ...:     optimizer='AdamOptimizer'\n"", '   ...: )\n', '```\n', '\n', '# Command-line interface\n', '\n', 'We include a command-line interface that allows users to access TGAN functionality. Currently only\n', 'one action is supported.\n', '\n', '## Random hyperparameter search\n', '\n', '### Input\n', '\n', 'To run random searchs for the best model hyperparameters for a given dataset, we will need:\n', '\n', '* A dataset, in a csv file, without any missing value, only columns of type `bool`, `str`, `int` or\n', '  `float` and only one type for column, as specified in the [Input Format](#input-format).\n', '\n', '* A JSON file containing the configuration for the search. This configuration shall contain:\n', '\n', '  * `name`: Name of the experiment. A folder with this name will be created.\n', '  * `num_random_search`: Number of iterations in hyper parameter search.\n', '  * `train_csv`: Path to the csv file containing the dataset.\n', '  * `continuous_cols`: List of column indices, starting at 0, to be considered continuous.\n', '  * `epoch`: Number of epoches to train the model.\n', '  * `steps_per_epoch`: Number of optimization steps in each epoch.\n', '  * `sample_rows`: Number of rows to sample when evaluating the model.\n', '\n', 'You can see an example of such a json file in [examples/config.json](examples/config.json), which you\n', 'can download and use as a template.\n', '\n', '### Execution\n', '\n', 'Once we have prepared everything we can launch the random hyperparameter search with this command:\n', '\n', '``` bash\n', 'tgan experiments config.json results.json\n', '```\n', '\n', 'Where the first argument, `config.json`, is the path to your configuration JSON, and the second,\n', '`results.json`, is the path to store the summary of the execution.\n', '\n', 'This will run the random search, wich basically consist of the folling steps:\n', '\n', '1. We fetch and split our data between test and train.\n', '2. We randomly select the hyperparameters to test.\n', '3. Then, for each hyperparameter combination, we train a TGAN model using the real training data T\n', '   and generate a synthetic training dataset Tsynth.\n', '4. We then train machine learning models on both the real and synthetic datasets.\n', '5. We use these trained models on real test data and see how well they perform.\n', '\n', '### Output\n', '\n', 'After the experiment has finished, the following can be found:\n', '\n', '* A JSON file, in the example above called `results.json`, containing a summary of the experiments.\n', '  This JSON will contain a key for each experiment `name`, and on it, an array of length\n', '  `num_random_search`, with the selected parameters and its evaluation score. For a configuration\n', '  like the example, the summary will look like this:\n', '\n', '``` python\n', '{\n', ""    'census': [\n"", '        {\n', '            ""steps_per_epoch"" : 10000,\n', '            ""num_gen_feature"" : 300,\n', '            ""num_dis_hidden"" : 300,\n', '            ""batch_size"" : 100,\n', '            ""num_gen_rnn"" : 400,\n', '            ""score"" : 0.937802280415988,\n', '            ""max_epoch"" : 5,\n', '            ""num_dis_layers"" : 4,\n', '            ""learning_rate"" : 0.0002,\n', '            ""z_dim"" : 100,\n', '            ""noise"" : 0.2\n', '        },\n', '        ... # 9 more nodes\n', '    ]\n', '}\n', '```\n', '\n', '* A set of folders, each one names after the `name` specified in the JSON configuration, contained\n', 'in the `experiments` folder. In each folder, sampled data and the models can be found. For a configuration\n', 'like the example, this will look like this:\n', '\n', '```\n', 'experiments/\n', '  census/\n', '    data/       # Sampled data with each of the models in the random search.\n', '    model_0/\n', '      logs/     # Training logs\n', '      model/    # Tensorflow model checkpoints\n', '    model_1/    # 9 more folders, one for each model in the random search\n', '    ...\n', '```\n', '\n', '# Research\n', '\n', 'The first **TGAN** version was built as the supporting software for the [Synthesizing Tabular Data using Generative Adversarial Networks](https://arxiv.org/pdf/1811.11264.pdf) paper by Lei Xu and Kalyan Veeramachaneni.\n', '\n', 'The exact version of software mentioned in the paper can be found in the releases section as the [research pre-release](https://github.com/sdv-dev/TGAN/releases/tag/research)\n', '\n', '# Citing TGAN\n', '\n', 'If you use TGAN for yor research, please consider citing the following paper (https://arxiv.org/pdf/1811.11264.pdf):\n', '\n', 'If you use TGAN, please cite the following work:\n', '\n', '> Lei Xu, Kalyan Veeramachaneni. 2018. Synthesizing Tabular Data using Generative Adversarial Networks.\n', '\n', '```LaTeX\n', '@article{xu2018synthesizing,\n', '  title={Synthesizing Tabular Data using Generative Adversarial Networks},\n', '  author={Xu, Lei and Veeramachaneni, Kalyan},\n', '  journal={arXiv preprint arXiv:1811.11264},\n', '  year={2018}\n', '}\n', '```\n']"
Synthetic+Data,Unity-Technologies/datasetinsights,Unity-Technologies,https://api.github.com/repos/Unity-Technologies/datasetinsights,82,16,16,"['https://api.github.com/users/adason', 'https://api.github.com/users/Saurav-D', 'https://api.github.com/users/BlairLee', 'https://api.github.com/users/86sanj', 'https://api.github.com/users/AdamPalmarUnity', 'https://api.github.com/users/sanjayuconn', 'https://api.github.com/users/rutvij-unity', 'https://api.github.com/users/JonathanHUnity', 'https://api.github.com/users/masonrubenstein', 'https://api.github.com/users/StevenBorkman', 'https://api.github.com/users/kalyanijagdale', 'https://api.github.com/users/davidwang-unity', 'https://api.github.com/users/mkamalza', 'https://api.github.com/users/alextha-scale', 'https://api.github.com/users/salehe-ee', 'https://api.github.com/users/leopoldo-zugasti']",Python,2023-03-09T15:21:37Z,https://raw.githubusercontent.com/Unity-Technologies/datasetinsights/master/README.md,"['# Dataset Insights\n', '\n', '[![PyPI python](https://img.shields.io/pypi/pyversions/datasetinsights)](https://pypi.org/project/datasetinsights)\n', '[![PyPI version](https://badge.fury.io/py/datasetinsights.svg)](https://pypi.org/project/datasetinsights)\n', '[![Downloads](https://pepy.tech/badge/datasetinsights)](https://pepy.tech/project/datasetinsights)\n', '[![Tests](https://github.com/Unity-Technologies/datasetinsights/actions/workflows/linting-and-unittests.yaml/badge.svg?branch=master&event=push)](https://github.com/Unity-Technologies/datasetinsights/actions/workflows/linting-and-unittests.yaml?query=branch%3Amaster+event%3Apush)\n', '[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)\n', '\n', 'Unity Dataset Insights is a python package for downloading, parsing and analyzing synthetic datasets generated using the Unity [Perception package](https://github.com/Unity-Technologies/com.unity.perception).\n', '\n', '## Installation\n', '\n', 'Datasetinsights is published to PyPI. You can simply run `pip install datasetinsights` command under a supported python environments:\n', '\n', '## Getting Started\n', '\n', '### Dataset Statistics\n', '\n', 'We provide a sample [notebook](notebooks/Perception_Statistics.ipynb) to help you load synthetic datasets generated using [Perception package](https://github.com/Unity-Technologies/com.unity.perception) and visualize dataset statistics. We plan to support other sample Unity projects in the future.\n', '\n', '### Load Datasets\n', '\n', 'The [Unity Perception](https://datasetinsights.readthedocs.io/en/latest/datasetinsights.datasets.unity_perception.html#datasetinsights-datasets-unity-perception) package provides datasets under this [schema](https://datasetinsights.readthedocs.io/en/latest/Synthetic_Dataset_Schema.html#synthetic-dataset-schema). The datasetinsighs package also provide convenient python modules to parse datasets.\n', '\n', 'For example, you can load `AnnotationDefinitions` into a python dictionary by providing the corresponding annotation definition ID:\n', '\n', '```python\n', 'from datasetinsights.datasets.unity_perception import AnnotationDefinitions\n', '\n', 'annotation_def = AnnotationDefinitions(data_root=dest, version=""my_schema_version"")\n', 'definition_dict = annotation_def.get_definition(def_id=""my_definition_id"")\n', '```\n', '\n', 'Similarly, for `MetricDefinitions`:\n', '```python\n', 'from datasetinsights.datasets.unity_perception import MetricDefinitions\n', '\n', 'metric_def = MetricDefinitions(data_root=dest, version=""my_schema_version"")\n', 'definition_dict = metric_def.get_definition(def_id=""my_definition_id"")\n', '```\n', '\n', 'The `Captures` table provide the collection of simulation captures and annotations. You can load these records directly as a Pandas `DataFrame`:\n', '\n', '```python\n', 'from datasetinsights.datasets.unity_perception import Captures\n', '\n', 'captures = Captures(data_root=dest, version=""my_schema_version"")\n', 'captures_df = captures.filter(def_id=""my_definition_id"")\n', '```\n', '\n', '\n', 'The `Metrics` table can store simulation metrics for a capture or annotation. You can also load these records as a Pandas `DataFrame`:\n', '\n', '```python\n', 'from datasetinsights.datasets.unity_perception import Metrics\n', '\n', 'metrics = Metrics(data_root=dest, version=""my_schema_version"")\n', 'metrics_df = metrics.filter_metrics(def_id=""my_definition_id"")\n', '```\n', '\n', '### Download Datasets\n', '\n', 'You can download the datasets using the [download](https://datasetinsights.readthedocs.io/en/latest/datasetinsights.commands.html#datasetinsights-commands-download) command:\n', '\n', '```bash\n', 'datasetinsights download --source-uri=<xxx> --output=$HOME/data\n', '```\n', '\n', 'The download command supports HTTP(s), and GCS.\n', '\n', 'Alternatively, you can download dataset directly from python [interface](https://datasetinsights.readthedocs.io/en/latest/datasetinsights.io.downloader.html#module-datasetinsights.io.downloader).\n', '\n', '`GCSDatasetDownloader` can download a dataset from GCS locations.\n', '```python\n', 'from datasetinsights.io.downloader import GCSDatasetDownloader\n', '\n', 'source_uri=gs://url/to/file.zip # or gs://url/to/folder\n', 'dest = ""~/data""\n', 'downloader = GCSDatasetDownloader()\n', 'downloader.download(source_uri=source_uri, output=dest)\n', '```\n', '\n', '`HTTPDatasetDownloader` can a dataset from any HTTP(S) url.\n', '```python\n', 'from datasetinsights.io.downloader import HTTPDatasetDownloader\n', '\n', 'source_uri=http://url.to.file.zip\n', 'dest = ""~/data""\n', 'downloader = HTTPDatasetDownloader()\n', 'downloader.download(source_uri=source_uri, output=dest)\n', '```\n', '\n', '### Convert Datasets\n', '\n', 'If you are interested in converting the synthetic dataset to COCO format for\n', 'annotations that COCO supports, you can run the `convert` command:\n', '\n', '```bash\n', 'datasetinsights convert -i <input-directory> -o <output-directory> -f COCO-Instances\n', '```\n', 'or\n', '```bash\n', 'datasetinsights convert -i <input-directory> -o <output-directory> -f COCO-Keypoints\n', '```\n', '\n', 'You will need to provide 2D bounding box definition ID in the synthetic dataset. We currently only support 2D bounding box and human keypoint annotations for COCO format.\n', '\n', '## Docker\n', '\n', 'You can use the pre-build docker image [unitytechnologies/datasetinsights](https://hub.docker.com/r/unitytechnologies/datasetinsights) to interact with datasets.\n', '\n', '## Documentation\n', '\n', 'You can find the API documentation on [readthedocs](https://datasetinsights.readthedocs.io/en/latest/).\n', '\n', '## Contributing\n', '\n', 'Please let us know if you encounter a bug by filing an issue. To learn more about making a contribution to Dataset Insights, please see our Contribution [page](CONTRIBUTING.md).\n', '\n', '## License\n', '\n', 'Dataset Insights is licensed under the Apache License, Version 2.0. See [LICENSE](LICENCE) for the full license text.\n', '\n', '## Citation\n', 'If you find this package useful, consider citing it using:\n', '```\n', '@misc{datasetinsights2020,\n', '    title={Unity {D}ataset {I}nsights Package},\n', '    author={{Unity Technologies}},\n', '    howpublished={\\url{https://github.com/Unity-Technologies/datasetinsights}},\n', '    year={2020}\n', '}\n', '```\n']"
Synthetic+Data,yuliangguo/3D_Lane_Synthetic_Dataset,yuliangguo,https://api.github.com/repos/yuliangguo/3D_Lane_Synthetic_Dataset,117,21,1,['https://api.github.com/users/yuliangguo'],Python,2023-03-27T17:53:53Z,https://raw.githubusercontent.com/yuliangguo/3D_Lane_Synthetic_Dataset/master/README.md,"['# A Synthetic Dataset for 3D lane Detection\n', '\n', '## Introduction\n', '\n', 'This is a synthetic dataset constructed to stimulate the development and evaluation of 3D lane detection methods \n', '(download dataset from [[google drive](https://drive.google.com/open?id=1Kisxoj7mYl1YyA_4xBKTE8GGWiNZVain)] [[baidu netdisk](https://pan.baidu.com/s/1y_d73-SaNreesif5nVXIVg?pwd=a852)]). \n', 'This dataset is an extension to [Apollo Synthetic Dataset](http://apollo.auto/synthetic.html).\n', 'The detailed strategy of construction and the evaluation method refer to our ECCV 2020 paper:\n', '\n', '""Gen-LaneNet: a generalized and scalable approach for 3D lane detection"", Y. Guo, etal., ECCV, 2020 [[eccv](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660664.pdf)][[arxiv](https://arxiv.org/abs/2003.10656)] [[code](https://github.com/yuliangguo/Pytorch_Generalized_3D_Lane_Detection)]\n', '\n', '<p align=""center"">\n', '  <img src=""figs/00_0000045.jpg"" width=""280"" />\n', '  <img src=""figs/08_0000003.jpg"" width=""280"" /> \n', '  <img src=""figs/16_0000077.jpg"" width=""280"" />\n', '</p>\n', '\n', '\n', '\n', '## Requirements\n', '\n', '* python                    3.7.3\n', '* numpy                     1.16.2\n', '* scipy                     1.2.1 \n', '* matplotlib                3.0.3 \n', '* opencv-python             4.1.0.25\n', '* py3-ortools               5.1.4041\n', '\n', '\n', '\n', '## Data preparation\n', '\n', '\n', 'You are welcome to proceed to the development and evaluation directly using the splits of the training and testing sets we provide.\n', 'Feel free to skip this section if you use our data split directly.\n', '\n', '    ```\n', '    data_splits\n', '    ├── standard\n', '    │   ├── 3D_LaneNet\n', '    |   |       └──test_pred_file.json\n', '    │   ├── Gen_LaneNet\n', '    |   |       └──test_pred_file.json\n', '    │   ├── train.json\n', '    │   └── test.json\n', '    │── rare_subset\n', '    │   ├── 3D_LaneNet\n', '    |   |       └──test_pred_file.json\n', '    │   ├── Gen_LaneNet\n', '    |   |       └──test_pred_file.json\n', '    │   ├── train.json\n', '    │   └── test.json\n', '    |── illus_chg\n', '    │   ├── 3D_LaneNet\n', '    |   |       └──test_pred_file.json\n', '    │   ├── Gen_LaneNet\n', '    |   |       └──test_pred_file.json\n', '    │   ├── train.json\n', '    │   └── test.json\n', '    ```\n', '\n', 'Meanwhile, we provide the helper functions needed to build your own split from the raw datasets downloaded. The following codes \n', 'need to be right in order.\n', '\n', '    parse_apollo_sim_raw_data.py\n', '\n', 'This code extracts lane-lanes and center-lanes in an interested top-view area. The code reasons about the foreground and background\n', 'occlusion based on the provided ground-truth depth maps and semantic segmentation map. Those lane segments in the distance occluded\n', ""by background are discarded, because in general they are not expected to recover from a lane detection method. By setting 'vis=True',\n"", 'this code will draw ground-true lane-lines and center-lines on each image and save them.\n', '\n', '    prepare_data_split.py\n', '\n', ""This code randomly splits the whole data into training and testing sets following a 'standard' five-fold split. Specifically, \n"", ""a subset generated from a difficult urban map are further extracted to be the test set for 'rare subset' data split.\n"", '\n', '    prepare_data_subset\n', ' \n', ""Given the standard split of data, this code excludes images corresponding to a certain 'illumination' condition (before dawn)\n"", 'from the training set. On contrary, in the testing set, only images corresponding to that illumination condition are kept.\n', '\n', '\n', '\n', '## Evaluation\n', '\n', '\n', '    eval_3D_lane.py\n', '    \n', ""You need to modify 'method_name', 'data_split' to specify the method and the data split to conduct evaluation.\n"", ""For example, the default setting compares 'data_splits/illus_chg/Gen_LaneNet/test_pred_file.json' against ground-truth\n"", ""'data_splits/illus_chg/test.json'.\n"", ""Optionally, set 'args.dataset_dir' to the folder containing the original dataset. The original images are only required for visualizing lane results, when setting 'vis = True'.\n"", '\n', 'In this dataset, each image sample is associated with a set of ground-truth 3D lane-lines and center-lines, as well as \n', 'the camera height and pitch angle. \n', 'Per image, the optimal bipartite match between a set of predicted lane curves and a set of ground-truth lane curves is sought via\n', 'solving a min-cost flow.\n', 'Precision and recall are computed via varying lane confidence thresholds. Overall, evaluation metrics include:\n', ' * Average Precision (AP)\n', ' * max F-score\n', ' * x-error in close range (0-40 m)\n', ' * x-error in far range (40-100 m)\n', ' * z-error in close range (0-40 m)\n', ' * z-error in far range (40-100 m)\n', '\n', ""Before running the evaluation, you need to make sure the predicted lanes are saved in the 'test_pred_file.json' file following\n"", ""the format included in our example. Specifically, each lane needs to be associated with a 'prob' score to calculate the\n"", ""precision and recall in full-range. Otherwise, you can only keep 'evaluator.bench_one_submit' in the main code to \n"", 'evaluate your algorithm at a single operation point.\n', '\n', '## Baselines Results\n', '\n', 'We show the evaluation results comparing two baseline methods: \n', '* ""3d-lanenet:  end-to-end 3d multiple lane detection"", N. Garnet, etal., ICCV 2019\n', '* ""Gen-LaneNet: a generalized and scalable approach for 3D lane detection"", Y. Guo, etal., Arxiv, 2020\n', '\n', 'Comparisons are conducted under three distinguished splits of the dataset. For simplicity, only lane-line results are reported here.\n', 'The results from the code is slightly different from that reported in the paper due to different random splits.\n', '\n', '- **Standard**\n', '\n', '| Method                 | AP     | F-Score | x error near (m) | x error far (m) | z error near (m) | z error far (m) |\n', '|------------------------|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|\n', '| 3D-LaneNet             |   89.3    | 86.4      | 0.068     | 0.477     | 0.015     | 0.202\n', '| Gen-LaneNet            |   90.1    | 88.1      | 0.061     | 0.496     | 0.012     | 0.214\n', '\n', '- **Rare Subset**\n', '\n', '| Method                 | AP     | F-Score | x error near (m) | x error far (m) | z error near (m) | z error far (m) |\n', '|------------------------|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|\n', '| 3D-LaneNet             |  74.6     | 72.0      | 0.166     | 0.855     | 0.039     | 0.521\n', '| Gen-LaneNet            |  79.0     | 78.0      | 0.139     | 0.903     | 0.030     | 0.539\n', '\n', '- **Illumination Change**\n', '\n', '| Method                 | AP     | F-Score | x error near (m) | x error far (m) | z error near (m) | z error far (m) |\n', '|------------------------|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|\n', '| 3D-LaneNet             |   74.9    | 72.5      | 0.115     | 0.601     | 0.032     | 0.230\n', '| Gen-LaneNet            |   87.2    | 85.3      | 0.074     | 0.538     | 0.015     | 0.232\n', '\n', '\n', '\n', '## Visualization\n', '\n', ""Visual comparisons to the ground truth can be generated per image when setting 'vis = True'.\n"", 'We show two examples for each method under the data split involving illumination change.\n', '\n', '* 3D-LaneNet\n', '\n', '<img src=""figs/3D_LaneNet/images_00_0000148.jpg"" width=""400""> <img src=""figs/3D_LaneNet/images_00_0000171.jpg"" width=""400"">\n', '\n', '* Gen-LaneNet\n', '\n', '<img src=""figs/Gen_LaneNet/images_00_0000148.jpg"" width=""400""> <img src=""figs/Gen_LaneNet/images_00_0000171.jpg"" width=""400"">\n', '\n', '\n', '## Citation\n', 'Please cite the paper in your publications if it helps your research: \n', '\n', '    @article{guo2020gen,\n', '      title={Gen-LaneNet: A Generalized and Scalable Approach for 3D Lane Detection},\n', '      author={Yuliang Guo, Guang Chen, Peitao Zhao, Weide Zhang, Jinghao Miao, Jingao Wang, and Tae Eun Choe},\n', '      booktitle={Computer Vision - {ECCV} 2020 - 16th European Conference},\n', '      year={2020}\n', '    }\n']"
Synthetic+Data,InsulatorData/InsulatorDataSet,InsulatorData,https://api.github.com/repos/InsulatorData/InsulatorDataSet,181,86,2,"['https://api.github.com/users/InsulatorData', 'https://api.github.com/users/RegisWang']",,2023-04-07T07:16:09Z,https://raw.githubusercontent.com/InsulatorData/InsulatorDataSet/master/README.md,"['# Insulator Data Set - Chinese Power Line Insulator Dataset (CPLID)\n', 'Provide normal insulator images captured by UAVs and synthetic defective insulator images.\n', '\n', '    \n', '    @article{tao2018detection,\n', '      title={Detection of Power Line Insulator Defects Using Aerial Images Analyzed With Convolutional Neural Networks},\n', '      author={Tao, Xian and Zhang, Dapeng and Wang, Zihao and Liu, Xilong and Zhang, Hongyan and Xu, De},\n', '      journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems},\n', '      year={2018},\n', '      publisher={IEEE}\n', '    }\n', 'This dataset is divided into two part:\n', '\n', '- `Normal_Insulators` contains the normal insulators capture by UAVs. The number of the normal insulator images is **600**.\n', '\n', '\n', ""- `Defective_Insulators` contains the insulators with defect. The number of the defective insulator images is **248**. Since we don't have too much defective insulators, the data augmentation method is applied. These images are synthesized by following process:\n"", '    - Use the algorithm in [TVSeg](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.154.6237) to segment the defective insulator from a small part original images, the segment results are the mask images;\n', '    - Use affine transform to augment the original images and their mask, the augmentation results is a lot of original-mask image pairs;\n', '    - Use these image pairs to train the [U-Net](https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28);\n', '    - Use the trained U-Net to segment the rest part of images;\n', '    - Attach the insulators in different backgrounds.\n', '\n', 'Both these two directories contain two subdirectories, one called `images` contains the image files, the other called `labels` contains the VOC2007 format annotations.\n', '\n', '- The `labels` of `Normal_Insulators` contains **only** the annotations of insulators;\n', '- The `labels` of `Defective_Insulators` contains not only the annotations of insulators but also the annotations of defects which on the insulators.\n', '\n', 'The images is provided by the State Grid Corporation of China, and the dataset is made by WANG Zi-Hao.\n', 'If you have any question about this dataset, feel free to contact [zhwang0721@gmail.com](mailto:zhwang0721@gmail.com).\n']"
Synthetic+Data,namebrandon/Sparkov_Data_Generation,namebrandon,https://api.github.com/repos/namebrandon/Sparkov_Data_Generation,91,44,3,"['https://api.github.com/users/streamnsight', 'https://api.github.com/users/namebrandon', 'https://api.github.com/users/kartik2112']",Python,2023-04-05T06:07:46Z,https://raw.githubusercontent.com/namebrandon/Sparkov_Data_Generation/master/README.md,"['# Generate Fake Credit Card Transaction Data, Including Fraudulent Transactions\n', '\n', 'Note: Version v1.0 behavior has changed in such a way that it runs much faster, however transaction files are chunked, so that several files get generated per profile. If your downstream process expects 1 file per profile, please checkout the v0.5 release branch `release/v0.5`.\n', '\n', '## General Usage\n', '\n', 'In this version, the general usage has changed:\n', '\n', 'Please run the datagen script as follow:\n', '\n', '```bash\n', 'python datagen.py -n <NUMBER_OF_CUSTOMERS_TO_GENERATE> -o <OUTPUT_FOLDER> <START_DATE> <END_DATE>\n', '```\n', '\n', 'To see the full list of options, use:\n', '\n', '```bash\n', 'python datagen.py -h\n', '```\n', '\n', 'You can pass additional options with the following flags:\n', '\n', '- `-config <CONFIG_FILE>`: pass the name of the config file, defaults to `./profiles/main_config.json`\n', '- `-seed <INT>`: pass a seed to the Faker class\n', '- `-c <CUSTOMER_FILE>`: pass the path to an already generated customer file\n', '- `-o <OUTPUT_FOLDER>`: folder to save files into\n', '\n', 'This version is modified from the version v0.5 to parallelize the work using `multiprocessing`, so as to take advantage of all available CPUs and bring a huge speed improvement.\n', '\n', 'Because of the way it parallelize the work (chunking transaction generation by chunking the customer list), there will be multiple transaction files generated per profile. Also not that if the number of customers is small, there may be empty files (i.e. files where no customer in the chunk matched the profile). This is expected.\n', '\n', 'With standard profiles, it was benchmarked as generating ~95MB/thread/min. With a 64 cores/128 threads AMD E3, I was able to generate 1.4TB of data, 4.5B transactions, in just under 2h, as opposed to days when running the previous versions.\n', '\n', 'The generation code is originally based on code by [Josh Plotkin](https://github.com/joshplotkin/data_generation). Change log of modifications to original code are below.\n', '\n', '## Change Log\n', '\n', '### v1.0\n', '\n', '- Parallelized version, bringing orders of magnitude faster generation depending on the hardware used.\n', '\n', '### v0.5\n', '\n', '- 12x speed up thanks to some code refactoring.\n', '\n', '### v0.4\n', '\n', '- Only surface-level changes done in scripts so that simulation can be done using Python3\n', '- Corrected bat files to generate transactions files.\n', '\n', '### v0.3\n', '\n', '- Completely re-worked profiles / segmentation of customers\n', '- introduced fraudulent transactions\n', '- introduced fraudulent profiles\n', '- modification of transaction amount generation via Gamma distribution\n', '- added 150k_ shell scripts for multi-threaded data generation (one python process for each segment launched in the background)\n', '\n', '### v0.2\n', '\n', '- Added unix time stamp for transactions for easier programamtic evaluation.\n', '- Individual profiles modified so that there is more variation in the data.\n', '- Modified random generation of age/gender. Original code did not appear to work correctly?\n', '- Added batch files for windows users\n', '\n', '### v0.1\n', '\n', '- Transaction times are now included instead of just dates\n', '- Profile specific spending windows (AM/PM with weighting of transaction times)\n', '- Merchant names (specific to spending categories) are now included (along with code for generation)\n', '- Travel probability is added, with profile specific options\n', '- Travel max distances is added, per profile option\n', '- Merchant location is randomized based on home location and profile travel probabilities\n', '- Simulated transaction numbers via faker MD5 hash (replacing sequential 0..n numbering)\n', '- Includes credit card number via faker\n', '- improved cross-platform file path compatibility\n']"
Synthetic+Data,finos/datahub,finos,https://api.github.com/repos/finos/datahub,78,12,6,"['https://api.github.com/users/mcleo-d', 'https://api.github.com/users/grovesy', 'https://api.github.com/users/zheyu-wang-tony', 'https://api.github.com/users/maoo', 'https://api.github.com/users/finos-admin', 'https://api.github.com/users/pGrovesy']",Python,2023-03-27T17:32:06Z,https://raw.githubusercontent.com/finos/datahub/master/README.md,"['<H1>DataHub</H1> \n', '\n', '![DataHub logo](https://raw.githubusercontent.com/finos/datahub/master/docs/logo.png) \n', '\n', '_Synthetic data generation_\n', '\n', 'DataHub is a set of python libraries dedicated to the production of synthetic data to be used in tests, machine learning training, statistical analysis, and other use cases [wiki](https://en.wikipedia.org/wiki/Synthetic_data). DataHub uses existing datasets to generate synthetic models. If no existing data is available it will use user-provided scripts and data rules to generate synthetic data using out-of-the-box helper datasets.\n', '\n', 'Synthetic datasets are simply artificiality manufactured sets, produced to a desired degree of accuracy. Real Data does play a part in synthetic generation, all depending on the realism\xa0you require. The product roadmaps details out the functionality planned in this respect.\n', '\n', ""DataHub's core is predominantly based around pandas data frames and object generation.\n"", 'A common question: Now that I have a data frame of synthetic-data, what do I do with it? The Pandas library comes with an array of options\xa0here - so for the time being sinking to databases is out of the scope of the core library, however see that examples in the test folder for some common patterns.\n', '\n', '**note** As we build out a config based synthetic spec generator, we will bring this back into scope - please see our roadmap/issue list and get involved in the discussion.\n', '\n', '## Key documents\n', '\n', '1. For information on how to get started with DataHub see our [Getting Started Guide](https://github.com/finos/datahub/blob/master/docs/GettingStarted.md)\n', '2. For more technical information about DataHub and how to customize it, see the [Developer Guide](https://github.com/finos/datahub/blob/master/docs/DeveloperGuide.md)\n', '3. For high-level project direction see [Road Map](https://github.com/finos/datahub/blob/master/docs/synthetic-data-roadmap/roadmap.md), [Requirements Gathering Approach](https://github.com/finos/datahub/blob/master/docs/synthetic-data-roadmap/synthetic-data-requirements-gathering.md) and [Delegated Action Groups](https://github.com/finos/datahub/tree/master/docs/delegated-action-groups).\n', '4. For Feature Development, Good First Issues, Help Wanted and Bug Tracking see [DataHub GitHub Issues](https://github.com/finos/datahub/issues). \n', '5. This project uses [Gravizo](https://g.gravizo.com) for all diagrams and charts as highlighted in [DataHub Issue 41](https://github.com/finos/datahub/issues/41).   \n', '\n', '## Overview of Synthetic data\n', '\n', ""- Synthetic data is information that's is artificially manufactured rather than\xa0generated by *real-world events.\n"", '- Synthetic data is created algorithmically, and can be used as a stand-in for\xa0 test datasets of production data\n', '- **Real data** does play a part in synthetic data generation - depending on how\n', 'realistic you want the output\n', '\n', '## License\n', '\n', 'Copyright 2020 Citigroup\n', '\n', 'Distributed under the [Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0).\n', '\n', 'SPDX-License-Identifier: [Apache-2.0](https://spdx.org/licenses/Apache-2.0)\n']"
Synthetic+Data,lmoroney/synthetic_datasets,lmoroney,https://api.github.com/repos/lmoroney/synthetic_datasets,35,16,1,['https://api.github.com/users/lmoroney'],Python,2022-06-16T17:57:18Z,https://raw.githubusercontent.com/lmoroney/synthetic_datasets/master/README.md,"['# synthetic_datasets\n', ""Repository for Synthetic datasets I'm creating\n"", '\n', 'This will contain subfolders that have the details on the specific dataset within.\n']"
Synthetic+Data,LBNL-ETA/AlphaBuilding-SyntheticDataset,LBNL-ETA,https://api.github.com/repos/LBNL-ETA/AlphaBuilding-SyntheticDataset,34,11,1,['https://api.github.com/users/tsbyq'],Python,2022-09-24T01:19:13Z,https://raw.githubusercontent.com/LBNL-ETA/AlphaBuilding-SyntheticDataset/master/README.md,"['# AlphaBuilding-SyntheticDataset\n', '\n', 'This repository is created for the AlphaBuilding-SyntheticDataset. Details about this dataset could be found on its [GitHub page](https://lbnl-eta.github.io/AlphaBuilding-SyntheticDataset/).\n', '\n', '## Generate synthetic building operation data\n', 'The source code to reproduce the dataset could be found in the code directory. Follow the steps below to generate synthetic building operation data:\n', '1. Install [OpenStudio v2.9.1](https://github.com/NREL/OpenStudio/releases/tag/v2.9.1). \n', 'Set up the full path of openstudio.rb in the [create_workflow.rb](https://github.com/LBNL-ETA/AlphaBuilding-SyntheticDataset/blob/master/code/create_workflow.rb#L23) script. \n', 'The openstudio.rb file could be found in the installed OpenStudio folder: <paht_to_openstudio_installation>/openstudio-2.9.1/Ruby/openstudio.rb.\n', '\n', '2. Clone the [OpenStudio-Standards](https://github.com/NREL/openstudio-standards) repository to your local machine. Set up the full path of openstudio-standards.rb in the [create_workflow.rb](https://github.com/LBNL-ETA/AlphaBuilding-SyntheticDataset/blob/master/code/create_workflow.rb#L24) scipt. The openstudio-standards.rb file could be found in the cloned OpenStudio-Standards repository.\n', '\n', '3. Make sure [Ruby v2.2.4](https://www.ruby-lang.org/en/downloads/) is installed.\n', '\n', '4. Set up the arguments in the [create_workflow.rb](https://github.com/LBNL-ETA/AlphaBuilding-SyntheticDataset/blob/master/code/create_workflow.rb#L340-L362).\n', 'This allows you to create models and run simulations for different building types, vintages, climate zones\n', '    * Step 1. Select the [climate zone(s)](https://github.com/LBNL-ETA/AlphaBuilding-SyntheticDataset/blob/master/code/create_workflow.rb#L376-L393) for simulation. \n', '    The available climate zones are in the following array. \n', '    Uncomment the line(s) to specify the climate zone(s) you want to include:\n', '        \n', '        ```ruby\n', '        climate_zones = [\n', ""            'ASHRAE 169-2006-1A',     # Considered in the synthetic operatin dataset\n"", ""            # 'ASHRAE 169-2006-2A',\n"", ""            # 'ASHRAE 169-2006-2B',\n"", ""            # 'ASHRAE 169-2006-3A',\n"", ""            # 'ASHRAE 169-2006-3B',\n"", ""            'ASHRAE 169-2006-3C',     # Considered in the synthetic operatin dataset\n"", ""            # 'ASHRAE 169-2006-4A',\n"", ""            # 'ASHRAE 169-2006-4B',\n"", ""            # 'ASHRAE 169-2006-4C',\n"", ""            'ASHRAE 169-2006-5A',     # Considered in the synthetic operatin dataset\n"", ""            # 'ASHRAE 169-2006-5B',\n"", ""            # 'ASHRAE 169-2006-6A',\n"", ""            # 'ASHRAE 169-2006-6B',\n"", ""            # 'ASHRAE 169-2006-7A',\n"", ""            # 'ASHRAE 169-2006-8A',\n"", '        ]\n', '        ```\n', '    * Step 2. [Prepare the weather files (EPWs) and map the their folder to the climate zones.](https://github.com/LBNL-ETA/AlphaBuilding-SyntheticDataset/blob/master/code/create_workflow.rb#L397-L402)\n', ""    For example, this repository provides 30 years' historical and a TMY3 weather files for three U.S. cities - \n"", '    Chicago, Miami, and San Francicso. The weather files are saved in ```./EPWs/<city name>_AMY```. And the Hash below\n', '    maps the climate zones of the three cities and the weather file to be used in the simulations. \n', '        ```ruby\n', '        hash_climate_epw = {\n', ""            # 'climate zone option' => 'EPWs folder name', (example convention)\n"", ""            'ASHRAE 169-2006-1A' => 'Miami_AMY',\n"", ""            'ASHRAE 169-2006-3C' => 'SF_AMY',\n"", ""            'ASHRAE 169-2006-5A' => 'Chicago_AMY',\n"", '        }\n', '        ```\n', '        You need to provide weather files and mapping rule for buildings in other climate zones.\n', '    \n', '    * Step 3. [Select the vintages you want to consider.](https://github.com/LBNL-ETA/AlphaBuilding-SyntheticDataset/blob/master/code/create_workflow.rb#L406-L411)\n', '        ```ruby\n', '        vintages = [\n', ""            # '90.1-2004',\n"", ""            # '90.1-2007',\n"", ""            # '90.1-2010',\n"", ""            '90.1-2013'     # Considered in the synthetic operatin dataset\n"", '        ]\n', '        ```\n', '    \n', '    * [Step 4. Select the building type to consider.](https://github.com/LBNL-ETA/AlphaBuilding-SyntheticDataset/blob/master/code/create_workflow.rb#L416-L442)\n', '      Please note that occupancy_simulator only works for office buildings.\n', '      ```ruby\n', '        building_types = [\n', '            ###############################################################\n', '            ## building types that support stochastic occupancy simulation\n', '            ###############################################################\n', ""            # 'SmallOffice',\n"", ""            # 'MediumOffice',\n"", ""            # 'LargeOffice',\n"", ""            # 'SmallOfficeDetailed',\n"", ""            'MediumOfficeDetailed',     # Considered in the synthetic operatin dataset\n"", ""            # 'LargeOfficeDetailed',\n"", '            ###############################################################\n', '            ## building types that do not support stochastic occupancy simulation\n', '            ###############################################################\n', ""            # 'SecondarySchool',\n"", ""            # 'PrimarySchool',\n"", ""            # 'SmallHotel',\n"", ""            # 'LargeHotel',\n"", ""            # 'Warehouse',\n"", ""            # 'RetailStandalone',\n"", ""            # 'RetailStripmall',\n"", ""            # 'QuickServiceRestaurant',\n"", ""            # 'FullServiceRestaurant',\n"", ""            # 'MidriseApartment',\n"", ""            # 'HighriseApartment',\n"", ""            # 'Hospital',\n"", ""            # 'Outpatient',\n"", '        ]\n', '        ```\n', '\n', '    * Step 5. [Set the number of stochastic occupancy simulations for each building model.](https://github.com/LBNL-ETA/AlphaBuilding-SyntheticDataset/blob/master/code/create_workflow.rb#L445)\n', '        ```ruby\n', '        number_of_stochastic_occupancy_simulation = 5\n', '        ```\n', '    \n', '    * Step 6. [Set the energy efficiency level (1 - low, 2 - standard, 3 - high) to run.](https://github.com/LBNL-ETA/AlphaBuilding-SyntheticDataset/blob/master/code/create_workflow.rb#L448)\n', '        ```ruby\n', '        efficiency_level = 2\n', '        ```\n', '\n', '5. Run the create_workflow.rb script with ```<ruby 2.2.4 command> create_workflow.rb``` The script will generate and run OpenStudio workflows to output the synthetic building operation data.\n', '\n', '6. Post-processing. The above routine automatically generates OpenStudio models and runs the simulations.\n', 'This [Python script](https://github.com/LBNL-ETA/AlphaBuilding-SyntheticDataset/blob/master/code/results_extraction_demo.py) shows an example of extracting the raw CSV outputs and saving them in a structured way.\n', 'Depending on their purpose, readers may need develop custom routines to process the simulation results. \n', '\n', '\n', '## License\n', 'Refer to [License.txt](https://github.com/LBNL-ETA/AlphaBuilding-SyntheticDataset/blob/master/License.txt)\n']"
Synthetic+Data,milaan9/Clustering-Datasets,milaan9,https://api.github.com/repos/milaan9/Clustering-Datasets,218,202,1,['https://api.github.com/users/milaan9'],,2023-04-05T12:57:03Z,https://raw.githubusercontent.com/milaan9/Clustering-Datasets/master/README.md,"['<p align=""center""> \n', '<a href=""https://github.com/milaan9""><img src=""https://img.shields.io/static/v1?logo=github&label=maintainer&message=milaan9&color=ff3300"" alt=""Last Commit""/></a> \n', '<!--<img src=""https://badges.pufler.dev/created/milaan9/Clustering-Datasets"" alt=""Created""/>-->\n', '<!--<a href=""https://github.com/milaan9/Clustering-Datasets/graphs/commit-activity""><img src=""https://img.shields.io/github/last-commit/milaan9/Clustering-Datasets.svg?colorB=ff8000&style=flat"" alt=""Last Commit""/></a>-->\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/pulse"" alt=""Activity""><img src=""https://img.shields.io/github/commit-activity/m/milaan9/Clustering-Datasets.svg?colorB=teal&style=flat"" /></a> \n', '<a href=""https://hits.seeyoufarm.com""><img src=""https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2Fmilaan9%2FClustering-Datasets&count_bg=%231DC92C&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=views&edge_flat=false""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/stargazers""><img src=""https://img.shields.io/github/stars/milaan9/Clustering-Datasets.svg?colorB=1a53ff"" alt=""Stars Badge""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/network/members""><img src=""https://img.shields.io/github/forks/milaan9/Clustering-Datasets"" alt=""Forks Badge""/> </a>\n', '<img src=""https://img.shields.io/github/repo-size/milaan9/Clustering-Datasets.svg?colorB=CC66FF&style=flat"" alt=""Size""/>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/pulls""><img src=""https://img.shields.io/github/issues-pr/milaan9/Clustering-Datasets.svg?colorB=yellow&style=flat"" alt=""Pull Requests Badge""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/issues""><img src=""https://img.shields.io/github/issues/milaan9/Clustering-Datasets.svg?colorB=yellow&style=flat"" alt=""Issues Badge""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/blob/master/LICENSE""><img src=""https://img.shields.io/badge/License-MIT-blueviolet.svg"" alt=""MIT License""/></a> \n', '<a href=""https://github.com/milaan9/Clustering-Datasets""><img src=""https://img.shields.io/static/v1?label=%F0%9F%8C%9F&message=If%20Useful&style=style=flat&color=BC4E99"" alt=""Star Badge""/>\n', '</p> \n', '<!--<img src=""https://badges.pufler.dev/contributors/milaan9/01_Python_Introduction?size=50&padding=5&bots=true"" alt=""milaan9""/>-->\n', '\n', '\n', '\n', '# Clustering-Datasets\n', '\n', 'This repository contains the collection of UCI (real-life)datasets and Synthetic (artificial) datasets(with cluster labels).\n', '\n', '  * [UCI (real-world) datasets](https://github.com/milaan9/Clustering-Datasets/tree/master/01.%20UCI)\n', '  * [Synthetic (artificial) datasets](https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic)\n', '\n', '### Artificial data\n', '\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/2d-10c.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/2d-10c.png"" alt=""2d-10c"" title=""2d-10c"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/2d-20c.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/2d-20c.png"" alt=""2d-20c-no0"" title=""2d-20c"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/2d-3c.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/2d-3c.png"" alt=""2d-3c-no123"" title=""2d-3c"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/2d-4c-2.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/2d-4c-2.png"" alt=""2d-4c-no4"" title=""2d-4c-2"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/2d-4c-3.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/2d-4c-3.png"" alt=""2d-4c-no9"" title=""2d-4c-3"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/2d-4c-1.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/2d-4c-1.png"" alt=""2d-4c"" title=""2d-4c-1"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/2sp2glob.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/2sp2glob.png"" alt=""2sp2glob"" title=""2sp2glob"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/3-spiral.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/3-spiral.png"" alt=""3-spiral"" title=""3-spiral"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/3MC.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/3mc.png"" alt=""3MC"" title=""3MC"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/D31.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/d31.png"" alt=""D31"" title=""D31"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/DS577.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/ds577.png"" alt=""DS577"" title=""DS577"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/DS850.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/ds850.png"" alt=""DS850"" title=""DS850"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/R15.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/r15.png"" alt=""R15"" title=""R15"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/aggregation.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/aggregation.png"" alt=""aggregation"" title=""aggregation"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/atom.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/atom.png"" alt=""atom"" title=""atom"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/banana.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/banana.png"" alt=""banana"" title=""banana"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/birch-rg1.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/birch-rg1.png"" alt=""birch-rg1"" title=""birch-rg1"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/birch-rg2.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/birch-rg2.png"" alt=""birch-rg2"" title=""birch-rg2"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/birch-rg3.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/birch-rg3.png"" alt=""birch-rg3"" title=""birch-rg3"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/chainlink.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/chainlink.png"" alt=""chainlink"" title=""chainlink"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/cluto-t4.8k.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/cluto-t4.8k.png"" alt=""cluto-t4.8k"" title=""cluto-t4.8k"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/cluto-t5.8k.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/cluto-t5.8k.png"" alt=""cluto-t5.8k"" title=""cluto-t5.8k"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/cluto-t7.10k.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/cluto-t7.10k.png"" alt=""cluto-t7.10k"" title=""cluto-t7.10k"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/cluto-t8.8k.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/cluto-t8.8k.png"" alt=""cluto-t8.8k"" title=""cluto-t8.8k"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/complex8.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/complex8.png"" alt=""complex8"" title=""complex8"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/complex9.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/complex9.png"" alt=""complex9"" title=""complex9"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/compound.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/compound.png"" alt=""compound"" title=""compound"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/cure-t0-2000n-2D.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/cure-t0-2000n-2d.png"" alt=""cure-t0-2000n-2D"" title=""cure-t0-2000n-2D"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/cure-t1-2000n-2D.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/cure-t1-2000n-2d.png"" alt=""cure-t1-2000n-2D"" title=""cure-t1-2000n-2D"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/cure-t2-4k.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/cure-t2-4k.png"" alt=""cure-t2-4k"" title=""cure-t2-4k"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/curves1.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/curves1.png"" alt=""curves1"" title=""curves1"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/curves2.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/curves2.png"" alt=""curves2"" title=""curves2"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/dartboard1.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/dartboard1.png"" alt=""dartboard1"" title=""dartboard1"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/dartboard2.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/dartboard2.png"" alt=""dartboard2"" title=""dartboard2"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/dense-disk-3000.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/dense-disk-3000.png"" alt=""dense-disk-3000"" title=""dense-disk-3000"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/dense-disk-5000.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/dense-disk-5000.png"" alt=""dense-disk-5000"" title=""dense-disk-5000"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/diamond9.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/diamond9.png"" alt=""diamond9"" title=""diamond9"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/disk-1000n.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/disk-1000n.png"" alt=""disk-1000n"" title=""disk-1000n"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/disk-3000n.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/disk-3000n.png"" alt=""disk-3000n"" title=""disk-3000n"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/disk-4000n.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/disk-4000n.png"" alt=""disk-4000n"" title=""disk-4000n"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/disk-4500n.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/disk-4500n.png"" alt=""disk-4500n"" title=""disk-4500n"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/disk-4600n.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/disk-4600n.png"" alt=""disk-4600n"" title=""disk-4600n"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/disk-5000n.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/disk-5000n.png"" alt=""disk-5000n"" title=""disk-5000n"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/disk-6000n.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/disk-6000n.png"" alt=""disk-6000n"" title=""disk-6000n"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/donut1.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/donut1.png"" alt=""donut1"" title=""donut1"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/donut2.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/donut2.png"" alt=""donut2"" title=""donut2"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/donut3.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/donut3.png"" alt=""donut3"" title=""donut3"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/donutcurves.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/donutcurves.png"" alt=""donutcurves"" title=""donutcurves"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/ds2c2sc13.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/ds2c2sc13.png"" alt=""ds2c2sc13"" title=""ds2c2sc13"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/ds3c3sc6.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/ds3c3sc6.png"" alt=""ds3c3sc6"" title=""ds3c3sc6"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/ds4c2sc8.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/ds4c2sc8.png"" alt=""ds4c2sc8"" title=""ds4c2sc8"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/elliptical_10_2.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/elliptical_10_2.png"" alt=""elliptical_10_2"" title=""elliptical_10_2"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/elly-2d10c13s.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/elly-2d10c13s.png"" alt=""elly-2d10c13s"" title=""elly-2d10c13s"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/engytime.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/engytime.png"" alt=""engytime"" title=""engytime"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/flame.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/flame.png"" alt=""flame"" title=""flame"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/fourty.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/fourty.png"" alt=""fourty"" title=""fourty"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/golfball.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/golfball.png"" alt=""golfball"" title=""golfball"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/hepta.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/hepta.png"" alt=""hepta"" title=""hepta"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/insect.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/insect.png"" alt=""insect"" title=""insect"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/jain.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/jain.png"" alt=""jain"" title=""jain"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/long1.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/long1.png"" alt=""long1"" title=""long1"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/long2.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/long2.png"" alt=""long2"" title=""long2"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/long3.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/long3.png"" alt=""long3"" title=""long3"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/longsquare.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/longsquare.png"" alt=""longsquare"" title=""longsquare"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/lsun.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/lsun.png"" alt=""lsun"" title=""lsun"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/mopsi-finland.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/mopsi-finland.png"" alt=""mopsi-finland"" title=""mopsi-finland"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/mopsi-joensuu.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/mopsi-joensuu.png"" alt=""mopsi-joensuu"" title=""mopsi-joensuu"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/pathbased.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/pathbased.png"" alt=""pathbased"" title=""pathbased"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/rings.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/rings.png"" alt=""rings"" title=""rings"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/s-set1.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/s-set1.png"" alt=""s-set1"" title=""s-set1"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/s-set2.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/s-set2.png"" alt=""s-set2"" title=""s-set2"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/s-set3.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/s-set3.png"" alt=""s-set3"" title=""s-set3"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/s-set4.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/s-set4.png"" alt=""s-set4"" title=""s-set4"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/sizes1.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/sizes1.png"" alt=""sizes1"" title=""sizes1"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/sizes2.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/sizes2.png"" alt=""sizes2"" title=""sizes2"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/sizes3.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/sizes3.png"" alt=""sizes3"" title=""sizes3"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/sizes4.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/sizes4.png"" alt=""sizes4"" title=""sizes4"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/sizes5.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/sizes5.png"" alt=""sizes5"" title=""sizes5"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/smile1.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/smile1.png"" alt=""smile1"" title=""smile1"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/smile2.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/smile2.png"" alt=""smile2"" title=""smile2"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/smile3.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/smile3.png"" alt=""smile3"" title=""smile3"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/spherical_4_3.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/spherical_4_3.png"" alt=""spherical_4_3"" title=""spherical_4_3"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/spherical_5_2.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/spherical_5_2.png"" alt=""spherical_5_2"" title=""spherical_5_2"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/spherical_6_2.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/spherical_6_2.png"" alt=""spherical_6_2"" title=""spherical_6_2"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/spiral.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/spiral.png"" alt=""spiral"" title=""spiral"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/spiralsquare.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/spiralsquare.png"" alt=""spiralsquare"" title=""spiralsquare"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/square1.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/square1.png"" alt=""square1"" title=""square1"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/square2.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/square2.png"" alt=""square2"" title=""square2"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/square3.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/square3.png"" alt=""square3"" title=""square3"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/square4.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/square4.png"" alt=""square4"" title=""square4"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/square5.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/square5.png"" alt=""square5"" title=""square5"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/st900.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/st900.png"" alt=""st900"" title=""st900"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/target.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/target.png"" alt=""target"" title=""target"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/tetra.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/tetra.png"" alt=""tetra"" title=""tetra"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/triangle1.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/triangle1.png"" alt=""triangle1"" title=""triangle1"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/triangle2.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/triangle2.png"" alt=""triangle2"" title=""triangle2"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/twenty.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/twenty.png"" alt=""twenty"" title=""twenty"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/twodiamonds.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/twodiamonds.png"" alt=""twodiamonds"" title=""twodiamonds"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/wingnut.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/wingnut.png"" alt=""wingnut"" title=""wingnut"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/xclara.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/xclara.png"" alt=""xclara"" title=""xclara"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synthetic/zelnik1.arff""><img src=""https://github.com/milaan9/Clustering-Datasets/blob/master/02.%20Synthetic/fig/zelnik1.png"" alt=""zelnik1"" title=""zelnik1"" width=""239px"" style=""max-width: 100%;float:left;""/></a>\n', '<a href=""https://github.com/milaan9/Clustering-Datasets/tree/master/02.%20Synt"
Synthetic+Data,mrtzh/PrivateMultiplicativeWeights.jl,mrtzh,https://api.github.com/repos/mrtzh/PrivateMultiplicativeWeights.jl,44,16,5,"['https://api.github.com/users/mrtzh', 'https://api.github.com/users/giladroyz', 'https://api.github.com/users/KristofferC', 'https://api.github.com/users/Quantisan', 'https://api.github.com/users/tkelman']",Julia,2023-01-26T12:19:09Z,https://raw.githubusercontent.com/mrtzh/PrivateMultiplicativeWeights.jl/master/README.md,"['# PrivateMultiplicativeWeights.jl\n', '\n', 'This package implements `MWEM`, a simple and practical algorithm for differentially private data release.\n', '\n', 'MIT Licensed. See `LICENSE.md`.\n', '\n', '## Installation\n', '\n', 'Install required packages, then open a Julia prompt and call: \n', '```\n', 'using Pkg\n', 'Pkg.add(""PrivateMultiplicativeWeights"")\n', '```\n', '\n', '## Main Features\n', '\n', '* Differentially private synthetic data preserving lower order marginals of an input data set\n', '* Optimized in-memory implementation for small number of data attributes\n', '* Scalable heuristic for large number of data attributes\n', '* Easy-to-use interfaces for custom query sets and data representations\n', '\n', '## Examples\n', '\n', '### Histogram approximations\n', '\n', '![Histogram approximation](https://github.com/mrtzh/PrivateMultiplicativeWeights.jl/blob/master/examples/histograms.png?raw=true)\n', '\n', 'Check out [`histograms.ipynb`](/examples/histograms.ipynb) for details on how to\n', 'use the algorithm to compute differentially private histogram approximations. \n', '\n', '### Marginal approximations\n', '\n', 'The package can also be used to create synthetic data that approximates the\n', 'lower order marginals of a data set with binary features.  For the sake of\n', 'illustration, we create a random data set with hidden correlations. Columns\n', 'correspond to data points.  \n', '\n', '```\n', 'd, n = 20, 1000\n', 'data_matrix = rand(0:1, d ,n)\n', 'data_matrix[3, :] = data_matrix[1, :] .* data_matrix[2, :]\n', '```\n', '\n', 'We can run MWEM to produce synthetic data accurate for 1st, 2nd, 3rd order marginals of the source data.\n', '```\n', 'using PrivateMultiplicativeWeights\n', 'mw = mwem(Parities(d, 3), Tabular(data_matrix))\n', '```\n', 'This will convert the data to its explicit histogram representation of size 2^d\n', 'and may not be useful when d is large. See section on factored histograms\n', 'for an alternative when the dimension d is large.\n', '\n', '### Convert histograms to matrices\n', '\n', 'We can convert synthetic data in histogram representation to a tabular \n', '(matrix) representation.\n', '```\n', 'table = Tabular(mw.synthetic, n)\n', '```\n', '\n', '### Compute error of approximation\n', 'Compute error achieved by MWEM:\n', '```\n', 'maximum_error(mw), mean_squared_error(mw)\n', '```\n', 'Note that these statistics are *not* differentially private.\n', '\n', '## Parameters\n', '\n', 'Parameters can be set flexibly with the `MWParameters` constructor:\n', '```\n', 'mw = mwem(Parities(d, 3),\n', '          Tabular(data_matrix),\n', '          MWParameters(epsilon=1.0,\n', '                       iterations=10,\n', '                       repetitions=10,\n', '                       verbose=false,\n', '                       noisy_init=false,\n', '                       init_budget=0.05,\n', '                       noisy_max_budget=0.5))\n', '```\n', 'Available parameters:\n', '\n', '| Name | Default | Description |\n', '| ---- | ------- | ----------- |\n', '| `epsilon` | `1.0` | Privacy parameter for the algorithm. Each iteration of MWEM is `epsilon`-differentially private. Total privacy guarantees follow via composition theorems.|\n', '| `iterations` | `10` | Number of iterations of MWEM. Each iteration corresponds to selecting one query via the exponential mechanism, evaluating the query on the data, and updating the internal state. |\n', '| `repetitions`| `10` | Number of times MWEM cycles through previously measured queries per iteration. This has no additional privacy cost. |\n', '| `noisy_init` | `false` | This requires part of the `epsilon` privacy cost.  When `noisy_init` is set to false, the initialization is uniform.  |\n', '| `init_budget` | `0.05` | In case the `noisy_init` flag is set to true, this flag decide what fraction of the `epsilon` privacy cost will be given for the noisy initialization. When `noisy_init` is set to false, all the budget will be used by the iterations. |\n', '| `noisy_max_budget` | `0.5` | Decise what fraction from the `epsion` privacy badget of every iteration will go to the ""noisy max"" step. (the rest is for the Exponential Mechanism)  |\n', '| `verbose` | `false` | print timing and error statistics per iteration (information is not differentially private)\n', '\n', 'The function `MWParameters` accepts any subset of parameters, e.g.,\n', '`MWParameter(epsilon=0.5, iterations=5)`.\n', '\n', '## Data representations\n', '\n', '### Histogram representation\n', '\n', 'By default, MWEM works with the histogram representation of a data sets. This\n', 'means that the data is represented by a vector whose length is equal to the size\n', 'of domain. For example, data consisting of `d` binary attributes would be\n', 'converted to an array of length `2^d`. MWEM needs to store and array of this\n', 'length in main memory, which is often the computational bottleneck.\n', '\n', '## Factored histograms\n', '\n', 'When the histogram representation is too large, try using factored histograms.\n', 'Factored histograms maintain a product distribution over clusters of attributes\n', 'of the data. Each component is represented using a single histogram. Components\n', 'are merged as it becomes necessary. This often allows to scale up MWEM by orders\n', 'of magnitude.  \n', '```\n', 'd, n = 100, 1000\n', 'data_matrix = rand(0:1, d, n)\n', 'data_matrix[3, :] = data_matrix[1, :] .* data_matrix[2, :]\n', 'mw = mwem(FactorParities(d, 3), Tabular(data_matrix))\n', '```\n', '\n', 'Also see `examples.jl`.\n', '\n', '## Query representations\n', '\n', 'There are two ways to define custom query sets.\n', '\n', '### Histogram queries\n', '\n', 'Histogram queries are linear functions in the histogram representation of the\n', 'data.  You can define custom query workloads by using\n', '`HistogramQueries(query_matrix)` instead of `Parities(d, 3)`. Here `query\n', 'matrix` is an `N x k` matrix specifying the query set in its Histogram\n', 'representation, `N` is the histogram length and `k` is the `k` is the number of\n', 'queries.\n', '\n', '### Custom query types\n', '\n', 'To build query sets with your own implicit representations, sub-type\n', '`Query` and `Queries`. Implement the functions specified in `src/interface.jl`.\n', '\n', 'See `src/parities.jl` for an example.\n', '\n', '### Available query sets\n', '\n', '- **Parities**(d, k)\n', '\n', '  Parities of `k` out of `d` attributes. This corresponds to approximating\n', '  `k`-way marginals of the original data.\n', '\n', '- **FactorParities**(d, k)\n', '\n', '  Parities of `k` out of `d` attributes for factored histogram representation.\n', '\n', '- **SeriesRangeQueries**(N)\n', '\n', '  Range queries corresponding to all interval queries over a histogram of length `N`.\n', '  \n', '  - *SeriesRangeQueries**(Intervals)\n', '\n', '  Range queries over histogram with length N, corresponding to intervals = {Interval1, Interval2, ...}\n', '  where Interval = (i, j) so that 1 <= i <= j <= N.\n', '\n', '## Contributing to this package\n', '\n', 'There are many ways to contribute to this repository:\n', '\n', '* Experiments\n', '* Additional query sets (e.g., two-dimensional range queries)\n', '* Additional tests, debugging, optimization\n', '* Additional documentation\n', '\n', '## Citing this package\n', '\n', 'The MWEM algorithm was presented in the following paper:\n', '```\n', '@inproceedings{HLM12,\n', '  author = ""Moritz Hardt and Katrina Ligett and Frank McSherry"",\n', '  title = ""A simple and practical algorithm for differentially-private data release"",\n', '  booktitle = {Proc.\\ $26$th Neural Information Processing Systems (NIPS)},\n', '  year = {2012},\n', '}\n', '```\n', '\n', '## Status\n', '\n', '[![Build\n', 'Status](https://travis-ci.org/mrtzh/PrivateMultiplicativeWeights.jl.svg?branch=master)](https://travis-ci.org/mrtzh/PrivateMultiplicativeWeights.jl)\n']"
Model Explainability,interpretml/interpret,interpretml,https://api.github.com/repos/interpretml/interpret,5457,664,30,"['https://api.github.com/users/interpret-ml', 'https://api.github.com/users/paulbkoch', 'https://api.github.com/users/msplants', 'https://api.github.com/users/luisffranca', 'https://api.github.com/users/wamartin-aml', 'https://api.github.com/users/Harsha-Nori', 'https://api.github.com/users/nopdive', 'https://api.github.com/users/dependabot%5Bbot%5D', 'https://api.github.com/users/imatiach-msft', 'https://api.github.com/users/ecederstrand', 'https://api.github.com/users/Ashton-Sidhu', 'https://api.github.com/users/blengerich', 'https://api.github.com/users/bamdevm', 'https://api.github.com/users/xiaohk', 'https://api.github.com/users/microsoftopensource', 'https://api.github.com/users/zhangxz1123', 'https://api.github.com/users/mtl-tony', 'https://api.github.com/users/ajyl', 'https://api.github.com/users/raethlein', 'https://api.github.com/users/daikikatsuragawa', 'https://api.github.com/users/eddy-geek', 'https://api.github.com/users/gliptak', 'https://api.github.com/users/itsoum', 'https://api.github.com/users/jruales', 'https://api.github.com/users/mczhu', 'https://api.github.com/users/msftgits', 'https://api.github.com/users/prateekiiest', 'https://api.github.com/users/vbernardes', 'https://api.github.com/users/chhetri22', 'https://api.github.com/users/ncherrier']",C++,2023-04-26T13:22:54Z,https://raw.githubusercontent.com/interpretml/interpret/develop/README.md,"['# InterpretML\n', '\n', '[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/interpretml/interpret/develop?labpath=examples%2Fpython%2FInterpretable_Classification_Methods.ipynb)\n', '![License](https://img.shields.io/github/license/interpretml/interpret.svg?style=flat-square)\n', '![Python Version](https://img.shields.io/pypi/pyversions/interpret.svg?style=flat-square)\n', '![Package Version](https://img.shields.io/pypi/v/interpret.svg?style=flat-square)\n', '![Conda](https://img.shields.io/conda/v/conda-forge/interpret)\n', '![Build Status](https://img.shields.io/azure-devops/build/ms/interpret/293/develop.svg?style=flat-square)\n', '![Coverage](https://img.shields.io/azure-devops/coverage/ms/interpret/293/develop.svg?style=flat-square)\n', '![Maintenance](https://img.shields.io/maintenance/yes/2023?style=flat-square)\n', '<br/>\n', '> ### In the beginning machines learned in darkness, and data scientists struggled in the void to explain them. \n', '> ### Let there be light.\n', '\n', ""InterpretML is an open-source package that incorporates state-of-the-art machine learning interpretability techniques under one roof. With this package, you can train interpretable glassbox models and explain blackbox systems. InterpretML helps you understand your model's global behavior, or understand the reasons behind individual predictions.\n"", '\n', 'Interpretability is essential for:\n', '- Model debugging - Why did my model make this mistake?\n', '- Feature Engineering - How can I improve my model?\n', '- Detecting fairness issues - Does my model discriminate?\n', ""- Human-AI cooperation - How can I understand and trust the model's decisions?\n"", '- Regulatory compliance - Does my model satisfy legal requirements?\n', '- High-risk applications - Healthcare, finance, judicial, ...\n', '\n', '![](https://github.com/interpretml/interpretml.github.io/blob/master/interpret-highlight.gif)\n', '\n', '# Installation\n', '\n', 'Python 3.7+ | Linux, Mac, Windows\n', '```sh\n', 'pip install interpret\n', '# OR\n', 'conda install -c conda-forge interpret\n', '```\n', '\n', '# Introducing the Explainable Boosting Machine (EBM)\n', '\n', 'EBM is an interpretable model developed at Microsoft Research<sup>[*](#citations)</sup>. It uses modern machine learning techniques like bagging, gradient boosting, and automatic interaction detection to breathe new life into traditional GAMs (Generalized Additive Models). This makes EBMs as accurate as state-of-the-art techniques like random forests and gradient boosted trees. However, unlike these blackbox models, EBMs produce exact explanations and are editable by domain experts.\n', '\n', '| Dataset/AUROC | Domain  | Logistic Regression | Random Forest | XGBoost         | Explainable Boosting Machine |\n', '|---------------|---------|:-------------------:|:-------------:|:---------------:|:----------------------------:|\n', '| Adult Income  | Finance | .907±.003           | .903±.002     | .927±.001       | **_.928±.002_**              |\n', '| Heart Disease | Medical | .895±.030           | .890±.008     | .851±.018       | **_.898±.013_**              |\n', '| Breast Cancer | Medical | **_.995±.005_**     | .992±.009     | .992±.010       | **_.995±.006_**              |\n', '| Telecom Churn | Business| .849±.005           | .824±.004     | .828±.010       | **_.852±.006_**              |\n', '| Credit Fraud  | Security| .979±.002           | .950±.007     | **_.981±.003_** | **_.981±.003_**              |\n', '\n', '[*Notebook for reproducing table*](https://nbviewer.jupyter.org/github/interpretml/interpret/blob/master/benchmarks/EBM%20Classification%20Comparison.ipynb)\n', '\n', '# Supported Techniques\n', '\n', '| Interpretability Technique  | Type               |\n', '|-----------------------------|--------------------|\n', '| [Explainable Boosting](https://interpret.ml/docs/ebm.html)        | glassbox model     |\n', '| [Decision Tree](https://interpret.ml/docs/dt.html)                | glassbox model     |\n', '| [Decision Rule List](https://interpret.ml/docs/dr.html)           | glassbox model     |\n', '| [Linear/Logistic Regression](https://interpret.ml/docs/lr.html)   | glassbox model     |\n', '| [SHAP Kernel Explainer](https://interpret.ml/docs/shap.html)      | blackbox explainer |\n', '| [LIME](https://interpret.ml/docs/lime.html)                       | blackbox explainer |\n', '| [Morris Sensitivity Analysis](https://interpret.ml/docs/msa.html) | blackbox explainer |\n', '| [Partial Dependence](https://interpret.ml/docs/pdp.html)          | blackbox explainer |\n', '\n', '# Train a glassbox model\n', '\n', ""Let's fit an Explainable Boosting Machine\n"", '\n', '```python\n', 'from interpret.glassbox import ExplainableBoostingClassifier\n', '\n', 'ebm = ExplainableBoostingClassifier()\n', 'ebm.fit(X_train, y_train)\n', '\n', '# or substitute with LogisticRegression, DecisionTreeClassifier, RuleListClassifier, ...\n', '# EBM supports pandas dataframes, numpy arrays, and handles ""string"" data natively.\n', '```\n', '\n', 'Understand the model\n', '```python\n', 'from interpret import show\n', '\n', 'ebm_global = ebm.explain_global()\n', 'show(ebm_global)\n', '```\n', '![Global Explanation Image](./examples/python/assets/readme_ebm_global_specific.PNG?raw=true)\n', '\n', '<br/>\n', '\n', 'Understand individual predictions\n', '```python\n', 'ebm_local = ebm.explain_local(X_test, y_test)\n', 'show(ebm_local)\n', '```\n', '![Local Explanation Image](./examples/python/assets/readme_ebm_local_specific.PNG?raw=true)\n', '\n', '<br/>\n', '\n', 'And if you have multiple model explanations, compare them\n', '```python\n', 'show([logistic_regression_global, decision_tree_global])\n', '```\n', '![Dashboard Image](./examples/python/assets/readme_dashboard.PNG?raw=true)\n', '\n', '<br/>\n', '\n', 'If you need to keep your data private, use Differentially Private EBMs (see [DP-EBMs](https://proceedings.mlr.press/v139/nori21a/nori21a.pdf))\n', '\n', '```python\n', 'from interpret.privacy import DPExplainableBoostingClassifier, DPExplainableBoostingRegressor\n', '\n', 'dp_ebm = DPExplainableBoostingClassifier(epsilon=1, delta=1e-5) # Specify privacy parameters\n', 'dp_ebm.fit(X_train, y_train)\n', '\n', 'show(dp_ebm.explain_global()) # Identical function calls to standard EBMs\n', '```\n', '\n', '<br/>\n', '<br/>\n', '\n', 'For more information, see the [documentation](https://interpret.ml/docs/getting-started.html).\n', '<br/>\n', '<br/>\n', '\n', '# Acknowledgements\n', '\n', 'InterpretML was originally created by (equal contributions): Samuel Jenkins, Harsha Nori, Paul Koch, and Rich Caruana\n', '\n', 'EBMs are fast derivative of GA2M, invented by: Yin Lou, Rich Caruana, Johannes Gehrke, and Giles Hooker\n', '\n', 'Many people have supported us along the way. Check out [ACKNOWLEDGEMENTS.md](./ACKNOWLEDGEMENTS.md)!\n', '\n', 'We also build on top of many great packages. Please check them out!\n', '\n', '[plotly](https://github.com/plotly/plotly.py) |\n', '[dash](https://github.com/plotly/dash) |\n', '[scikit-learn](https://github.com/scikit-learn/scikit-learn) |\n', '[lime](https://github.com/marcotcr/lime) |\n', '[shap](https://github.com/slundberg/shap) |\n', '[salib](https://github.com/SALib/SALib) |\n', '[skope-rules](https://github.com/scikit-learn-contrib/skope-rules) |\n', '[treeinterpreter](https://github.com/andosa/treeinterpreter) |\n', '[gevent](https://github.com/gevent/gevent) |\n', '[joblib](https://github.com/joblib/joblib) |\n', '[pytest](https://github.com/pytest-dev/pytest) |\n', '[jupyter](https://github.com/jupyter/notebook)\n', '\n', '# <a name=""citations"">Citations</a>\n', '\n', '<details open>\n', '  <summary><strong>InterpretML</strong></summary>\n', '  <hr/>\n', '\n', '  <details open>\n', '    <summary>\n', '      <em>""InterpretML: A Unified Framework for Machine Learning Interpretability"" (H. Nori, S. Jenkins, P. Koch, and R. Caruana 2019)</em>\n', '    </summary>\n', '    <br/>\n', '    <pre>\n', '@article{nori2019interpretml,\n', '  title={InterpretML: A Unified Framework for Machine Learning Interpretability},\n', '  author={Nori, Harsha and Jenkins, Samuel and Koch, Paul and Caruana, Rich},\n', '  journal={arXiv preprint arXiv:1909.09223},\n', '  year={2019}\n', '}\n', '    </pre>\n', '    <a href=""https://arxiv.org/pdf/1909.09223.pdf"">Paper link</a>\n', '  </details>\n', '\n', '  <hr/>\n', '</details>\n', '\n', '<details>\n', '  <summary><strong>Explainable Boosting</strong></summary>\n', '  <hr/>\n', '\n', '  <details>\n', '    <summary>\n', '      <em>""Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission"" (R. Caruana, Y. Lou, J. Gehrke, P. Koch, M. Sturm, and N. Elhadad 2015)</em>\n', '    </summary>\n', '    <br/>\n', '    <pre>\n', '@inproceedings{caruana2015intelligible,\n', '  title={Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission},\n', '  author={Caruana, Rich and Lou, Yin and Gehrke, Johannes and Koch, Paul and Sturm, Marc and Elhadad, Noemie},\n', '  booktitle={Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},\n', '  pages={1721--1730},\n', '  year={2015},\n', '  organization={ACM}\n', '}\n', '    </pre>\n', '    <a href=""https://www.microsoft.com/en-us/research/wp-content/uploads/2017/06/KDD2015FinalDraftIntelligibleModels4HealthCare_igt143e-caruanaA.pdf"">Paper link</a>\n', '  </details>\n', '\n', '  <details>\n', '    <summary>\n', '      <em>""Accurate intelligible models with pairwise interactions"" (Y. Lou, R. Caruana, J. Gehrke, and G. Hooker 2013)</em>\n', '    </summary>\n', '    <br/>\n', '    <pre>\n', '@inproceedings{lou2013accurate,\n', '  title={Accurate intelligible models with pairwise interactions},\n', '  author={Lou, Yin and Caruana, Rich and Gehrke, Johannes and Hooker, Giles},\n', '  booktitle={Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining},\n', '  pages={623--631},\n', '  year={2013},\n', '  organization={ACM}\n', '}\n', '    </pre>\n', '    <a href=""https://www.cs.cornell.edu/~yinlou/papers/lou-kdd13.pdf"">Paper link</a>\n', '  </details>\n', '\n', '  <details>\n', '    <summary>\n', '      <em>""Intelligible models for classification and regression"" (Y. Lou, R. Caruana, and J. Gehrke 2012)</em>\n', '    </summary>\n', '    <br/>\n', '    <pre>\n', '@inproceedings{lou2012intelligible,\n', '  title={Intelligible models for classification and regression},\n', '  author={Lou, Yin and Caruana, Rich and Gehrke, Johannes},\n', '  booktitle={Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining},\n', '  pages={150--158},\n', '  year={2012},\n', '  organization={ACM}\n', '}\n', '    </pre>\n', '    <a href=""https://www.cs.cornell.edu/~yinlou/papers/lou-kdd12.pdf"">Paper link</a>\n', '  </details>\n', '\n', '  <details>\n', '    <summary>\n', '      <em>""Interpretability, Then What? Editing Machine Learning Models to Reflect Human Knowledge and Values"" (Zijie J. Wang, Alex Kale, Harsha Nori, Peter Stella, Mark E. Nunnally, Duen Horng Chau, Mihaela Vorvoreanu, Jennifer Wortman Vaughan, Rich Caruana 2022)</em>\n', '    </summary>\n', '    <br/>\n', '    <pre>\n', '@article{wang2022interpretability,\n', '  title={Interpretability, Then What? Editing Machine Learning Models to Reflect Human Knowledge and Values},\n', '  author={Wang, Zijie J and Kale, Alex and Nori, Harsha and Stella, Peter and Nunnally, Mark E and Chau, Duen Horng and Vorvoreanu, Mihaela and Vaughan, Jennifer Wortman and Caruana, Rich},\n', '  journal={arXiv preprint arXiv:2206.15465},\n', '  year={2022}\n', '}\n', '    </pre>\n', '    <a href=""https://arxiv.org/pdf/2206.15465.pdf"">Paper link</a>\n', '  </details>\n', '\n', '  <details>\n', '    <summary>\n', '      <em>""Axiomatic Interpretability for Multiclass Additive Models"" (X. Zhang, S. Tan, P. Koch, Y. Lou, U. Chajewska, and R. Caruana 2019)</em>\n', '    </summary>\n', '    <br/>\n', '    <pre>\n', '@inproceedings{zhang2019axiomatic,\n', '  title={Axiomatic Interpretability for Multiclass Additive Models},\n', '  author={Zhang, Xuezhou and Tan, Sarah and Koch, Paul and Lou, Yin and Chajewska, Urszula and Caruana, Rich},\n', '  booktitle={Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \\& Data Mining},\n', '  pages={226--234},\n', '  year={2019},\n', '  organization={ACM}\n', '}\n', '    </pre>\n', '    <a href=""https://arxiv.org/pdf/1810.09092.pdf"">Paper link</a>\n', '  </details>\n', '\n', '  <details>\n', '    <summary>\n', '      <em>""Distill-and-compare: auditing black-box models using transparent model distillation"" (S. Tan, R. Caruana, G. Hooker, and Y. Lou 2018)</em>\n', '    </summary>\n', '    <br/>\n', '    <pre>\n', '@inproceedings{tan2018distill,\n', '  title={Distill-and-compare: auditing black-box models using transparent model distillation},\n', '  author={Tan, Sarah and Caruana, Rich and Hooker, Giles and Lou, Yin},\n', '  booktitle={Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},\n', '  pages={303--310},\n', '  year={2018},\n', '  organization={ACM}\n', '}\n', '    </pre>\n', '    <a href=""https://arxiv.org/pdf/1710.06169"">Paper link</a>\n', '  </details>\n', '\n', '  <details>\n', '    <summary>\n', '      <em>""Purifying Interaction Effects with the Functional ANOVA: An Efficient Algorithm for Recovering Identifiable Additive Models"" (B. Lengerich, S. Tan, C. Chang, G. Hooker, R. Caruana 2019)</em>\n', '    </summary>\n', '    <br/>\n', '    <pre>\n', '@article{lengerich2019purifying,\n', '  title={Purifying Interaction Effects with the Functional ANOVA: An Efficient Algorithm for Recovering Identifiable Additive Models},\n', '  author={Lengerich, Benjamin and Tan, Sarah and Chang, Chun-Hao and Hooker, Giles and Caruana, Rich},\n', '  journal={arXiv preprint arXiv:1911.04974},\n', '  year={2019}\n', '}\n', '    </pre>\n', '    <a href=""https://arxiv.org/pdf/1911.04974.pdf"">Paper link</a>\n', '  </details>\n', '\n', '  <details>\n', '    <summary>\n', '      <em>""Interpreting Interpretability: Understanding Data Scientists\' Use of Interpretability Tools for Machine Learning"" (H. Kaur, H. Nori, S. Jenkins, R. Caruana, H. Wallach, J. Wortman Vaughan 2020)</em>\n', '    </summary>\n', '    <br/>\n', '    <pre>\n', '@inproceedings{kaur2020interpreting,\n', ""  title={Interpreting Interpretability: Understanding Data Scientists' Use of Interpretability Tools for Machine Learning},\n"", '  author={Kaur, Harmanpreet and Nori, Harsha and Jenkins, Samuel and Caruana, Rich and Wallach, Hanna and Wortman Vaughan, Jennifer},\n', '  booktitle={Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},\n', '  pages={1--14},\n', '  year={2020}\n', '}\n', '    </pre>\n', '    <a href=""https://www.microsoft.com/en-us/research/publication/interpreting-interpretability-understanding-data-scientists-use-of-interpretability-tools-for-machine-learning/"">Paper link</a>\n', '  </details>\n', '\n', '  <details>\n', '    <summary>\n', '      <em>""How Interpretable and Trustworthy are GAMs?"" (C. Chang, S. Tan, B. Lengerich, A. Goldenberg, R. Caruana 2020)</em>\n', '    </summary>\n', '    <br/>\n', '    <pre>\n', '@article{chang2020interpretable,\n', '  title={How Interpretable and Trustworthy are GAMs?},\n', '  author={Chang, Chun-Hao and Tan, Sarah and Lengerich, Ben and Goldenberg, Anna and Caruana, Rich},\n', '  journal={arXiv preprint arXiv:2006.06466},\n', '  year={2020}\n', '}\n', '    </pre>\n', '    <a href=""https://arxiv.org/pdf/2006.06466.pdf"">Paper link</a>\n', '  </details>\n', '\n', '  <hr/>\n', '</details>\n', '\n', '<details>\n', '  <summary><strong>Differential Privacy</strong></summary>\n', '  <hr/>\n', '\n', '  <details>\n', '    <summary>\n', '      <em>""Accuracy, Interpretability, and Differential Privacy via Explainable Boosting"" (H. Nori, R. Caruana, Z. Bu, J. Shen, J. Kulkarni 2021)</em>\n', '    </summary>\n', '    <br/>\n', '    <pre>\n', '@inproceedings{pmlr-v139-nori21a,\n', '  title = \t {Accuracy, Interpretability, and Differential Privacy via Explainable Boosting},\n', '  author =       {Nori, Harsha and Caruana, Rich and Bu, Zhiqi and Shen, Judy Hanwen and Kulkarni, Janardhan},\n', '  booktitle = \t {Proceedings of the 38th International Conference on Machine Learning},\n', '  pages = \t {8227--8237},\n', '  year = \t {2021},\n', '  volume = \t {139},\n', '  series = \t {Proceedings of Machine Learning Research},\n', '  publisher =    {PMLR}\n', '}\n', '    </pre>\n', '    <a href=""https://proceedings.mlr.press/v139/nori21a/nori21a.pdf"">Paper link</a>\n', '  </details>\n', '\n', '  <hr/>\n', '</details>\n', '\n', '<details>\n', '  <summary><strong>LIME</strong></summary>\n', '  <hr/>\n', '\n', '  <details>\n', '    <summary>\n', '      <em>""Why should i trust you?: Explaining the predictions of any classifier"" (M. T. Ribeiro, S. Singh, and C. Guestrin 2016)</em>\n', '    </summary>\n', '    <br/>\n', '    <pre>\n', '@inproceedings{ribeiro2016should,\n', '  title={Why should i trust you?: Explaining the predictions of any classifier},\n', '  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},\n', '  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},\n', '  pages={1135--1144},\n', '  year={2016},\n', '  organization={ACM}\n', '}\n', '    </pre>\n', '    <a href=""https://arxiv.org/pdf/1602.04938.pdf"">Paper link</a>\n', '  </details>\n', '\n', '  <hr/>\n', '</details>\n', '\n', '<details>\n', '  <summary><strong>SHAP</strong></summary>\n', '  <hr/>\n', '\n', '  <details>\n', '    <summary>\n', '      <em>""A Unified Approach to Interpreting Model Predictions"" (S. M. Lundberg and S.-I. Lee 2017)</em>\n', '    </summary>\n', '    <br/>\n', '    <pre>\n', '@incollection{NIPS2017_7062,\n', ' title = {A Unified Approach to Interpreting Model Predictions},\n', ' author = {Lundberg, Scott M and Lee, Su-In},\n', ' booktitle = {Advances in Neural Information Processing Systems 30},\n', ' editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n', ' pages = {4765--4774},\n', ' year = {2017},\n', ' publisher = {Curran Associates, Inc.},\n', ' url = {https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf}\n', '}\n', '    </pre>\n', '    <a href=""https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf"">Paper link</a>\n', '  </details>\n', '\n', '  <details>\n', '    <summary>\n', '      <em>""Consistent individualized feature attribution for tree ensembles"" (Lundberg, Scott M and Erion, Gabriel G and Lee, Su-In 2018)</em>\n', '    </summary>\n', '    <br/>\n', '    <pre>\n', '@article{lundberg2018consistent,\n', '  title={Consistent individualized feature attribution for tree ensembles},\n', '  author={Lundberg, Scott M and Erion, Gabriel G and Lee, Su-In},\n', '  journal={arXiv preprint arXiv:1802.03888},\n', '  year={2018}\n', '}\n', '    </pre>\n', '    <a href=""https://arxiv.org/pdf/1802.03888"">Paper link</a>\n', '  </details>\n', '\n', '  <details>\n', '    <summary>\n', '      <em>""Explainable machine-learning predictions for the prevention of hypoxaemia during surgery"" (S. M. Lundberg et al. 2018)</em>\n', '    </summary>\n', '    <br/>\n', '    <pre>\n', '@article{lundberg2018explainable,\n', '  title={Explainable machine-learning predictions for the prevention of hypoxaemia during surgery},\n', '  author={Lundberg, Scott M and Nair, Bala and Vavilala, Monica S and Horibe, Mayumi and Eisses, Michael J and Adams, Trevor and Liston, David E and Low, Daniel King-Wai and Newman, Shu-Fang and Kim, Jerry and others},\n', '  journal={Nature Biomedical Engineering},\n', '  volume={2},\n', '  number={10},\n', '  pages={749},\n', '  year={2018},\n', '  publisher={Nature Publishing Group}\n', '}\n', '    </pre>\n', '    <a href=""https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6467492/pdf/nihms-1505578.pdf"">Paper link</a>\n', '  </details>\n', '\n', '  <hr/>\n', '</details>\n', '\n', '<details>\n', '  <summary><strong>Sensitivity Analysis</strong></summary>\n', '  <hr/>\n', '\n', '  <details>\n', '    <summary>\n', '      <em>""SALib: An open-source Python library for Sensitivity Analysis"" (J. D. Herman and W. Usher 2017)</em>\n', '    </summary>\n', '    <br/>\n', '    <pre>\n', '@article{herman2017salib,\n', '  title={SALib: An open-source Python library for Sensitivity Analysis.},\n', '  author={Herman, Jonathan D and Usher, Will},\n', '  journal={J. Open Source Software},\n', '  volume={2},\n', '  number={9},\n', '  pages={97},\n', '  year={2017}\n', '}\n', '    </pre>\n', '    <a href=""https://www.researchgate.net/profile/Will_Usher/publication/312204236_SALib_An_open-source_Python_library_for_Sensitivity_Analysis/links/5ac732d64585151e80a39547/SALib-An-open-source-Python-library-for-Sensitivity-Analysis.pdf?origin=publication_detail"">Paper link</a>\n', '  </details>\n', '\n', '  <details>\n', '    <summary>\n', '      <em>""Factorial sampling plans for preliminary computational experiments"" (M. D. Morris 1991)</em>\n', '    </summary>\n', '    <br/>\n', '    <pre>\n', '@article{morris1991factorial,\n', '  title={},\n', '  author={Morris, Max D},\n', '  journal={Technometrics},\n', '  volume={33},\n', '  number={2},\n', '  pages={161--174},\n', '  year={1991},\n', '  publisher={Taylor \\& Francis Group}\n', '}\n', '    </pre>\n', '    <a href=""https://abe.ufl.edu/Faculty/jjones/ABE_5646/2010/Morris.1991%20SA%20paper.pdf"">Paper link</a>\n', '  </details>\n', '\n', '  <hr/>\n', '</details>\n', '\n', '<details>\n', '  <summary><strong>Partial Dependence</strong></summary>\n', '  <hr/>\n', '\n', '  <details>\n', '    <summary>\n', '      <em>""Greedy function approximation: a gradient boosting machine"" (J. H. Friedman 2001)</em>\n', '    </summary>\n', '    <br/>\n', '    <pre>\n', '@article{friedman2001greedy,\n', '  title={Greedy function approximation: a gradient boosting machine},\n', '  author={Friedman, Jerome H},\n', '  journal={Annals of statistics},\n', '  pages={1189--1232},\n', '  year={2001},\n', '  publisher={JSTOR}\n', '}\n', '    </pre>\n', '    <a href=""https://projecteuclid.org/download/pdf_1/euclid.aos/1013203451"">Paper link</a>\n', '  </details>\n', '\n', '  <hr/>\n', '</details>\n', '\n', '<details>\n', '  <summary><strong>Open Source Software</strong></summary>\n', '  <hr/>\n', '\n', '  <details>\n', '    <summary>\n', '      <em>""Scikit-learn: Machine learning in Python"" (F. Pedregosa et al. 2011)</em>\n', '    </summary>\n', '    <br/>\n', '    <pre>\n', '@article{pedregosa2011scikit,\n', '  title={Scikit-learn: Machine learning in Python},\n', '  author={Pedregosa, Fabian and Varoquaux, Ga{\\""e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},\n', '  journal={Journal of machine learning research},\n', '  volume={12},\n', '  number={Oct},\n', '  pages={2825--2830},\n', '  year={2011}\n', '}\n', '    </pre>\n', '    <a href=""https://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf"">Paper link</a>\n', '  </details>\n', '\n', '  <details>\n', '    <summary>\n', '      <em>""Collaborative data science"" (Plotly Technologies Inc. 2015)</em>\n', '    </summary>\n', '    <br/>\n', '    <pre>\n', '@online{plotly, \n', '  author = {Plotly Technologies Inc.}, \n', '  title = {Collaborative data science}, \n', '  publisher = {Plotly Technologies Inc.}, \n', '  address = {Montreal, QC}, \n', '  year = {2015}, \n', '  url = {https://plot.ly}\n', '}\n', '    </pre>\n', '    <a href=""https://plot.ly"">Link</a>\n', '  </details>\n', '  \n', '  <details>\n', '    <summary>\n', '      <em>""Joblib: running python function as pipeline jobs"" (G. Varoquaux and O. Grisel 2009)</em>\n', '    </summary>\n', '    <br/>\n', '    <pre>\n', '@article{varoquaux2009joblib,\n', '  title={Joblib: running python function as pipeline jobs},\n', '  author={Varoquaux, Ga{\\""e}l and Grisel, O},\n', '  journal={packages. python. org/joblib},\n', '  year={2009}\n', '}\n', '    </pre>\n', '    <a href=""https://joblib.readthedocs.io/en/latest/"">Link</a>\n', '  </details>\n', '  \n', '  <hr/>\n', '</details>\n', '\n', '# Videos\n', '\n', '- [The Science Behind InterpretML: Explainable Boosting Machine](https://www.youtube.com/watch?v=MREiHgHgl0k)\n', '- [How to Explain Models with InterpretML Deep Dive](https://www.youtube.com/watch?v=WwBeKMQ0-I8)\n', '- [Black-Box and Glass-Box Explanation in Machine Learning](https://youtu.be/7uzNKY8pEhQ)\n', '- [Explainable AI explained!  By-design interpretable models with Microsofts InterpretML](https://www.youtube.com/watch?v=qPn9m30ojfc)\n', '- [Interpreting Machine Learning Models with InterpretML](https://www.youtube.com/watch?v=ERNuFfsknhk)\n', '\n', '# External links\n', '\n', '- [Interpretable or Accurate? Why Not Both?](https://towardsdatascience.com/interpretable-or-accurate-why-not-both-4d9c73512192)\n', '- [The Explainable Boosting Machine. As accurate as gradient boosting, as interpretable as linear regression.](https://towardsdatascience.com/the-explainable-boosting-machine-f24152509ebb)\n', '- [Performance And Explainability With EBM](https://blog.oakbits.com/ebm-algorithm.html)\n', '- [InterpretML: Another Way to Explain Your Model](https://towardsdatascience.com/interpretml-another-way-to-explain-your-model-b7faf0a384f8)\n', '- [A gentle introduction to GA2Ms, a white box model](https://www.fiddler.ai/blog/a-gentle-introduction-to-ga2ms-a-white-box-model)\n', '- [Model Interpretation with Microsoft’s Interpret ML](https://medium.com/@sand.mayur/model-interpretation-with-microsofts-interpret-ml-85aa0ad697ae)\n', '- [Explaining Model Pipelines With InterpretML](https://medium.com/@mariusvadeika/explaining-model-pipelines-with-interpretml-a9214f75400b)\n', '- [Explain Your Model with Microsoft’s InterpretML](https://medium.com/@Dataman.ai/explain-your-model-with-microsofts-interpretml-5daab1d693b4)\n', '- [On Model Explainability: From LIME, SHAP, to Explainable Boosting](https://everdark.github.io/k9/notebooks/ml/model_explain/model_explain.nb.html)\n', '- [Dealing with Imbalanced Data (Mortgage loans defaults)](https://mikewlange.github.io/ImbalancedData-/index.html)\n', '- [The right way to compute your Shapley Values](https://towardsdatascience.com/the-right-way-to-compute-your-shapley-values-cfea30509254)\n', '- [The Art of Sprezzatura for Machine Learning](https://towardsdatascience.com/the-art-of-sprezzatura-for-machine-learning-e2494c0db727)\n', '- [Mixing Art into the Science of Model Explainability](https://towardsdatascience.com/mixing-art-into-the-science-of-model-explainability-312b8216fa95)\n', '\n', '# Papers that use or compare EBMs\n', '\n', '\n', '- [Model Interpretability in Credit Insurance](http://hdl.handle.net/10400.5/27507)\n', '- [Federated Boosted Decision Trees with Differential Privacy](https://arxiv.org/pdf/2210.02910.pdf)\n', '- [GAM(E) CHANGER OR NOT? AN EVALUATION OF INTERPRETABLE MACHINE LEARNING MODELS](https://arxiv.org/pdf/2204.09123.pdf)\n', '- [GAM Coach: Towards Interactive and User-centered Algorithmic Recourse](https://arxiv.org/pdf/2302.14165.pdf)\n', '- [Revealing the Galaxy-Halo Connection Through Machine Learning](https://arxiv.org/pdf/2204.10332.pdf)\n', '- [Explainable Artificial Intelligence for COVID-19 Diagnosis Through Blood Test Variables](https://link.springer.com/content/pdf/10.1007/s40313-021-00858-y.pdf)\n', '- [Using Explainable Boosting Machines (EBMs) to Detect Common Flaws in Data](https://link.springer.com/chapter/10.1007/978-3-030-93736-2_40)\n', '- [Differentially Private Gradient Boosting on Linear Learners for Tabular Data Analysis](https://assets.amazon.science/fa/3a/a62ba73f4bbda1d880b678c39193/differentially-private-gradient-boosting-on-linear-learners-for-tabular-data-analysis.pdf)\n', '- [Concrete compressive strength prediction using an explainable boosting machine model](https://www.sciencedirect.com/science/article/pii/S2214509523000244/pdfft?md5=171c275b6bcae8897cef03d931e908e2&pid=1-s2.0-S2214509523000244-main.pdf)\n', '- [Estimate Deformation Capacity of Non-Ductile RC Shear Walls Using Explainable Boosting Machine](https://arxiv.org/pdf/2301.04652.pdf)\n', '- [Introducing the Rank-Biased Overlap as Similarity Measure for Feature Importance in Explainable Machine Learning: A Case Study on Parkinson’s Disease](https://link.springer.com/chapter/10.1007/978-3-031-15037-1_11)\n', '- [Targeting resources efficiently and justifiably by combining causal machine learning and theory](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9768181/pdf/frai-05-1015604.pdf)\n', '- [Extractive Text Summarization Using Generalized Additive Models with Interactions for Sentence Selection](https://arxiv.org/pdf/2212.10707.pdf)\n', '- [Death by Round Numbers: Glass-Box Machine Learning Uncovers Biases in Medical Practice](https://www.medrxiv.org/content/medrxiv/early/2022/11/28/2022.04.30.22274520.full.pdf)\n', '- [Post-Hoc Interpretation of Transformer Hyperparameters with Explainable Boosting Machines](https://www.cs.jhu.edu/~xzhan138/papers/BLACK2022.pdf)\n', '- [Interpretable machine learning for predicting pathologic complete response in patients treated with chemoradiation therapy for rectal adenocarcinoma](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9771385/pdf/frai-05-1059033.pdf)\n', '- [Exploring the Balance between Interpretability and Performance with carefully designed Constrainable Neural Additive Models](https://deliverypdf.ssrn.com/delivery.php?ID=998105006000069122073098120102102121021040051018055094125029122011041003059093125102072122106122077081069015087124028097016003127095087091028087010007035098086102086081014043013113004081117108011028041097095064071100112069081100069120077067116088100069070097093080074087115080072064086111126&EXT=pdf&INDEX=TRUE)\n', '- [Estimating Discontinuous Time-Varying Risk Factors and Treatment Benefits for COVID-19 with Interpretable ML](https://arxiv.org/pdf/2211.08991.pdf)\n', '- [Pest Presence Prediction Using Interpretable Machine Learning](https://arxiv.org/pdf/2205.07723.pdf)\n', '- [epitope1D: Accurate Taxonomy-Aware B-Cell Linear Epitope Prediction](https://www.biorxiv.org/content/10.1101/2022.10.17.512613v1.full.pdf)\n', '- [Explainable Boosting Machines for Slope Failure Spatial Predictive Modeling](https://www.mdpi.com/2072-4292/13/24/4991/htm)\n', '- [Micromodels for Efficient, Explainable, and Reusable Systems: A Case Study on Mental Health](https://arxiv.org/pdf/2109.13770.pdf)\n', '- [Identifying main and interaction effects of risk factors to predict intensive care admission in patients hospitalized with COVID-19](https://www.medrxiv.org/content/10.1101/2020.06.30.20143651v1.full.pdf)\n', '- [Comparing the interpretability of machine learning classifiers for brain tumour survival prediction](https://deliverypdf.ssrn.com/delivery.php?ID=760122118067103094108090123091079011028032009009023085005014014002123105085114025022024005047078031019089073120012025117073002064031071072113006066035001068125027021087087083085026100009018045107092063001023068071002124070107120120007014102094103069089119026110104107005031095001092090&EXT=pdf&INDEX=TRUE)\n', '- [Using Interpretable Machine Learning to Predict Maternal and Fetal Outcomes](https://arxiv.org/pdf/2207.05322.pdf)\n', '- [Calibrate: Interactive Analysis of Probabilistic Model Output](https://arxiv.org/pdf/2207.13770.pdf)\n', '- [Neural Additive Models: Interpretable Machine Learning with Neural Nets](https://arxiv.org/pdf/2004.13912.pdf)\n', '- [NODE-GAM: Neural Generalized Additive Model for Interpretable Deep Learning](https://arxiv.org/pdf/2106.01613.pdf)\n', '- [Scalable Interpretability via Polynomials](https://arxiv.org/pdf/2205.14108v1.pdf)\n', '- [Neural Basis Models for Interpretability](https://arxiv.org/pdf/2205.14120.pdf)\n', '- [ILMART: Interpretable Ranking with Constrained LambdaMART](https://arxiv.org/pdf/2206.00473.pdf)\n', '- [Integrating Co-Clustering and Interpretable Machine Learning for the Prediction of Intravenous Immunoglobulin Resistance in Kawasaki Disease](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9097874)\n', '- [GAMI-Net: An Explainable Neural Network based on Generalized Additive Models with Structured Interactions](https://arxiv.org/pdf/2003.07132v1.pdf)\n', '- [A Concept and Argumentation based Interpretable Model in High Risk Domains](https://arxiv.org/pdf/2208.08149.pdf)\n', '- [Analyzing the Differences be"
Model Explainability,RexYing/gnn-model-explainer,RexYing,https://api.github.com/repos/RexYing/gnn-model-explainer,681,153,6,"['https://api.github.com/users/RexYing', 'https://api.github.com/users/dtsbourg', 'https://api.github.com/users/JiaxuanYou', 'https://api.github.com/users/hnaik', 'https://api.github.com/users/devloop0', 'https://api.github.com/users/Stannislav']",Python,2023-04-26T08:44:24Z,https://raw.githubusercontent.com/RexYing/gnn-model-explainer/master/README.md,"['# gnn-explainer\n', '\n', 'This repository contains the source code for the paper `GNNExplainer: Generating Explanations for Graph Neural Networks` by [Rex Ying](https://cs.stanford.edu/people/rexy/), [Dylan Bourgeois](https://dtsbourg.me/), [Jiaxuan You](https://cs.stanford.edu/~jiaxuan/), [Marinka Zitnik](http://helikoid.si/cms/) & [Jure Leskovec](https://cs.stanford.edu/people/jure/), presented at [NeurIPS 2019](nips.cc).\n', '\n', '[[Arxiv]](https://arxiv.org/abs/1903.03894) [[BibTex]](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1903-03894) [[Google Scholar]](https://scholar.google.com/scholar?q=GNNExplainer%3A%20Generating%20Explanations%20for%20Graph%20Neural%20Networks%20Rex%20arXiv%202019)\n', '\n', '```\n', '@misc{ying2019gnnexplainer,\n', '    title={GNNExplainer: Generating Explanations for Graph Neural Networks},\n', '    author={Rex Ying and Dylan Bourgeois and Jiaxuan You and Marinka Zitnik and Jure Leskovec},\n', '    year={2019},\n', '    eprint={1903.03894},\n', '    archivePrefix={arXiv},\n', '    primaryClass={cs.LG}\n', '}\n', '```\n', '\n', '## Using the explainer\n', '\n', '### Installation\n', '\n', 'See [INSTALLATION.md](#)\n', '\n', ""### Replicating the paper's results\n"", '\n', '#### Training a GCN model \n', '\n', 'This is the model that will be explained. We do provide [pre-trained models](#TODO) for all of the experiments\n', 'that are shown in the paper. To re-train these models, run the following:\n', '\n', '```\n', 'python train.py --dataset=EXPERIMENT_NAME\n', '```\n', '\n', 'where `EXPERIMENT_NAME` is the experiment you want to replicate. \n', '\n', 'For a complete list of options in training the GCN models:\n', '\n', '```\n', 'python train.py --help\n', '```\n', '\n', '> TODO: Explain outputs\n', '\n', '#### Explaining a GCN model\n', '\n', 'To run the explainer, run the following:\n', '\n', '```\n', 'python explainer_main.py --dataset=EXPERIMENT_NAME\n', '```\n', '\n', 'where `EXPERIMENT_NAME` is the experiment you want to replicate. \n', '\n', '\n', 'For a complete list of options provided by the explainer:\n', '\n', '```\n', 'python train.py --help\n', '```\n', '\n', '#### Visualizing the explanations\n', '\n', '##### Tensorboard\n', '\n', 'The result of the optimization can be visualized through Tensorboard.\n', '\n', '```\n', 'tensorboard --logdir log\n', '```\n', '\n', 'You should then have access to visualizations served from `localhost`.\n', '\n', '#### Jupyter Notebook\n', '\n', 'We provide an example visualization through Jupyter Notebooks in the `notebook` folder. To try it:\n', '\n', '```\n', 'jupyter notebook\n', '```\n', '\n', 'The default visualizations are provided in `notebook/GNN-Explainer-Viz.ipynb`.\n', '\n', '> Note: For an interactive version, you must enable ipywidgets\n', '>\n', '> ```\n', '> jupyter nbextension enable --py widgetsnbextension\n', '> ```\n', '\n', 'You can now play around with the mask threshold in the `GNN-Explainer-Viz-interactive.ipynb`.\n', '> TODO: Explain outputs + visualizations + baselines\n', '\n', '#### D3,js\n', '\n', 'We provide export functionality so the generated masks can be visualized in other data visualization \n', 'frameworks, for example [d3.js](http://observablehq.com). We provide [an example visualization in Observable](https://observablehq.com/d/00c5dc74f359e7a1).\n', '\n', '#### Included experiments\n', '\n', '| Name     | `EXPERIMENT_NAME` | Description  |\n', '|----------|:-------------------:|--------------|\n', '| Synthetic #1 | `syn1`  | Random BA graph with House attachments.  |\n', '| Synthetic #2 | `syn2`  | Random BA graph with community features. | \n', '| Synthetic #3 | `syn3`  | Random BA graph with grid attachments.  |\n', '| Synthetic #4 | `syn4`  | Random Tree with cycle attachments. |\n', '| Synthetic #5 | `syn5`  | Random Tree with grid attachments. | \n', '| Enron        | `enron` | Enron email dataset [source](https://www.cs.cmu.edu/~enron/). |\n', '| PPI          | `ppi_essential` | Protein-Protein interaction dataset. |\n', '| | | |\n', '| Reddit*      | `REDDIT-BINARY`  | Reddit-Binary Graphs ([source](https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets)). |\n', '| Mutagenicity*      | `Mutagenicity`  | Predicting the mutagenicity of molecules ([source](https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets)). |\n', ""| Tox 21*      | `Tox21_AHR`  | Predicting a compound's toxicity ([source](https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets)). |\n"", '\n', '> Datasets with a * are passed with the `--bmname` parameter rather than `--dataset` as they require being downloaded manually.\n', '\n', ""> TODO: Provide all data for experiments packaged so we don't have to split the two.\n"", '\n', '\n', '### Using the explainer on other models\n', 'A graph attention model is provided. This repo is still being actively developed to support other\n', 'GNN models in the future.\n', '\n', '## Changelog\n', '\n', 'See [CHANGELOG.md](#)\n']"
Model Explainability,Trusted-AI/AIX360,Trusted-AI,https://api.github.com/repos/Trusted-AI/AIX360,1323,286,23,"['https://api.github.com/users/vijay-arya', 'https://api.github.com/users/kmyusk', 'https://api.github.com/users/sadhamanus', 'https://api.github.com/users/dennislwei', 'https://api.github.com/users/floidgilbert', 'https://api.github.com/users/rluss', 'https://api.github.com/users/ImgBotApp', 'https://api.github.com/users/pronics2004', 'https://api.github.com/users/Tomcli', 'https://api.github.com/users/jamescodella', 'https://api.github.com/users/rahulnair23ibm', 'https://api.github.com/users/animeshsingh', 'https://api.github.com/users/karthikeyansh', 'https://api.github.com/users/michaelhind', 'https://api.github.com/users/monindersingh', 'https://api.github.com/users/fabianlim', 'https://api.github.com/users/cclauss', 'https://api.github.com/users/kant', 'https://api.github.com/users/gaborpelesz', 'https://api.github.com/users/gdequeiroz', 'https://api.github.com/users/ishapuri', 'https://api.github.com/users/Marleen1', 'https://api.github.com/users/asm582']",Python,2023-04-26T02:59:05Z,https://raw.githubusercontent.com/Trusted-AI/AIX360/master/README.md,"['# AI Explainability 360 (v0.2.1) \n', '\n', '[![Build](https://github.com/Trusted-AI/AIX360/actions/workflows/Build.yml/badge.svg)](https://github.com/Trusted-AI/AIX360/actions/workflows/Build.yml)\n', '[![Documentation Status](https://readthedocs.org/projects/aix360/badge/?version=latest)](https://aix360.readthedocs.io/en/latest/?badge=latest)\n', '[![PyPI version](https://badge.fury.io/py/aix360.svg)](https://badge.fury.io/py/aix360)\n', '\n', 'The AI Explainability 360 toolkit is an open-source library that supports interpretability and explainability of datasets and machine learning models. The AI Explainability 360 Python package includes a comprehensive set of algorithms that cover different dimensions of explanations along with proxy explainability metrics.           \n', '\n', 'The [AI Explainability 360 interactive experience](http://aix360.mybluemix.net/data) provides a gentle introduction to the concepts and capabilities by walking through an example use case for different consumer personas. The [tutorials and example notebooks](./examples) offer a deeper, data scientist-oriented introduction. The complete API is also available. \n', '\n', 'There is no single approach to explainability that works best. There are many ways to explain: data vs. model, directly interpretable vs. post hoc explanation, local vs. global, etc. It may therefore be confusing to figure out which algorithms are most appropriate for a given use case. To help, we have created some [guidance material](http://aix360.mybluemix.net/resources#guidance) and a [chart](./aix360/algorithms/README.md) that can be consulted. \n', '\n', 'We have developed the package with extensibility in mind. This library is still in development. We encourage you to contribute your explainability algorithms, metrics, and use cases. To get started as a contributor, please join the [AI Explainability 360 Community on Slack](https://aix360.slack.com) by requesting an invitation [here](https://join.slack.com/t/aix360/shared_invite/enQtNzEyOTAwOTk1NzY2LTM1ZTMwM2M4OWQzNjhmNGRiZjg3MmJiYTAzNDU1MTRiYTIyMjFhZTI4ZDUwM2M1MGYyODkwNzQ2OWQzMThlN2Q). Please review the instructions to contribute code and python notebooks [here](CONTRIBUTING.md).\n', '\n', '## Supported explainability algorithms\n', '\n', '### Data explanation\n', '\n', '- ProtoDash ([Gurumoorthy et al., 2019](https://arxiv.org/abs/1707.01212))\n', '- Disentangled Inferred Prior VAE ([Kumar et al., 2018](https://openreview.net/forum?id=H1kG7GZAW))\n', '\n', '### Local post-hoc explanation \n', '\n', '- ProtoDash ([Gurumoorthy et al., 2019](https://arxiv.org/abs/1707.01212))\n', '- Contrastive Explanations Method ([Dhurandhar et al., 2018](https://papers.nips.cc/paper/7340-explanations-based-on-the-missing-towards-contrastive-explanations-with-pertinent-negatives))\n', '- Contrastive Explanations Method with Monotonic Attribute Functions ([Luss et al., 2019](https://arxiv.org/abs/1905.12698))\n', '- LIME ([Ribeiro et al. 2016](https://arxiv.org/abs/1602.04938),  [Github](https://github.com/marcotcr/lime))\n', '- SHAP ([Lundberg, et al. 2017](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions),  [Github](https://github.com/slundberg/shap))\n', '\n', '### Local direct explanation\n', '\n', '- Teaching AI to Explain its Decisions ([Hind et al., 2019](https://doi.org/10.1145/3306618.3314273)) \n', '- Order Constraints in Optimal Transport ([Lim et al.,2022](https://arxiv.org/abs/2110.07275), [Github](https://github.com/IBM/otoc))\n', '\n', '### Global direct explanation\n', '\n', '- CoFrNets (Continued Fraction Nets) ([Puri et al., 2021](https://papers.nips.cc/paper/2021/file/b538f279cb2ca36268b23f557a831508-Paper.pdf))\n', '- Boolean Decision Rules via Column Generation (Light Edition) ([Dash et al., 2018](https://papers.nips.cc/paper/7716-boolean-decision-rules-via-column-generation))\n', '- Generalized Linear Rule Models ([Wei et al., 2019](http://proceedings.mlr.press/v97/wei19a.html))\n', '- Fast Effective Rule Induction (Ripper) ([William W Cohen, 1995](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.107.2612&rep=rep1&type=pdf))\n', '\n', '### Global post-hoc explanation\xa0\n', '\n', '- ProfWeight ([Dhurandhar et al., 2018](https://papers.nips.cc/paper/8231-improving-simple-models-with-confidence-profiles))\n', '\n', '\n', '## Supported explainability metrics\n', '- Faithfulness ([Alvarez-Melis and Jaakkola, 2018](https://papers.nips.cc/paper/8003-towards-robust-interpretability-with-self-explaining-neural-networks))\n', '- Monotonicity ([Luss et al., 2019](https://arxiv.org/abs/1905.12698))\n', '\n', '## Setup\n', '\n', 'Supported Configurations:\n', '\n', '| OS      | Python version |\n', '| ------- | -------------- |\n', '| macOS   | 3.6  |\n', '| Ubuntu  | 3.6  |\n', '| Windows | 3.6  |\n', '\n', '### (Optional) Create a virtual environment\n', '\n', 'AI Explainability 360 requires specific versions of many Python packages which may conflict\n', 'with other projects on your system. A virtual environment manager is strongly\n', 'recommended to ensure dependencies may be installed safely. If you have trouble installing the toolkit, try this first.\n', '\n', '#### Conda\n', '\n', 'Conda is recommended for all configurations though Virtualenv is generally\n', 'interchangeable for our purposes. Miniconda is sufficient (see [the difference between Anaconda and\n', 'Miniconda](https://conda.io/docs/user-guide/install/download.html#anaconda-or-miniconda)\n', 'if you are curious) and can be installed from\n', '[here](https://conda.io/miniconda.html) if you do not already have it.\n', '\n', 'Then, to create a new Python 3.6 environment, run:\n', '\n', '```bash\n', 'conda create --name aix360 python=3.6\n', 'conda activate aix360\n', '```\n', '\n', 'The shell should now look like `(aix360) $`. To deactivate the environment, run:\n', '\n', '```bash\n', '(aix360)$ conda deactivate\n', '```\n', '\n', 'The prompt will return back to `$ ` or `(base)$`.\n', '\n', 'Note: Older versions of conda may use `source activate aix360` and `source\n', 'deactivate` (`activate aix360` and `deactivate` on Windows).\n', '\n', '\n', '### Installation\n', '\n', 'Clone the latest version of this repository:\n', '\n', '```bash\n', '(aix360)$ git clone https://github.com/Trusted-AI/AIX360\n', '```\n', '\n', ""If you'd like to run the examples and tutorial notebooks, download the datasets now and place them in\n"", 'their respective folders as described in\n', '[aix360/data/README.md](aix360/data/README.md).\n', '\n', 'Then, navigate to the root directory of the project which contains `setup.py` file and run:\n', '\n', '```bash\n', '(aix360)$ pip install -e .\n', '```\n', '\n', 'If you face any issues, please try upgrading pip and setuptools and uninstall any previous versions of aix360 before attempting the above step again. \n', '\n', '```bash\n', '(aix360)$ pip install --upgrade pip setuptools\n', '(aix360)$ pip uninstall aix360\n', '```\n', '\n', '## Running in Docker\n', '\n', '* Under `AIX360` directory build the container image from Dockerfile using `docker build -t aix360_docker .`\n', '* Start the container image using command `docker run -it -p 8888:8888 aix360_docker:latest bash` assuming port 8888 is free on your machine.\n', '* Inside the container start jupuyter lab using command `jupyter lab --allow-root --ip 0.0.0.0 --port 8888 --no-browser`\n', '* Access the sample tutorials on your machine using URL `localhost:8888`\n', '\n', '## PIP Installation of AI Explainability 360\n', '\n', 'If you would like to quickly start using the AI explainability 360 toolkit without cloning this repository, then you can install the [aix360 pypi package](https://pypi.org/project/aix360/) as follows. \n', '\n', '```bash\n', '(your environment)$ pip install aix360\n', '```\n', '\n', 'If you follow this approach, you may need to download the notebooks in the [examples](./examples) folder separately. \n', '\n', '\n', '## Using AI Explainability 360\n', '\n', 'The `examples` directory contains a diverse collection of jupyter notebooks\n', 'that use AI Explainability 360 in various ways. Both examples and tutorial notebooks illustrate\n', 'working code using the toolkit. Tutorials provide additional discussion that walks\n', 'the user through the various steps of the notebook. See the details about\n', 'tutorials and examples [here](examples/README.md). \n', '\n', '## Citing AI Explainability 360\n', '\n', 'If you are using AI Explainability 360 for your work, we encourage you to\n', '\n', '* Cite the following [paper](https://arxiv.org/abs/1909.03012). The bibtex entry is as follows: \n', '\n', '```\n', '@misc{aix360-sept-2019,\n', 'title = ""One Explanation Does Not Fit All: A Toolkit and Taxonomy of AI Explainability Techniques"",\n', 'author = {Vijay Arya and Rachel K. E. Bellamy and Pin-Yu Chen and Amit Dhurandhar and Michael Hind\n', ""and Samuel C. Hoffman and Stephanie Houde and Q. Vera Liao and Ronny Luss and Aleksandra Mojsilovi\\'c\n"", 'and Sami Mourad and Pablo Pedemonte and Ramya Raghavendra and John Richards and Prasanna Sattigeri\n', 'and Karthikeyan Shanmugam and Moninder Singh and Kush R. Varshney and Dennis Wei and Yunfeng Zhang},\n', 'month = sept,\n', 'year = {2019},\n', 'url = {https://arxiv.org/abs/1909.03012}\n', '}\n', '```\n', '\n', '* Put a star on this repository.\n', '\n', '* Share your success stories with us and others in the [AI Explainability 360 Community](https://aix360.slack.com). \n', '\n', '## AIX360 Videos\n', '\n', '* Introductory [video](https://www.youtube.com/watch?v=Yn4yduyoQh4) to AI\n', '  Explainability 360 by Vijay Arya and Amit Dhurandhar, September 5, 2019 (35 mins)\n', '\n', '## Acknowledgements\n', '\n', 'AIX360 is built with the help of several open source packages. All of these are listed in setup.py and some of these include: \n', '* Tensorflow https://www.tensorflow.org/about/bib\n', '* Pytorch https://github.com/pytorch/pytorch\n', '* scikit-learn https://scikit-learn.org/stable/about.html\n', '\n', '## License Information\n', '\n', 'Please view both the [LICENSE](https://github.com/vijay-arya/AIX360/blob/master/LICENSE) file and the folder [supplementary license](https://github.com/vijay-arya/AIX360/tree/master/supplementary%20license) present in the root directory for license information. \n', '\n']"
Model Explainability,slundberg/shap,slundberg,https://api.github.com/repos/slundberg/shap,19070,2866,30,"['https://api.github.com/users/slundberg', 'https://api.github.com/users/ryserrao', 'https://api.github.com/users/vivekchettiar', 'https://api.github.com/users/imatiach-msft', 'https://api.github.com/users/gabrieltseng', 'https://api.github.com/users/RAMitchell', 'https://api.github.com/users/QuentinAmbard', 'https://api.github.com/users/anusham1990', 'https://api.github.com/users/dependabot%5Bbot%5D', 'https://api.github.com/users/floidgilbert', 'https://api.github.com/users/jsu27', 'https://api.github.com/users/lrjball', 'https://api.github.com/users/kodonnell', 'https://api.github.com/users/JasonTam', 'https://api.github.com/users/maggiewu19', 'https://api.github.com/users/alexisdrakopoulos', 'https://api.github.com/users/jorgecarleitao', 'https://api.github.com/users/tlabarta', 'https://api.github.com/users/KOLANICH', 'https://api.github.com/users/alexander-pv', 'https://api.github.com/users/xzzxxzzx', 'https://api.github.com/users/moritzaugustin', 'https://api.github.com/users/parsatorb', 'https://api.github.com/users/JiechengZhao', 'https://api.github.com/users/sbrugman', 'https://api.github.com/users/SachinVarghese', 'https://api.github.com/users/ihopethiswillfi', 'https://api.github.com/users/mizukasai', 'https://api.github.com/users/JacobMcc', 'https://api.github.com/users/jeremiahpslewis']",Python,2023-04-26T14:23:30Z,https://raw.githubusercontent.com/slundberg/shap/master/README.md,"['\n', '\n', '<p align=""center"">\n', '  <img src=""https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/shap_header.svg"" width=""800"" />\n', '</p>\n', '\n', '---\n', '![example workflow](https://github.com/slundberg/shap/actions/workflows/run_tests.yml/badge.svg)\n', '[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/slundberg/shap/master)\n', '[![Documentation Status](https://readthedocs.org/projects/shap/badge/?version=latest)](https://shap.readthedocs.io/en/latest/?badge=latest)\n', '\n', '**SHAP (SHapley Additive exPlanations)** is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions (see [papers](#citations) for details and citations).\n', '\n', '<!--**SHAP (SHapley Additive exPlanations)** is a unified approach to explain the output of any machine learning model. SHAP connects game theory with local explanations, uniting several previous methods [1-7] and representing the only possible consistent and locally accurate additive feature attribution method based on expectations (see our [papers](#citations) for details and citations).-->\n', '\n', '\n', '\n', '## Install\n', '\n', 'SHAP can be installed from either [PyPI](https://pypi.org/project/shap) or [conda-forge](https://anaconda.org/conda-forge/shap):\n', '\n', '<pre>\n', 'pip install shap\n', '<i>or</i>\n', 'conda install -c conda-forge shap\n', '</pre>\n', '\n', '## Tree ensemble example (XGBoost/LightGBM/CatBoost/scikit-learn/pyspark models)\n', '\n', 'While SHAP can explain the output of any machine learning model, we have developed a high-speed exact algorithm for tree ensemble methods (see our [Nature MI paper](https://rdcu.be/b0z70)). Fast C++ implementations are supported for *XGBoost*, *LightGBM*, *CatBoost*, *scikit-learn* and *pyspark* tree models:\n', '\n', '```python\n', 'import xgboost\n', 'import shap\n', '\n', '# train an XGBoost model\n', 'X, y = shap.datasets.boston()\n', 'model = xgboost.XGBRegressor().fit(X, y)\n', '\n', ""# explain the model's predictions using SHAP\n"", '# (same syntax works for LightGBM, CatBoost, scikit-learn, transformers, Spark, etc.)\n', 'explainer = shap.Explainer(model)\n', 'shap_values = explainer(X)\n', '\n', ""# visualize the first prediction's explanation\n"", 'shap.plots.waterfall(shap_values[0])\n', '```\n', '\n', '<p align=""center"">\n', '  <img width=""616"" src=""https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_waterfall.png"" />\n', '</p>\n', '\n', 'The above explanation shows features each contributing to push the model output from the base value (the average model output over the training dataset we passed) to the model output. Features pushing the prediction higher are shown in red, those pushing the prediction lower are in blue. Another way to visualize the same explanation is to use a force plot (these are introduced in our [Nature BME paper](https://rdcu.be/baVbR)):\n', '\n', '```python\n', ""# visualize the first prediction's explanation with a force plot\n"", 'shap.plots.force(shap_values[0])\n', '```\n', '\n', '<p align=""center"">\n', '  <img width=""811"" src=""https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_instance.png"" />\n', '</p>\n', '\n', 'If we take many force plot explanations such as the one shown above, rotate them 90 degrees, and then stack them horizontally, we can see explanations for an entire dataset (in the notebook this plot is interactive):\n', '\n', '```python\n', '# visualize all the training set predictions\n', 'shap.plots.force(shap_values)\n', '```\n', '\n', '<p align=""center"">\n', '  <img width=""811"" src=""https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_dataset.png"" />\n', '</p>\n', '\n', ""To understand how a single feature effects the output of the model we can plot the SHAP value of that feature vs. the value of the feature for all the examples in a dataset. Since SHAP values represent a feature's responsibility for a change in the model output, the plot below represents the change in predicted house price as RM (the average number of rooms per house in an area) changes. Vertical dispersion at a single value of RM represents interaction effects with other features. To help reveal these interactions we can color by another feature. If we pass the whole explanation tensor to the `color` argument the scatter plot will pick the best feature to color by. In this case it picks RAD (index of accessibility to radial highways) since that highlights that the average number of rooms per house has less impact on home price for areas with a high RAD value.\n"", '\n', '```python\n', '# create a dependence scatter plot to show the effect of a single feature across the whole dataset\n', 'shap.plots.scatter(shap_values[:,""RM""], color=shap_values)\n', '```\n', '\n', '<p align=""center"">\n', '  <img width=""544"" src=""https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_scatter.png"" />\n', '</p>\n', '\n', '\n', 'To get an overview of which features are most important for a model we can plot the SHAP values of every feature for every sample. The plot below sorts features by the sum of SHAP value magnitudes over all samples, and uses SHAP values to show the distribution of the impacts each feature has on the model output. The color represents the feature value (red high, blue low). This reveals for example that a high LSTAT (% lower status of the population) lowers the predicted home price.\n', '\n', '```python\n', '# summarize the effects of all the features\n', 'shap.plots.beeswarm(shap_values)\n', '```\n', '\n', '<p align=""center"">\n', '  <img width=""583"" src=""https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_beeswarm.png"" />\n', '</p>\n', '\n', 'We can also just take the mean absolute value of the SHAP values for each feature to get a standard bar plot (produces stacked bars for multi-class outputs):\n', '\n', '```python\n', 'shap.plots.bar(shap_values)\n', '```\n', '\n', '<p align=""center"">\n', '  <img width=""570"" src=""https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_global_bar.png"" />\n', '</p>\n', '\n', '## Natural language example (transformers)\n', '\n', 'SHAP has specific support for natural language models like those in the Hugging Face transformers library. By adding coalitional rules to traditional Shapley values we can form games that explain large modern NLP model using very few function evaluations. Using this functionality is as simple as passing a supported transformers pipeline to SHAP:\n', '\n', '```python\n', 'import transformers\n', 'import shap\n', '\n', '# load a transformers pipeline model\n', ""model = transformers.pipeline('sentiment-analysis', return_all_scores=True)\n"", '\n', '# explain the model on two sample inputs\n', 'explainer = shap.Explainer(model) \n', 'shap_values = explainer([""What a great movie! ...if you have no taste.""])\n', '\n', ""# visualize the first prediction's explanation for the POSITIVE output class\n"", 'shap.plots.text(shap_values[0, :, ""POSITIVE""])\n', '```\n', '\n', '<p align=""center"">\n', '  <img width=""811"" src=""https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/sentiment_analysis_plot.png"" />\n', '</p>\n', '\n', '## Deep learning example with DeepExplainer (TensorFlow/Keras models)\n', '\n', 'Deep SHAP is a high-speed approximation algorithm for SHAP values in deep learning models that builds on a connection with [DeepLIFT](https://arxiv.org/abs/1704.02685) described in the SHAP NIPS paper. The implementation here differs from the original DeepLIFT by using a distribution of background samples instead of a single reference value, and using Shapley equations to linearize components such as max, softmax, products, divisions, etc. Note that some of these enhancements have also been since integrated into DeepLIFT. TensorFlow models and Keras models using the TensorFlow backend are supported (there is also preliminary support for PyTorch):\n', '\n', '```python\n', '# ...include code from https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n', '\n', 'import shap\n', 'import numpy as np\n', '\n', '# select a set of background examples to take an expectation over\n', 'background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]\n', '\n', '# explain predictions of the model on four images\n', 'e = shap.DeepExplainer(model, background)\n', '# ...or pass tensors directly\n', '# e = shap.DeepExplainer((model.layers[0].input, model.layers[-1].output), background)\n', 'shap_values = e.shap_values(x_test[1:5])\n', '\n', '# plot the feature attributions\n', 'shap.image_plot(shap_values, -x_test[1:5])\n', '```\n', '\n', '<p align=""center"">\n', '  <img width=""820"" src=""https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/mnist_image_plot.png"" />\n', '</p>\n', '\n', ""The plot above explains ten outputs (digits 0-9) for four different images. Red pixels increase the model's output while blue pixels decrease the output. The input images are shown on the left, and as nearly transparent grayscale backings behind each of the explanations. The sum of the SHAP values equals the difference between the expected model output (averaged over the background dataset) and the current model output. Note that for the 'zero' image the blank middle is important, while for the 'four' image the lack of a connection on top makes it a four instead of a nine.\n"", '\n', '\n', '## Deep learning example with GradientExplainer (TensorFlow/Keras/PyTorch models)\n', '\n', 'Expected gradients combines ideas from [Integrated Gradients](https://arxiv.org/abs/1703.01365), SHAP, and [SmoothGrad](https://arxiv.org/abs/1706.03825) into a single expected value equation. This allows an entire dataset to be used as the background distribution (as opposed to a single reference value) and allows local smoothing. If we approximate the model with a linear function between each background data sample and the current input to be explained, and we assume the input features are independent then expected gradients will compute approximate SHAP values. In the example below we have explained how the 7th intermediate layer of the VGG16 ImageNet model impacts the output probabilities.\n', '\n', '```python\n', 'from keras.applications.vgg16 import VGG16\n', 'from keras.applications.vgg16 import preprocess_input\n', 'import keras.backend as K\n', 'import numpy as np\n', 'import json\n', 'import shap\n', '\n', '# load pre-trained model and choose two images to explain\n', ""model = VGG16(weights='imagenet', include_top=True)\n"", 'X,y = shap.datasets.imagenet50()\n', 'to_explain = X[[39,41]]\n', '\n', '# load the ImageNet class names\n', 'url = ""https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json""\n', 'fname = shap.datasets.cache(url)\n', 'with open(fname) as f:\n', '    class_names = json.load(f)\n', '\n', '# explain how the input to the 7th layer of the model explains the top two classes\n', 'def map2layer(x, layer):\n', '    feed_dict = dict(zip([model.layers[0].input], [preprocess_input(x.copy())]))\n', '    return K.get_session().run(model.layers[layer].input, feed_dict)\n', 'e = shap.GradientExplainer(\n', '    (model.layers[7].input, model.layers[-1].output),\n', '    map2layer(X, 7),\n', '    local_smoothing=0 # std dev of smoothing noise\n', ')\n', 'shap_values,indexes = e.shap_values(map2layer(to_explain, 7), ranked_outputs=2)\n', '\n', '# get the names for the classes\n', 'index_names = np.vectorize(lambda x: class_names[str(x)][1])(indexes)\n', '\n', '# plot the explanations\n', 'shap.image_plot(shap_values, to_explain, index_names)\n', '```\n', '\n', '<p align=""center"">\n', '  <img width=""500"" src=""https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/gradient_imagenet_plot.png"" />\n', '</p>\n', '\n', 'Predictions for two input images are explained in the plot above. Red pixels represent positive SHAP values that increase the probability of the class, while blue pixels represent negative SHAP values the reduce the probability of the class. By using `ranked_outputs=2` we explain only the two most likely classes for each input (this spares us from explaining all 1,000 classes).\n', '\n', '## Model agnostic example with KernelExplainer (explains any function)\n', '\n', 'Kernel SHAP uses a specially-weighted local linear regression to estimate SHAP values for any model. Below is a simple example for explaining a multi-class SVM on the classic iris dataset.\n', '\n', '```python\n', 'import sklearn\n', 'import shap\n', 'from sklearn.model_selection import train_test_split\n', '\n', '# print the JS visualization code to the notebook\n', 'shap.initjs()\n', '\n', '# train a SVM classifier\n', 'X_train,X_test,Y_train,Y_test = train_test_split(*shap.datasets.iris(), test_size=0.2, random_state=0)\n', ""svm = sklearn.svm.SVC(kernel='rbf', probability=True)\n"", 'svm.fit(X_train, Y_train)\n', '\n', '# use Kernel SHAP to explain test set predictions\n', 'explainer = shap.KernelExplainer(svm.predict_proba, X_train, link=""logit"")\n', 'shap_values = explainer.shap_values(X_test, nsamples=100)\n', '\n', '# plot the SHAP values for the Setosa output of the first instance\n', 'shap.force_plot(explainer.expected_value[0], shap_values[0][0,:], X_test.iloc[0,:], link=""logit"")\n', '```\n', '<p align=""center"">\n', '  <img width=""810"" src=""https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/iris_instance.png"" />\n', '</p>\n', '\n', 'The above explanation shows four features each contributing to push the model output from the base value (the average model output over the training dataset we passed) towards zero. If there were any features pushing the class label higher they would be shown in red.\n', '\n', 'If we take many explanations such as the one shown above, rotate them 90 degrees, and then stack them horizontally, we can see explanations for an entire dataset. This is exactly what we do below for all the examples in the iris test set:\n', '\n', '```python\n', '# plot the SHAP values for the Setosa output of all instances\n', 'shap.force_plot(explainer.expected_value[0], shap_values[0], X_test, link=""logit"")\n', '```\n', '<p align=""center"">\n', '  <img width=""813"" src=""https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/iris_dataset.png"" />\n', '</p>\n', '\n', '## SHAP Interaction Values\n', '\n', 'SHAP interaction values are a generalization of SHAP values to higher order interactions. Fast exact computation of pairwise interactions are implemented for tree models with `shap.TreeExplainer(model).shap_interaction_values(X)`. This returns a matrix for every prediction, where the main effects are on the diagonal and the interaction effects are off-diagonal. These values often reveal interesting hidden relationships, such as how the increased risk of death peaks for men at age 60 (see the NHANES notebook for details):\n', '\n', '<p align=""center"">\n', '  <img width=""483"" src=""https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/nhanes_age_sex_interaction.png"" />\n', '</p>\n', '\n', '## Sample notebooks\n', '\n', 'The notebooks below demonstrate different use cases for SHAP. Look inside the notebooks directory of the repository if you want to try playing with the original notebooks yourself.\n', '\n', '### TreeExplainer\n', '\n', 'An implementation of Tree SHAP, a fast and exact algorithm to compute SHAP values for trees and ensembles of trees.\n', '\n', '- [**NHANES survival model with XGBoost and SHAP interaction values**](https://slundberg.github.io/shap/notebooks/NHANES%20I%20Survival%20Model.html) - Using mortality data from 20 years of followup this notebook demonstrates how to use XGBoost and `shap` to uncover complex risk factor relationships.\n', '\n', '- [**Census income classification with LightGBM**](https://slundberg.github.io/shap/notebooks/tree_explainer/Census%20income%20classification%20with%20LightGBM.html) - Using the standard adult census income dataset, this notebook trains a gradient boosting tree model with LightGBM and then explains predictions using `shap`.\n', '\n', '- [**League of Legends Win Prediction with XGBoost**](https://slundberg.github.io/shap/notebooks/League%20of%20Legends%20Win%20Prediction%20with%20XGBoost.html) - Using a Kaggle dataset of 180,000 ranked matches from League of Legends we train and explain a gradient boosting tree model with XGBoost to predict if a player will win their match.\n', '\n', '### DeepExplainer\n', '\n', 'An implementation of Deep SHAP, a faster (but only approximate) algorithm to compute SHAP values for deep learning models that is based on connections between SHAP and the DeepLIFT algorithm.\n', '\n', '- [**MNIST Digit classification with Keras**](https://slundberg.github.io/shap/notebooks/deep_explainer/Front%20Page%20DeepExplainer%20MNIST%20Example.html) - Using the MNIST handwriting recognition dataset, this notebook trains a neural network with Keras and then explains predictions using `shap`.\n', '\n', '- [**Keras LSTM for IMDB Sentiment Classification**](https://slundberg.github.io/shap/notebooks/deep_explainer/Keras%20LSTM%20for%20IMDB%20Sentiment%20Classification.html) - This notebook trains an LSTM with Keras on the IMDB text sentiment analysis dataset and then explains predictions using `shap`. \n', '\n', '### GradientExplainer\n', '\n', 'An implementation of expected gradients to approximate SHAP values for deep learning models. It is based on connections between SHAP and the Integrated Gradients algorithm. GradientExplainer is slower than DeepExplainer and makes different approximation assumptions.\n', '\n', '- [**Explain an Intermediate Layer of VGG16 on ImageNet**](https://slundberg.github.io/shap/notebooks/gradient_explainer/Explain%20an%20Intermediate%20Layer%20of%20VGG16%20on%20ImageNet.html) - This notebook demonstrates how to explain the output of a pre-trained VGG16 ImageNet model using an internal convolutional layer.\n', '\n', '### LinearExplainer\n', '\n', 'For a linear model with independent features we can analytically compute the exact SHAP values. We can also account for feature correlation if we are willing to estimate the feature covariance matrix. LinearExplainer supports both of these options.\n', '\n', '- [**Sentiment Analysis with Logistic Regression**](https://slundberg.github.io/shap/notebooks/linear_explainer/Sentiment%20Analysis%20with%20Logistic%20Regression.html) - This notebook demonstrates how to explain a linear logistic regression sentiment analysis model.\n', '\n', '### KernelExplainer\n', '\n', 'An implementation of Kernel SHAP, a model agnostic method to estimate SHAP values for any model. Because it makes no assumptions about the model type, KernelExplainer is slower than the other model type specific algorithms.\n', '\n', '- [**Census income classification with scikit-learn**](https://slundberg.github.io/shap/notebooks/Census%20income%20classification%20with%20scikit-learn.html) - Using the standard adult census income dataset, this notebook trains a k-nearest neighbors classifier using scikit-learn and then explains predictions using `shap`.\n', '\n', ""- [**ImageNet VGG16 Model with Keras**](https://slundberg.github.io/shap/notebooks/ImageNet%20VGG16%20Model%20with%20Keras.html) - Explain the classic VGG16 convolutional nerual network's predictions for an image. This works by applying the model agnostic Kernel SHAP method to a super-pixel segmented image.\n"", '\n', '- [**Iris classification**](https://slundberg.github.io/shap/notebooks/Iris%20classification%20with%20scikit-learn.html) - A basic demonstration using the popular iris species dataset. It explains predictions from six different models in scikit-learn using `shap`.\n', '\n', '## Documentation notebooks\n', '\n', 'These notebooks comprehensively demonstrate how to use specific functions and objects. \n', '\n', '- [`shap.decision_plot` and `shap.multioutput_decision_plot`](https://slundberg.github.io/shap/notebooks/plots/decision_plot.html)\n', '\n', '- [`shap.dependence_plot`](https://slundberg.github.io/shap/notebooks/plots/dependence_plot.html)\n', '\n', '## Methods Unified by SHAP\n', '\n', '1. *LIME:* Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. ""Why should i trust you?: Explaining the predictions of any classifier."" Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2016.\n', '\n', '2. *Shapley sampling values:* Strumbelj, Erik, and Igor Kononenko. ""Explaining prediction models and individual predictions with feature contributions."" Knowledge and information systems 41.3 (2014): 647-665.\n', '\n', '3. *DeepLIFT:* Shrikumar, Avanti, Peyton Greenside, and Anshul Kundaje. ""Learning important features through propagating activation differences."" arXiv preprint arXiv:1704.02685 (2017).\n', '\n', '4. *QII:* Datta, Anupam, Shayak Sen, and Yair Zick. ""Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems."" Security and Privacy (SP), 2016 IEEE Symposium on. IEEE, 2016.\n', '\n', '5. *Layer-wise relevance propagation:* Bach, Sebastian, et al. ""On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation."" PloS one 10.7 (2015): e0130140.\n', '\n', '6. *Shapley regression values:* Lipovetsky, Stan, and Michael Conklin. ""Analysis of regression in game theory approach."" Applied Stochastic Models in Business and Industry 17.4 (2001): 319-330.\n', '\n', '7. *Tree interpreter:* Saabas, Ando. Interpreting random forests. http://blog.datadive.net/interpreting-random-forests/\n', '\n', '## Citations\n', '\n', ""The algorithms and visualizations used in this package came primarily out of research in [Su-In Lee's lab](https://suinlee.cs.washington.edu) at the University of Washington, and Microsoft Research. If you use SHAP in your research we would appreciate a citation to the appropriate paper(s):\n"", '\n', '- For general use of SHAP you can read/cite our [NeurIPS paper](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions) ([bibtex](https://raw.githubusercontent.com/slundberg/shap/master/docs/references/shap_nips.bib)). \n', '- For TreeExplainer you can read/cite our [Nature Machine Intelligence paper](https://www.nature.com/articles/s42256-019-0138-9) ([bibtex](https://raw.githubusercontent.com/slundberg/shap/master/docs/references/tree_explainer.bib); [free access](https://rdcu.be/b0z70)).\n', '- For GPUTreeExplainer you can read/cite [this article](https://arxiv.org/abs/2010.13972).\n', '- For `force_plot` visualizations and medical applications you can read/cite our [Nature Biomedical Engineering paper](https://www.nature.com/articles/s41551-018-0304-0) ([bibtex](https://raw.githubusercontent.com/slundberg/shap/master/docs/references/nature_bme.bib); [free access](https://rdcu.be/baVbR)).\n', '\n', '<img height=""1"" width=""1"" style=""display:none"" src=""https://www.facebook.com/tr?id=189147091855991&ev=PageView&noscript=1"" />\n']"
Model Explainability,SeldonIO/alibi,SeldonIO,https://api.github.com/repos/SeldonIO/alibi,2010,222,18,"['https://api.github.com/users/jklaise', 'https://api.github.com/users/RobertSamoilescu', 'https://api.github.com/users/ascillitoe', 'https://api.github.com/users/dependabot%5Bbot%5D', 'https://api.github.com/users/mauicv', 'https://api.github.com/users/alexcoca', 'https://api.github.com/users/arnaudvl', 'https://api.github.com/users/gipster', 'https://api.github.com/users/MarcoGorelli', 'https://api.github.com/users/adriangonz', 'https://api.github.com/users/ahousley', 'https://api.github.com/users/ChristopherGS', 'https://api.github.com/users/daavoo', 'https://api.github.com/users/jimbudarz', 'https://api.github.com/users/sanjass', 'https://api.github.com/users/VinceShieh', 'https://api.github.com/users/abs428', 'https://api.github.com/users/oscarfco']",Python,2023-04-26T13:11:54Z,https://raw.githubusercontent.com/SeldonIO/alibi/master/README.md,"['<p align=""center"">\n', '  <img src=""https://raw.githubusercontent.com/SeldonIO/alibi/master/doc/source/_static/Alibi_Explain_Logo_rgb.png"" alt=""Alibi Logo"" width=""50%"">\n', '</p>\n', '\n', '<!--- BADGES: START --->\n', '\n', '[![Build Status](https://github.com/SeldonIO/alibi-detect/workflows/CI/badge.svg?branch=master)][#build-status]\n', '[![Documentation Status](https://readthedocs.org/projects/alibi/badge/?version=latest)][#docs-package]\n', '[![codecov](https://codecov.io/gh/SeldonIO/alibi/branch/master/graph/badge.svg)](https://codecov.io/gh/SeldonIO/alibi)\n', '[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/alibi?logo=pypi&style=flat&color=blue)][#pypi-package]\n', '[![PyPI - Package Version](https://img.shields.io/pypi/v/alibi?logo=pypi&style=flat&color=orange)][#pypi-package]\n', '[![Conda (channel only)](https://img.shields.io/conda/vn/conda-forge/alibi?logo=anaconda&style=flat&color=orange)][#conda-forge-package]\n', '[![GitHub - License](https://img.shields.io/github/license/SeldonIO/alibi?logo=github&style=flat&color=green)][#github-license]\n', '[![Slack channel](https://img.shields.io/badge/chat-on%20slack-e51670.svg)][#slack-channel]\n', '\n', '<!--- Hide platform for now as platform agnostic --->\n', '<!--- [![Conda - Platform](https://img.shields.io/conda/pn/conda-forge/alibi?logo=anaconda&style=flat)][#conda-forge-package]--->\n', '\n', '[#github-license]: https://github.com/SeldonIO/alibi/blob/master/LICENSE\n', '[#pypi-package]: https://pypi.org/project/alibi/\n', '[#conda-forge-package]: https://anaconda.org/conda-forge/alibi\n', '[#docs-package]: https://docs.seldon.io/projects/alibi/en/stable/\n', '[#build-status]: https://github.com/SeldonIO/alibi/actions?query=workflow%3A%22CI%22\n', '[#slack-channel]: https://join.slack.com/t/seldondev/shared_invite/zt-vejg6ttd-ksZiQs3O_HOtPQsen_labg\n', '<!--- BADGES: END --->\n', '---\n', '\n', '[Alibi](https://docs.seldon.io/projects/alibi) is an open source Python library aimed at machine learning model inspection and interpretation.\n', 'The focus of the library is to provide high-quality implementations of black-box, white-box, local and global\n', 'explanation methods for classification and regression models.\n', '*  [Documentation](https://docs.seldon.io/projects/alibi/en/stable/)\n', '\n', ""If you're interested in outlier detection, concept drift or adversarial instance detection, check out our sister project [alibi-detect](https://github.com/SeldonIO/alibi-detect).\n"", '\n', '<table>\n', '  <tr valign=""top"">\n', '    <td width=""50%"" >\n', '        <a href=""https://docs.seldon.io/projects/alibi/en/stable/examples/anchor_image_imagenet.html"">\n', '            <br>\n', '            <b>Anchor explanations for images</b>\n', '            <br>\n', '            <br>\n', '            <img src=""https://github.com/SeldonIO/alibi/raw/master/doc/source/_static/anchor_image.png"">\n', '        </a>\n', '    </td>\n', '    <td width=""50%"">\n', '        <a href=""https://docs.seldon.io/projects/alibi/en/stable/examples/integrated_gradients_imdb.html"">\n', '            <br>\n', '            <b>Integrated Gradients for text</b>\n', '            <br>\n', '            <br>\n', '            <img src=""https://github.com/SeldonIO/alibi/raw/master/doc/source/_static/ig_text.png"">\n', '        </a>\n', '    </td>\n', '  </tr>\n', '  <tr valign=""top"">\n', '    <td width=""50%"">\n', '        <a href=""https://docs.seldon.io/projects/alibi/en/stable/methods/CFProto.html"">\n', '            <br>\n', '            <b>Counterfactual examples</b>\n', '            <br>\n', '            <br>\n', '            <img src=""https://github.com/SeldonIO/alibi/raw/master/doc/source/_static/cf.png"">\n', '        </a>\n', '    </td>\n', '    <td width=""50%"">\n', '        <a href=""https://docs.seldon.io/projects/alibi/en/stable/methods/ALE.html"">\n', '            <br>\n', '            <b>Accumulated Local Effects</b>\n', '            <br>\n', '            <br>\n', '            <img src=""https://github.com/SeldonIO/alibi/raw/master/doc/source/_static/ale.png"">\n', '        </a>\n', '    </td>\n', '  </tr>\n', '</table>\n', '\n', '## Table of Contents\n', '\n', '* [Installation and Usage](#installation-and-usage)\n', '* [Supported Methods](#supported-methods)\n', '  * [Model Explanations](#model-explanations)\n', '  * [Model Confidence](#model-confidence)\n', '  * [Prototypes](#prototypes)\n', '  * [References and Examples](#references-and-examples)\n', '* [Citations](#citations)\n', '\n', '## Installation and Usage\n', 'Alibi can be installed from:\n', '\n', '- PyPI or GitHub source (with `pip`)\n', '- Anaconda (with `conda`/`mamba`)\n', '\n', '### With pip\n', '\n', '- Alibi can be installed from [PyPI](https://pypi.org/project/alibi):\n', '\n', '  ```bash\n', '  pip install alibi\n', '  ```\n', '  \n', '- Alternatively, the development version can be installed:\n', '  ```bash\n', '  pip install git+https://github.com/SeldonIO/alibi.git \n', '  ```\n', '\n', '- To take advantage of distributed computation of explanations, install `alibi` with `ray`:\n', '  ```bash\n', '  pip install alibi[ray]\n', '  ```\n', '\n', '- For SHAP support, install `alibi` as follows:\n', '  ```bash\n', '  pip install alibi[shap]\n', '  ```\n', '\n', '### With conda \n', '\n', 'To install from [conda-forge](https://conda-forge.org/) it is recommended to use [mamba](https://mamba.readthedocs.io/en/stable/), \n', 'which can be installed to the *base* conda enviroment with:\n', '\n', '```bash\n', 'conda install mamba -n base -c conda-forge\n', '```\n', '\n', '- For the standard Alibi install:\n', '  ```bash\n', '  mamba install -c conda-forge alibi\n', '  ```\n', '\n', '- For distributed computing support:\n', '  ```bash\n', '  mamba install -c conda-forge alibi ray\n', '  ```\n', '\n', '- For SHAP support:\n', '  ```bash\n', '  mamba install -c conda-forge alibi shap\n', '  ```\n', '\n', '### Usage\n', 'The alibi explanation API takes inspiration from `scikit-learn`, consisting of distinct initialize,\n', 'fit and explain steps. We will use the [AnchorTabular](https://docs.seldon.io/projects/alibi/en/stable/methods/Anchors.html)\n', 'explainer to illustrate the API:\n', '\n', '```python\n', 'from alibi.explainers import AnchorTabular\n', '\n', '# initialize and fit explainer by passing a prediction function and any other required arguments\n', 'explainer = AnchorTabular(predict_fn, feature_names=feature_names, category_map=category_map)\n', 'explainer.fit(X_train)\n', '\n', '# explain an instance\n', 'explanation = explainer.explain(x)\n', '```\n', '\n', 'The explanation returned is an `Explanation` object with attributes `meta` and `data`. `meta` is a dictionary\n', 'containing the explainer metadata and any hyperparameters and `data` is a dictionary containing everything\n', 'related to the computed explanation. For example, for the Anchor algorithm the explanation can be accessed\n', ""via `explanation.data['anchor']` (or `explanation.anchor`). The exact details of available fields varies\n"", 'from method to method so we encourage the reader to become familiar with the\n', '[types of methods supported](https://docs.seldon.io/projects/alibi/en/stable/overview/algorithms.html).\n', ' \n', '\n', '## Supported Methods\n', 'The following tables summarize the possible use cases for each method.\n', '\n', '### Model Explanations\n', '| Method                                                                                                       |    Models    |     Explanations      | Classification | Regression | Tabular | Text | Images | Categorical features | Train set required | Distributed |\n', '|:-------------------------------------------------------------------------------------------------------------|:------------:|:---------------------:|:--------------:|:----------:|:-------:|:----:|:------:|:--------------------:|:------------------:|:-----------:|\n', '| [ALE](https://docs.seldon.io/projects/alibi/en/stable/methods/ALE.html)                                      |      BB      |        global         |       ✔        |     ✔      |    ✔    |      |        |                      |                    |             |\n', '| [Partial Dependence](https://docs.seldon.io/projects/alibi/en/stable/methods/PartialDependence.html)         |    BB WB     |        global         |       ✔        |     ✔      |    ✔    |      |        |          ✔           |                    |             |\n', '| [PD Variance](https://docs.seldon.io/projects/alibi/en/stable/methods/PartialDependenceVariance.html)        |    BB WB     |        global         |       ✔        |     ✔      |    ✔    |      |        |          ✔           |                    |             |\n', '| [Permutation Importance](https://docs.seldon.io/projects/alibi/en/stable/methods/PermutationImportance.html) |      BB      |        global         |       ✔        |     ✔      |    ✔    |      |        |          ✔           |                    |             |\n', '| [Anchors](https://docs.seldon.io/projects/alibi/en/stable/methods/Anchors.html)                              |      BB      |         local         |       ✔        |            |    ✔    |  ✔   |   ✔    |          ✔           |    For Tabular     |             |\n', '| [CEM](https://docs.seldon.io/projects/alibi/en/stable/methods/CEM.html)                                      | BB* TF/Keras |         local         |       ✔        |            |    ✔    |      |   ✔    |                      |      Optional      |             |\n', '| [Counterfactuals](https://docs.seldon.io/projects/alibi/en/stable/methods/CF.html)                           | BB* TF/Keras |         local         |       ✔        |            |    ✔    |      |   ✔    |                      |         No         |             |\n', '| [Prototype Counterfactuals](https://docs.seldon.io/projects/alibi/en/stable/methods/CFProto.html)            | BB* TF/Keras |         local         |       ✔        |            |    ✔    |      |   ✔    |          ✔           |      Optional      |             |\n', '| [Counterfactuals with RL](https://docs.seldon.io/projects/alibi/en/stable/methods/CFRL.html)                 |      BB      |         local         |       ✔        |            |    ✔    |      |   ✔    |          ✔           |         ✔          |             |\n', '| [Integrated Gradients](https://docs.seldon.io/projects/alibi/en/stable/methods/IntegratedGradients.html)     |   TF/Keras   |         local         |       ✔        |     ✔      |    ✔    |  ✔   |   ✔    |          ✔           |      Optional      |             |\n', '| [Kernel SHAP](https://docs.seldon.io/projects/alibi/en/stable/methods/KernelSHAP.html)                       |      BB      | local <br></br>global |       ✔        |     ✔      |    ✔    |      |        |          ✔           |         ✔          |      ✔      |\n', '| [Tree SHAP](https://docs.seldon.io/projects/alibi/en/stable/methods/TreeSHAP.html)                           |      WB      | local <br></br>global |       ✔        |     ✔      |    ✔    |      |        |          ✔           |      Optional      |             |\n', '| [Similarity explanations](https://docs.seldon.io/projects/alibi/en/stable/methods/Similarity.html)           |      WB      |         local         |       ✔        |     ✔      |    ✔    |  ✔   |   ✔    |          ✔           |         ✔          |             |\n', '\n', '### Model Confidence\n', 'These algorithms provide **instance-specific** scores measuring the model confidence for making a\n', 'particular prediction.\n', '\n', '|Method|Models|Classification|Regression|Tabular|Text|Images|Categorical Features|Train set required|\n', '|:---|:---|:---:|:---:|:---:|:---:|:---:|:---:|:---|\n', '|[Trust Scores](https://docs.seldon.io/projects/alibi/en/stable/methods/TrustScores.html)|BB|✔| |✔|✔(1)|✔(2)| |Yes|\n', '|[Linearity Measure](https://docs.seldon.io/projects/alibi/en/stable/methods/LinearityMeasure.html)|BB|✔|✔|✔| |✔| |Optional|\n', '\n', 'Key:\n', ' - **BB** - black-box (only require a prediction function)\n', ' - **BB\\*** - black-box but assume model is differentiable\n', ' - **WB** - requires white-box model access. There may be limitations on models supported\n', ' - **TF/Keras** - TensorFlow models via the Keras API\n', ' - **Local** - instance specific explanation, why was this prediction made?\n', ' - **Global** - explains the model with respect to a set of instances\n', ' - **(1)** -  depending on model\n', ' - **(2)** -  may require dimensionality reduction\n', '\n', '### Prototypes\n', 'These algorithms provide a **distilled** view of the dataset and help construct a 1-KNN **interpretable** classifier.\n', '\n', '|Method|Classification|Regression|Tabular|Text|Images|Categorical Features|Train set labels|\n', '|:-----|:-------------|:---------|:------|:---|:-----|:-------------------|:---------------|\n', '|[ProtoSelect](https://docs.seldon.io/projects/alibi/en/latest/methods/ProtoSelect.html)|✔| |✔|✔|✔|✔| Optional       |\n', '\n', '\n', '## References and Examples\n', '- Accumulated Local Effects (ALE, [Apley and Zhu, 2016](https://arxiv.org/abs/1612.08468))\n', '  - [Documentation](https://docs.seldon.io/projects/alibi/en/stable/methods/ALE.html)\n', '  - Examples:\n', '    [California housing dataset](https://docs.seldon.io/projects/alibi/en/stable/examples/ale_regression_california.html),\n', '    [Iris dataset](https://docs.seldon.io/projects/alibi/en/stable/examples/ale_classification.html)\n', '\n', '- Partial Dependence ([J.H. Friedman, 2001](https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boostingmachine/10.1214/aos/1013203451.full))\n', '  - [Documentation](https://docs.seldon.io/projects/alibi/en/stable/methods/PartialDependence.html)\n', '  - Examples:\n', '    [Bike rental](https://docs.seldon.io/projects/alibi/en/stable/examples/pdp_regression_bike.html)\n', '\n', '- Partial Dependence Variance([Greenwell et al., 2018](https://arxiv.org/abs/1805.04755))\n', '  - [Documentation](https://docs.seldon.io/projects/alibi/en/stable/methods/PartialDependenceVariance.html)\n', '  - Examples:\n', '    [Friedman’s regression problem](https://docs.seldon.io/projects/alibi/en/stable/examples/pd_variance_regression_friedman.html)\n', '\n', '- Permutation Importance([Breiman, 2001](https://link.springer.com/article/10.1023/A:1010933404324); [Fisher et al., 2018](https://arxiv.org/abs/1801.01489))\n', '  - [Documentation](https://docs.seldon.io/projects/alibi/en/stable/methods/PermutationImportance.html)\n', '  - Examples:\n', ""    [Who's Going to Leave Next?](https://docs.seldon.io/projects/alibi/en/stable/examples/permutation_importance_classification_leave.html)\n"", '\n', '- Anchor explanations ([Ribeiro et al., 2018](https://homes.cs.washington.edu/~marcotcr/aaai18.pdf))\n', '  - [Documentation](https://docs.seldon.io/projects/alibi/en/stable/methods/Anchors.html)\n', '  - Examples:\n', '    [income prediction](https://docs.seldon.io/projects/alibi/en/stable/examples/anchor_tabular_adult.html),\n', '    [Iris dataset](https://docs.seldon.io/projects/alibi/en/stable/examples/anchor_tabular_iris.html),\n', '    [movie sentiment classification](https://docs.seldon.io/projects/alibi/en/stable/examples/anchor_text_movie.html),\n', '    [ImageNet](https://docs.seldon.io/projects/alibi/en/stable/examples/anchor_image_imagenet.html),\n', '    [fashion MNIST](https://docs.seldon.io/projects/alibi/en/stable/examples/anchor_image_fashion_mnist.html)\n', '\n', '- Contrastive Explanation Method (CEM, [Dhurandhar et al., 2018](https://papers.nips.cc/paper/7340-explanations-based-on-the-missing-towards-contrastive-explanations-with-pertinent-negatives))\n', '  - [Documentation](https://docs.seldon.io/projects/alibi/en/stable/methods/CEM.html)\n', '  - Examples: [MNIST](https://docs.seldon.io/projects/alibi/en/stable/examples/cem_mnist.html),\n', '    [Iris dataset](https://docs.seldon.io/projects/alibi/en/stable/examples/cem_iris.html)\n', '\n', '- Counterfactual Explanations (extension of\n', '  [Wachter et al., 2017](https://arxiv.org/abs/1711.00399))\n', '  - [Documentation](https://docs.seldon.io/projects/alibi/en/stable/methods/CF.html)\n', '  - Examples: \n', '    [MNIST](https://docs.seldon.io/projects/alibi/en/stable/examples/cf_mnist.html)\n', '\n', '- Counterfactual Explanations Guided by Prototypes ([Van Looveren and Klaise, 2019](https://arxiv.org/abs/1907.02584))\n', '  - [Documentation](https://docs.seldon.io/projects/alibi/en/stable/methods/CFProto.html)\n', '  - Examples:\n', '    [MNIST](https://docs.seldon.io/projects/alibi/en/stable/examples/cfproto_mnist.html),\n', '    [California housing dataset](https://docs.seldon.io/projects/alibi/en/stable/examples/cfproto_housing.html),\n', '    [Adult income (one-hot)](https://docs.seldon.io/projects/alibi/en/stable/examples/cfproto_cat_adult_ohe.html),\n', '    [Adult income (ordinal)](https://docs.seldon.io/projects/alibi/en/stable/examples/cfproto_cat_adult_ord.html)\n', '\n', '- Model-agnostic Counterfactual Explanations via RL([Samoilescu et al., 2021](https://arxiv.org/abs/2106.02597))\n', '  - [Documentation](https://docs.seldon.io/projects/alibi/en/stable/methods/CFRL.html)\n', '  - Examples:\n', '    [MNIST](https://docs.seldon.io/projects/alibi/en/stable/examples/cfrl_mnist.html),\n', '    [Adult income](https://docs.seldon.io/projects/alibi/en/stable/examples/cfrl_adult.html)\n', '\n', '- Integrated Gradients ([Sundararajan et al., 2017](https://arxiv.org/abs/1703.01365))\n', '  - [Documentation](https://docs.seldon.io/projects/alibi/en/stable/methods/IntegratedGradients.html),\n', '  - Examples:\n', '    [MNIST example](https://docs.seldon.io/projects/alibi/en/stable/examples/integrated_gradients_mnist.html),\n', '    [Imagenet example](https://docs.seldon.io/projects/alibi/en/stable/examples/integrated_gradients_imagenet.html),\n', '    [IMDB example](https://docs.seldon.io/projects/alibi/en/stable/examples/integrated_gradients_imdb.html).\n', '\n', '- Kernel Shapley Additive Explanations ([Lundberg et al., 2017](https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions))\n', '  - [Documentation](https://docs.seldon.io/projects/alibi/en/stable/methods/KernelSHAP.html)\n', '  - Examples:\n', '    [SVM with continuous data](https://docs.seldon.io/projects/alibi/en/stable/examples/kernel_shap_wine_intro.html),\n', '    [multinomial logistic regression with continous data](https://docs.seldon.io/projects/alibi/en/stable/examples/kernel_shap_wine_lr.html),\n', '    [handling categorical variables](https://docs.seldon.io/projects/alibi/en/stable/examples/kernel_shap_adult_lr.html)\n', '    \n', '- Tree Shapley Additive Explanations ([Lundberg et al., 2020](https://www.nature.com/articles/s42256-019-0138-9))\n', '  - [Documentation](https://docs.seldon.io/projects/alibi/en/stable/methods/TreeSHAP.html)\n', '  - Examples:\n', '    [Interventional (adult income, xgboost)](https://docs.seldon.io/projects/alibi/en/stable/examples/interventional_tree_shap_adult_xgb.html),\n', '    [Path-dependent (adult income, xgboost)](https://docs.seldon.io/projects/alibi/en/stable/examples/path_dependent_tree_shap_adult_xgb.html)\n', '    \n', '- Trust Scores ([Jiang et al., 2018](https://arxiv.org/abs/1805.11783))\n', '  - [Documentation](https://docs.seldon.io/projects/alibi/en/stable/methods/TrustScores.html)\n', '  - Examples:\n', '    [MNIST](https://docs.seldon.io/projects/alibi/en/stable/examples/trustscore_mnist.html),\n', '    [Iris dataset](https://docs.seldon.io/projects/alibi/en/stable/examples/trustscore_mnist.html)\n', '\n', '- Linearity Measure\n', '  - [Documentation](https://docs.seldon.io/projects/alibi/en/stable/methods/LinearityMeasure.html)\n', '  - Examples:\n', '    [Iris dataset](https://docs.seldon.io/projects/alibi/en/stable/examples/linearity_measure_iris.html),\n', '    [fashion MNIST](https://docs.seldon.io/projects/alibi/en/stable/examples/linearity_measure_fashion_mnist.html)\n', '\n', '- ProtoSelect\n', '  - [Documentation](https://docs.seldon.io/projects/alibi/en/latest/methods/ProtoSelect.html)\n', '  - Examples:\n', '    [Adult Census & CIFAR10](https://docs.seldon.io/projects/alibi/en/latest/examples/protoselect_adult_cifar10.html)\n', '\n', '- Similarity explanations\n', '  - [Documentation](https://docs.seldon.io/projects/alibi/en/stable/methods/Similarity.html)\n', '  - Examples:\n', '    [20 news groups dataset](https://docs.seldon.io/projects/alibi/en/stable/examples/similarity_explanations_20ng.html),\n', '    [ImageNet dataset](https://docs.seldon.io/projects/alibi/en/stable/examples/similarity_explanations_imagenet.html),\n', '    [MNIST dataset](https://docs.seldon.io/projects/alibi/en/stable/examples/similarity_explanations_mnist.html)\n', '\n', '## Citations\n', 'If you use alibi in your research, please consider citing it.\n', '\n', 'BibTeX entry:\n', '\n', '```\n', '@article{JMLR:v22:21-0017,\n', '  author  = {Janis Klaise and Arnaud Van Looveren and Giovanni Vacanti and Alexandru Coca},\n', '  title   = {Alibi Explain: Algorithms for Explaining Machine Learning Models},\n', '  journal = {Journal of Machine Learning Research},\n', '  year    = {2021},\n', '  volume  = {22},\n', '  number  = {181},\n', '  pages   = {1-7},\n', '  url     = {http://jmlr.org/papers/v22/21-0017.html}\n', '}\n', '```\n']"
Model Explainability,cdpierse/transformers-interpret,cdpierse,https://api.github.com/repos/cdpierse/transformers-interpret,1006,85,7,"['https://api.github.com/users/cdpierse', 'https://api.github.com/users/pabvald', 'https://api.github.com/users/cwenner', 'https://api.github.com/users/lalitpagaria', 'https://api.github.com/users/Voyz', 'https://api.github.com/users/rinapch', 'https://api.github.com/users/dependabot%5Bbot%5D']",Python,2023-04-26T03:54:18Z,https://raw.githubusercontent.com/cdpierse/transformers-interpret/master/README.md,"['<p align=""center"">\n', '    <a id=""transformers-intepret"" href=""#transformers-intepret"">\n', '        <img src=""https://github.com/cdpierse/transformers-interpret/blob/master/images/tight%401920x_transparent.png"" alt=""Transformers Intepret Title"" title=""Transformers Intepret Title"" width=""600""/>\n', '    </a>\n', '</p>\n', '\n', '<p align=""center""> Explainability for any 🤗 Transformers models in 2 lines.</p>\n', '\n', '<h1 align=""center""></h1>\n', '\n', '<p align=""center"">\n', '    <a href=""https://opensource.org/licenses/Apache-2.0"">\n', '        <img src=""https://img.shields.io/badge/License-Apache%202.0-blue.svg""/>\n', '    <a href=""https://github.com/cdpierse/transformers-interpret/actions/workflows/unit_tests.yml"">\n', '        <img src=""https://github.com/cdpierse/transformers-interpret/actions/workflows/unit_tests.yml/badge.svg"">\n', '    </a>\n', '            <a href=""https://github.com/cdpierse/transformers-interpret/releases"">\n', '        <img src=""https://img.shields.io/pypi/v/transformers_interpret?label=version""/>\n', '    </a>\n', '        <a href=""https://pepy.tech/project/transformers-interpret"">\n', '        <img src=""https://static.pepy.tech/personalized-badge/transformers-interpret?period=total&units=abbreviation&left_color=black&right_color=brightgreen&left_text=Downloads"">\n', '    </a>\n', '</p>\n', '\n', 'Transformers Interpret is a model explainability tool designed to work exclusively with the 🤗 [transformers][transformers] package.\n', '\n', 'In line with the philosophy of the Transformers package Transformers Interpret allows any transformers model to be explained in just two lines. Explainers are available for both text and computer vision models. Visualizations are also available in notebooks and as savable png and html files.\n', '\n', 'Check out the streamlit [demo app here](https://share.streamlit.io/cdpierse/transformers-interpret-streamlit/main/app.py)\n', '\n', '## Install\n', '\n', '```posh\n', 'pip install transformers-interpret\n', '```\n', '\n', '## Quick Start\n', '\n', '### Text Explainers\n', '\n', '<details><summary>Sequence Classification Explainer and Pairwise Sequence Classification</summary>\n', '\n', '<p>\n', ""Let's start by initializing a transformers' model and tokenizer, and running it through the `SequenceClassificationExplainer`.\n"", '\n', 'For this example we are using `distilbert-base-uncased-finetuned-sst-2-english`, a distilbert model finetuned on a sentiment analysis task.\n', '\n', '```python\n', 'from transformers import AutoModelForSequenceClassification, AutoTokenizer\n', 'model_name = ""distilbert-base-uncased-finetuned-sst-2-english""\n', 'model = AutoModelForSequenceClassification.from_pretrained(model_name)\n', 'tokenizer = AutoTokenizer.from_pretrained(model_name)\n', '\n', '# With both the model and tokenizer initialized we are now able to get explanations on an example text.\n', '\n', 'from transformers_interpret import SequenceClassificationExplainer\n', 'cls_explainer = SequenceClassificationExplainer(\n', '    model,\n', '    tokenizer)\n', 'word_attributions = cls_explainer(""I love you, I like you"")\n', '```\n', '\n', 'Which will return the following list of tuples:\n', '\n', '```python\n', '>>> word_attributions\n', ""[('[CLS]', 0.0),\n"", "" ('i', 0.2778544699186709),\n"", "" ('love', 0.7792370723380415),\n"", "" ('you', 0.38560088858031094),\n"", "" (',', -0.01769750505546915),\n"", "" ('i', 0.12071898121557832),\n"", "" ('like', 0.19091105304734457),\n"", "" ('you', 0.33994871536713467),\n"", "" ('[SEP]', 0.0)]\n"", '```\n', '\n', 'Positive attribution numbers indicate a word contributes positively towards the predicted class, while negative numbers indicate a word contributes negatively towards the predicted class. Here we can see that **I love you** gets the most attention.\n', '\n', ""You can use `predicted_class_index` in case you'd want to know what the predicted class actually is:\n"", '\n', '```python\n', '>>> cls_explainer.predicted_class_index\n', 'array(1)\n', '```\n', '\n', 'And if the model has label names for each class, we can see these too using `predicted_class_name`:\n', '\n', '```python\n', '>>> cls_explainer.predicted_class_name\n', ""'POSITIVE'\n"", '```\n', '\n', '#### Visualize Classification attributions\n', '\n', ""Sometimes the numeric attributions can be difficult to read particularly in instances where there is a lot of text. To help with that we also provide the `visualize()` method that utilizes Captum's in built viz library to create a HTML file highlighting the attributions.\n"", '\n', 'If you are in a notebook, calls to the `visualize()` method will display the visualization in-line. Alternatively you can pass a filepath in as an argument and an HTML file will be created, allowing you to view the explanation HTML in your browser.\n', '\n', '```python\n', 'cls_explainer.visualize(""distilbert_viz.html"")\n', '```\n', '\n', '<a href=""https://github.com/cdpierse/transformers-interpret/blob/master/images/distilbert_example.png"">\n', '<img src=""https://github.com/cdpierse/transformers-interpret/blob/master/images/distilbert_example.png"" width=""80%"" height=""80%"" align=""center""/>\n', '</a>\n', '\n', '#### Explaining Attributions for Non Predicted Class\n', '\n', ""Attribution explanations are not limited to the predicted class. Let's test a more complex sentence that contains mixed sentiments.\n"", '\n', 'In the example below we pass `class_name=""NEGATIVE""` as an argument indicating we would like the attributions to be explained for the **NEGATIVE** class regardless of what the actual prediction is. Effectively because this is a binary classifier we are getting the inverse attributions.\n', '\n', '```python\n', 'cls_explainer = SequenceClassificationExplainer(model, tokenizer)\n', 'attributions = cls_explainer(""I love you, I like you, I also kinda dislike you"", class_name=""NEGATIVE"")\n', '```\n', '\n', 'In this case, `predicted_class_name` still returns a prediction of the **POSITIVE** class, because the model has generated the same prediction but nonetheless we are interested in looking at the attributions for the negative class regardless of the predicted result.\n', '\n', '```python\n', '>>> cls_explainer.predicted_class_name\n', ""'POSITIVE'\n"", '```\n', '\n', 'But when we visualize the attributions we can see that the words ""**...kinda dislike**"" are contributing to a prediction of the ""NEGATIVE""\n', 'class.\n', '\n', '```python\n', 'cls_explainer.visualize(""distilbert_negative_attr.html"")\n', '```\n', '\n', '<a href=""https://github.com/cdpierse/transformers-interpret/blob/master/images/distilbert_example_negative.png"">\n', '<img src=""https://github.com/cdpierse/transformers-interpret/blob/master/images/distilbert_example_negative.png"" width=""80%"" height=""80%"" align=""center"" />\n', '</a>\n', '\n', 'Getting attributions for different classes is particularly insightful for multiclass problems as it allows you to inspect model predictions for a number of different classes and sanity-check that the model is ""looking"" at the right things.\n', '\n', 'For a detailed explanation of this example please checkout this [multiclass classification notebook.](notebooks/multiclass_classification_example.ipynb)\n', '\n', '### Pairwise Sequence Classification\n', '\n', ""The `PairwiseSequenceClassificationExplainer` is a variant of the the `SequenceClassificationExplainer` that is designed to work with classification models that expect the input sequence to be two inputs separated by a models' separator token. Common examples of this are [NLI models](https://arxiv.org/abs/1705.02364) and [Cross-Encoders ](https://www.sbert.net/docs/pretrained_cross-encoders.html) which are commonly used to score two inputs similarity to one another.\n"", '\n', 'This explainer calculates pairwise attributions for two passed inputs `text1` and `text2` using the model\n', 'and tokenizer given in the constructor.\n', '\n', 'Also, since a common use case for pairwise sequence classification is to compare two inputs similarity - models of this nature typically only have a single output node rather than multiple for each class. The pairwise sequence classification has some useful utility functions to make interpreting single node outputs clearer.\n', '\n', 'By default for models that output a single node the attributions are with respect to the inputs pushing the scores closer to 1.0, however if you want to see the\n', 'attributions with respect to scores closer to 0.0 you can pass `flip_sign=True`. For similarity\n', 'based models this is useful, as the model might predict a score closer to 0.0 for the two inputs\n', 'and in that case we would flip the attributions sign to explain why the two inputs are dissimilar.\n', '\n', ""Let's start by initializing a cross-encoder model and tokenizer from the suite of [pre-trained cross-encoders ](https://www.sbert.net/docs/pretrained_cross-encoders.html)provided by [sentence-transformers](https://github.com/UKPLab/sentence-transformers).\n"", '\n', 'For this example we are using `""cross-encoder/ms-marco-MiniLM-L-6-v2""`, a high quality cross-encoder trained on the [MSMarco dataset](https://github.com/microsoft/MSMARCO-Passage-Ranking) a passage ranking dataset for question answering and machine reading comprehension.\n', '\n', '```python\n', 'from transformers import AutoModelForSequenceClassification, AutoTokenizer\n', '\n', 'from transformers_interpret import PairwiseSequenceClassificationExplainer\n', '\n', 'model = AutoModelForSequenceClassification.from_pretrained(""cross-encoder/ms-marco-MiniLM-L-6-v2"")\n', 'tokenizer = AutoTokenizer.from_pretrained(""cross-encoder/ms-marco-MiniLM-L-6-v2"")\n', '\n', 'pairwise_explainer = PairwiseSequenceClassificationExplainer(model, tokenizer)\n', '\n', '# the pairwise explainer requires two string inputs to be passed, in this case given the nature of the model\n', '# we pass a query string and a context string. The question we are asking of our model is ""does this context contain a valid answer to our question""\n', '# the higher the score the better the fit.\n', '\n', 'query = ""How many people live in Berlin?""\n', 'context = ""Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.""\n', 'pairwise_attr = pairwise_explainer(query, context)\n', '```\n', '\n', 'Which returns the following attributions:\n', '\n', '```python\n', '>>> pairwise_attr\n', ""[('[CLS]', 0.0),\n"", "" ('how', -0.037558652124213034),\n"", "" ('many', -0.40348581975409786),\n"", "" ('people', -0.29756140282349425),\n"", "" ('live', -0.48979015417391764),\n"", "" ('in', -0.17844527885888117),\n"", "" ('berlin', 0.3737346097442739),\n"", "" ('?', -0.2281428913480142),\n"", "" ('[SEP]', 0.0),\n"", "" ('berlin', 0.18282430604641564),\n"", "" ('has', 0.039114659489254834),\n"", "" ('a', 0.0820056652212297),\n"", "" ('population', 0.35712150914643026),\n"", "" ('of', 0.09680870840224687),\n"", "" ('3', 0.04791760029513795),\n"", "" (',', 0.040330986539774266),\n"", "" ('520', 0.16307677913176166),\n"", "" (',', -0.005919693904602767),\n"", "" ('03', 0.019431649515841844),\n"", "" ('##1', -0.0243808667024702),\n"", "" ('registered', 0.07748341753369632),\n"", "" ('inhabitants', 0.23904087299731255),\n"", "" ('in', 0.07553221327346359),\n"", "" ('an', 0.033112821611999875),\n"", "" ('area', -0.025378852244447532),\n"", "" ('of', 0.026526373859562906),\n"", "" ('89', 0.0030700151809002147),\n"", "" ('##1', -0.000410387092186983),\n"", "" ('.', -0.0193147139126114),\n"", "" ('82', 0.0073800833347678774),\n"", "" ('square', 0.028988305990861576),\n"", "" ('kilometers', 0.02071182933829008),\n"", "" ('.', -0.025901070914318036),\n"", "" ('[SEP]', 0.0)]\n"", '```\n', '\n', '#### Visualize Pairwise Classification attributions\n', '\n', 'Visualizing the pairwise attributions is no different to the sequence classification explaine. We can see that in both the `query` and `context` there is a lot of positive attribution for the word `berlin` as well the words `population` and `inhabitants` in the `context`, good signs that our model understands the textual context of the question asked.\n', '\n', '```python\n', 'pairwise_explainer.visualize(""cross_encoder_attr.html"")\n', '```\n', '\n', '<a href=""https://github.com/cdpierse/transformers-interpret/blob/master/images/pairwise_cross_encoder_example.png"">\n', '<img src=""https://github.com/cdpierse/transformers-interpret/blob/master/images/pairwise_cross_encoder_example.png"" width=""100%"" height=""100%"" align=""center"" />\n', '</a>\n', '\n', 'If we were more interested in highlighting the input attributions that pushed the model away from the positive class of this single node output we could pass:\n', '\n', '```python\n', 'pairwise_attr = explainer(query, context, flip_sign=True)\n', '```\n', '\n', 'This simply inverts the sign of the attributions ensuring that they are with respect to the model outputting 0 rather than 1.\n', '\n', '</details>\n', '\n', '<details><summary>MultiLabel Classification Explainer</summary>\n', '<p>\n', '\n', ""This explainer is an extension of the `SequenceClassificationExplainer` and is thus compatible with all sequence classification models from the Transformers package. The key change in this explainer is that it caclulates attributions for each label in the model's config and returns a dictionary of word attributions w.r.t to each label. The `visualize()` method also displays a table of attributions with attributions calculated per label.\n"", '\n', '```python\n', 'from transformers import AutoModelForSequenceClassification, AutoTokenizer\n', 'from transformers_interpret import MultiLabelClassificationExplainer\n', '\n', 'model_name = ""j-hartmann/emotion-english-distilroberta-base""\n', 'model = AutoModelForSequenceClassification.from_pretrained(model_name)\n', 'tokenizer = AutoTokenizer.from_pretrained(model_name)\n', '\n', '\n', 'cls_explainer = MultiLabelClassificationExplainer(model, tokenizer)\n', '\n', '\n', 'word_attributions = cls_explainer(""There were many aspects of the film I liked, but it was frightening and gross in parts. My parents hated it."")\n', '```\n', '\n', ""This produces a dictionary of word attributions mapping labels to a list of tuples for each word and it's attribution score.\n"", '\n', '<details><summary>Click to see word attribution dictionary</summary>\n', '\n', '```python\n', '>>> word_attributions\n', ""{'anger': [('<s>', 0.0),\n"", ""           ('There', 0.09002208622000409),\n"", ""           ('were', -0.025129709879675187),\n"", ""           ('many', -0.028852677974079328),\n"", ""           ('aspects', -0.06341968013631565),\n"", ""           ('of', -0.03587626320752477),\n"", ""           ('the', -0.014813095892961287),\n"", ""           ('film', -0.14087587475098232),\n"", ""           ('I', 0.007367876912617766),\n"", ""           ('liked', -0.09816592066307557),\n"", ""           (',', -0.014259517291745674),\n"", ""           ('but', -0.08087144668471376),\n"", ""           ('it', -0.10185214349220136),\n"", ""           ('was', -0.07132244710777856),\n"", ""           ('frightening', -0.4125361737439814),\n"", ""           ('and', -0.021761663818889918),\n"", ""           ('gross', -0.10423745223600908),\n"", ""           ('in', -0.02383646952201854),\n"", ""           ('parts', -0.027137622525091033),\n"", ""           ('.', -0.02960415694062459),\n"", ""           ('My', 0.05642774605113695),\n"", ""           ('parents', 0.11146648216326158),\n"", ""           ('hated', 0.8497975489280364),\n"", ""           ('it', 0.05358116678115284),\n"", ""           ('.', -0.013566277162080632),\n"", ""           ('', 0.09293256725788422),\n"", ""           ('</s>', 0.0)],\n"", "" 'disgust': [('<s>', 0.0),\n"", ""             ('There', -0.035296263203072),\n"", ""             ('were', -0.010224922196739717),\n"", ""             ('many', -0.03747571761725605),\n"", ""             ('aspects', 0.007696321643436715),\n"", ""             ('of', 0.0026740873113235107),\n"", ""             ('the', 0.0025752851265661335),\n"", ""             ('film', -0.040890035285783645),\n"", ""             ('I', -0.014710007408208579),\n"", ""             ('liked', 0.025696806663391577),\n"", ""             (',', -0.00739107098314569),\n"", ""             ('but', 0.007353791868893654),\n"", ""             ('it', -0.00821368234753605),\n"", ""             ('was', 0.005439709067819798),\n"", ""             ('frightening', -0.8135974168445725),\n"", ""             ('and', -0.002334953123414774),\n"", ""             ('gross', 0.2366024374426269),\n"", ""             ('in', 0.04314772995234148),\n"", ""             ('parts', 0.05590472194035334),\n"", ""             ('.', -0.04362554293972562),\n"", ""             ('My', -0.04252694977895808),\n"", ""             ('parents', 0.051580790911406944),\n"", ""             ('hated', 0.5067406070057585),\n"", ""             ('it', 0.0527491071885104),\n"", ""             ('.', -0.008280280618652273),\n"", ""             ('', 0.07412384603053103),\n"", ""             ('</s>', 0.0)],\n"", "" 'fear': [('<s>', 0.0),\n"", ""          ('There', -0.019615758046045408),\n"", ""          ('were', 0.008033402634196246),\n"", ""          ('many', 0.027772367717635423),\n"", ""          ('aspects', 0.01334130725685673),\n"", ""          ('of', 0.009186049991879768),\n"", ""          ('the', 0.005828877177384549),\n"", ""          ('film', 0.09882910753644959),\n"", ""          ('I', 0.01753565003544039),\n"", ""          ('liked', 0.02062597344466885),\n"", ""          (',', -0.004469530636560965),\n"", ""          ('but', -0.019660439408176984),\n"", ""          ('it', 0.0488084071292538),\n"", ""          ('was', 0.03830859527501167),\n"", ""          ('frightening', 0.9526443954511705),\n"", ""          ('and', 0.02535156284103706),\n"", ""          ('gross', -0.10635301961551227),\n"", ""          ('in', -0.019190425328209065),\n"", ""          ('parts', -0.01713006453323631),\n"", ""          ('.', 0.015043169035757302),\n"", ""          ('My', 0.017068079071414916),\n"", ""          ('parents', -0.0630781275517486),\n"", ""          ('hated', -0.23630028921273583),\n"", ""          ('it', -0.056057044429020306),\n"", ""          ('.', 0.0015102052077844612),\n"", ""          ('', -0.010045048665404609),\n"", ""          ('</s>', 0.0)],\n"", "" 'joy': [('<s>', 0.0),\n"", ""         ('There', 0.04881772670614576),\n"", ""         ('were', -0.0379316152427468),\n"", ""         ('many', -0.007955371089444285),\n"", ""         ('aspects', 0.04437296429416574),\n"", ""         ('of', -0.06407011137335743),\n"", ""         ('the', -0.07331568926973099),\n"", ""         ('film', 0.21588462483311055),\n"", ""         ('I', 0.04885724513463952),\n"", ""         ('liked', 0.5309510543276107),\n"", ""         (',', 0.1339765195225006),\n"", ""         ('but', 0.09394079060730279),\n"", ""         ('it', -0.1462792330432028),\n"", ""         ('was', -0.1358591558323458),\n"", ""         ('frightening', -0.22184169339341142),\n"", ""         ('and', -0.07504142930419291),\n"", ""         ('gross', -0.005472075984252812),\n"", ""         ('in', -0.0942152657437379),\n"", ""         ('parts', -0.19345218754215965),\n"", ""         ('.', 0.11096247277185402),\n"", ""         ('My', 0.06604512262645984),\n"", ""         ('parents', 0.026376541098236207),\n"", ""         ('hated', -0.4988319510231699),\n"", ""         ('it', -0.17532499366236615),\n"", ""         ('.', -0.022609976138939034),\n"", ""         ('', -0.43417114685294833),\n"", ""         ('</s>', 0.0)],\n"", "" 'neutral': [('<s>', 0.0),\n"", ""             ('There', 0.045984598036642205),\n"", ""             ('were', 0.017142566357474697),\n"", ""             ('many', 0.011419348619472542),\n"", ""             ('aspects', 0.02558593440287365),\n"", ""             ('of', 0.0186162232003498),\n"", ""             ('the', 0.015616416841815963),\n"", ""             ('film', -0.021190511300570092),\n"", ""             ('I', -0.03572427925026324),\n"", ""             ('liked', 0.027062554960050455),\n"", ""             (',', 0.02089914209290366),\n"", ""             ('but', 0.025872618597570115),\n"", ""             ('it', -0.002980407262316265),\n"", ""             ('was', -0.022218157611174086),\n"", ""             ('frightening', -0.2982516449116045),\n"", ""             ('and', -0.01604643529040792),\n"", ""             ('gross', -0.04573829263548096),\n"", ""             ('in', -0.006511536166676108),\n"", ""             ('parts', -0.011744224307968652),\n"", ""             ('.', -0.01817041167875332),\n"", ""             ('My', -0.07362312722231429),\n"", ""             ('parents', -0.06910711601816408),\n"", ""             ('hated', -0.9418903509267312),\n"", ""             ('it', 0.022201795222373488),\n"", ""             ('.', 0.025694319747309045),\n"", ""             ('', 0.04276690822325994),\n"", ""             ('</s>', 0.0)],\n"", "" 'sadness': [('<s>', 0.0),\n"", ""             ('There', 0.028237893283377526),\n"", ""             ('were', -0.04489910545229568),\n"", ""             ('many', 0.004996044977269471),\n"", ""             ('aspects', -0.1231292680125582),\n"", ""             ('of', -0.04552690725956671),\n"", ""             ('the', -0.022077819961347042),\n"", ""             ('film', -0.14155752357877663),\n"", ""             ('I', 0.04135347872193571),\n"", ""             ('liked', -0.3097732540526099),\n"", ""             (',', 0.045114660009053134),\n"", ""             ('but', 0.0963352125332619),\n"", ""             ('it', -0.08120617610094617),\n"", ""             ('was', -0.08516150809170213),\n"", ""             ('frightening', -0.10386889639962761),\n"", ""             ('and', -0.03931986389970189),\n"", ""             ('gross', -0.2145059013625132),\n"", ""             ('in', -0.03465423285571697),\n"", ""             ('parts', -0.08676627134611635),\n"", ""             ('.', 0.19025217371906333),\n"", ""             ('My', 0.2582092561303794),\n"", ""             ('parents', 0.15432351476960307),\n"", ""             ('hated', 0.7262186310977987),\n"", ""             ('it', -0.029160655114499095),\n"", ""             ('.', -0.002758524253450406),\n"", ""             ('', -0.33846410359182094),\n"", ""             ('</s>', 0.0)],\n"", "" 'surprise': [('<s>', 0.0),\n"", ""              ('There', 0.07196110795254315),\n"", ""              ('were', 0.1434314520711312),\n"", ""              ('many', 0.08812238369489701),\n"", ""              ('aspects', 0.013432396769890982),\n"", ""              ('of', -0.07127508805657243),\n"", ""              ('the', -0.14079766624810955),\n"", ""              ('film', -0.16881201614906485),\n"", ""              ('I', 0.040595668935112135),\n"", ""              ('liked', 0.03239855530171577),\n"", ""              (',', -0.17676382558158257),\n"", ""              ('but', -0.03797939330341559),\n"", ""              ('it', -0.029191325089641736),\n"", ""              ('was', 0.01758013584108571),\n"", ""              ('frightening', -0.221738963726823),\n"", ""              ('and', -0.05126920277135527),\n"", ""              ('gross', -0.33986913466614044),\n"", ""              ('in', -0.018180366628697),\n"", ""              ('parts', 0.02939418603252064),\n"", ""              ('.', 0.018080129971003226),\n"", ""              ('My', -0.08060162218059498),\n"", ""              ('parents', 0.04351719139081836),\n"", ""              ('hated', -0.6919028585285265),\n"", ""              ('it', 0.0009574844165327357),\n"", ""              ('.', -0.059473118237873344),\n"", ""              ('', -0.465690452620123),\n"", ""              ('</s>', 0.0)]}\n"", '```\n', '\n', '</details>\n', '\n', '#### Visualize MultiLabel Classification attributions\n', '\n', ""Sometimes the numeric attributions can be difficult to read particularly in instances where there is a lot of text. To help with that we also provide the `visualize()` method that utilizes Captum's in built viz library to create a HTML file highlighting the attributions. For this explainer attributions will be show w.r.t to each label.\n"", '\n', 'If you are in a notebook, calls to the `visualize()` method will display the visualization in-line. Alternatively you can pass a filepath in as an argument and an HTML file will be created, allowing you to view the explanation HTML in your browser.\n', '\n', '```python\n', 'cls_explainer.visualize(""multilabel_viz.html"")\n', '```\n', '\n', '<a href=""https://github.com/cdpierse/transformers-interpret/blob/master/images/multilabel_example.png"">\n', '<img src=""https://github.com/cdpierse/transformers-interpret/blob/master/images/multilabel_example.png"" width=""80%"" height=""80%"" align=""center""/>\n', '</a>\n', '\n', '</details>\n', '\n', '<details><summary>Zero Shot Classification Explainer</summary>\n', '\n', '_Models using this explainer must be previously trained on NLI classification downstream tasks and have a label in the model\'s config called either ""entailment"" or ""ENTAILMENT""._\n', '\n', 'This explainer allows for attributions to be calculated for zero shot classification like models. In order to achieve this we use the same methodology employed by Hugging face. For those not familiar method employed by Hugging Face to achieve zero shot classification the way this works is by exploiting the ""entailment"" label of NLI models. Here is a [link](https://arxiv.org/abs/1909.00161) to a paper explaining more about it. A list of NLI models guaranteed to be compatible with this explainer can be found on the [model hub](https://huggingface.co/models?filter=pytorch&pipeline_tag=zero-shot-classification).\n', '\n', ""Let's start by initializing a transformers' sequence classification model and tokenizer trained specifically on a NLI task, and passing it to the ZeroShotClassificationExplainer.\n"", '\n', 'For this example we are using `facebook/bart-large-mnli` which is a checkpoint for a bart-large model trained on the\n', '[MNLI dataset](https://huggingface.co/datasets/multi_nli). This model typically predicts whether a sentence pair are an entailment, neutral, or a contradiction, however for zero-shot we only look the entailment label.\n', '\n', 'Notice that we pass our own custom labels `[""finance"", ""technology"", ""sports""]` to the class instance. Any number of labels can be passed including as little as one. Whichever label scores highest for entailment can be accessed via `predicted_label`, however the attributions themselves are calculated for every label. If you want to see the attributions for a particular label it is recommended just to pass in that one label and then the attributions will be guaranteed to be calculated w.r.t. that label.\n', '\n', '```python\n', 'from transformers import AutoModelForSequenceClassification, AutoTokenizer\n', 'from transformers_interpret import ZeroShotClassificationExplainer\n', '\n', 'tokenizer = AutoTokenizer.from_pretrained(""facebook/bart-large-mnli"")\n', '\n', 'model = AutoModelForSequenceClassification.from_pretrained(""facebook/bart-large-mnli"")\n', '\n', '\n', 'zero_shot_explainer = ZeroShotClassificationExplainer(model, tokenizer)\n', '\n', '\n', 'word_attributions = zero_shot_explainer(\n', '    ""Today apple released the new Macbook showing off a range of new features found in the proprietary silicon chip computer. "",\n', '    labels = [""finance"", ""technology"", ""sports""],\n', ')\n', '\n', '```\n', '\n', 'Which will return the following dict of attribution tuple lists for each label:\n', '\n', '```python\n', '>>> word_attributions\n', ""{'finance': [('<s>', 0.0),\n"", ""  ('Today', 0.0),\n"", ""  ('apple', -0.016100065046282107),\n"", ""  ('released', 0.3348383988281792),\n"", ""  ('the', -0.8932952916127369),\n"", ""  ('new', 0.14207183688642497),\n"", ""  ('Mac', 0.016309545780430777),\n"", ""  ('book', -0.06956802041125129),\n"", ""  ('showing', -0.12661404114316252),\n"", ""  ('off', -0.11470154900720078),\n"", ""  ('a', -0.03299250484912159),\n"", ""  ('range', -0.002532332125100561),\n"", ""  ('of', -0.022451943898971004),\n"", ""  ('new', -0.01859870581213379),\n"", ""  ('features', -0.020774327263810944),\n"", ""  ('found', -0.007734346326330102),\n"", ""  ('in', 0.005100588658589585),\n"", ""  ('the', 0.04711084622588314),\n"", ""  ('proprietary', 0.046352064964644286),\n"", ""  ('silicon', -0.0033502000158946127),\n"", ""  ('chip', -0.010419324929115785),\n"", ""  ('computer', -0.11507972995022273),\n"", ""  ('.', 0.12237840300907425)],\n"", "" 'technology': [('<s>', 0.0),\n"", ""  ('Today', 0.0),\n"", ""  ('apple', 0.22505152647747717),\n"", ""  ('released', -0.16164146624851905),\n"", ""  ('the', 0.5026975657258089),\n"", ""  ('new', 0.052589263167955536),\n"", ""  ('Mac', 0.2528325960993759),\n"", ""  ('book', -0.06445090203729663),\n"", ""  ('showing', -0.21204922293777534),\n"", ""  ('off', 0.06319714817612732),\n"", ""  ('a', 0.032048012090796815),\n"", ""  ('range', 0.08553079346908955),\n"", ""  ('of', 0.1409201107994034),\n"", ""  ('new', 0.0515261917112576),\n"", ""  ('features', -0.09656406466213506),\n"", ""  ('found', 0.02336613296843605),\n"", ""  ('in', -0.0011649894272190678),\n"", ""  ('the', 0.14229640664777807),\n"", ""  ('proprietary', -0.23169065661847646),\n"", ""  ('silicon', 0.5963924257008087),\n"", ""  ('chip', -0.19908474233975806),\n"", ""  ('computer', 0.030620295844734646),\n"", ""  ('.', 0.1995076958535378)],\n"", "" 'sports': [('<s>', 0.0),\n"", ""  ('Today', 0.0),\n"", ""  ('apple', 0.1776618164760026),\n"", ""  ('released', 0.10067773539491479),\n"", ""  ('the', 0.4813466937627506),\n"", ""  ('new', -0.018555244191949295),\n"", ""  ('Mac', 0.016338241133536224),\n"", ""  ('book', 0.39311969562943677),\n"", ""  ('showing', 0.03579210145504227),\n"", ""  ('off', 0.0016710813632476176),\n"", ""  ('a', 0.04367940034297261),\n"", ""  ('range', 0.06076859006993011),\n"", ""  ('of', 0.11039711284328052),\n"", ""  ('new', 0.003932416031994724),\n"", ""  ('features', -0.009660883377622588),\n"", ""  ('found', -0.06507586539836184),\n"", ""  ('in', 0.2957812911667922),\n"", ""  ('the', 0.1584106228974514),\n"", ""  ('proprietary', 0.0005789280604917397),\n"", ""  ('silicon', -0.04693795680472678),\n"", ""  ('chip', -0.1699508539245465),\n"", ""  ('computer', -0.4290823663975582),\n"", ""  ('.', 0.469314992542427)]}\n"", '```\n', '\n', 'We can find out which label was predicted with:\n', '\n', '```python\n', '>>> zero_shot_explainer.predicted_label\n', ""'technology'\n"", '```\n', '\n', '#### Visualize Zero Shot Classification attributions\n', '\n', 'For the `ZeroShotClassificationExplainer` the visualize() method returns a table similar to the `SequenceClassificationExplainer` but with attributions for every label.\n', '\n', '```python\n', 'zero_shot_explainer.visualize(""zero_shot.html"")\n', '```\n', '\n', '<a href=""https://github.com/cdpierse/transformers-interpret/blob/master/images/zero_shot_example.png"">\n', '<img src=""https://github.com/cdpierse/transformers-interpret/blob/master/images/zero_shot_example.png"" width=""100%"" height=""100%"" align=""center"" />\n', '</a>\n', '\n', '</details>\n', '\n', '<details><summary>Question Answering Explainer</summary>\n', '\n', ""Let's start by initializing a transformers' Question Answering model and tokenizer, and running it through the `QuestionAnsweringExplainer`.\n"", '\n', 'For this example we are using `bert-large-uncased-whole-word-masking-finetuned-squad`, a bert model finetuned on a SQuAD.\n', '\n', '```python\n', 'from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n', 'from transformers_interpret import QuestionAnsweringExplainer\n', '\n', 'tokenizer = AutoTokenizer.from_pretrained(""bert-large-uncased-whole-word-masking-finetuned-squad"")\n', 'model = AutoModelForQuestionAnswering.from_pretrained(""bert-large-uncased-whole-word-masking-finetuned-squad"")\n', '\n', 'qa_explainer = QuestionAnsweringExplainer(\n', '    model,\n', '    tokenizer,\n', ')\n', '\n', 'context = """"""\n', 'In Artificial Intelligence and machine learning, Natural Language Processing relates to the usage of machines to process and understand human language.\n', 'Many researchers currently work in this space.\n', '""""""\n', '\n', 'word_attributions = qa_explainer(\n', '    ""What is natural language processing ?"",\n', '    context,\n', ')\n', '```\n', '\n', 'Which will return the following dict containing word attributions for both the predicted start and end positions for the answer.\n', '\n', '```python\n', '>>> word_attributions\n', ""{'start': [('[CLS]', 0.0),\n"", ""  ('what', 0.9177170660377296),\n"", ""  ('is', 0.13382234898765258),\n"", ""  ('natural', 0.08061747350142005),\n"", ""  ('language', 0.013138062762511409),\n"", ""  ('processing', 0.11135923869816286),\n"", ""  ('?', 0.00858057388924361),\n"", ""  ('[SEP]', -0.09646373141894966),\n"", ""  ('in', 0.01545633993975799),\n"", ""  ('artificial', 0.0472082598707737),\n"", ""  ('intelligence', 0.026687249355110867),\n"", ""  ('and', 0.01675371260058537),\n"", ""  ('machine', -0.08429502436554961),\n"", ""  ('learning', 0.0044827685126163355),\n"", ""  (',', -0.02401013152520878),\n"", ""  ('natural', -0.0016756080249823537),\n"", ""  ('language', 0.0026815068421401885),\n"", ""  ('processing', 0.06773157580722854),\n"", ""  ('relates', 0.03884601576992908),\n"", ""  ('to', 0.009783797821526368),\n"", ""  ('the', -0.026650922910540952),\n"", ""  ('usage', -0.01067"
Model Explainability,jphall663/interpretable_machine_learning_with_python,jphall663,https://api.github.com/repos/jphall663/interpretable_machine_learning_with_python,633,200,5,"['https://api.github.com/users/jphall663', 'https://api.github.com/users/esztiorm', 'https://api.github.com/users/ntache', 'https://api.github.com/users/aosama', 'https://api.github.com/users/APorterOReilly']",Python,2023-03-10T14:09:23Z,https://raw.githubusercontent.com/jphall663/interpretable_machine_learning_with_python/master/README.md,"['# Responsible Machine Learning with Python\n', 'Examples of techniques for training interpretable machine learning (ML) models, explaining ML models, and debugging ML models for accuracy, discrimination, and security.\n', '\n', '\n', '### Overview\n', '\n', ""Usage of artificial intelligence (AI) and ML models is likely to become more commonplace as larger swaths of the economy embrace automation and data-driven decision-making. While these predictive systems can be quite accurate, they have often been inscrutable and unappealable black boxes that produce only numeric predictions with no accompanying explanations. Unfortunately, recent studies and recent events have drawn attention to mathematical and sociological flaws in prominent weak AI and ML systems, but practitioners don’t often have the right tools to pry open ML models and debug them. This series of notebooks introduces several approaches that increase transparency, accountability, and trustworthiness in ML models. If you are a data scientist or analyst and you want to train accurate, interpretable ML models, explain ML models to your customers or managers, test those models for security vulnerabilities or social discrimination, or if you have concerns about documentation, validation, or regulatory requirements, then this series of Jupyter notebooks is for you! (But *please* don't take these notebooks or associated materials as legal compliance advice.)\n"", '\n', 'The notebooks highlight techniques such as:\n', '* [Monotonic XGBoost models, partial dependence, individual conditional expectation plots, and Shapley explanations](https://github.com/jphall663/interpretable_machine_learning_with_python#enhancing-transparency-in-machine-learning-models-with-python-and-xgboost---notebook)\n', '* [Decision tree surrogates, reason codes, and ensembles of explanations](https://github.com/jphall663/interpretable_machine_learning_with_python#increase-transparency-and-accountability-in-your-machine-learning-project-with-python---notebook)\n', '* [Disparate impact analysis](https://github.com/jphall663/interpretable_machine_learning_with_python#increase-fairness-in-your-machine-learning-project-with-disparate-impact-analysis-using-python-and-h2o---notebook)\n', '* [LIME](https://github.com/jphall663/interpretable_machine_learning_with_python#explain-your-predictive-models-to-business-stakeholders-with-lime-using-python-and-h2o---notebook)\n', '* [Sensitivity and residual analysis](https://github.com/jphall663/interpretable_machine_learning_with_python#testing-machine-learning-models-for-accuracy-trustworthiness-and-stability-with-python-and-h2o---notebook)\n', '  * [Advanced sensitivity analysis for model debugging](https://github.com/jphall663/interpretable_machine_learning_with_python#part-1-sensitivity-analysis---notebook)\n', '  * [Advanced residual analysis for model debugging](https://github.com/jphall663/interpretable_machine_learning_with_python#part-2-residual-analysis---notebook)\n', '* [Detailed model comparison and model selection by cross-validated ranking](https://github.com/jphall663/interpretable_machine_learning_with_python#from-glm-to-gbm-building-the-case-for-complexity---notebook)\n', '\n', 'The notebooks can be accessed through:\n', '* [H2O Aquarium (Recommended)](https://github.com/jphall663/interpretable_machine_learning_with_python#h2o-aquarium-recommended)\n', '* [Virtualenv (Advanced)](https://github.com/jphall663/interpretable_machine_learning_with_python#virtualenv-installation)\n', '* [Docker container (Advanced)](https://github.com/jphall663/interpretable_machine_learning_with_python#docker-installation)\n', '* [Manual installation (Advanced)](https://github.com/jphall663/interpretable_machine_learning_with_python#manual-installation)\n', '\n', '#### Further reading:\n', '* [*Machine Learning: Considerations for fairly and transparently expanding access to credit*](http://info.h2o.ai/rs/644-PKX-778/images/Machine%20Learning%20-%20Considerations%20for%20Fairly%20and%20Transparently%20Expanding%20Access%20to%20Credit.pdf)\n', '* [*A Responsible Machine Learning Workflow with Focus on Interpretable Models, Post-hoc Explanation, and Discrimination Testing*](https://www.mdpi.com/2078-2489/11/3/137)\n', '* [*An Introduction to Machine Learning Interpretability, 2nd Edition*](https://www.h2o.ai/wp-content/uploads/2019/08/An-Introduction-to-Machine-Learning-Interpretability-Second-Edition.pdf)\n', '* [*On the Art and Science of Explainable Machine Learning*](https://arxiv.org/pdf/1810.02909.pdf)\n', '* [*Proposals for model vulnerability and security*](https://www.oreilly.com/ideas/proposals-for-model-vulnerability-and-security)\n', '* [*Proposed Guidelines for the Responsible Use of Explainable Machine Learning*](https://arxiv.org/pdf/1906.03533.pdf)\n', '* [*Real-World Strategies for Model Debugging*](https://medium.com/@jphall_22520/strategies-for-model-debugging-aa822f1097ce)\n', '* [*Warning Signs: Security and Privacy in an Age of Machine Learning*](https://fpf.org/wp-content/uploads/2019/09/FPF_WarningSigns_Report.pdf)\n', '* [*Why you should care about debugging machine learning models*](https://www.oreilly.com/radar/why-you-should-care-about-debugging-machine-learning-models/)\n', '\n', '***\n', '\n', '### Enhancing Transparency in Machine Learning Models with Python and XGBoost - [Notebook](https://nbviewer.jupyter.org/github/jphall663/interpretable_machine_learning_with_python/blob/master/xgboost_pdp_ice.ipynb)\n', '\n', '![](./readme_pics/pdp_ice.png)\n', '\n', 'Monotonicity constraints can turn opaque, complex models into transparent, and potentially regulator-approved models, by ensuring predictions only increase or only decrease for any change in a given input variable. In this notebook, I will demonstrate how to use monotonicity constraints in the popular open source gradient boosting package XGBoost to train an interpretable and accurate nonlinear classifier on the UCI credit card default data.\n', '\n', 'Once we have trained a monotonic XGBoost model, we will use partial dependence plots and individual conditional expectation (ICE) plots to investigate the internal mechanisms of the model and to verify its monotonic behavior. Partial dependence plots show us the way machine-learned response functions change based on the values of one or two input variables of interest while averaging out the effects of all other input variables. ICE plots can be used to create more localized descriptions of model predictions, and ICE plots pair nicely with partial dependence plots. An example of generating regulator mandated reason codes from high fidelity Shapley explanations for any model prediction is also presented. The combination of monotonic XGBoost, partial dependence, ICE, and Shapley explanations is likely one of the most direct ways to create an interpretable machine learning model today.\n', '\n', '\n', '### Increase Transparency and Accountability in Your Machine Learning Project with Python - [Notebook](https://nbviewer.jupyter.org/github/jphall663/interpretable_machine_learning_with_python/blob/master/dt_surrogate_loco.ipynb)\n', '\n', '![](./readme_pics/dt_surrogate.png)\n', '\n', 'Gradient boosting machines (GBMs) and other complex machine learning models are popular and accurate prediction tools, but they can be difficult to interpret. Surrogate models, feature importance, and reason codes can be used to explain and increase transparency in machine learning models. In this notebook, we will train a GBM on the UCI credit card default data. Then we’ll train a decision tree surrogate model on the original inputs and predictions of the complex GBM model and see how the variable importance and interactions displayed in the surrogate model yield an overall, approximate flowchart of the complex model’s predictions. We will also analyze the global variable importance of the GBM and compare this information to the surrogate model, our domain expertise, and our reasonable expectations.\n', '\n', 'To get a better picture of the complex model’s local behavior and to enhance the accountability of the model’s predictions, we will use a variant of the leave-one-covariate-out (LOCO) technique. LOCO enables us to calculate the local contribution each input variable makes toward each model prediction. We will then rank the local contributions to generate reason codes that describe, in plain English, the model’s decision process for every prediction.\n', '\n', '### Increase Fairness in Your Machine Learning Project with Disparate Impact Analysis using Python and H2O - [Notebook](https://nbviewer.jupyter.org/github/jphall663/interpretable_machine_learning_with_python/blob/master/dia.ipynb)\n', '\n', '<img src=""./readme_pics/dia.png"" height=""400"">\n', '\n', 'Fairness is an incredibly important, but highly complex entity. So much so that leading scholars have yet to agree on a strict definition. However, there is a practical way to discuss and handle observational fairness, or how your model predictions affect different groups of people. This procedure is often known as disparate impact analysis (DIA). DIA is far from perfect, as it relies heavily on user-defined thresholds and reference levels to measure disparity and does not attempt to remediate disparity or provide information on sources of disparity, but it is a fairly straightforward method to quantify your model’s behavior across sensitive demographic segments or other potentially interesting groups of observations. Some types of DIA are also an accepted, regulation-compliant tool for fair-lending purposes in the U.S. financial services industry. If it’s good enough for multibillion-dollar credit portfolios, it’s probably good enough for your project.\n', '\n', 'This example DIA notebook starts by training a monotonic gradient boosting machine (GBM) classifier on the UCI credit card default data using the popular open source library, h2o. A probability cutoff for making credit decisions is selected by maximizing the F1 statistic and confusion matrices are generated to summarize the GBM’s decisions across men and women. A basic DIA procedure is then conducted using the information stored in the confusion matrices and some traditional fair lending measures.\n', '\n', '### Explain Your Predictive Models to Business Stakeholders with LIME using Python and H2O - [Notebook](https://nbviewer.jupyter.org/github/jphall663/interpretable_machine_learning_with_python/blob/master/lime.ipynb)\n', '\n', '![](./readme_pics/lime.png)\n', '\n', 'Machine learning can create very accurate predictive models, but these models can be almost impossible to explain to your boss, your customers, or even your regulators. This notebook will use (Local Interpretable Model-agnostic Explanations) LIME to increase transparency and accountability in a complex GBM model trained on the UCI credit card default data. LIME is a method for building linear surrogate models for local regions in a data set, often single rows of data. LIME sheds light on how model predictions are made and describes local model mechanisms for specific rows of data. Because the LIME sampling process may feel abstract to some practitioners, this notebook will also introduce a more straightforward method of creating local samples for LIME.\n', '\n', 'Once local samples have been generated, we will fit LIME models to understand local trends in the complex model’s predictions. LIME can also tell us the local contribution of each input variable toward each model prediction, and these contributions can be sorted to create reason codes -- plain English explanations of every model prediction. We will also validate the fit of the LIME model to enhance trust in our explanations using the local model’s R2 statistic and a ranked prediction plot.\n', '\n', '### Testing Machine Learning Models for Accuracy, Trustworthiness, and Stability with Python and H2O - [Notebook](https://nbviewer.jupyter.org/github/jphall663/interpretable_machine_learning_with_python/blob/master/resid_sens_analysis.ipynb)\n', '\n', '![](./readme_pics/resid.png)\n', '\n', 'Because ML model predictions can vary drastically for small changes in input variable values, especially outside of training input domains, sensitivity analysis is perhaps the most important validation technique for increasing trust in ML model predictions. Sensitivity analysis investigates whether model behavior and outputs remain stable when input data is intentionally perturbed, or other changes are simulated in input data. In this notebook, we will enhance trust in a complex credit default model by testing and debugging its predictions with sensitivity analysis.\n', '\n', 'We’ll further enhance trust in our model using residual analysis. Residuals refer to the difference between the recorded value of a target variable and the predicted value of a target variable for each row in a data set. Generally, the residuals of a well-fit model should be randomly distributed, because good models will account for most phenomena in a data set, except for random error. In this notebook, we will create residual plots for a complex model to debug any accuracy problems arising from overfitting or outliers.\n', '\n', '### Machine Learning Model Debugging with Python: All Models are Wrong ... but Why is _My_ Model Wrong? (And Can I Fix It?)\n', '\n', '##### Part 1: Sensitivity Analysis - [Notebook](https://nbviewer.jupyter.org/github/jphall663/interpretable_machine_learning_with_python/blob/master/debugging_sens_analysis_redux.ipynb)\n', '\n', '![](/readme_pics/sa.png)\n', '\n', 'Sensitivity analysis is the perturbation of data under a trained model. It can take many forms and arguably Shapley feature importance, partial dependence, individual conditional expectation, and adversarial examples are all types of sensitivity analysis. This notebook focuses on using these different types of sensitivity analysis to discover error mechanisms and security vulnerabilities and to assess stability and fairness in a trained XGBoost model. It begins by loading the UCI credit card default data and then training an interpretable, monotonically constrained XGBoost model. After the model is trained, global and local Shapley feature importance is calculated. These Shapley values help inform the application of partial dependence and ICE, and together these results guide a search for adversarial examples. The notebook closes by exposing the trained model to a random attack and analyzing the attack results.\n', '\n', 'These model debugging exercises uncover accuracy, drift, and security problems such as over-emphasis of important features and impactful yet non-robust interactions. Several remediation mechanisms are proposed including editing of final model artifacts to remove or fix errors, missing value injection or regularization during training to lessen the impact of certain features or interactions, and assertion-based missing value injection during scoring to mitigate the effect of non-robust interactions.\n', '\n', '##### Part 2: Residual Analysis - [Notebook](https://nbviewer.jupyter.org/github/jphall663/interpretable_machine_learning_with_python/blob/master/debugging_resid_analysis_redux.ipynb)\n', '\n', '![](readme_pics/resid2.png)\n', '\n', 'In general, residual analysis can be characterized as a careful study of when and how models make mistakes. A better understanding of mistakes will hopefully lead to fewer of them. This notebook uses variants of residual analysis to find error mechanisms and security vulnerabilities and to assess stability and fairness in a trained XGBoost model. It begins by loading the UCI credit card default data and then training an interpretable, monotonically constrained XGBoost gradient boosting machine (GBM) model. (Pearson correlation with the prediction target is used to determine the direction of the monotonicity constraints for each input variable.) After the model is trained, its logloss residuals are analyzed and explained thoroughly and the constrained GBM is compared to a benchmark linear model. These model debugging exercises uncover accuracy, drift, and security problems such as over-emphasis of important variables and strong signal in model residuals. Several remediation mechanisms are proposed including missing value injection during training, additional data collection, and use of assertions to correct known problems during scoring.\n', '\n', '### From GLM to GBM: Building the Case For Complexity - [Notebook](https://nbviewer.jupyter.org/github/jphall663/interpretable_machine_learning_with_python/blob/master/glm_mgbm_gbm.ipynb)\n', '\n', '![](readme_pics/hist_pd_ice_lo.png)\n', '\n', 'This notebook uses the same credit card default scenario to show how monotonicity constraints, Shapley values and other post-hoc explanations, and discrimination testing can enable practitioners to create direct comparisons between GLM and GBM models. Several candidate probability of default models are selected for comparison using feature selection methods, like LASSO, and by cross-validated ranking. Comparisons then enable building from GLM to more complex GBM models in a step-by-step manner, while retaining model transparency and the ability to test for discrimination. This notebook shows that GBMs can yield better accuracy, more revenue, and that GBMs are also likely to fulfill many model documentation, adverse action notice, and discrimination testing requirements.\n', '\n', '## Using the Examples\n', '\n', '### H2O Aquarium (recommended)\n', '\n', 'H2O Aquarium is a free educational environment that hosts versions of these notebooks among many other H2o-related resources. To use these notebooks in Aquarium:\n', '\n', '1. Navigate to the Aquarium URL: https://aquarium.h2o.ai.\n', '\n', '2. Create a new Aquarium account.\n', '\n', '3. Check the registered email inbox and use the temporary password to login to Aquarium.\n', '\n', '4. Click `Browse Labs`.\n', '\n', '5. Click `View Detail` under *Open Source MLI Workshop*.\n', '\n', '6. Click `Start Lab` (this can take several minutes).\n', '\n', '7. Click on the *Jupyter URL* when it becomes available.\n', '\n', '8. Enter the token `h2o`.\n', '\n', '9. Click the `patrick_hall_mli` folder.\n', '\n', '10. Browse/run the Jupyter notebooks.\n', '\n', '11. Click `End Lab` when you are finished.\n', '\n', '### Virtualenv Installation\n', '\n', 'For avid Python users, creating a Python virtual environment is a convenient way to run these notebooks.\n', '\n', '1. Install [Git](https://git-scm.com/downloads).\n', '\n', '2. Clone this repository with the examples.</br>\n', '`$ git clone https://github.com/jphall663/interpretable_machine_learning_with_python.git`\n', '\n', '3. Install Anaconda Python 5.1.0 from the [Anaconda archives](https://repo.continuum.io/archive/) and add it to your system path.\n', '\n', '4. Change directories into the cloned repository.</br>\n', '`$ cd interpretable_machine_learning_with_python`\n', '\n', '5. Create a Python 3.6 virtual environment.</br>\n', '`$ virtualenv -p /path/to/anaconda3/bin/python3.6 env_iml`\n', '\n', '6. Activate the virtual environment.</br>\n', '`$ source env_iml/bin/activate`\n', '\n', '7. Install the correct packages for the example notebooks.</br>\n', '`$ pip install -r requirements.txt`\n', '\n', '8. Start Jupyter.</br>\n', '`$ jupyter notebook`\n', '\n', '### Docker Installation\n', '\n', 'A Dockerfile is provided to build a docker container with all necessary packages and dependencies. This is a way to use these examples if you are on Mac OS X, \\*nix, or Windows 10. To do so:\n', '\n', '1. Install and start [docker](https://www.docker.com/).\n', '\n', 'From a terminal:\n', '\n', '2. Create a directory for the Dockerfile.</br>\n', '`$ mkdir anaconda_py36_h2o_xgboost_graphviz_shap`\n', '\n', '3. Fetch the Dockerfile.</br>\n', '`$ curl https://raw.githubusercontent.com/jphall663/interpretable_machine_learning_with_python/master/anaconda_py36_h2o_xgboost_graphviz_shap/Dockerfile > anaconda_py36_h2o_xgboost_graphviz_shap/Dockerfile`\n', '\n', '4. Build a docker image from the Dockefile.</br>\n', '`docker build -t iml anaconda_py36_h2o_xgboost_graphviz_shap`\n', '\n', '5. Start the docker image and the Jupyter notebook server.</br>\n', ' `docker run -i -t -p 8888:8888 iml:latest /bin/bash -c ""/opt/conda/bin/jupyter notebook --notebook-dir=/interpretable_machine_learning_with_python --allow-root --ip=\'*\' --port=8888 --no-browser""`\n', '\n', '6. Navigate to port 8888 on your machine, probably `http://localhost:8888/`.\n', '\n', '\n', '### Manual Installation\n', '\n', '1. Anaconda Python 5.1.0 from the [Anaconda archives](https://repo.continuum.io/archive/).\n', '2. [Java](https://java.com/download).\n', '3. The latest stable [h2o](https://www.h2o.ai/download/) Python package.\n', '4. [Git](https://git-scm.com/downloads).\n', '5. [XGBoost](https://github.com/dmlc/xgboost) with Python bindings.\n', '6. [GraphViz](http://www.graphviz.org/).\n', '7. [Seaborn](https://pypi.org/project/seaborn/) package.\n', '8. [Shap](https://pypi.org/project/shap/) package.  \n', '\n', 'Anaconda Python, Java, Git, and GraphViz must be added to your system path.\n', '\n', 'From a terminal:\n', '\n', '9. Clone the repository with examples.</br>\n', '`$ git clone https://github.com/jphall663/interpretable_machine_learning_with_python.git`\n', '\n', '10. `$ cd interpretable_machine_learning_with_python`\n', '\n', '11. Start the Jupyter notebook server.</br>\n', '`$ jupyter notebook`\n', '\n', '12. Navigate to the port Jupyter directs you to on your machine, probably `http://localhost:8888/`.\n']"
Model Explainability,MAIF/shapash,MAIF,https://api.github.com/repos/MAIF/shapash,2164,268,23,"['https://api.github.com/users/ThomasBouche', 'https://api.github.com/users/ThibaudReal', 'https://api.github.com/users/yg79', 'https://api.github.com/users/Francesco-Marini', 'https://api.github.com/users/MaxGdr', 'https://api.github.com/users/MLecardonnel', 'https://api.github.com/users/guillaume-vignal', 'https://api.github.com/users/FlorineGreciet', 'https://api.github.com/users/GAP01', 'https://api.github.com/users/SebastienBidault', 'https://api.github.com/users/mathisbarthere', 'https://api.github.com/users/blanoe', 'https://api.github.com/users/githubyako', 'https://api.github.com/users/YL79', 'https://api.github.com/users/dependabot%5Bbot%5D', 'https://api.github.com/users/johannmartin95', 'https://api.github.com/users/peterdhansen', 'https://api.github.com/users/amnaabbassi', 'https://api.github.com/users/amorea04', 'https://api.github.com/users/ptitFicus', 'https://api.github.com/users/DragonWarrior15', 'https://api.github.com/users/susmitpy', 'https://api.github.com/users/yvanzubro']",Python,2023-04-26T09:53:26Z,https://raw.githubusercontent.com/MAIF/shapash/master/README.md,"['<p align=""center"">\n', '<img src=""https://raw.githubusercontent.com/MAIF/shapash/master/docs/_static/shapash-resize.png"" width=""300"" title=""shapash-logo"">\n', '</p>\n', '\n', '\n', '<p align=""center"">\n', '  <!-- Tests -->\n', '  <a href=""https://github.com/MAIF/shapash/workflows/Build%20%26%20Test/badge.svg"">\n', '    <img src=""https://github.com/MAIF/shapash/workflows/Build%20%26%20Test/badge.svg"" alt=""tests"">\n', '  </a>\n', '  <!-- PyPi -->\n', '  <a href=""https://img.shields.io/pypi/v/shapash"">\n', '    <img src=""https://img.shields.io/pypi/v/shapash"" alt=""pypi"">\n', '  </a>\n', '  <!-- Downloads -->\n', '  <a href=""https://static.pepy.tech/personalized-badge/shapash?period=total&units=international_system&left_color=grey&right_color=orange&left_text=Downloads"">\n', '    <img src=""https://static.pepy.tech/personalized-badge/shapash?period=total&units=international_system&left_color=grey&right_color=orange&left_text=Downloads"" alt=""downloads"">\n', '  </a>\n', '  <!-- Python Version -->\n', '  <a href=""https://img.shields.io/pypi/pyversions/shapash"">\n', '    <img src=""https://img.shields.io/pypi/pyversions/shapash"" alt=""pyversion"">\n', '  </a>\n', '  <!-- License -->\n', '  <a href=""https://img.shields.io/pypi/l/shapash"">\n', '    <img src=""https://img.shields.io/pypi/l/shapash"" alt=""license"">\n', '  </a>\n', '  <!-- Doc -->\n', '  <a href=""https://shapash.readthedocs.io/en/latest/"">\n', '    <img src=""https://readthedocs.org/projects/shapash/badge/?version=latest"" alt=""doc"">\n', '  </a>\n', '</p>\n', '\n', ""## 🎉 What's new ?\n"", '\n', '\n', '| Version       | New Feature                                                                           | Description                                                                                                                            | Tutorial |\n', '|:-------------:|:-------------------------------------------------------------------------------------:|:--------------------------------------------------------------------------------------------------------------------------------------:|:--------:|\n', '| 2.3.x         |  Additional dataset columns <br> [New demo](https://shapash-demo.ossbymaif.fr/) <br> [Article](https://pub.towardsai.net/shapash-2-3-0-comprehensive-model-interpretation-40b50157c2fb)                                                                | In Webapp: Target and error columns added to dataset and possibility to add features outside the model for more filtering options            |  [<img src=""https://raw.githubusercontent.com/MAIF/shapash/master/docs/_static/add_column_icon.png"" width=""50"" title=""add_column"">](https://github.com/MAIF/shapash/blob/master/tutorial/webapp/tuto-webapp01-additional-data.ipynb)\n', '| 2.3.x         |  Identity card <br> [New demo](https://shapash-demo.ossbymaif.fr/) <br> [Article](https://pub.towardsai.net/shapash-2-3-0-comprehensive-model-interpretation-40b50157c2fb)                                                                  | In Webapp: New identity card to summarize the information of the selected sample                  |  [<img src=""https://raw.githubusercontent.com/MAIF/shapash/master/docs/_static/identity_card.png"" width=""50"" title=""identity"">](https://github.com/MAIF/shapash/blob/master/tutorial/webapp/tuto-webapp01-additional-data.ipynb)\n', '| 2.2.x         |  Picking samples <br> [Article](https://www.kdnuggets.com/2022/11/picking-examples-understand-machine-learning-model.html)                                                                | New tab in the webapp for picking samples. The graph represents the ""True Values Vs Predicted Values""            |  [<img src=""https://raw.githubusercontent.com/MAIF/shapash/master/docs/_static/picking.png"" width=""50"" title=""picking"">](https://github.com/MAIF/shapash/blob/master/tutorial/plot/tuto-plot06-prediction_plot.ipynb)\n', '| 2.2.x         |  Dataset Filter <br>                                                              | New tab in the webapp to filter data. And several improvements in the webapp: subtitles, labels, screen adjustments                   |  [<img src=""https://raw.githubusercontent.com/MAIF/shapash/master/docs/_static/webapp.png"" width=""50"" title=""webapp"">](https://github.com/MAIF/shapash/blob/master/tutorial/tutorial01-Shapash-Overview-Launch-WebApp.ipynb)\n', '| 2.0.x         |  Refactoring Shapash <br>                                                                   | Refactoring attributes of compile methods and init. Refactoring implementation for new backends                   |  [<img src=""https://raw.githubusercontent.com/MAIF/shapash/master/docs/_static/modular.png"" width=""50"" title=""modular"">](https://github.com/MAIF/shapash/blob/master/tutorial/backend/tuto-backend-01.ipynb)\n', '| 1.7.x         |  Variabilize Colors <br>                                                                   | Giving possibility to have your own colour palette for outputs adapted to your design                   |  [<img src=""https://raw.githubusercontent.com/MAIF/shapash/master/docs/_static/variabilize-colors.png"" width=""50"" title=""variabilize-colors"">](https://github.com/MAIF/shapash/blob/master/tutorial/common/tuto-common02-colors.ipynb)\n', '| 1.6.x         |  Explainability Quality Metrics <br> [Article](https://towardsdatascience.com/building-confidence-on-explainability-methods-66b9ee575514)                                                                   | To help increase confidence in explainability methods, you can evaluate the relevance of your explainability using 3 metrics: **Stability**, **Consistency** and **Compacity**                   |  [<img src=""https://raw.githubusercontent.com/MAIF/shapash/master/docs/_static/quality-metrics.png"" width=""50"" title=""quality-metrics"">](https://github.com/MAIF/shapash/blob/master/tutorial/explainability_quality/tuto-quality01-Builing-confidence-explainability.ipynb) \n', '| 1.5.x         |  ACV Backend <br>                                                                     | A new way of estimating Shapley values using ACV. [More info about ACV here](https://towardsdatascience.com/the-right-way-to-compute-your-shapley-values-cfea30509254).                   |  [<img src=""https://raw.githubusercontent.com/MAIF/shapash/master/docs/_static/wheel.png"" width=""50"" title=""wheel-acv-backend"">](tutorial/explainer/tuto-expl03-Shapash-acv-backend.ipynb)    |\n', '| 1.4.x         |  Groups of features <br> [Demo](https://shapash-demo2.ossbymaif.fr/)                  | You can now regroup features that share common properties together. <br>This option can be useful if your model has a lot of features. |  [<img src=""https://raw.githubusercontent.com/MAIF/shapash/master/docs/_static/groups_features.gif"" width=""120"" title=""groups-features"">](https://github.com/MAIF/shapash/blob/master/tutorial/common/tuto-common01-groups_of_features.ipynb)    | \n', '| 1.3.x         |  Shapash Report <br> [Demo](https://shapash.readthedocs.io/en/latest/report.html)     | A standalone HTML report that constitutes a basis of an audit document.                                                                |  [<img src=""https://raw.githubusercontent.com/MAIF/shapash/master/docs/_static/report-icon.png"" width=""50"" title=""shapash-report"">](https://github.com/MAIF/shapash/blob/master/tutorial/report/tuto-shapash-report01.ipynb)    | \n', '\n', '\n', '## 🔍 Overview\n', '\n', '**Shapash** is a Python library which aims to make machine learning interpretable and understandable by everyone.\n', 'It provides several types of visualization that display explicit labels that everyone can understand. \n', '\n', 'Data Scientists can understand their models easily and share their results. End users can understand the decision proposed by a model using a summary of the most influential criteria.\n', '\n', 'Shapash also contributes to data science auditing by displaying usefull information about any model and data in a unique report. \n', '\n', '- Readthedocs: [![documentation badge](https://readthedocs.org/projects/shapash/badge/?version=latest)](https://shapash.readthedocs.io/en/latest/)\n', '- [Presentation video for french speakers](https://www.youtube.com/watch?v=r1R_A9B9apk)\n', '- Medium:\n', '  - [Understand your model with Shapash - Towards AI](https://pub.towardsai.net/shapash-making-ml-models-understandable-by-everyone-8f96ad469eb3) \n', '  - [Model auditability - Towards DS](https://towardsdatascience.com/shapash-1-3-2-announcing-new-features-for-more-auditable-ai-64a6db71c919)\n', '  - [Group of features - Towards AI](https://pub.towardsai.net/machine-learning-6011d5d9a444)\n', '  - [Building confidence on explainability - Towards DS](https://towardsdatascience.com/building-confidence-on-explainability-methods-66b9ee575514)\n', '  - [Picking Examples to Understand Machine Learning Model](https://www.kdnuggets.com/2022/11/picking-examples-understand-machine-learning-model.html)\n', '  - [Enhancing Webapp Built-In Features for Comprehensive Machine Learning Model Interpretation](https://pub.towardsai.net/shapash-2-3-0-comprehensive-model-interpretation-40b50157c2fb)\n', '\n', '\n', '<p align=""center"">\n', '  <img src=""https://raw.githubusercontent.com/MAIF/shapash/master/docs/_static/shapash_global.gif"" width=""800"">\n', '</p>\n', '\n', '## 🤝 Contributors\n', '\n', '<div align=""left"">\n', '  <div style=""display: flex; align-items: flex-start;"">\n', '    <img align=middle src=""https://github.com/MAIF/shapash/blob/master/docs/_static/logo_maif.png"" width=""18%""/>\n', '    <img align=middle src=""https://github.com/MAIF/shapash/blob/master/docs/_static/logo_quantmetry.png"" width=""18%"" />\n', '    <img align=middle src=""https://github.com/MAIF/shapash/blob/master/docs/_static/logo_societe_generale.png"" width=""18%"" /> \n', '    <img align=middle src=""https://github.com/MAIF/shapash/blob/master/docs/_static/logo_groupe_vyv.png"" width=""18%"" /> \n', '    <img align=middle src=""https://github.com/MAIF/shapash/blob/master/docs/_static/logo_SixfoisSept.png"" width=""18%"" /> \n', '  </div>\n', '</div>\n', '\n', '\n', '## 🏆 Awards\n', '\n', '<a href=""https://raw.githubusercontent.com/MAIF/shapash/master/docs/_static/awards-argus-or.png"">\n', '  <img align=""left"" src=""https://raw.githubusercontent.com/MAIF/shapash/master/docs/_static/awards-argus-or.png"" width=""180"" />\n', '</a>\n', '\n', '<a href=""https://www.kdnuggets.com/2021/04/shapash-machine-learning-models-understandable.html"">\n', '  <img src=""https://www.kdnuggets.com/images/tkb-2104-g.png?raw=true"" width=""65"" />\n', '</a>  \n', '\n', '\n', '## 🔥 Features\n', '\n', '- Display clear and understandable results: plots and outputs use **explicit labels** for each feature and its values\n', '\n', '<p align=""center"">\n', '  <img align=""left"" src=""https://github.com/MAIF/shapash/blob/master/docs/_static/shapash-grid-images-02.png?raw=true"" width=""28%""/>\n', '  <img src=""https://github.com/MAIF/shapash/blob/master/docs/_static/shapash-grid-images-06.png?raw=true"" width=""28%"" />\n', '  <img align=""right"" src=""https://github.com/MAIF/shapash/blob/master/docs/_static/shapash-grid-images-04.png?raw=true"" width=""28%"" /> \n', '</p>\n', '\n', '<p align=""center"">\n', '  <img align=""left"" src=""https://github.com/MAIF/shapash/blob/master/docs/_static/shapash-grid-images-01.png?raw=true"" width=""28%"" />\n', '  <img src=""https://github.com/MAIF/shapash/blob/master/docs/_static/shapash-resize.png?raw=true"" width=""18%"" />\n', '  <img align=""right"" src=""https://github.com/MAIF/shapash/blob/master/docs/_static/shapash-grid-images-13.png?raw=true"" width=""28%"" /> \n', '</p>\n', '\n', '<p align=""center"">\n', '  <img align=""left"" src=""https://github.com/MAIF/shapash/blob/master/docs/_static/shapash-grid-images-12.png?raw=true"" width=""33%"" />\n', '  <img src=""https://github.com/MAIF/shapash/blob/master/docs/_static/shapash-grid-images-03.png?raw=true"" width=""28%"" />\n', '  <img align=""right"" src=""https://github.com/MAIF/shapash/blob/master/docs/_static/shapash-grid-images-10.png?raw=true"" width=""25%"" /> \n', '</p>\n', '\n', '\n', '- Allow Data Scientists to quickly understand their models by using a **webapp** to easily navigate between global and local explainability, and understand how the different features contribute: [Live Demo Shapash-Monitor](https://shapash-demo.ossbymaif.fr/)\n', '\n', '- **Summarize and export** the local explanation\n', '> **Shapash** proposes a short and clear local explanation. It allows each user, whatever their Data background, to understand a local prediction of a supervised model thanks to a summarized and explicit explanation\n', '\n', '\n', '- **Evaluate** the quality of your explainability using different metrics\n', '\n', '- Easily share and discuss results with non-Data users\n', '\n', '- Select subsets for further analysis of explainability by filtering on explanatory and additional features, correct or wrong predictions. [Picking Examples to Understand Machine Learning Model](https://www.kdnuggets.com/2022/11/picking-examples-understand-machine-learning-model.html)\n', '\n', '- Deploy interpretability part of your project: From model training to deployment (API or Batch Mode)\n', '\n', '- Contribute to the **auditability of your model** by generating a **standalone HTML report** of your projects. [Report Example](https://shapash.readthedocs.io/en/latest/report.html) \n', '>We hope that this report will bring a valuable support to auditing models and data related to a better AI governance. \n', 'Data Scientists can now deliver to anyone who is interested in their project **a document that freezes different aspects of their work as a basis of an audit report**. \n', 'This document can be easily shared across teams (internal audit, DPO, risk, compliance...).\n', '\n', '<p align=""center"">\n', '  <img src=""https://raw.githubusercontent.com/MAIF/shapash/master/docs/_static/shapash-report-demo.gif"" width=""800"">\n', '</p>\n', '\n', '## ⚙️ How Shapash works \n', '**Shapash** is an overlay package for libraries dedicated to the interpretability of models. It uses Shap or Lime backend\n', 'to compute contributions.\n', '**Shapash** builds on the different steps necessary to build a machine learning model to make the results understandable\n', '\n', '<p align=""center"">\n', '  <img src=""https://raw.githubusercontent.com/MAIF/shapash/master/docs/_static/shapash-diagram.png"" width=""700"" title=""diagram"">\n', '</p>\n', '\n', '**Shapash** works for Regression, Binary Classification or Multiclass problem. <br />\n', 'It is compatible with many models: *Catboost*, *Xgboost*, *LightGBM*, *Sklearn Ensemble*, *Linear models*, *SVM*. <br />\n', 'Shapash can use category-encoders object, sklearn ColumnTransformer or simply features dictionary. <br />\n', '- Category_encoder: *OneHotEncoder*, *OrdinalEncoder*, *BaseNEncoder*, *BinaryEncoder*, *TargetEncoder*\n', '- Sklearn ColumnTransformer: *OneHotEncoder*, *OrdinalEncoder*, *StandardScaler*, *QuantileTransformer*, *PowerTransformer*\n', '\n', '## 🛠 Installation\n', '\n', 'Shapash is intended to work with Python versions 3.8 to 3.10. Installation can be done with pip:\n', '\n', '```\n', 'pip install shapash\n', '```\n', '\n', 'In order to generate the Shapash Report some extra requirements are needed.\n', 'You can install these using the following command :  \n', '```\n', 'pip install shapash[report]\n', '```\n', '\n', 'If you encounter **compatibility issues** you may check the corresponding section in the Shapash documentation [here](https://shapash.readthedocs.io/en/latest/installation-instructions/index.html).\n', '\n', '## 🕐 Quickstart\n', '\n', 'The 4 steps to display results:\n', '\n', '- Step 1: Declare SmartExplainer Object\n', '  > There 1 mandatory parameter in compile method: Model\n', '  > You can declare features dict here to specify the labels to display\n', '\n', '```\n', 'from shapash import SmartExplainer\n', 'xpl = SmartExplainer(\n', '  model=regressor,\n', '  features_dict=house_dict,  # Optional parameter\n', '  preprocessing=encoder, # Optional: compile step can use inverse_transform method\n', '  postprocessing=postprocess, # Optional: see tutorial postprocessing  \n', ')\n', '```\n', '\n', '- Step 2: Compile  Dataset, ...\n', '  > There 1 mandatory parameter in compile method: Dataset\n', ' \n', '```\n', 'xpl.compile(\n', '    x=Xtest,    \n', '    y_pred=y_pred, # Optional: for your own prediction (by default: model.predict)\n', '    y_target=yTest, # Optional: allows to display True Values vs Predicted Values\n', '    additional_data=X_additional, # Optional: additional dataset of features for Webapp\n', '    additional_features_dict=features_dict_additional, # Optional: dict additional data    \n', ')\n', '```  \n', '\n', '- Step 3: Display output\n', '  > There are several outputs and plots available. for example, you can launch the web app:\n', '\n', '```\n', 'app = xpl.run_app()\n', '``` \n', '\n', '[Live Demo Shapash-Monitor](https://shapash-demo.ossbymaif.fr/)\n', '\n', '- Step 4: Generate the Shapash Report\n', '  > This step allows to generate a standalone html report of your project using the different splits\n', '  of your dataset and also the metrics you used:\n', '\n', '```\n', 'xpl.generate_report(\n', ""    output_file='path/to/output/report.html',\n"", ""    project_info_file='path/to/project_info.yml',\n"", '    x_train=Xtrain,\n', '    y_train=ytrain,\n', '    y_test=ytest,\n', '    title_story=""House prices report"",\n', '    title_description=""""""This document is a data science report of the kaggle house prices tutorial project.\n', '        It was generated using the Shapash library."""""",\n', '    metrics=[{‘name’: ‘MSE’, ‘path’: ‘sklearn.metrics.mean_squared_error’}]\n', ')\n', '```\n', '\n', '[Report Example](https://shapash.readthedocs.io/en/latest/report.html)\n', '\n', '- Step 5: From training to deployment : SmartPredictor Object\n', '  > Shapash provides a SmartPredictor object to deploy the summary of local explanation for the operational needs.\n', '  It is an object dedicated to deployment, lighter than SmartExplainer with additional consistency checks.\n', '  SmartPredictor can be used with an API or in batch mode. It provides predictions, detailed or summarized local \n', '  explainability using appropriate wording.\n', '  \n', '```\n', 'predictor = xpl.to_smartpredictor()\n', '```\n', 'See the tutorial part to know how to use the SmartPredictor object\n', '\n', '## 📖  Tutorials\n', 'This github repository offers many tutorials to allow you to easily get started with Shapash.\n', '\n', '\n', '<details><summary><b>Overview</b> </summary>\n', '\n', '- [Launch the webapp with a concrete use case](tutorial/tutorial01-Shapash-Overview-Launch-WebApp.ipynb)\n', '- [Jupyter Overviews - The main outputs and methods available with the SmartExplainer object](tutorial/tutorial02-Shapash-overview-in-Jupyter.ipynb)\n', '- [Shapash in production: From model training to deployment (API or Batch Mode)](tutorial/tutorial03-Shapash-overview-model-in-production.ipynb)\n', '- [Use groups of features](tutorial/common/tuto-common01-groups_of_features.ipynb)\n', '- [Deploy local explainability in production with SmartPredictor](tutorial/predictor/tuto-smartpredictor-introduction-to-SmartPredictor.ipynb)\n', '\n', '</details>\n', '\n', '<details><summary><b>Charts and plots</b> </summary>\n', '\n', '- [**Shapash** Features Importance](tutorial/plot/tuto-plot03-features-importance.ipynb)\n', '- [Contribution plot to understand how one feature affects a prediction](tutorial/plot/tuto-plot02-contribution_plot.ipynb)\n', '- [Summarize, display and export local contribution using filter and local_plot method](tutorial/plot/tuto-plot01-local_plot-and-to_pandas.ipynb)\n', '- [Contributions Comparing plot to understand why predictions on several individuals are different](tutorial/plot/tuto-plot04-compare_plot.ipynb)\n', '- [Visualize interactions between couple of variables](tutorial/plot/tuto-plot05-interactions-plot.ipynb)\n', '- [Customize colors in Webapp, plots and report](tutorial/common/tuto-common02-colors.ipynb)\n', '\n', '</details>\n', '\n', '<details><summary><b>Different ways to use Encoders and Dictionaries</b> </summary>\n', '\n', '- [Use Category_Encoder & inverse transformation](tutorial/encoder/tuto-encoder01-using-category_encoder.ipynb)\n', '- [Use ColumnTransformers](tutorial/encoder/tuto-encoder02-using-columntransformer.ipynb)\n', '- [Use Simple Python Dictionnaries](tutorial/encoder/tuto-encoder03-using-dict.ipynb)\n', '\n', '</details>\n', '\n', '<details><summary><b>Displaying data with postprocessing</b> </summary>\n', '\n', '[Using postprocessing parameter in compile method](tutorial/postprocess/tuto-postprocess01.ipynb)\n', '\n', '</details>\n', '\n', '<details><summary><b>Using different backends</b> </summary>\n', '\n', '- [Compute Shapley Contributions using **Shap**](tutorial/explainer/tuto-expl01-Shapash-Viz-using-Shap-contributions.ipynb)\n', '- [Use **Lime** to compute local explanation, Summarize-it with **Shapash**](tutorial/explainer/tuto-expl02-Shapash-Viz-using-Lime-contributions.ipynb)\n', '- [Use **ACV backend** to compute Active Shapley Values and SDP global importance](tutorial/explainer/tuto-expl03-Shapash-acv-backend.ipynb)\n', '- [Compile faster Lime and consistency of contributions](tutorial/explainer/tuto-expl04-Shapash-compute-Lime-faster.ipynb)\n', '\n', '</details>\n', '\n', '<details><summary><b>Evaluating the quality of your explainability</b> </summary>\n', '\n', '- [Building confidence on explainability methods using **Stability**, **Consistency** and **Compacity** metrics](tutorial/explainability_quality/tuto-quality01-Builing-confidence-explainability.ipynb)\n', '\n', '</details>\n', '\n', '<details><summary><b>Generate a report of your project</b> </summary>\n', '\n', '- [Generate a standalone HTML report of your project with generate_report](tutorial/report/tuto-shapash-report01.ipynb)\n', '\n', '</details>\n', '\n', '<details><summary><b>Analysing your model via Shapash WebApp</b> </summary>\n', '\n', '- [Add features outside of the model for more exploration options](tutorial/webapp/tuto-webapp01-additional-data.ipynb)\n', '\n', '</details>\n']"
Model Explainability,dylan-slack/Modeling-Uncertainty-Local-Explainability,dylan-slack,https://api.github.com/repos/dylan-slack/Modeling-Uncertainty-Local-Explainability,27,10,1,['https://api.github.com/users/dylan-slack'],Python,2023-03-14T23:25:14Z,https://raw.githubusercontent.com/dylan-slack/Modeling-Uncertainty-Local-Explainability/main/README.md,"['# Reliable Post hoc Explanations: Modeling Uncertainty in Explainability\n', '\n', 'Welcome to the code for our paper, Reliable Post hoc Explanations: Modeling Uncertainty in Explainability, published at NeurIPS 2021. We encourage you to read the [full paper](https://arxiv.org/abs/2008.05030).\n', '\n', 'Visualizing the posteriors of BayesLIME explanations on an image of a dog and COMPAS:\n', '\n', '<p float=""middle"">\n', '<img src=""visualization/diego.gif"" width=""300"" heigh=""300"">\n', '<img src=""data/posteriors_fig_1.png"" width=""500"" heigh=""300"">\n', '</p>\n', '\n', '## Citation\n', '\n', 'If you found this work useful, please cite us:\n', '\n', '```\n', '@inproceedings{reliableposthoc:neurips21,\n', '  author = {Dylan Slack and Sophie Hilgard and Sameer Singh and Himabindu Lakkaraju},\n', '  title = { {Reliable Post hoc Explanations Modeling Uncertainty in Explainability} },\n', '  booktitle = {Neural Information Processing Systems (NeurIPS)},\n', '  year = {2021}\n', '}\n', '```\n', '\n', '## Examples\n', '\n', ""An example usage of the explainer is provided in `./visualization/image_posterior_example.py`, where we visualize the posterior of a BayesLIME explanation on an image of the first author's dog.\n"", '\n', '## Experiments\n', '\n', '### Data\n', '\n', '#### Tabular Data\n', '\n', 'The German Credit + COMPAS datasets are included in the `./data` folder. Within experiments, the german credit data set is called as `--dataset german` and compas is called as `--dataset compas`.\n', '\n', '#### MNIST\n', '\n', 'The MNIST data is set to download automatically on the first run.\n', '\n', 'In places where the MNIST data is accepted, by specifying the `--dataset` flag, it is possible to select the digit on which to run the experiment by specifying, for example, `--dataset mnist_1` for the 1 digit or `--dataset mnist_3` for the 3 digit, and so on.\n', '\n', '#### ImageNet\n', '\n', 'To download the ImageNet data, use [this script](https://github.com/mf1024/ImageNet-Datasets-Downloader), selecting the appropriate class indices (e.g., n02108915 is the French Bulldog class used in the paper). For example, to download the French Bulldog data, run:\n', '\n', '```python\n', 'python ./downloader.py \n', '    -data_root ./data/imagenet/frenchbulldog \\\n', '    -use_class_list True \\\n', '    -class_list n02108915 \\\n', '    -images_per_class 100 \n', '```\n', '\n', 'Once the imagenet dataset is installed, it can be called with `--dataset imagenet_classname` where `classname` is the name of the folder where the data is stored (for instance `frenchbulldog` running the script above).\n', '\n', '### Models\n', '\n', 'The tabular models are trained when they are called in experiments. The pre-trained MNIST model is provided in the `./data/mnist` subfolder. The VGG16 IMAGENET model will be downloaded when it is called.\n', '\n', '### Experiments\n', '\n', 'Code to run experiments from the paper is included in the `./experiments` directory within the project.\n', '\n', '### Hardware Requirements\n', '\n', 'For image experiments, GPU/TPU acceleration is recommended. I ran most of the experiments for this paper with a single NVIDIA 2080TI and a few with a NVIDIA Titan RTX.\n', '\n', ""For the tabular experiments, it's possible to run them on CPU. I tested this using a 1.4 GHz Intel Core i5 from a 2019 MacBook Pro, and it seemed to work fine. In places in the experiments where multithreading is used (`--n_threads`) in the experiments, be careful to use a value less than the avaliable cores on your CPU. I noticed that if I set `--n_threads` value too high on the MacBook, it caused it to freeze. \n"", '\n', '### Questions\n', '\n', 'You can reach out to [dslack@uci.edu](mailto:dslack@uci.edu) with any questions.\n', '\n']"
Model Explainability,pbiecek/ema,pbiecek,https://api.github.com/repos/pbiecek/ema,159,35,8,"['https://api.github.com/users/pbiecek', 'https://api.github.com/users/tomaszbur', 'https://api.github.com/users/xiaochi-liu', 'https://api.github.com/users/hbaniecki', 'https://api.github.com/users/jtr13', 'https://api.github.com/users/MartinHoldrege', 'https://api.github.com/users/mvwestendorp', 'https://api.github.com/users/FrieseWoudloper']",Python,2023-04-20T12:17:34Z,https://raw.githubusercontent.com/pbiecek/ema/master/README.md,"['<img width=""300"" align=""right"" src=""figure/front4.png"">\n', '\n', '# Explanatory Model Analysis\n', '\n', '## Explore, Explain, and Examine Predictive Models\n', '\n', 'See the html website here: https://pbiecek.github.io/ema/\n', '\n', 'A note to readers: this text is a work in progress. \n', '\n', ""We've released this initial version to get more feedback. Feedback can be given at the GitHub repo https://github.com/pbiecek/ema/issues. We are primarily interested in the organization and consistency of the content, but any comments will be welcommed.\n"", '\n', '### Bibtex entry\n', '\n', '```\n', '@Book{,\n', '  author = {Przemyslaw Biecek and Tomasz Burzykowski},\n', '  title = {{Explanatory Model Analysis}},\n', '  publisher = {Chapman and Hall/CRC, New York},\n', '  year = {2021},\n', '  isbn = {9780367135591},\n', '  url = {https://ema.drwhy.ai/},\n', '}\n', '```\n', '\n', '### Model Studio\n', '\n', 'In this book we have used several predictive models. One can explore them with the Model Studio under following links\n', '\n', '* `titanic_rf_v4` model https://titanic-rf-v4.netlify.com\n', '* `titanic_rf_v6` model https://titanic-rf-v6.netlify.com\n', '* `titanic_gbm_v6` model https://titanic-gbm-v6.netlify.com\n', '* `titanic_lmr_v6` model https://titanic-lmr-v6.netlify.com\n', '\n', '![figure/titanic_rf_v6.png](figure/titanic_rf_v6.png)\n', '\n']"
Model Explainability,erikjhordan-rey/Android-Spotify-MVP,erikjhordan-rey,https://api.github.com/repos/erikjhordan-rey/Android-Spotify-MVP,183,64,1,['https://api.github.com/users/erikjhordan-rey'],Java,2023-03-05T09:31:40Z,https://raw.githubusercontent.com/erikjhordan-rey/Android-Spotify-MVP/master/README.md,"['# Android - Spotify + Model View Presenter (MVP) [![Build Status](https://travis-ci.org/erikjhordan-rey/Android-Spotify-MVP.svg?branch=master)](https://travis-ci.org/erikjhordan-rey/Android-Spotify-MVP)\n', 'Android Model View Presenter used to explain how to use this pattern in our android applications.\n', '\n', 'This example was created to support an article explanation [Model View Presenter en Android][1] (spanish).\n', '\n', 'Libraries used on the sample project\n', '------------------------------------\n', '* [AppCompat, CardView, RecyclerView, Material][2]\n', '* [Retrofit 2][4]\n', '* [RxJava & RxAndroid][5]\n', '* [Gradle Retrolambda Plugin][6]\n', '\n', '\n', '# Demo\n', '\n', '![](./art/spotify-mvp.png)\n', '\n', '\n', '# Access Token \n', '\n', 'The Spotify Api has been changed an Access Token is required. The app sample probably will response `401 unauthorized code`.\n', '\n', '1- * Get Your Access Token from [Spotify Api Doc](https://developer.spotify.com/web-api/console/get-search-item/)\n', '\n', '![](./art/token_spotify.png)\n', '\n', '2- The class `Constans` has a constant variable called `ACCESS_TOKEN` replace with your access token  \n', '\n', '\n', '3- Run the app, it should work!!\n', '\n', '\n', '# how does it work?\n', '\n', '![](./art/Telecine_2015-11-25-17-19-04.gif)\n', '\n', '[8]: http://mockito.org/\n', '[7]: http://robolectric.org/\n', '[6]: https://github.com/evant/gradle-retrolambda\n', '[5]: https://github.com/ReactiveX/RxAndroid\n', '[4]: http://square.github.io/retrofit/\n', '[2]: http://developer.android.com/intl/es/tools/support-library/index.html\n', '[1]: https://erikjhordan-rey.github.io/blog/2015/11/02/ANDROID-mvp.html\n', '\n', '\n', 'Do you want to contribute?\n', '--------------------------\n', '\n', 'Feel free to report or add any useful feature, I will be glad to improve it with your help.\n', '\n', '\n', 'Developed By\n', '------------\n', '\n', '* Erik Jhordan Rey  - <erikjhordan.rey@gmail.com>\n', '\n', 'License\n', '-------\n', '\n', '    Copyright 2016 Erik Jhordan Rey\n', '\n', '    Licensed under the Apache License, Version 2.0 (the ""License"");\n', '    you may not use this file except in compliance with the License.\n', '    You may obtain a copy of the License at\n', '\n', '       http://www.apache.org/licenses/LICENSE-2.0\n', '\n', '    Unless required by applicable law or agreed to in writing, software\n', '    distributed under the License is distributed on an ""AS IS"" BASIS,\n', '    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n', '    See the License for the specific language governing permissions and\n', '    limitations under the License.\n', '\n', '\n']"
Model Explainability,eludadev/css-docs,eludadev,https://api.github.com/repos/eludadev/css-docs,318,44,1,['https://api.github.com/users/eludadev'],,2023-04-26T16:49:46Z,https://raw.githubusercontent.com/eludadev/css-docs/main/README.md,"['## :sparkles: CSS Selectors\n', '\n', '| Preview                           | Selector                                                     | Description                                                  |\n', '| --------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |\n', '| [![](./assets/css_selectors_1.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/Child_combinator) | <p align=""center""><strong>a > b</strong><br /><a href=""https://developer.mozilla.org/en-US/docs/Web/CSS/Child_combinator"">Child Combinator</a></p> | Select all b elements that are directly inside of a elements. |\n', '| [![](./assets/css_selectors_2.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/Descendant_combinator) | <p align=""center""><strong>a &nbsp; b</strong><br /><a href=""https://developer.mozilla.org/en-US/docs/Web/CSS/Descendant_combinator"">Descendant Combinator</a></p> | Select all b elements that are anywhere inside of a elements. |\n', '| [![](./assets/css_selectors_3.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/Adjacent_sibling_combinator) | <p align=""center""><strong>a + b</strong><br /><a href=""https://developer.mozilla.org/en-US/docs/Web/CSS/Adjacent_sibling_combinator"">Adjacent sibling combinator</a></p> | Select all b elements that are immediately next to a elements. |\n', '| [![](./assets/css_selectors_4.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/General_sibling_combinator) | <p align=""center""><strong>a ~ b</strong><br /><a href=""https://developer.mozilla.org/en-US/docs/Web/CSS/General_sibling_combinator"">General sibling combinator</a></p> | Select all b elements that are anywhere after a elements. |\n', '| [![](./assets/css_selectors_5.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/Class_selectors) | <p align=""center""><strong>.cl</strong><br /><a href=""https://developer.mozilla.org/en-US/docs/Web/CSS/Class_selectors"">Class selector</a></p> | Select all elements that have the cl class name. |\n', '| [![](./assets/css_selectors_6.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/Type_selectors) | <p align=""center""><strong>a.cl</strong><br /><a href=""https://developer.mozilla.org/en-US/docs/Web/CSS/Type_selectors"">Tag + Class selector</a></p> | Select all a elements that have the cl class name. |\n', '| [![](./assets/css_selectors_7.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/Class_selectors) | <p align=""center""><strong>.cl1.cl2</strong><br /><a href=""https://developer.mozilla.org/en-US/docs/Web/CSS/Class_selectors"">Multiclass selector</a></p> | Select all elements that have both the cl1 and cl2 class names. |\n', '| [![](./assets/css_selectors_8.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/Attribute_selectors) | <p align=""center""><strong>a\\[x=y\\]</strong><br /><a href=""https://developer.mozilla.org/en-US/docs/Web/CSS/Attribute_selectors"">Attribute selector</a></p> | Select all a elements that have the x attribute set to y. |\n', '| [![](./assets/css_selectors_9.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/ID_selectors) | <p align=""center""><strong>#id1</strong><br /><a href=""https://developer.mozilla.org/en-US/docs/Web/CSS/ID_selectors"">ID selector</a></p> | Select the element with the id1 ID name. |\n', '| [![](./assets/css_selectors_10.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/Universal_selectors) | <p align=""center""><strong>*</strong><br /><a href=""https://developer.mozilla.org/en-US/docs/Web/CSS/Universal_selectors"">Universal selector</a></p> | Select all elements. |\n', '\n', '| High Resolution | Grayscale Print |\n', '| --------------- | --------------- |\n', '| [![](./assets/lowres-css_selectors.png)](./assets/css_selectors.png) | [![](./assets/lowres-css_selectors_print.png)](./assets/css_selectors_print.png) |\n', '\n', '## :sparkles: CSS Box Model\n', '\n', '| Preview                           | Property                                                     | Description                                                  |\n', '| --------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |\n', '| [![](./assets/css_box_model_1.png)](https://developer.mozilla.org/en-US/docs/Learn/CSS/Building_blocks/The_box_model) | <p align=""center"" markdown=""true"">[`box-sizing: border-box`](https://developer.mozilla.org/en-US/docs/Learn/CSS/Building_blocks/The_box_model)</p> | The `width` and `height` have the size of `content`+`padding`+`border` |\n', '| [![](./assets/css_box_model_2.png)](https://developer.mozilla.org/en-US/docs/Learn/CSS/Building_blocks/The_box_model) | <p align=""center"" markdown=""true"">[`box-sizing: content-box`](https://developer.mozilla.org/en-US/docs/Learn/CSS/Building_blocks/The_box_model)</p> | The `width` and `height` have the size of just `content` |\n', '\n', '| High Resolution |\n', '| --------------- |\n', '| [![](./assets/lowres-css_box_model.png)](./assets/css_box_model.png) |\n', '\n', '## :sparkles: CSS Grid Layout\n', '\n', '| Align Content                             |\n', '| --------------------------------- |\n', '| <p align=""center"" markdown=""true"">Distribute content along the horizontal axis.</p> |\n', '| <table><tr><td markdown=""true"">[![](./assets/css_grid_align_content_start.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td><td markdown=""true"">[`align-content: start`](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td><td markdown=""true"">[![](./assets/css_grid_align_content_space_around.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td><td markdown=""true"">[`align-content: space-around`](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td></tr><tr><td markdown=""true"">[![](./assets/css_grid_align_content_center.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td><td markdown=""true"">[`align-content: center`](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td><td markdown=""true"">[![](./assets/css_grid_align_content_space_between.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td><td markdown=""true"">[`align-content: space-between`](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td></tr><tr><td markdown=""true"">[![](./assets/css_grid_align_content_end.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td><td markdown=""true"">[`align-content: end`](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td><td markdown=""true"">[![](./assets/css_grid_align_content_stretch.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td><td markdown=""true"">[`align-content: stretch`](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td></tr></table> |\n', '\n', '| Justify Content                             |\n', '| --------------------------------- |\n', '| <p align=""center"" markdown=""true"">Distribute content along the vertical axis.</p> |\n', '| <table><tr><td markdown=""true"">[![](./assets/css_grid_justify_content_start.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td><td markdown=""true"">[`justify-content: start`](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td><td markdown=""true"">[![](./assets/css_grid_justify_content_space_around.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td><td markdown=""true"">[`justify-content: space-around`](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td></tr><tr><td markdown=""true"">[![](./assets/css_grid_justify_content_center.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td><td markdown=""true"">[`justify-content: center`](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td><td markdown=""true"">[![](./assets/css_grid_justify_content_space_between.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td><td markdown=""true"">[`justify-content: space-between`](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td></tr><tr><td markdown=""true"">[![](./assets/css_grid_justify_content_end.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td><td markdown=""true"">[`justify-content: end`](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td><td markdown=""true"">[![](./assets/css_grid_justify_content_stretch.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td><td markdown=""true"">[`justify-content: stretch`](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td></tr></table> |\n', '\n', '| Align Items                             |\n', '| --------------------------------- |\n', '| <p align=""center"" markdown=""true"">Distribute content along the horizontal axis within their grid area.</p> |\n', '| <table><tr><td markdown=""true"">[![](./assets/css_grid_align_items_start.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/align-items)</td><td markdown=""true"">[`align-items: start`](https://developer.mozilla.org/en-US/docs/Web/CSS/align-items)</td><td markdown=""true"">[![](./assets/css_grid_align_items_center.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/align-items)</td><td markdown=""true"">[`align-items: center`](https://developer.mozilla.org/en-US/docs/Web/CSS/align-items)</td></tr><tr><td markdown=""true"">[![](./assets/css_grid_align_items_end.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/align-items)</td><td markdown=""true"">[`align-items: end`](https://developer.mozilla.org/en-US/docs/Web/CSS/align-items)</td><td markdown=""true"">[![](./assets/css_grid_align_items_stretch.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/align-items)</td><td markdown=""true"">[`align-items: stretch`](https://developer.mozilla.org/en-US/docs/Web/CSS/align-items)</td></tr></table> |\n', '\n', '| Justify Items                            |\n', '| --------------------------------- |\n', '| <p align=""center"" markdown=""true"">Distribute content along the vertical axis within their grid area.</p> |\n', '| <table><tr><td markdown=""true"">[![](./assets/css_grid_justify_items_start.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-items)</td><td markdown=""true"">[`justify-items: start`](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-items)</td><td markdown=""true"">[![](./assets/css_grid_justify_items_center.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-items)</td><td markdown=""true"">[`justify-items: center`](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-items)</td></tr><tr><td markdown=""true"">[![](./assets/css_grid_justify_items_end.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-items)</td><td markdown=""true"">[`justify-items: end`](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-items)</td><td markdown=""true"">[![](./assets/css_grid_justify_items_stretch.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-items)</td><td markdown=""true"">[`justify-items: stretch`](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-items)</td></tr></table> |\n', '\n', '| High Resolution | Grayscale Print |\n', '| --------------- | --------------- |\n', '| [![](./assets/lowres-css_grid.png)](./assets/css_grid.png) | [![](./assets/lowres-css_grid_print.png)](./assets/css_grid_print.png) |\n', '\n', '## :sparkles: CSS Flexbox Layout\n', '\n', '| Flex Direction                             |\n', '| --------------------------------- |\n', '| <p align=""center"" markdown=""true"">The flex-direction CSS property sets how flex items are placed in the flex container defining the main axis and the direction (normal or reversed).</p> |\n', '| <table><tr><td markdown=""true"">[![](./assets/css_flexbox_flex_direction_row.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/flex-direction)</td><td markdown=""true"">[`flex-direction: row`](https://developer.mozilla.org/en-US/docs/Web/CSS/flex-direction)</td><td markdown=""true"">[![](./assets/css_flexbox_flex_direction_column.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/flex-direction)</td><td markdown=""true"">[`flex-direction: column`](https://developer.mozilla.org/en-US/docs/Web/CSS/flex-direction)</td></tr><tr><td markdown=""true"">[![](./assets/css_flexbox_flex_direction_row_reverse.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/flex-direction)</td><td markdown=""true"">[`flex-direction: row-reverse`](https://developer.mozilla.org/en-US/docs/Web/CSS/flex-direction)</td><td markdown=""true"">[![](./assets/css_flexbox_flex_direction_column_reverse.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/flex-direction)</td><td markdown=""true"">[`flex-direction: column-reverse`](https://developer.mozilla.org/en-US/docs/Web/CSS/flex-direction)</td></tr></table> |\n', '\n', '| Align Content                             |\n', '| --------------------------------- |\n', '| <p align=""center"" markdown=""true"">The CSS align-content property sets the distribution of space between and around content items along a flexbox\'s cross-axis.</p> |\n', '| <table><tr><td markdown=""true"">[![](./assets/css_flexbox_align_content_flex_start.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td><td markdown=""true"">[`align-content: flex-start`](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td><td markdown=""true"">[![](./assets/css_flexbox_align_content_space_around.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td><td markdown=""true"">[`align-content: space-around`](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td></tr><tr><td markdown=""true"">[![](./assets/css_flexbox_align_content_center.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td><td markdown=""true"">[`align-content: center`](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td><td markdown=""true"">[![](./assets/css_flexbox_align_content_space_between.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td><td markdown=""true"">[`align-content: space-between`](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td></tr><tr><td markdown=""true"">[![](./assets/css_flexbox_align_content_flex_end.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td><td markdown=""true"">[`align-content: flex-end`](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td><td markdown=""true"">[![](./assets/css_flexbox_align_content_stretch.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td><td markdown=""true"">[`align-content: stretch`](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td></tr></table> |\n', '\n', '| Justify Content                             |\n', '| --------------------------------- |\n', '| <p align=""center"" markdown=""true"">The CSS justify-content property defines how the browser distributes space between and around content items along the main-axis of a flex container.</p> |\n', '| <table><tr><td markdown=""true"">[![](./assets/css_flexbox_justify_content_flex_start.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td><td markdown=""true"">[`justify-content: flex-start`](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td><td markdown=""true"">[![](./assets/css_flexbox_justify_content_space_around.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td><td markdown=""true"">[`justify-content: space-around`](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td></tr><tr><td markdown=""true"">[![](./assets/css_flexbox_justify_content_center.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td><td markdown=""true"">[`justify-content: center`](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td><td markdown=""true"">[![](./assets/css_flexbox_justify_content_space_between.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td><td markdown=""true"">[`justify-content: space-between`](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td></tr><tr><td markdown=""true"">[![](./assets/css_flexbox_justify_content_flex_end.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td><td markdown=""true"">[`justify-content: flex-end`](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td><td markdown=""true"">[![](./assets/css_flexbox_justify_content_stretch.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td><td markdown=""true"">[`justify-content: stretch`](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td></tr></table> |\n', '\n', '| Align Items                             |\n', '| --------------------------------- |\n', '| <p align=""center"" markdown=""true"">The CSS align-items property sets the align-self value on all direct children as a group. In Flexbox, it controls the alignment of items on the Cross Axis.</p> |\n', '| <table><tr><td markdown=""true"">[![](./assets/css_flexbox_align_items_flex_start.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/align-items)</td><td markdown=""true"">[`align-items: flex-start`](https://developer.mozilla.org/en-US/docs/Web/CSS/align-items)</td><td markdown=""true"">[![](./assets/css_flexbox_align_items_center.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/align-items)</td><td markdown=""true"">[`align-items: center`](https://developer.mozilla.org/en-US/docs/Web/CSS/align-items)</td></tr><tr><td markdown=""true"">[![](./assets/css_flexbox_align_items_flex_end.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/align-items)</td><td markdown=""true"">[`align-items: flex-end`](https://developer.mozilla.org/en-US/docs/Web/CSS/align-items)</td><td markdown=""true"">[![](./assets/css_flexbox_align_items_stretch.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/align-items)</td><td markdown=""true"">[`align-items: stretch`](https://developer.mozilla.org/en-US/docs/Web/CSS/align-items)</td></tr></table> |\n', '\n', '| High Resolution | Grayscale Print |\n', '| --------------- | --------------- |\n', '| [![](./assets/lowres-css_flexbox.png)](./assets/css_flexbox.png) | [![](./assets/lowres-css_flexbox_print.png)](./assets/css_flexbox_print.png) |']"
Model Explainability,ur-whitelab/exmol,ur-whitelab,https://api.github.com/repos/ur-whitelab/exmol,221,36,7,"['https://api.github.com/users/whitead', 'https://api.github.com/users/geemi725', 'https://api.github.com/users/hgandhi2411', 'https://api.github.com/users/maclandrol', 'https://api.github.com/users/eltociear', 'https://api.github.com/users/navneeth3005', 'https://api.github.com/users/aditis44']",Python,2023-04-06T15:49:33Z,https://raw.githubusercontent.com/ur-whitelab/exmol/main/README.md,"['# Explaining why that molecule\n', '\n', '[![GitHub](https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white)](https://github.com/ur-whitelab/exmol)\n', '[![tests](https://github.com/ur-whitelab/exmol/actions/workflows/tests.yml/badge.svg)](https://github.com/ur-whitelab/exmol) [![paper](https://github.com/ur-whitelab/exmol/actions/workflows/paper.yml/badge.svg)](https://github.com/ur-whitelab/exmol) [![docs](https://github.com/ur-whitelab/exmol/actions/workflows/docs.yml/badge.svg)](https://ur-whitelab.github.io/exmol/)\n', '[![PyPI version](https://badge.fury.io/py/exmol.svg)](https://badge.fury.io/py/exmol)\n', '[![MIT license](https://img.shields.io/badge/License-MIT-blue.svg)](https://lbesson.mit-license.org/)\n', '\n', '- [Explaining why that molecule](#explaining-why-that-molecule)\n', '  - [Install](#install)\n', '  - [Quickstart](#quickstart)\n', '  - [Counterfactual Generation](#counterfactual-generation)\n', '  - [Descriptor Attribution](#descriptor-attribution)\n', '  - [Usage](#usage)\n', '  - [Further Examples](#further-examples)\n', '  - [Chemical Space](#chemical-space)\n', '  - [SVG](#svg)\n', '  - [Disable Progress Bars](#disable-progress-bars)\n', '  - [API and Docs](#api-and-docs)\n', '  - [Developing](#developing)\n', '  - [Citation](#citation)\n', '\n', '\n', '`exmol` is a package to explain black-box predictions of molecules. The package uses model agnostic explanations to help users understand why a molecule is predicted to have a property.\n', '\n', '## Install\n', '\n', '```sh\n', 'pip install exmol\n', '```\n', '\n', '## Quickstart\n', '\n', 'See [the tutorial](https://ur-whitelab.github.io/exmol/paper2_LIME/Tutorial.html) to give an overview of the basic usage of exmol.\n', '\n', '## Counterfactual Generation\n', '\n', 'Our package implements the Model Agnostic Counterfactual Compounds with STONED to generate counterfactuals.\n', 'A counterfactual can explain a prediction by showing what would have to change in the molecule to change its predicted class. Here is an example of a counterfactual:\n', '\n', '> This package is not popular. If the package had a logo, it would be popular.\n', '\n', 'In addition to having a changed prediction, a molecular counterfactual must be similar to its base molecule as much as possible. Here is an example of a molecular counterfactual:\n', '\n', '<img alt=""counterfactual demo"" src=""https://raw.githubusercontent.com/ur-whitelab/exmol/main/paper1_CFs/svg_figs/counterfactual.png"" width=""400"">\n', '\n', 'The counterfactual shows that if the carboxylic acid were an ester, the molecule would be active. It is up to the user to translate this set of structures into a meaningful sentence.\n', '\n', '## Descriptor Attribution\n', 'This package also implements Model Agnostic Descriptor Attribution for molecules using LIME.\n', 'Descriptor attributions can explain a prediction by computing QSARs for molecular structure properties independent of features used for model predictions. Here is an example of descriptor attribution:\n', '\n', '<img alt=""descriptor demo"" src=""https://raw.githubusercontent.com/ur-whitelab/exmol/main/paper2_LIME/descriptor.png"" width=""800"">\n', '\n', 'The descriptor t-statistics show which chemical properties or substructures influence properety prediction for the pictured molecule. LIME is a perturbation based method and the descriptor attributions depend on the perturbed chemical space created around the molecule of interest.\n', '\n', '## Usage\n', '\n', ""Let's assume you have a deep learning model `my_model(s)` that takes in one SMILES string and outputs a predicted binary class. We first expand chemical space around the prediction of interest\n"", '\n', '```py\n', 'import exmol\n', '\n', '# mol of interest\n', ""base = 'Cc1onc(-c2ccccc2Cl)c1C(=O)NC1C(=O)N2C1SC(C)(C)C2C(=O)O'\n"", '\n', 'samples = exmol.sample_space(base, my_model, batched=False)\n', '```\n', '\n', 'We uses `batched=False` to indicate `my_model` cannot handle a batch of SMILES, just one at a time.  If your model takes SELFIES, just pass `use_selfies=True` to `sample_space`. Now we select counterfactuals from that space and plot them.\n', '\n', '```py\n', 'cfs = exmol.cf_explain(samples)\n', 'exmol.plot_cf(cfs)\n', '```\n', '\n', '<img alt=""set of counterfactuals"" src=""https://raw.githubusercontent.com/ur-whitelab/exmol/main/paper1_CFs/svg_figs/rf-simple.png"" width=""500"">\n', '\n', 'We can also plot the space around the counterfactual. This is computed via PCA of the affinity matrix -- the similarity (Tanimoto of ECFP4) with the base molecule.\n', 'Due to how similarity is calculated, the base is going to be the farthest from all other molecules. Thus your base should fall on the left (or right) extreme of your plot.\n', '\n', '```py\n', 'cfs = exmol.cf_explain(samples)\n', 'exmol.plot_space(samples, cfs)\n', '```\n', '<img alt=""chemical space"" src=""https://raw.githubusercontent.com/ur-whitelab/exmol/main/paper1_CFs/svg_figs/rf-space.png"" width=""600"">\n', '\n', 'Each counterfactual is a Python `dataclass` with information allowing it to be used in your own analysis:\n', '\n', '```py\n', 'print(cfs[1])\n', '```\n', '```\n', '{\n', ""'smiles': 'Cc1onc(-c2ccccc2Cl)c1C(=O)NC1C(=O)N2C1SC(C)(C)C2C',\n"", ""'selfies': '[C][C][O][N][=C][Branch1_1][Branch2_3][C][=C][C][=C][C][=C][Ring1][Branch1_2][Cl][C]\n"", '            [Expl=Ring1][N][C][Branch1_2][C][=O][N][C][C][Branch1_2][C][=O][N][C][Ring1][Branch1_1][S][C]\n', ""            [Branch1_1][C][C][Branch1_1][C][C][C][Ring1][Branch1_3][C]',\n"", ""'similarity': 0.8,\n"", ""'yhat': 1,\n"", ""'index': 1813,\n"", ""'position': array([-7.8032394 ,  0.51781263]),\n"", ""'is_origin': False,\n"", ""'cluster': -1,\n"", ""'label': 'Counterfactual 1'\n"", '}\n', '```\n', '\n', ""We can use the same chemical space to get descriptor attributions for the molecule. Along with `samples`, we also need to specify the `descriptor_type` to get attributions. You can select from `Classic` Rdkit descriptors, `MACCS` fingerprint descriptors, `ECFP` substructure descriptors. The default `descriptor_type` is `MACCS`. If you'd like to use regression coefficients for analysis, specify `return_beta=True`. The descriptor t-statistics are stored in `descriptors.tstats` attribute for the base molecule and can be accessed using `space_tstats = space[0].descriptors.tstats`. `plot_descriptors` saves a plot as shown below in the `output_file`.\n"", '\n', '```py\n', ""beta = exmol.lime_explain(samples, descriptor_type='ECFP', return_beta=True)\n"", ""exmol.plot_descriptors(samples, output_file='ecfp.svg')\n"", '```\n', '<img alt=""ecfp descriptors"" src=""https://raw.githubusercontent.com/ur-whitelab/exmol/main/paper2_LIME/ECFP.svg"" width=""400"">\n', '\n', 'You can use a more typical atom attribution plot as well, although note that some information is lost in this representation.\n', '\n', '```py\n', 'exmol.plot_utils.similarity_map_using_tstats(samples[0])\n', '```\n', '<img alt=""molecule attribution by coloring each atom"" src=""https://raw.githubusercontent.com/ur-whitelab/exmol/main/paper2_LIME/mol-attr.png"">\n', '\n', '\n', 'You can also plot the chemical space colored by fit to see how well the regression fits the original model. To plot by fit, regression coefficients `beta` need to be passed in as an argument.\n', '\n', '```py\n', 'exmol.plot_utils.plot_space_by_fit(\n', '    samples,\n', '    [samples[0]],\n', '    beta=beta,\n', '    mol_size=(300, 250),\n', ""    figure_kwargs={'figsize': (7,5)},\n"", ')\n', '```\n', '<img alt=""chemical space by fit"" src=""https://raw.githubusercontent.com/ur-whitelab/exmol/main/paper2_LIME/space_by_fit.png"" width=""500"">\n', '\n', 'It is also possible to get global attributions for multiple base molecules. For this, the user should create a space around each instance of interest and concatenate these spaces. Then use this joint space to do lime explanations:\n', '\n', '```py\n', ""beta = exmol.lime_explain(joint_space, descriptor_type='ECFP', return_beta=True, multiple_bases=True)\n"", '```\n', '\n', ""`lime_explain()` uses a linear surrogate model for descriptor explanations. You can also use a custom surrogate model instead of a linear model. To do so, just add desired descriptors to the chemical space using the `add_descriptors()` function and then use a custom model on samples to get explanations. For example, add ECFP descriptors using `exmol.add_descriptors(samples, descriptor_type='ECFP')`.\n"", '\n', '## Further Examples\n', '\n', 'You can find more examples by looking at the exact code used to generate all figures from our paper [in the docs webpage](https://ur-whitelab.github.io/exmol/toc.html).\n', '\n', '## Chemical Space\n', '\n', 'When calling `exmol.sample_space` you can pass `preset=<preset>`, which can be\n', 'one of the following:\n', '\n', ""* `'narrow'`: Only one change to molecular structure, reduced set of possible bonds/elements\n"", ""* `'medium'`: Default. One or two changes to molecular structure, reduced set of possible bonds/elements\n"", ""* `'wide'`: One through five changes to molecular structure, large set of possible bonds/elements\n"", ""* `'chemed'`: A restricted set where only pubchem molecules are considered.\n"", '* `\'custom\'`: A restricted set where only molecules provided by the ""data"" key are considered.\n', ""* `'synspace'`: Chemical space is generated by running retro and forward synthesis reactions, so all generaterd molecules are synthetically feasible. Uses [synspace](https://github.com/whitead/synspace) package.\n"", '\n', 'You can also pass `num_samples` as a ""request"" for number of samples. You will typically end up with less due to\n', 'degenerate molecules. See API for complete description.\n', '\n', '## SVG\n', '\n', 'Molecules are by default drawn as PNGs. If you would like to have them drawn as SVGs, call `insert_svg` after calling\n', '`plot_space` or `plot_cf`\n', '\n', '```py\n', 'import skunk\n', 'exmol.plot_cf(exps)\n', 'svg = exmol.insert_svg(exps, mol_fontsize=16)\n', '\n', '# for Jupyter Notebook\n', 'skunk.display(svg)\n', '\n', '# To save to file\n', ""with open('myplot.svg', 'w') as f:\n"", '    f.write(svg)\n', '```\n', '\n', 'This is done with the [skunk🦨 library](https://github.com/whitead/skunk).\n', '\n', '## Disable Progress Bars\n', '\n', 'If `exmol` is being too loud, add `quiet = True` to `sample_space` arguments.\n', '\n', '## API and Docs\n', '\n', '[Read API here](https://ur-whitelab.github.io/exmol/api.html). You should also read the paper (see below) for a more exact\n', 'description of the methods and implementation.\n', '\n', '## Developing\n', '\n', 'This repo uses pre-commit, so after cloning run `pip install -r requirements.txt` and `pre-commit install` prior to committing.\n', '\n', '## Citation\n', '\n', 'For the counterfactual work, please cite [Wellawatte et al.](https://pubs.rsc.org/en/content/articlelanding/2022/sc/d1sc05259d#!divAbstract)\n', '\n', '```bibtex\n', '@Article{wellawatte_seshadri_white_2021,\n', 'author =""Wellawatte, Geemi P. and Seshadri, Aditi and White, Andrew D."",\n', 'title  =""Model agnostic generation of counterfactual explanations for molecules"",\n', 'journal  =""Chem. Sci."",\n', 'year  =""2022"",\n', 'pages  =""-"",\n', 'publisher  =""The Royal Society of Chemistry"",\n', 'doi  =""10.1039/D1SC05259D"",\n', 'url  =""http://dx.doi.org/10.1039/D1SC05259D"",\n', '}\n', '```\n', '\n', 'For the descriptor explanations, please cite [Gandhi et. al.](https://doi.org/10.26434/chemrxiv-2022-v5p6m-v2)\n', '\n', '```bibtex\n', '@Article{gandhi_white_2022,\n', 'place = ""Cambridge"",\n', 'title = ""Explaining structure-activity relationships using locally faithful surrogate models"",\n', 'DOI = ""10.26434/chemrxiv-2022-v5p6m"",\n', 'url = ""https://doi.org/10.26434/chemrxiv-2022-v5p6m""\n', 'journal = ""ChemRxiv"",\n', 'publisher = ""Cambridge Open Engage"",\n', 'author = ""Gandhi, Heta A. and White, Andrew D."",\n', 'year = ""2022""\n', '}\n', '```\n']"
Model Explainability,oegedijk/explainerdashboard,oegedijk,https://api.github.com/repos/oegedijk/explainerdashboard,1668,218,15,"['https://api.github.com/users/oegedijk', 'https://api.github.com/users/oegesam', 'https://api.github.com/users/raybellwaves', 'https://api.github.com/users/mekomlusa', 'https://api.github.com/users/Simon-Free', 'https://api.github.com/users/brandonserna', 'https://api.github.com/users/rajgupt', 'https://api.github.com/users/yanhong-zhao-ef', 'https://api.github.com/users/achimgaedke', 'https://api.github.com/users/absynthe', 'https://api.github.com/users/jenoOvchi', 'https://api.github.com/users/hugocool', 'https://api.github.com/users/haizadtarik', 'https://api.github.com/users/tunayokumus', 'https://api.github.com/users/woochan-jang']",Python,2023-04-24T20:01:48Z,https://raw.githubusercontent.com/oegedijk/explainerdashboard/master/README.md,"['![GitHub Workflow Status (branch)](https://img.shields.io/github/actions/workflow/status/oegedijk/explainerdashboard/explainerdashboard.yml?branch=main)\r\n', '![https://pypi.python.org/pypi/explainerdashboard/](https://img.shields.io/pypi/v/explainerdashboard.svg)\r\n', '![https://anaconda.org/conda-forge/explainerdashboard/](https://anaconda.org/conda-forge/explainerdashboard/badges/version.svg)\r\n', '[![codecov](https://codecov.io/gh/oegedijk/explainerdashboard/branch/master/graph/badge.svg?token=0XU6HNEGBK)](undefined)\r\n', '[![Downloads](https://static.pepy.tech/badge/explainerdashboard)](https://pepy.tech/project/explainerdashboard)\r\n', '\r\n', '# explainerdashboard\r\n', 'by: Oege Dijk\r\n', '\r\n', 'This package makes it convenient to quickly deploy a dashboard web app\r\n', 'that explains the workings of a (scikit-learn compatible) machine \r\n', 'learning model. The dashboard provides interactive plots on model performance, \r\n', 'feature importances, feature contributions to individual predictions, \r\n', '""what if"" analysis,\r\n', 'partial dependence plots, SHAP (interaction) values, visualisation of individual\r\n', 'decision trees, etc. \r\n', '\r\n', 'You can also interactively explore components of the dashboard in a \r\n', 'notebook/colab environment (or just launch a dashboard straight from there). \r\n', 'Or design a dashboard with your own [custom layout](https://explainerdashboard.readthedocs.io/en/latest/buildcustom.html) \r\n', 'and explanations (thanks to the modular design of the library). And you can combine multiple dashboards into\r\n', 'a single [ExplainerHub](https://explainerdashboard.readthedocs.io/en/latest/hub.html).\r\n', '\r\n', 'Dashboards can be exported to static html directly from a running dashboard, or \r\n', 'programmatically as an artifact as part of an automated CI/CD deployment process.\r\n', '\r\n', ' Examples deployed at: [titanicexplainer.herokuapp.com](http://titanicexplainer.herokuapp.com), \r\n', ' detailed documentation at [explainerdashboard.readthedocs.io](http://explainerdashboard.readthedocs.io), \r\n', ' example notebook on how to launch dashboard for different models [here](notebooks/dashboard_examples.ipynb), and an example notebook on how to interact with the explainer object [here](notebooks/explainer_examples.ipynb).\r\n', '\r\n', ' Works with `scikit-learn`, `xgboost`, `catboost`, `lightgbm`, and `skorch` \r\n', ' (sklearn wrapper for tabular PyTorch models) and others.\r\n', '\r\n', ' ## Installation\r\n', '\r\n', 'You can install the package through pip:\r\n', '\r\n', '`pip install explainerdashboard`\r\n', '\r\n', 'or conda-forge:\r\n', '\r\n', '`conda install -c conda-forge explainerdashboard`\r\n', '\r\n', '## Demonstration:\r\n', '\r\n', '![explainerdashboard.gif](explainerdashboard.gif)\r\n', '\r\n', '<!-- [![Dashboard Screenshot](https://i.postimg.cc/Gm8RnKVb/Screenshot-2020-07-01-at-13-25-19.png)](https://postimg.cc/PCj9mWd7) -->\r\n', '(for live demonstration see [titanicexplainer.herokuapp.com](http://titanicexplainer.herokuapp.com))\r\n', '## Background\r\n', '\r\n', 'In a lot of organizations, especially governmental, but with the GDPR also increasingly in private sector, it is becoming more and more important to be able to explain the inner workings of your machine learning algorithms. Customers have to some extent a right to an explanation why they received a certain prediction, and more and more internal and external regulators require it. With recent innovations in explainable AI (e.g. SHAP values) the old black box trope is no longer valid, but it can still take quite a bit of data wrangling and plot manipulation to get the explanations out of a model. This library aims to make this easy.\r\n', '\r\n', 'The goal is manyfold:\r\n', '- Make it easy for data scientists to quickly inspect the workings and performance of their model in a few lines of code\r\n', '- Make it possible for non data scientist stakeholders such as managers, directors, internal and external watchdogs to interactively inspect the inner workings of the model without having to depend on a data scientist to generate every plot and table\r\n', '- Make it easy to build an application that explains individual predictions of your model for customers that ask for an explanation\r\n', ""- Explain the inner workings of the model to the people working (human-in-the-loop) with it so that they gain understanding what the model does and doesn't do. This is important so that they can gain an intuition for when the model is likely missing information and may have to be overruled. \r\n"", '\r\n', '\r\n', 'The library includes:\r\n', '- *Shap values* (i.e. what is the contributions of each feature to each individual prediction?)\r\n', '- *Permutation importances* (how much does the model metric deteriorate when you shuffle a feature?)\r\n', '- *Partial dependence plots* (how does the model prediction change when you vary a single feature?\r\n', '- *Shap interaction values* (decompose the shap value into a direct effect an interaction effects)\r\n', '- For Random Forests and xgboost models: visualisation of individual decision trees\r\n', '- Plus for classifiers: precision plots, confusion matrix, ROC AUC plot, PR AUC plot, etc\r\n', '- For regression models: goodness-of-fit plots, residual plots, etc. \r\n', '\r\n', 'The library is designed to be modular so that it should be easy to design your own interactive dashboards with plotly dash, with most of the work of calculating and formatting data, and rendering plots and tables handled by `explainerdashboard`, so that you can focus on the layout\r\n', 'and project specific textual explanations. (i.e. design it so that it will be interpretable for business users in your organization, not just data scientists)\r\n', '\r\n', 'Alternatively, there is a built-in standard dashboard with pre-built tabs (that you can switch off individually)\r\n', '\r\n', '## Examples of use\r\n', '\r\n', 'Fitting a model, building the explainer object, building the dashboard, and then running it can be as simple as:\r\n', '\r\n', '```python\r\n', 'ExplainerDashboard(ClassifierExplainer(RandomForestClassifier().fit(X_train, y_train), X_test, y_test)).run()\r\n', '```\r\n', '\r\n', 'Below a multi-line example, adding a few extra parameters. \r\n', 'You can group onehot encoded categorical variables together using the `cats` \r\n', 'parameter. You can either pass a dict specifying a list of onehot cols per\r\n', 'categorical feature, or if you encode using e.g. \r\n', ""`pd.get_dummies(df.Name, prefix=['Name'])` (resulting in column names `'Name_Adam', 'Name_Bob'`) \r\n"", ""you can simply pass the prefix `'Name'`:\r\n"", '\r\n', '```python\r\n', 'from sklearn.ensemble import RandomForestClassifier\r\n', 'from explainerdashboard import ClassifierExplainer, ExplainerDashboard\r\n', 'from explainerdashboard.datasets import titanic_survive, titanic_names\r\n', '\r\n', 'feature_descriptions = {\r\n', '    ""Sex"": ""Gender of passenger"",\r\n', '    ""Gender"": ""Gender of passenger"",\r\n', '    ""Deck"": ""The deck the passenger had their cabin on"",\r\n', '    ""PassengerClass"": ""The class of the ticket: 1st, 2nd or 3rd class"",\r\n', '    ""Fare"": ""The amount of money people paid"", \r\n', '    ""Embarked"": ""the port where the passenger boarded the Titanic. Either Southampton, Cherbourg or Queenstown"",\r\n', '    ""Age"": ""Age of the passenger"",\r\n', '    ""No_of_siblings_plus_spouses_on_board"": ""The sum of the number of siblings plus the number of spouses on board"",\r\n', '    ""No_of_parents_plus_children_on_board"" : ""The sum of the number of parents plus the number of children on board"",\r\n', '}\r\n', '\r\n', 'X_train, y_train, X_test, y_test = titanic_survive()\r\n', 'train_names, test_names = titanic_names()\r\n', 'model = RandomForestClassifier(n_estimators=50, max_depth=5)\r\n', 'model.fit(X_train, y_train)\r\n', '\r\n', 'explainer = ClassifierExplainer(model, X_test, y_test, \r\n', ""                                cats=['Deck', 'Embarked',\r\n"", ""                                    {'Gender': ['Sex_male', 'Sex_female', 'Sex_nan']}],\r\n"", ""                                cats_notencoded={'Embarked': 'Stowaway'}, # defaults to 'NOT_ENCODED'\r\n"", '                                descriptions=feature_descriptions, # adds a table and hover labels to dashboard\r\n', ""                                labels=['Not survived', 'Survived'], # defaults to ['0', '1', etc]\r\n"", '                                idxs = test_names, # defaults to X.index\r\n', '                                index_name = ""Passenger"", # defaults to X.index.name\r\n', '                                target = ""Survival"", # defaults to y.name\r\n', '                                )\r\n', '\r\n', 'db = ExplainerDashboard(explainer, \r\n', '                        title=""Titanic Explainer"", # defaults to ""Model Explainer""\r\n', '                        shap_interaction=False, # you can switch off tabs with bools\r\n', '                        )\r\n', 'db.run(port=8050)\r\n', '```\r\n', '\r\n', 'For a regression model you can also pass the units of the target variable (e.g. \r\n', 'dollars):\r\n', '\r\n', '```python\r\n', 'X_train, y_train, X_test, y_test = titanic_fare()\r\n', 'model = RandomForestRegressor().fit(X_train, y_train)\r\n', '\r\n', 'explainer = RegressionExplainer(model, X_test, y_test, \r\n', ""                                cats=['Deck', 'Embarked', 'Sex'],\r\n"", '                                descriptions=feature_descriptions, \r\n', '                                units = ""$"", # defaults to """"\r\n', '                                )\r\n', '\r\n', 'ExplainerDashboard(explainer).run()\r\n', '```\r\n', '\r\n', '`y_test` is actually optional, although some parts of the dashboard like performance\r\n', 'metrics will obviously not be available: `ExplainerDashboard(ClassifierExplainer(model, X_test)).run()`.\r\n', '\r\n', ""You can export a dashboard to static html with `db.save_html('dashboard.html')`.\r\n"", '\r\n', 'For a simplified single page dashboard try `ExplainerDashboard(explainer, simple=True)`.\r\n', '\r\n', '<details><summary>Show simplified dashboard screenshot</summary>\r\n', '<p>\r\n', '\r\n', '\r\n', '![docs/source/screenshots/simple_classifier_dashboard.png](docs/source/screenshots/simple_classifier_dashboard.png)\r\n', '\r\n', '</p>\r\n', '</details>\r\n', '<p></p>\r\n', '\r\n', '### ExplainerHub\r\n', '\r\n', 'You can combine multiple dashboards and host them in a single place using \r\n', '[ExplainerHub](https://explainerdashboard.readthedocs.io/en/latest/hub.html):\r\n', '\r\n', '```python\r\n', 'db1 = ExplainerDashboard(explainer1, title=""Classifier Explainer"", \r\n', '         description=""Model predicting survival on H.M.S. Titanic"")\r\n', 'db2 = ExplainerDashboard(explainer2, title=""Regression Explainer"",\r\n', '         description=""Model predicting ticket price on H.M.S. Titanic"")\r\n', 'hub = ExplainerHub([db1, db2])\r\n', 'hub.run()\r\n', '```\r\n', '\r\n', 'You can adjust titles and descriptions, manage users and logins, store and load \r\n', 'from config, manage the hub through a CLI and more. See the \r\n', '[ExplainerHub documentation](https://explainerdashboard.readthedocs.io/en/latest/hub.html).\r\n', '\r\n', '<details><summary>Show ExplainerHub screenshot</summary>\r\n', '<p>\r\n', '\r\n', '\r\n', '![docs/source/screenshots/explainerhub.png](docs/source/screenshots/explainerhub.png)\r\n', '\r\n', '</p>\r\n', '</details>\r\n', '<p></p>\r\n', '\r\n', '\r\n', '### Dealing with slow calculations\r\n', '\r\n', 'Some of the calculations for the dashboard such as calculating SHAP (interaction) values\r\n', 'and permutation importances can be slow for large datasets and complicated models. \r\n', 'There are a few tricks to make this less painful:\r\n', '\r\n', '1. Switching off the interactions tab (`shap_interaction=False`) and disabling\r\n', '    permutation importances (`no_permutations=True`). Especially SHAP interaction\r\n', '    values can be very slow to calculate, and often are not needed for analysis.\r\n', '    For permutation importances you can set the `n_jobs` parameter to speed up\r\n', '    the calculation in parallel.\r\n', '2. Storing the explainer. The calculated properties are only calculated once\r\n', '    for each instance, however each time when you instantiate a new explainer\r\n', '    instance they will have to be recalculated. You can store them with\r\n', '    `explainer.dump(""explainer.joblib"")` and load with e.g. \r\n', '    `ClassifierExplainer.from_file(""explainer.joblib"")`. All calculated properties\r\n', '    are stored along with the explainer.\r\n', '3. Using a smaller (test) dataset, or using smaller decision trees. \r\n', '    TreeShap computational complexity is `O(TLD^2)`, where `T` is the \r\n', '    number of trees, `L` is the maximum number of leaves in any tree and \r\n', '    `D` the maximal depth of any tree. So reducing the number of leaves or average\r\n', '    depth in the decision tree can really speed up SHAP calculations.\r\n', '4. Pre-computing shap values. Perhaps you already have calculated the shap values\r\n', '    somewhere, or you can calculate them off on a giant cluster somewhere, or\r\n', '    your model supports [GPU generated shap values](https://github.com/rapidsai/gputreeshap). \r\n', '    You can simply add these pre-calculated shap values to the explainer \r\n', '    with `explainer.set_shap_values()` and `explainer.set_shap_interaction_values()` methods.\r\n', '5. Plotting only a random sample of points. When you have a lots of observations,\r\n', '    simply rendering the plots may get slow as well. You can pass the `plot_sample`\r\n', '    parameter to render a (different each time) random sample of observations\r\n', '    for the various scatter plots in the dashboard. E.g.: \r\n', '    `ExplainerDashboard(explainer, plot_sample=1000).run()`\r\n', '\r\n', '## Launching from within a notebook\r\n', '\r\n', 'When working inside Jupyter or Google Colab you can use \r\n', ""`ExplainerDashboard(mode='inline')`, `ExplainerDashboard(mode='external')` or\r\n"", ""`ExplainerDashboard(mode='jupyterlab')`, to run the dashboard inline in the notebook,\r\n"", ""or in a seperate tab but keep the notebook interactive. (`db.run(mode='inline')` \r\n"", 'now also works)\r\n', '\r\n', 'There is also a specific interface for quickly displaying interactive components\r\n', 'inline in your notebook: `InlineExplainer()`. For example you can use \r\n', '`InlineExplainer(explainer).shap.dependence()` to display the shap dependence\r\n', 'component interactively in your notebook output cell.\r\n', '\r\n', '## Command line tool\r\n', '\r\n', 'You can store explainers to disk with `explainer.dump(""explainer.joblib"")`\r\n', 'and then run them from the command-line:\r\n', '\r\n', '```bash\r\n', '$ explainerdashboard run explainer.joblib\r\n', '```\r\n', '\r\n', 'Or store the full configuration of a dashboard to `.yaml` with e.g.\r\n', '`dashboard.to_yaml(""dashboard.yaml"", explainerfile=""explainer.joblib"", dump_explainer=True)` and run it with:\r\n', '\r\n', '```bash\r\n', '$ explainerdashboard run dashboard.yaml\r\n', '```\r\n', '\r\n', 'You can also build explainers from the commandline with `explainerdashboard build`.\r\n', 'See [explainerdashboard CLI documentation](https://explainerdashboard.readthedocs.io/en/latest/cli.html)\r\n', 'for details. \r\n', '\r\n', '## Customizing your dashboard\r\n', '\r\n', 'The dashboard is highly modular and customizable so that you can adjust it your\r\n', 'own needs and project. \r\n', '\r\n', '### Changing bootstrap theme\r\n', '\r\n', 'You can change the bootstrap theme by passing a link to the appropriate css\r\n', 'file. You can use the convenient [themes](https://dash-bootstrap-components.opensource.faculty.ai/docs/themes/) module of \r\n', '[dash_bootstrap_components](https://dash-bootstrap-components.opensource.faculty.ai/docs/) to generate\r\n', 'the css url for you:\r\n', '\r\n', '```python\r\n', 'import dash_bootstrap_components as dbc\r\n', '\r\n', 'ExplainerDashboard(explainer, bootstrap=dbc.themes.FLATLY).run()\r\n', '```\r\n', '\r\n', 'See the [dbc themes documentation](https://dash-bootstrap-components.opensource.faculty.ai/docs/themes/)\r\n', 'and [bootwatch website](https://bootswatch.com/) for the different themes that are supported.\r\n', '\r\n', '### Switching off tabs\r\n', '\r\n', 'You can switch off individual tabs using boolean flags. This also makes sure\r\n', ""that expensive calculations for that tab don't get executed:\r\n"", '\r\n', '```python\r\n', 'ExplainerDashboard(explainer,\r\n', '                    importances=False,\r\n', '                    model_summary=True,\r\n', '                    contributions=True,\r\n', '                    whatif=True,\r\n', '                    shap_dependence=True,\r\n', '                    shap_interaction=False,\r\n', '                    decision_trees=True)\r\n', '```\r\n', '\r\n', '### Hiding components\r\n', '\r\n', 'You can also hide individual components on the various tabs:\r\n', '\r\n', '```python\r\n', '    ExplainerDashboard(explainer, \r\n', '        # importances tab:\r\n', '        hide_importances=True,\r\n', '        # classification stats tab:\r\n', '        hide_globalcutoff=True, hide_modelsummary=True, \r\n', '        hide_confusionmatrix=True, hide_precision=True, \r\n', '        hide_classification=True, hide_rocauc=True, \r\n', '        hide_prauc=True, hide_liftcurve=True, hide_cumprecision=True,\r\n', '        # regression stats tab:\r\n', '        # hide_modelsummary=True, \r\n', '        hide_predsvsactual=True, hide_residuals=True, \r\n', '        hide_regvscol=True,\r\n', '        # individual predictions tab:\r\n', '        hide_predindexselector=True, hide_predictionsummary=True,\r\n', '        hide_contributiongraph=True, hide_pdp=True, \r\n', '        hide_contributiontable=True,\r\n', '        # whatif tab:\r\n', '        hide_whatifindexselector=True, hide_whatifprediction=True,\r\n', '        hide_inputeditor=True, hide_whatifcontributiongraph=True, \r\n', '        hide_whatifcontributiontable=True, hide_whatifpdp=True,\r\n', '        # shap dependence tab:\r\n', '        hide_shapsummary=True, hide_shapdependence=True,\r\n', '        # shap interactions tab:\r\n', '        hide_interactionsummary=True, hide_interactiondependence=True,\r\n', '        # decisiontrees tab:\r\n', '        hide_treeindexselector=True, hide_treesgraph=True, \r\n', '        hide_treepathtable=True, hide_treepathgraph=True,\r\n', '        ).run()\r\n', '```\r\n', '\r\n', '### Hiding toggles and dropdowns inside components\r\n', '\r\n', 'You can also hide individual toggles and dropdowns using `**kwargs`. However they\r\n', 'are not individually targeted, so if you pass `hide_cats=True` then the group\r\n', 'cats toggle will be hidden on every component that has one:\r\n', '\r\n', '```python\r\n', 'ExplainerDashboard(explainer, \r\n', '                    no_permutations=True, # do not show or calculate permutation importances\r\n', '                    hide_poweredby=True, # hide the poweredby:explainerdashboard footer\r\n', ""                    hide_popout=True, # hide the 'popout' button from each graph\r\n"", '                    hide_depth=True, # hide the depth (no of features) dropdown\r\n', '                    hide_sort=True, # hide sort type dropdown in contributions graph/table\r\n', '                    hide_orientation=True, # hide orientation dropdown in contributions graph/table\r\n', '                    hide_type=True, # hide shap/permutation toggle on ImportancesComponent \r\n', '                    hide_dropna=True, # hide dropna toggle on pdp component\r\n', '                    hide_sample=True, # hide sample size input on pdp component\r\n', '                    hide_gridlines=True, # hide gridlines on pdp component\r\n', '                    hide_gridpoints=True, # hide gridpoints input on pdp component\r\n', '                    hide_cats_sort=True, # hide the sorting option for categorical features\r\n', '                    hide_cutoff=True, # hide cutoff selector on classification components\r\n', '                    hide_percentage=True, # hide percentage toggle on classificaiton components\r\n', '                    hide_log_x=True, # hide x-axis logs toggle on regression plots\r\n', '                    hide_log_y=True, # hide y-axis logs toggle on regression plots\r\n', '                    hide_ratio=True, # hide the residuals type dropdown\r\n', '                    hide_points=True, # hide the show violin scatter markers toggle\r\n', '                    hide_winsor=True, # hide the winsorize input\r\n', '                    hide_wizard=True, # hide the wizard toggle in lift curve component\r\n', '                    hide_range=True, # hide the range subscript on feature input\r\n', ""                    hide_star_explanation=True, # hide the '* indicates observed label` text\r\n"", ')\r\n', '```\r\n', '\r\n', '### Setting default values\r\n', '\r\n', 'You can also set default values for the various dropdowns and toggles. \r\n', 'All the components with their parameters can be found [in the documentation](https://explainerdashboard.readthedocs.io/en/latest/components.html).\r\n', 'Some examples of useful parameters to pass:\r\n', '\r\n', '```python\r\n', 'ExplainerDashboard(explainer, \r\n', '                    higher_is_better=False, # flip green and red in contributions graph\r\n', '                    n_input_cols=3, # divide feature inputs into 3 columns on what if tab\r\n', ""                    col='Fare', # initial feature in shap graphs\r\n"", ""                    color_col='Age', # color feature in shap dependence graph\r\n"", ""                    interact_col='Age', # interaction feature in shap interaction\r\n"", '                    depth=5, # only show top 5 features\r\n', ""                    sort = 'low-to-high', # sort features from lowest shap to highest in contributions graph/table\r\n"", '                    cats_topx=3, # show only the top 3 categories for categorical features\r\n', ""                    cats_sort='alphabet', # short categorical features alphabetically\r\n"", ""                    orientation='horizontal', # horizontal bars in contributions graph\r\n"", ""                    index='Rugg, Miss. Emily', # initial index to display\r\n"", ""                    pdp_col='Fare', # initial pdp feature\r\n"", '                    cutoff=0.8, # cutoff for classification plots\r\n', '                    round=2 # rounding to apply to floats\r\n', ""                    show_metrics=['accuracy', 'f1', custom_metric] # only show certain metrics \r\n"", '                    plot_sample=1000, # only display a 1000 random markers in scatter plots\r\n', '                    )\r\n', '```\r\n', '\r\n', '\r\n', '### Designing your own layout\r\n', '\r\n', 'All the components in the dashboard are modular and re-usable, which means that \r\n', 'you can build your own custom [dash](https://dash.plotly.com/) dashboards \r\n', 'around them.\r\n', '\r\n', 'By using the built-in `ExplainerComponent` class it is easy to build your\r\n', 'own layouts, with just a bare minimum of knowledge of HTML and [bootstrap](https://dash-bootstrap-components.opensource.faculty.ai/docs/quickstart/). For\r\n', 'example if you only wanted to display the `ConfusionMatrixComponent` and \r\n', '`ShapContributionsGraphComponent`, but hide\r\n', 'a few toggles:\r\n', '\r\n', '```python\r\n', 'from explainerdashboard.custom import *\r\n', '\r\n', 'class CustomDashboard(ExplainerComponent):\r\n', '    def __init__(self, explainer, name=None):\r\n', '        super().__init__(explainer, title=""Custom Dashboard"")\r\n', '        self.confusion = ConfusionMatrixComponent(explainer, name=self.name+""cm"",\r\n', '                            hide_selector=True, hide_percentage=True,\r\n', '                            cutoff=0.75)\r\n', '        self.contrib = ShapContributionsGraphComponent(explainer, name=self.name+""contrib"",\r\n', '                            hide_selector=True, hide_cats=True, \r\n', '                            hide_depth=True, hide_sort=True,\r\n', ""                            index='Rugg, Miss. Emily')\r\n"", '        \r\n', '    def layout(self):\r\n', '        return dbc.Container([\r\n', '            dbc.Row([\r\n', '                dbc.Col([\r\n', '                    html.H1(""Custom Demonstration:""),\r\n', '                    html.H3(""How to build your own layout using ExplainerComponents."")\r\n', '                ])\r\n', '            ]),\r\n', '            dbc.Row([\r\n', '                dbc.Col([\r\n', '                    self.confusion.layout(),\r\n', '                ]),\r\n', '                dbc.Col([\r\n', '                    self.contrib.layout(),\r\n', '                ])\r\n', '            ])\r\n', '        ])\r\n', '\r\n', 'db = ExplainerDashboard(explainer, CustomDashboard, hide_header=True).run()\r\n', '```\r\n', '\r\n', '<details><summary>Show example custom dashboard screenshot</summary>\r\n', '<p>\r\n', '\r\n', '\r\n', '![docs/source/screenshots/custom_dashboard.png](docs/source/screenshots/custom_dashboard.png)\r\n', '\r\n', '</p>\r\n', '\r\n', '</details>\r\n', '<p></p>\r\n', '\r\n', '\r\n', 'You can use this to define your own layouts, specifically tailored to your\r\n', 'own model, project and needs. You can use the [ExplainerComposites](https://github.com/oegedijk/explainerdashboard/blob/master/explainerdashboard/dashboard_components/composites.py) that\r\n', 'are used for the tabs of the default dashboard as a starting point, and edit\r\n', 'them to reorganize components, add text, etc. \r\n', 'See [custom dashboard documentation](https://explainerdashboard.readthedocs.io/en/latest/custom.html)\r\n', 'for more details. A deployed custom dashboard can be found [here](http://titanicexplainer.herokuapp.com/custom/)([source code](https://github.com/oegedijk/explainingtitanic/blob/master/buildcustom.py)).\r\n', '\r\n', '## Deployment\r\n', '\r\n', 'If you wish to use e.g. `gunicorn` or `waitress` to deploy the dashboard you should add \r\n', '`app = db.flask_server()` to your code to expose the Flask server. You can then \r\n', 'start the server with e.g. `gunicorn dashboard:app` \r\n', '(assuming the file you defined the dashboard in was called `dashboard.py`). \r\n', 'See also the [ExplainerDashboard section](https://explainerdashboard.readthedocs.io/en/latest/dashboards.html) \r\n', 'and the [deployment section of the documentation](https://explainerdashboard.readthedocs.io/en/latest/deployment.html).\r\n', '\r\n', 'It can be helpful to store your `explainer` and dashboard layout to disk, and \r\n', 'then reload, e.g.:\r\n', '\r\n', '**generate_dashboard.py**:\r\n', '```python\r\n', 'from explainerdashboard import ClassifierExplainer, ExplainerDashboard\r\n', 'from explainerdashboard.custom import *\r\n', '\r\n', 'explainer = ClassifierExplainer(model, X_test, y_test)\r\n', '\r\n', '# building an ExplainerDashboard ensures that all necessary properties \r\n', '# get calculated:\r\n', 'db = ExplainerDashboard(explainer, [ShapDependenceComposite, WhatIfComposite],\r\n', ""                        title='Awesome Dashboard', hide_whatifpdp=True)\r\n"", '\r\n', '# store both the explainer and the dashboard configuration:\r\n', 'db.to_yaml(""dashboard.yaml"", explainerfile=""explainer.joblib"", dump_explainer=True)\r\n', '```\r\n', '\r\n', 'You can then reload it in **dashboard.py**:\r\n', '```python\r\n', 'from explainerdashboard import ClassifierExplainer, ExplainerDashboard\r\n', '\r\n', '# you can override params during load from_config:\r\n', 'db = ExplainerDashboard.from_config(""dashboard.yaml"", title=""Awesomer Title"")\r\n', '\r\n', 'app = db.flask_server()\r\n', '```\r\n', '\r\n', 'And then run it with:\r\n', '\r\n', '```sh\r\n', '    $ gunicorn dashboard:app\r\n', '```\r\n', '\r\n', 'or with waitress (also works on Windows):\r\n', '\r\n', '```sh\r\n', '    $ waitress-serve dashboard:app\r\n', '```\r\n', '\r\n', '### Minimizing memory usage\r\n', '\r\n', 'When you deploy a dashboard with a dataset with a large number of rows (`n`) and columns (`m`),\r\n', 'the memory usage of the dashboard can be substantial. You can check the (approximate)\r\n', 'memory usage with `explainer.memory_usage()`. (as a side note: if you have lots\r\n', 'of rows, you probably want to set the `plot_sample` parameter as well)\r\n', '\r\n', 'In order to reduce the memory footprint there are a number of things you can do:\r\n', '\r\n', '1. Not including shap interaction tab: shap interaction values are shape (`n*m*m`),\r\n', '    so can take a subtantial amount of memory.\r\n', ""2. Setting a lower precision. By default shap values are stored as `'float64'`,\r\n"", ""    but you can store them as `'float32'` instead and save half the space:\r\n"", ""    ```ClassifierExplainer(model, X_test, y_test, precision='float32')```. You \r\n"", '    can also set a lower precision on your `X_test` dataset yourself ofcourse.\r\n', '3. For multi class classifier, by default `ClassifierExplainer` calculates\r\n', ""    shap values for all classes. If you're only interested in a single class\r\n"", '    you can drop the other shap values: `explainer.keep_shap_pos_label_only(pos_label)`\r\n', '4. Storing data externally. You can for example only store a subset of 10.000 rows in\r\n', '    the explainer itself (enough to generate importance and dependence plots),\r\n', '    and store the rest of your millions of rows of input data in an external file \r\n', '    or database:\r\n', '    - with `explainer.set_X_row_func()` you can set a function that takes \r\n', '        an `index` as argument and returns a single row dataframe with model\r\n', '        compatible input data for that index. This function can include a query\r\n', '        to a database or fileread. \r\n', '    - with `explainer.set_y_func()` you can set a function that takes \r\n', '        and `index` as argument and returns the observed outcome `y` for\r\n', '        that index.\r\n', '    - with `explainer.set_index_list_func()` you can set a function \r\n', '        that returns a list of available indexes that can be queried. Only gets\r\n', '        called upon start of the dashboard.\r\n', '\r\n', '    If you have a very large number of indexes and the user is able to look\r\n', '    them up elsewhere, you can also replace the index dropdowns with a simple free\r\n', '    text field with `index_dropdown=False`. Only valid indexes (i.e. in the \r\n', '    `get_index_list()` list) get propagated\r\n', '    to other components by default, but this can be overriden with `index_check=False`. \r\n', '    Instead of an ``index_list_func`` you can also set an \r\n', '    ``explainer.set_index_check_func(func)`` which should return a bool whether\r\n', '    the ``index`` exists or not. \r\n', '\r\n', '    Important: these function can be called multiple times by multiple independent\r\n', '    components, so probably best to implement some kind of caching functionality.\r\n', '    The functions you pass can be also methods, so you have access to all of the\r\n', '    internals of the explainer.\r\n', '    \r\n', '\r\n', '## Documentation\r\n', '\r\n', 'Documentation can be found at [explainerdashboard.readthedocs.io](https://explainerdashboard.readthedocs.io/en/latest/).\r\n', '\r\n', 'Example notebook on how to launch dashboards for different model types here: [dashboard_examples.ipynb](notebooks/dashboard_examples.ipynb).\r\n', '\r\n', 'Example notebook on how to interact with the explainer object here: [explainer_examples.ipynb](notebooks/explainer_examples.ipynb).\r\n', '\r\n', 'Example notebook on how to design a custom dashboard: [custom_examples.ipynb](notebooks/custom_examples.ipynb).\r\n', '\r\n', '\r\n', '\r\n', '## Deployed example:\r\n', '\r\n', 'You can find an example dashboard at [titanicexplainer.herokuapp.com](http://titanicexplainer.herokuapp.com) \r\n', '\r\n', '(source code at [https://github.com/oegedijk/explainingtitanic](https://github.com/oegedijk/explainingtitanic))\r\n', '\r\n', '## Citation:\r\n', '\r\n', 'A doi can be found at [zenodo](https://zenodo.org/record/7633294)\r\n']"
Model Explainability,llSourcell/world_models,llSourcell,https://api.github.com/repos/llSourcell/world_models,79,27,1,['https://api.github.com/users/llSourcell'],Python,2023-02-11T01:56:44Z,https://raw.githubusercontent.com/llSourcell/world_models/master/README.md,"['# Overview\n', '\n', 'This is the code for [this](https://youtu.be/rV1SIOJzj0c) video on Youtube by Siraj Raval.  An implementation of the ideas from this paper https://arxiv.org/pdf/1803.10122.pdf Code base adapted from https://github.com/hardmaru/estool . For full installation and run instructions see this blog post: https://applied-data.science/blog/hallucinogenic-deep-reinforcement-learning-using-python-and-keras\n', '\n', '## Credits\n', '\n', 'Credits for this code go to [AppliedDataSciencePartners](https://github.com/AppliedDataSciencePartners)\n']"
Model Explainability,xiangwang1223/tree_enhanced_embedding_model,xiangwang1223,https://api.github.com/repos/xiangwang1223/tree_enhanced_embedding_model,73,14,1,['https://api.github.com/users/xiangwang1223'],,2023-01-15T06:59:25Z,https://raw.githubusercontent.com/xiangwang1223/tree_enhanced_embedding_model/master/README.md,"['# Tree-enhanced Embedding Model\n', 'This is our project for the paper:\n', "">Xiang Wang, Xiangnan He, Fuli Feng, Liqiang Nie and Tat-Seng Chua (2018). [TEM: Tree-enhanced Embedding Model for Explainable Recommendation](https://dl.acm.org/citation.cfm?id=3178876.3186066). In WWW'18, Lyon, France, April 23–27, 2018.\n"", '\n', 'Author: Dr. Xiang Wang (xiangwang at u.nus.edu)\n', '\n', '## Introduction\n', 'Tree-enhanced Embedding Mode (TEM) is a new recommendation framework, which combines the strong representation ability of embeddingbased and interpretability of tree-based models. At its core is an easy-to-interpret decision-tree and attention network, making the recommendation process fully transparent and explainable.\n', '\n', '## Citation \n', 'If you want to use our codes and datasets in your research, please cite:\n', '```\n', '@inproceedings{TEM2018,\n', '  author    = {Xiang Wang and\n', '               Xiangnan He and\n', '               Fuli Feng and\n', '               Liqiang Nie and\n', '               Tat{-}Seng Chua},\n', '  title     = {{TEM:} Tree-enhanced Embedding Model for Explainable Recommendation},\n', '  booktitle = {{WWW}},\n', '  pages     = {1543--1552},\n', '  year      = {2018},\n', '}\n', '```\n', '## Codes\n', 'We are finding license suitable to release this software. Currently codes are under request and will be released later.\n', '\n', '## Dataset\n', 'We provide two rich-attribute datasets: London-Attractions (LON-A) and New-York-City-Restaurant (NYC-R) datasets, which have user profiles and item attributes, and are collected from [TripAdvisor](https://www.tripadvisor.com.sg/).\n', '* `London_Attractions_Complete_Review.csv`\n', '  * All positive instances.\n', ""  * Each line is a review, where the fields of user profiles and item attributes start with 'u' and 'i', respectively.\n"", '\n', '* `New_York_City_Restaurant_Complete_Review.csv`\n', '  * All positive instances.\n', ""  * Each line is a review, where the fields of user profiles and item attributes start with 'u' and 'i', respectively.\n""]"
Model Explainability,smazzanti/tds_black_box_models_more_explainable,smazzanti,https://api.github.com/repos/smazzanti/tds_black_box_models_more_explainable,67,41,1,['https://api.github.com/users/smazzanti'],Python,2023-04-12T09:22:56Z,https://raw.githubusercontent.com/smazzanti/tds_black_box_models_more_explainable/master/README.md,"['# Black-Box models are actually more explainable than a Logistic Regression\n', '\n', 'Jupyter Notebook used for writing the article ""Black-Box models are actually more explainable than a Logistic Regression"" published in Towards Data Science: https://towardsdatascience.com/black-box-models-are-actually-more-explainable-than-a-logistic-regression-f263c22795d\n']"
Model Explainability,mark-watson/cancer-deep-learning-model,mark-watson,https://api.github.com/repos/mark-watson/cancer-deep-learning-model,68,47,1,['https://api.github.com/users/mark-watson'],Python,2023-04-12T11:44:09Z,https://raw.githubusercontent.com/mark-watson/cancer-deep-learning-model/master/README.md,"['# Keras Deep Neural Network using Breast Cancer Data with Explanation of Predictions\n', '\n', 'This model is trained on 497 training examples and is tested for accuracy on 151 different testing examples. The accuracy is about 97%.\n', '\n', 'The Python example code provides a simple example of using CSV data files with TensorFlow and training a model with three hidden layers.\n', '\n', 'I assume that you have Keras and TensorFlow installed.\n', '\n', '## Donate on Patreon to support all of my projects\n', '\n', 'Please visit [https://www.patreon.com/markwatson](https://www.patreon.com/markwatson) and sign up to donate $1/month\n', '\n', '## Uses the IntegratedVarients library to explain predictions made by a trained model\n', '\n', 'Please [read this excellent paper](https://arxiv.org/pdf/1703.01365.pdf)\n', 'by Mukund Sundararajan, Ankur Taly, and Qiqi Yan\n', '\n', 'When making a prediction, you can get a scaling of which input features most contributed to a classifiaction made by the model.\n', '\n', 'For example:\n', '\n', '````````\n', '** Contributions to classification for sample type  benign sample  **\n', '\t Clump Thickness :\t -15\n', '\t Uniformity of Cell Size :\t 19\n', '\t Uniformity of Cell Shape :\t -5\n', '\t Marginal Adhesion :\t -15\n', '\t Single Epithelial Cell Size :\t -100\n', '\t Bare Nuclei :\t -5\n', '\t Bland Chromatin :\t -70\n', '\t Normal Nucleoli :\t -5\n', '\t Mitoses :\t 9\n', '** Contributions to classification for sample type  malignant sample  **\n', '\t Clump Thickness :\t 27\n', '\t Uniformity of Cell Size :\t 8\n', '\t Uniformity of Cell Shape :\t 15\n', '\t Marginal Adhesion :\t -21\n', '\t Single Epithelial Cell Size :\t -8\n', '\t Bare Nuclei :\t 100\n', '\t Bland Chromatin :\t 20\n', '\t Normal Nucleoli :\t 5\n', '\t Mitoses :\t 3\n', '````````\n', '## A version of this code was used in a book I wrote\n', '\n', 'The [github repository for my book ""Introduction to Cognitive Computing""](https://github.com/mark-watson/cognitive-computing-book)\n', 'contains an older version of this example.\n', '\n', '# Universary of Wisconcin Cancer Data\n', '\n', '````````\n', '- 0 Clump Thickness               1 - 10\n', '- 1 Uniformity of Cell Size       1 - 10\n', '- 2 Uniformity of Cell Shape      1 - 10\n', '- 3 Marginal Adhesion             1 - 10\n', '- 4 Single Epithelial Cell Size   1 - 10\n', '- 5 Bare Nuclei                   1 - 10\n', '- 6 Bland Chromatin               1 - 10\n', '- 7 Normal Nucleoli               1 - 10\n', '- 8 Mitoses                       1 - 10\n', '- 9 Class (0 for benign, 1 for malignant)\n', '````````\n', '\n', 'I modified the original data slightly by removing the randomized patient ID and changing the target class values from (2,4) to (0,1) for (no cancer, cancer).\n', '\n', 'The CSV file loader in the TensorFlow contrib learn library expects header lines. The following is the first few lines of train.csv:\n', '\n', '````````\n', '10,10,10,8,6,1,8,9,1,1\n', '6,2,1,1,1,1,7,1,1,0\n', '2,5,3,3,6,7,7,5,1,1\n', '````````\n', '\n', 'The last value on each input line is 0 or 1 indicating the target classification.\n', '\n', 'This example just has 2 target classifications, but you can have any number. Label target class values 0, 1, 2, etc.\n']"
Model Explainability,koriavinash1/BioExp,koriavinash1,https://api.github.com/repos/koriavinash1/BioExp,26,6,3,"['https://api.github.com/users/parthnatekar', 'https://api.github.com/users/zsfVishnu', 'https://api.github.com/users/koriavinash1']",Python,2023-02-19T07:04:41Z,https://raw.githubusercontent.com/koriavinash1/BioExp/master/README.md,"['# BioExp\n', '[![Build Status](https://travis-ci.org/koriavinash1/BioExp.svg?branch=master)](https://travis-ci.org/koriavinash1/BioExp)\n', '[![Documentation Status](https://readthedocs.org/projects/bioexp/badge/?version=latest)](https://bioexp.readthedocs.io/en/latest/?badge=latest)\n', '[![PyPI version](https://badge.fury.io/py/BioExp.svg)](https://badge.fury.io/py/BioExp)\n', '[![Downloads](https://pepy.tech/badge/bioexp)](https://pepy.tech/project/bioexp)\n', '[![arXiv](https://img.shields.io/badge/arXiv-2008.06457-<COLOR>.svg)](https://arxiv.org/abs/2008.06457)\n', '[![arXiv](https://img.shields.io/badge/arXiv-1909.01498-<COLOR>.svg)](https://arxiv.org/abs/1909.01498)\n', '[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n', '\n', 'Explaining Deep Learning Models which perform various image processing tasks in the medical images and natural images.\n', '\n', '# Features\n', '\n', '- [x] Dissection Analysis\n', '- [x] Ablation Analysis\n', '- [x] Uncertainity Analysis\n', '   - [x] Epistemic Uncertainty using Bayesian Dropout\n', '   - [x] Aleatoric Uncertainty using Test Time Augmentation\n', '- [x] Activation Maximization\n', '- [x] CAM Analysis\n', '- [x] RCT on input and concept space \n', '- [x] Concept generation clustering analysis\n', '   - [x] wts based clustering\n', '   - [x] feature based clustering\n', '- [x] Concept Identification\n', '  - [x] Dissection based\n', '  - [x] Flow based\n', '- [x] Causal Graph \n', '- [x] Inference Methods\n', '- [ ] Counterfactuals on Visual Trails\n', '- [ ] Counterfactual Generation\n', '- [ ] Ante-hoc methods (Meta-Causation)\n', '\n', '# Citations\n', 'If you use BioExp, please cite the following papers:\n', '\n', '```\n', '@article{kori2020abstracting,\n', '  title={Abstracting Deep Neural Networks into Concept Graphs for Concept Level Interpretability},\n', '  author={Kori, Avinash and Natekar, Parth and Krishnamurthi, Ganapathy and Srinivasan, Balaji},\n', '  journal={arXiv preprint arXiv:2008.06457},\n', '  year={2020}\n', '}\n', '\n', '@article{natekar2020demystifying,\n', '  title={Demystifying Brain Tumor Segmentation Networks: Interpretability and Uncertainty Analysis},\n', '  author={Natekar, Parth and Kori, Avinash and Krishnamurthi, Ganapathy},\n', '  journal={Frontiers in Computational Neuroscience},\n', '  volume={14},\n', '  pages={6},\n', '  year={2020},\n', '  publisher={Frontiers}\n', '}\n', '```\n', '\n', '# Defined Pipeline\n', '![pipeline](./imgs/pipeline.png)\n', '\n', '# Installation\n', 'Running of the explainability pipeline requires a GPU and several deep learning modules. \n', '\n', '### Requirements\n', ""- 'pandas'\n"", ""- 'numpy'\n"", ""- 'scipy==1.6.0'\n"", ""- 'matplotlib'\n"", ""- 'pillow'\n"", ""- 'simpleITK'\n"", ""- 'opencv-python'\n"", ""- 'tensorflow-gpu==1.14'\n"", ""- 'keras'\n"", ""- 'keras-vis'\n"", ""- 'lucid'\n"", '\n', 'The following command will install only the dependencies listed above.\n', '\n', '```\n', 'pip install BioExp\n', '```\n', '\n', '# Ablation\n', '\n', '## Usage\n', '```\n', 'from BioExp.spatial import Ablation\n', '\n', 'A = spatial.Ablation(model = model, \n', '\t\t\t\tweights_pth = weights_path, \n', '\t\t\t\tmetric      = dice_label_coef, \n', '\t\t\t\tlayer_name  = layer_name, \n', '\t\t\t\ttest_image  = test_image, \n', '\t\t\t\tgt \t    = gt, \n', '\t\t\t\tclasses     = infoclasses, \n', '\t\t\t\tnclasses    = 4)\n', '\n', 'df = A.ablate_filter(step = 1)\n', '```\n', '\n', '# Dissection\n', '\n', '## Usage\n', '```\n', 'from BioExp.spatial import Dissector\n', '\n', ""layer_name = 'conv2d_3'\n"", 'infoclasses = {}\n', ""for i in range(1): infoclasses['class_'+str(i)] = (i,)\n"", ""infoclasses['whole'] = (1,2,3)\n"", '\n', 'dissector = Dissector(model=model,\n', '                        layer_name = layer_name)\n', '\n', 'threshold_maps = dissector.get_threshold_maps(dataset_path = data_root_path,\n', '                                                save_path  = savepath,\n', '                                                percentile = 85)\n', 'dissector.apply_threshold(image, threshold_maps, \n', '                        nfeatures =9, \n', '                        save_path = savepath, \n', '                        ROI       = ROI)\n', '\n', 'dissector.quantify_gt_features(image, gt, \n', '                        threshold_maps, \n', '                        nclasses   = infoclass, \n', '                        nfeatures  = 9, \n', '                        save_path  = savepath,\n', '                        save_fmaps = False, \n', '                        ROI        = ROI)\n', '```\n', '## Results\n', '\n', '![dissection](./imgs/dissection.png)\n', '\n', '\n', '# GradCAM\n', '\n', '## Usage\n', '```\n', 'from BioExp.spatial import cam\n', '\n', 'dice = flow.cam(model, img, gt, \n', '\t\t\t\tnclasses = nclasses, \n', '\t\t\t\tsave_path = save_path, \n', '\t\t\t\tlayer_idx = -1, \n', '\t\t\t\tthreshol = 0.5,\n', ""\t\t\t\tmodifier = 'guided')\n"", '\n', '```\n', '## Results\n', '![gradcam](./imgs/gradcam.png)\n', '\n', '\n', '# Activation Maximization\n', '\n', '## Usage\n', '```\n', 'from BioExp.concept.feature import Feature_Visualizer\n', '\n', 'class Load_Model(Model):\n', '\n', ""  model_path = '../../saved_models/model_flair_scaled/model.pb'\n"", '  image_shape = [None, 1, 240, 240]\n', '  image_value_range = (0, 10)\n', ""  input_name = 'input_1'\n"", '\n', ""E = Feature_Visualizer(Load_Model, savepath = '../results/', regularizer_params={'L1':1e-3, 'rotate':8})\n"", ""a = E.run(layer = 'conv2d_17', class_ = 'None', channel = 95, transforms=True)\n"", '\n', '```\n', '\n', '##Activation Results\n', '![lucid](./imgs/lucid.png)\n', '\n', '\n', '# Uncertainty\n', '\n', '## Usage\n', '```\n', 'from BioExp.uncertainty import uncertainty\n', '\n', 'D = uncertainty(test_image)\n', '            \n', '# for aleatoric\n', 'mean, var = D.aleatoric(model, iterations = 50)\n', '\n', '# for epistemic\n', 'mean, var = D.epistemic(model, iterations = 50)\n', ' \n', '# for combined\n', 'mean, var = D.combined(model, iterations = 50)\n', '\n', '```\n', '## Results\n', '![un](./imgs/uncertainty.png)\n', '\n', '\n', '# Radiomics\n', '## Usage\n', '```\n', 'from BioExp.helpers import radfeatures\n', 'feat_extractor = radfeatures.ExtractRadiomicFeatures(image, mask, save_path = pth)\n', 'df = feat_extractor.all_features()\n', '```\n', '\n', '# Causal Inference Pipeline\n', '![un](./imgs/causal_pipeline.png)\n', '\n', '# Contact\n', '- Avinash Kori (koriavinash1@gmail.com)\n', '- Parth Natekar (parth@smail.iitm.ac.in)\n']"
Model Explainability,HelmholtzAI-Consultants-Munich/fg-clustering,HelmholtzAI-Consultants-Munich,https://api.github.com/repos/HelmholtzAI-Consultants-Munich/fg-clustering,23,6,4,"['https://api.github.com/users/lisa-sousa', 'https://api.github.com/users/DoTha', 'https://api.github.com/users/hpelin', 'https://api.github.com/users/georgii-helmholtz']",Python,2023-03-31T06:19:20Z,https://raw.githubusercontent.com/HelmholtzAI-Consultants-Munich/fg-clustering/main/README.md,"['<div align=""center"">\n', '\n', '<img src=""https://raw.githubusercontent.com/HelmholtzAI-Consultants-Munich/fg-clustering/main/docs/source/_figures/FGC_Logo.png"" width=""200"">\n', '\t\n', '\n', '# *Forest-Guided Clustering* - Shedding light into the Random Forest Black Box \n', '\n', '[![test](https://github.com/HelmholtzAI-Consultants-Munich/fg-clustering/actions/workflows/test.yml/badge.svg)](https://github.com/HelmholtzAI-Consultants-Munich/fg-clustering/actions/workflows/test.yml)\n', '[![PyPI](https://img.shields.io/pypi/v/fgclustering.svg)](https://pypi.org/project/fgclustering)\n', '[![stars](https://img.shields.io/github/stars/HelmholtzAI-Consultants-Munich/forest_guided_clustering?logo=GitHub&color=yellow)](https://github.com/HelmholtzAI-Consultants-Munich/forest_guided_clustering/stargazers)\n', '[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n', '[![cite](https://zenodo.org/badge/397931780.svg)](https://zenodo.org/badge/latestdoi/397931780)\n', '\t\n', '[Docs] | [Tutorials]\n', '\n', '[Docs]: https://forest-guided-clustering.readthedocs.io/en/latest/\n', '[Tutorials]: https://github.com/HelmholtzAI-Consultants-Munich/fg-clustering/tree/main/tutorials\n', '\n', '</div>\n', '\n', 'Forest-Guided Clustering (FGC) is an explainability method for Random Forest models. Standard explainability methods (e.g. feature importance) assume independence of model features and hence, are not suited in the presence of correlated features. The Forest-Guided Clustering algorithm does not assume independence of model features, because it computes the feature importance based on subgroups of instances that follow similar decision rules within the Random Forest model. Hence, this method is well suited for cases with high correlation among model features. \n', '\n', 'For a detailed comparison of FGC and Permutation Feature Importance, please have a look at the Notebook [Introduction to FGC: Comparison of Forest-Guided Clustering and Feature Importance](https://github.com/HelmholtzAI-Consultants-Munich/fg-clustering/blob/main/tutorials/introduction_to_FGC_comparing_FGC_to_FI.ipynb).\n', '\n', '## Documentation\n', '\n', 'Please see [here](https://forest-guided-clustering.readthedocs.io/) for full documentation on:\n', '\n', '- Getting Started (installation, basic usage)\n', '- Theoretical Background (introduction, general algorith, feature importance)\n', '- Tutorials (simple use cases, special cases)\n', '- API documentation\n', '\n', 'For a short introduction to Forest-Guided Clustering, click below:\n', '\n', '<div align=""center"">\n', '\n', '[![Video](http://i.vimeocdn.com/video/1501376117-3e402fde211d1a52080fb16b317efc3786a34d0be852a81cfe3a03aa89adc475-d_295x166)](https://vimeo.com/746443233/07ddf2290b)\n', '\n', '</div>\n', '\n', '## Installation\n', '\n', '### Requirements\n', '\n', 'This packages was tested for ```Python 3.7 - 3.11``` on ubuntu, macos and windows. It depends on the ```kmedoids``` python package. If you are using windows or macos, you may need to first install Rust/Cargo with:\n', '\n', '```\n', 'conda install -c conda-forge rust\n', '```\n', '\n', 'If this does not work, please try to install Cargo from source:\n', '\n', '```\n', 'git clone https://github.com/rust-lang/cargo\n', 'cd cargo\n', 'cargo build --release\n', '```\n', '\n', 'For further information on the kmedoids package, please visit [this page](https://pypi.org/project/kmedoids/).\n', '\n', 'All other required packages are automatically installed if installation is done via ```pip```.\n', '\n', '\n', '### Install Options\n', '\n', 'The installation of the package is done via pip. Note: if you are using conda, first install pip with: ```conda install pip```.\n', '\n', 'PyPI install:\n', '\n', '```\n', 'pip install fgclustering\n', '```\n', '\n', '\n', 'Installation from source:\n', '\n', '```\n', 'git clone https://github.com/HelmholtzAI-Consultants-Munich/fg-clustering.git\n', '```\n', '\n', '- Installation as python package (run inside directory):\n', '\n', '\t\tpip install .   \n', '\n', '\n', '- Development Installation as python package (run inside directory):\n', '\n', '\t\tpip install -e . [dev]\n', '\n', '\n', '## Basic Usage\n', '\n', 'To get explainability of your Random Forest model via Forest-Guided Clustering, you simply need to run the following commands:\n', '\n', '```python\n', 'from fgclustering import FgClustering\n', '   \n', '# initialize and run fgclustering object\n', ""fgc = FgClustering(model=rf, data=data, target_column='target')\n"", 'fgc.run()\n', '   \n', '# visualize results\n', 'fgc.plot_global_feature_importance()\n', 'fgc.plot_local_feature_importance()\n', 'fgc.plot_decision_paths()\n', '   \n', '# obtain optimal number of clusters and vector that contains the cluster label of each data point\n', 'optimal_number_of_clusters = fgc.k\n', 'cluster_labels = fgc.cluster_labels\n', '```\n', '\n', 'where \n', '\n', '- ```model=rf``` is a Random Forest Classifier or Regressor object,\n', '- ```data=data``` is a dataset containing the same features as required by the Random Forest model, and\n', ""- ```target_column='target'``` is the name of the target column (i.e. *target*) in the provided dataset. \n"", '\n', 'For detailed instructions, please have a look at the Notebook [Introduction to FGC: Simple Use Cases](https://github.com/HelmholtzAI-Consultants-Munich/fg-clustering/blob/main/tutorials/introduction_to_FGC_use_cases.ipynb).\n', '\n', '**Usage on big datasets**\n', '\n', 'If you are working with the dataset containing large number of samples, you can use one of the following strategies:\n', '\n', '- Use the cores you have at your disposal to parallelize the optimization of the cluster number. You can do so by setting the parameter ```n_jobs``` to a value > 1 in the ```run()``` function.\n', '- Use the faster implementation of the pam method that K-Medoids algorithm uses to find the clusters by setting the parameter  ```method_clustering``` to *fasterpam* in the ```run()``` function.\n', '- Use subsampling technique\n', '\n', 'For detailed instructions, please have a look at the Notebook [Special Case: FGC for Big Datasets](https://github.com/HelmholtzAI-Consultants-Munich/fg-clustering/blob/main/tutorials/special_case_big_data_with_FGC.ipynb).\n', '\n', '## Contributing\n', ' \n', ""Contributions are more than welcome! Everything from code to notebooks to examples and documentation are all equally valuable so please don't feel you can't contribute. To contribute please fork the project make your changes and submit a pull request. We will do our best to work through any issues with you and get your code merged into the main branch.\n"", '\n', '## How to cite\n', '\n', 'If Forest-Guided Clustering is useful for your research, consider citing the package:\n', '\n', '```\n', '@software{lisa_sousa_2022_7823042,\n', '    author       = {Lisa Barros de Andrade e Sousa,\n', '                     Helena Pelin,\n', '                     Dominik Thalmeier,\n', '                     Marie Piraud},\n', '    title        = {{Forest-Guided Clustering - Explainability for Random Forest Models}},\n', '    month        = april,\n', '    year         = 2022,\n', '    publisher    = {Zenodo},\n', '    version      = {v1.0.3},\n', '    doi          = {10.5281/zenodo.7823042},\n', '    url          = {https://doi.org/10.5281/zenodo.7823042}\n', '}\n', '```\n', '\n', '## License\n', '\n', '```fgclustering``` is released under the MIT license. See [LICENSE](https://github.com/HelmholtzAI-Consultants-Munich/fg-clustering/blob/main/LICENSE) for additional details about it.\n']"
Model Explainability,JoaoLages/diffusers-interpret,JoaoLages,https://api.github.com/repos/JoaoLages/diffusers-interpret,215,10,3,"['https://api.github.com/users/JoaoLages', 'https://api.github.com/users/TomPham97', 'https://api.github.com/users/andrewizbatista']",Python,2023-04-26T11:51:11Z,https://raw.githubusercontent.com/JoaoLages/diffusers-interpret/main/README.md,"['<div align=""center"">\r\n', '\r\n', '# Diffusers-Interpret 🤗🧨🕵️\u200d♀️\r\n', '\r\n', '![PyPI Latest Package Version](https://img.shields.io/pypi/v/diffusers-interpret?logo=pypi&style=flat&color=orange) ![GitHub License](https://img.shields.io/github/license/JoaoLages/diffusers-interpret?logo=github&style=flat&color=green) \r\n', '\r\n', '`diffusers-interpret` is a model explainability tool built on top of [🤗 Diffusers](https://github.com/huggingface/diffusers)\r\n', '</div>\r\n', '\r\n', '## Installation\r\n', '\r\n', 'Install directly from PyPI:\r\n', '\r\n', '    pip install --upgrade diffusers-interpret\r\n', '\r\n', '## Usage\r\n', '\r\n', ""Let's see how we can interpret the **[new 🎨🎨🎨 Stable Diffusion](https://github.com/huggingface/diffusers#new--stable-diffusion-is-now-fully-compatible-with-diffusers)!**\r\n"", '\r\n', '1. [Explanations for StableDiffusionPipeline](#explanations-for-stablediffusionpipeline)\r\n', '2. [Explanations for StableDiffusionImg2ImgPipeline](#explanations-for-stablediffusionimg2imgpipeline)\r\n', '3. [Explanations for StableDiffusionInpaintPipeline](#explanations-for-stablediffusioninpaintpipeline)\r\n', '\r\n', '### Explanations for StableDiffusionPipeline\r\n', '[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JoaoLages/diffusers-interpret/blob/main/notebooks/stable_diffusion_example_colab.ipynb)\r\n', '\r\n', '```python\r\n', 'import torch\r\n', 'from diffusers import StableDiffusionPipeline\r\n', 'from diffusers_interpret import StableDiffusionPipelineExplainer\r\n', '\r\n', 'pipe = StableDiffusionPipeline.from_pretrained(\r\n', '    ""CompVis/stable-diffusion-v1-4"", \r\n', '    use_auth_token=True,\r\n', ""    revision='fp16',\r\n"", '    torch_dtype=torch.float16\r\n', "").to('cuda')\r\n"", '\r\n', '# optional: reduce memory requirement with a speed trade off \r\n', 'pipe.enable_attention_slicing()\r\n', '\r\n', '# pass pipeline to the explainer class\r\n', 'explainer = StableDiffusionPipelineExplainer(pipe)\r\n', '\r\n', '# generate an image with `explainer`\r\n', 'prompt = ""A cute corgi with the Eiffel Tower in the background""\r\n', ""with torch.autocast('cuda'):\r\n"", '    output = explainer(\r\n', '        prompt, \r\n', '        num_inference_steps=15\r\n', '    )\r\n', '```\r\n', '\r\n', 'If you are having GPU memory problems, try reducing `n_last_diffusion_steps_to_consider_for_attributions`, `height`, `width` and/or `num_inference_steps`.\r\n', '```\r\n', 'output = explainer(\r\n', '    prompt, \r\n', '    num_inference_steps=15,\r\n', '    height=448,\r\n', '    width=448,\r\n', '    n_last_diffusion_steps_to_consider_for_attributions=5\r\n', ')\r\n', '```\r\n', '\r\n', 'You can completely deactivate token/pixel attributions computation by passing `n_last_diffusion_steps_to_consider_for_attributions=0`.  \r\n', '\r\n', 'Gradient checkpointing also reduces GPU usage, but makes computations a bit slower:\r\n', '```\r\n', 'explainer = StableDiffusionPipelineExplainer(pipe, gradient_checkpointing=True)\r\n', '```\r\n', '\r\n', 'To see the final generated image:\r\n', '```python\r\n', 'output.image\r\n', '```\r\n', '\r\n', '![](assets/corgi_eiffel_tower.png)\r\n', '\r\n', 'You can also check all the images that the diffusion process generated at the end of each step:\r\n', '```python\r\n', 'output.all_images_during_generation.show()\r\n', '```\r\n', '![](assets/image_slider_cropped.gif)\r\n', '\r\n', 'To analyse how a token in the input `prompt` influenced the generation, you can study the token attribution scores:\r\n', '```python\r\n', '>>> output.token_attributions # (token, attribution)\r\n', ""[('a', 1063.0526),\r\n"", "" ('cute', 415.62888),\r\n"", "" ('corgi', 6430.694),\r\n"", "" ('with', 1874.0208),\r\n"", "" ('the', 1223.2847),\r\n"", "" ('eiffel', 4756.4556),\r\n"", "" ('tower', 4490.699),\r\n"", "" ('in', 2463.1294),\r\n"", "" ('the', 655.4624),\r\n"", "" ('background', 3997.9395)]\r\n"", '```\r\n', '\r\n', 'Or their computed normalized version, in percentage:\r\n', '```python\r\n', '>>> output.token_attributions.normalized # (token, attribution_percentage)\r\n', ""[('a', 3.884),\r\n"", "" ('cute', 1.519),\r\n"", "" ('corgi', 23.495),\r\n"", "" ('with', 6.847),\r\n"", "" ('the', 4.469),\r\n"", "" ('eiffel', 17.378),\r\n"", "" ('tower', 16.407),\r\n"", "" ('in', 8.999),\r\n"", "" ('the', 2.395),\r\n"", "" ('background', 14.607)]\r\n"", '```\r\n', '\r\n', 'Or plot them!\r\n', '```python\r\n', 'output.token_attributions.plot(normalize=True)\r\n', '```\r\n', '![](assets/token_attributions_1.png)\r\n', '\r\n', '\r\n', '`diffusers-interpret` also computes these token/pixel attributions for generating a particular part of the image. \r\n', '\r\n', 'To do that, call `explainer` with a particular 2D bounding box defined in `explanation_2d_bounding_box`:\r\n', '\r\n', '```python\r\n', ""with torch.autocast('cuda'):\r\n"", '    output = explainer(\r\n', '        prompt, \r\n', '        num_inference_steps=15, \r\n', '        explanation_2d_bounding_box=((70, 180), (400, 435)), # (upper left corner, bottom right corner)\r\n', '    )\r\n', 'output.image\r\n', '```\r\n', '![](assets/corgi_eiffel_tower_box_1.png)\r\n', '\r\n', 'The generated image now has a <span style=""color:red""> **red bounding box** </span> to indicate the region of the image that is being explained.\r\n', '\r\n', 'The attributions are now computed only for the area specified in the image.\r\n', '\r\n', '```python\r\n', '>>> output.token_attributions.normalized # (token, attribution_percentage)\r\n', ""[('a', 1.891),\r\n"", "" ('cute', 1.344),\r\n"", "" ('corgi', 23.115),\r\n"", "" ('with', 11.995),\r\n"", "" ('the', 7.981),\r\n"", "" ('eiffel', 5.162),\r\n"", "" ('tower', 11.603),\r\n"", "" ('in', 11.99),\r\n"", "" ('the', 1.87),\r\n"", "" ('background', 23.05)]\r\n"", '```\r\n', '\r\n', '### Explanations for StableDiffusionImg2ImgPipeline\r\n', '[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JoaoLages/diffusers-interpret/blob/main/notebooks/stable_diffusion_img2img_example.ipynb)\r\n', '\r\n', '```python\r\n', 'import torch\r\n', 'import requests\r\n', 'from PIL import Image\r\n', 'from io import BytesIO\r\n', 'from diffusers import StableDiffusionImg2ImgPipeline\r\n', 'from diffusers_interpret import StableDiffusionImg2ImgPipelineExplainer\r\n', '\r\n', '\r\n', 'pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\r\n', '    ""CompVis/stable-diffusion-v1-4"", \r\n', '    use_auth_token=True,\r\n', "").to('cuda')\r\n"", '\r\n', 'explainer = StableDiffusionImg2ImgPipelineExplainer(pipe)\r\n', '\r\n', 'prompt = ""A fantasy landscape, trending on artstation""\r\n', '\r\n', ""# let's download an initial image\r\n"", 'url = ""https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg""\r\n', '\r\n', 'response = requests.get(url)\r\n', 'init_image = Image.open(BytesIO(response.content)).convert(""RGB"")\r\n', 'init_image = init_image.resize((448, 448))\r\n', '\r\n', ""with torch.autocast('cuda'):\r\n"", '    output = explainer(\r\n', '        prompt=prompt, init_image=init_image, strength=0.75\r\n', '    )\r\n', '```\r\n', '\r\n', '`output` will have all the properties that were presented for [StableDiffusionPipeline](#explanations-for-stablediffusionpipeline).\r\n', 'For example, to see the gif version of all the images during generation:\r\n', '```python\r\n', 'output.all_images_during_generation.gif()\r\n', '```\r\n', '![](assets/img2img_1.gif)\r\n', '\r\n', 'Additionally, it is also possible to visualize pixel attributions of the input image as a saliency map:\r\n', '```python\r\n', 'output.input_saliency_map.show()\r\n', '```\r\n', '![](assets/pixel_attributions_1.png)\r\n', '\r\n', 'or access their values directly:\r\n', '```python\r\n', '>>> output.pixel_attributions\r\n', 'array([[ 1.2714844 ,  4.15625   ,  7.8203125 , ...,  2.7753906 ,\r\n', '         2.1308594 ,  0.66552734],\r\n', '       [ 5.5078125 , 11.1953125 ,  4.8125    , ...,  5.6367188 ,\r\n', '         6.8828125 ,  3.0136719 ],\r\n', '       ...,\r\n', '       [ 0.21386719,  1.8867188 ,  2.2109375 , ...,  3.0859375 ,\r\n', '         2.7421875 ,  0.7871094 ],\r\n', '       [ 0.85791016,  0.6694336 ,  1.71875   , ...,  3.8496094 ,\r\n', '         1.4589844 ,  0.5727539 ]], dtype=float32)\r\n', '```\r\n', 'or the normalized version:\r\n', '```python\r\n', '>>> output.pixel_attributions.normalized \r\n', 'array([[7.16054201e-05, 2.34065039e-04, 4.40411852e-04, ...,\r\n', '        1.56300011e-04, 1.20002325e-04, 3.74801020e-05],\r\n', '       [3.10180156e-04, 6.30479713e-04, 2.71022669e-04, ...,\r\n', '        3.17439699e-04, 3.87615233e-04, 1.69719147e-04],\r\n', '       ...,\r\n', '       [1.20442292e-05, 1.06253210e-04, 1.24512037e-04, ...,\r\n', '        1.73788882e-04, 1.54430119e-04, 4.43271674e-05],\r\n', '       [4.83144104e-05, 3.77000870e-05, 9.67938031e-05, ...,\r\n', '        2.16796136e-04, 8.21647482e-05, 3.22554370e-05]], dtype=float32)\r\n', '```\r\n', '\r\n', '**Note:** Passing `explanation_2d_bounding_box` to the `explainer` will also change these values to explain a specific part of the **output** image. \r\n', ""<ins>The attributions are always calculated for the model's input (image and text) with respect to the output image.</ins>\r\n"", '\r\n', '### Explanations for StableDiffusionInpaintPipeline\r\n', '[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JoaoLages/diffusers-interpret/blob/main/notebooks/stable_diffusion_inpaint_example.ipynb)\r\n', '\r\n', 'Same as [StableDiffusionImg2ImgPipeline](#explanations-for-stablediffusionimg2imgpipeline), but now we also pass a `mask_image` argument to `explainer`.\r\n', '\r\n', '```python\r\n', 'import torch\r\n', 'import requests\r\n', 'from PIL import Image\r\n', 'from io import BytesIO\r\n', 'from diffusers import StableDiffusionInpaintPipeline\r\n', 'from diffusers_interpret import StableDiffusionInpaintPipelineExplainer\r\n', '\r\n', '\r\n', 'def download_image(url):\r\n', '    response = requests.get(url)\r\n', '    return Image.open(BytesIO(response.content)).convert(""RGB"")\r\n', '\r\n', '\r\n', 'pipe = StableDiffusionInpaintPipeline.from_pretrained(\r\n', '    ""CompVis/stable-diffusion-v1-4"", \r\n', '    use_auth_token=True,\r\n', "").to('cuda')\r\n"", '\r\n', 'explainer = StableDiffusionInpaintPipelineExplainer(pipe)\r\n', '\r\n', 'prompt = ""a cat sitting on a bench""\r\n', '\r\n', 'img_url = ""https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png""\r\n', 'mask_url = ""https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png""\r\n', '\r\n', 'init_image = download_image(img_url).resize((448, 448))\r\n', 'mask_image = download_image(mask_url).resize((448, 448))\r\n', '\r\n', ""with torch.autocast('cuda'):\r\n"", '    output = explainer(\r\n', '        prompt=prompt, init_image=init_image, mask_image=mask_image, strength=0.75\r\n', '    )\r\n', '```\r\n', '\r\n', '`output` will have all the properties that were presented for [StableDiffusionImg2ImgPipeline](#explanations-for-stablediffusionimg2imgpipeline) and [StableDiffusionPipeline](#explanations-for-stablediffusionpipeline).  \r\n', 'For example, to see the gif version of all the images during generation:\r\n', '```python\r\n', 'output.all_images_during_generation.gif()\r\n', '```\r\n', '![](assets/inpaint_1.gif)\r\n', '\r\n', 'The only difference in `output` now, is that we can now see the masked part of the image:\r\n', '```python\r\n', 'output.input_saliency_map.show()\r\n', '```\r\n', '![](assets/pixel_attributions_inpaint_1.png)\r\n', '\r\n', 'Check other functionalities and more implementation examples in [here](https://github.com/JoaoLages/diffusers-interpret/blob/main/notebooks/).\r\n', '\r\n', '## Future Development\r\n', '- [x] ~~Add interactive display of all the images that were generated in the diffusion process~~\r\n', '- [x] ~~Add explainer for StableDiffusionImg2ImgPipeline~~\r\n', '- [x] ~~Add explainer for StableDiffusionInpaintPipeline~~\r\n', '- [ ] Add attentions visualization \r\n', '- [ ] Add unit tests\r\n', '- [ ] Website for documentation\r\n', '- [ ] Do not require another generation every time the `explanation_2d_bounding_box` argument is changed\r\n', '- [ ] Add interactive bounding-box and token attributions visualization\r\n', '- [ ] Add more explainability methods\r\n', '\r\n', '## Contributing\r\n', ""Feel free to open an [Issue](https://github.com/JoaoLages/diffusers-interpret/issues) or create a [Pull Request](https://github.com/JoaoLages/diffusers-interpret/pulls) and let's get started 🚀\r\n"", '\r\n', '## Credits\r\n', '\r\n', 'A special thanks to:\r\n', '- [@andrewizbatista](https://github.com/andrewizbatista) for creating a great [image slider](https://github.com/JoaoLages/diffusers-interpret/pull/1) to show all the generated images during diffusion! 💪 \r\n', '- [@TomPham97](https://github.com/TomPham97) for README improvements, the [GIF visualization](https://github.com/JoaoLages/diffusers-interpret/pull/9) and the [token attributions plot](https://github.com/JoaoLages/diffusers-interpret/pull/13) 😁\r\n']"
Model Explainability,gearsuccess/DER,gearsuccess,https://api.github.com/repos/gearsuccess/DER,39,21,1,['https://api.github.com/users/gearsuccess'],Python,2022-11-25T08:36:55Z,https://raw.githubusercontent.com/gearsuccess/DER/master/README.md,"['## Requirements\n', '\n', '- Python 2.7\n', '- TensorFlow 1.8\n', '- numpy 1.14\n', '\n', '\n', '## Project Structure\n', '\n', '    .\n', '    ├── conf                    # Config files\n', '    ├── data\n', '        ├── der_data            # Experiment data   \n', '    ├── results                 # Results saving\n', '    ├── model          \n', '        ├── DER.py              # The graph of DER\n', '    ├── data_loader.py          \n', '    ├── evaluate.py             # Evaluation code\n', '    ├── main.py                 # The entrance of the project\n', '    └── solver.py               # The solver of the project     \n', '\n', '\n', '\n', '\n', '## Config\n', '\n', 'example: \n', '\n', '[path]\n', '\n', 'root_path = your-local-path/DER\n', '\n', 'input_data_type = data/der_data/category\n', '\n', 'output_path = results\n', '\n', 'log_conf_path = conf/logging.conf\n', '\n', '\n', '[parameters]\n', '\n', 'global_dimension = 8\n', '\n', 'word_dimension = 64\n', '\n', 'batch_size = 50\n', '\n', 'epoch = 50\n', '\n', 'learning_rate = 0.001\n', '\n', 'reg = 1\n', '\n', 'mode = validation\n', '\n', 'merge = FM\n', '\n', 'concat = 1\n', '\n', 'item_review_combine = add\n', '\n', 'item_review_combine_c = 0.5\n', '\n', 'lmd = 1\n', '\n', 'drop_out_rate = 0.7\n', '\n', '\n', '## Usage\n', '\n', '1. Install all the required packages\n', '\n', '2. process the raw data into the formats according to data/der_data/data_format\n', '\n', '3. Run python main.py\n', '\n', '\n', '## Author# DER\n']"
Model Explainability,AI4LIFE-GROUP/OpenXAI,AI4LIFE-GROUP,https://api.github.com/repos/AI4LIFE-GROUP/OpenXAI,149,20,8,"['https://api.github.com/users/chirag126', 'https://api.github.com/users/jiaqima', 'https://api.github.com/users/y12uc231', 'https://api.github.com/users/s1682978', 'https://api.github.com/users/eshika', 'https://api.github.com/users/hnaik', 'https://api.github.com/users/Y0mingZhang', 'https://api.github.com/users/ehsankiakojouri']",JavaScript,2023-04-24T17:43:52Z,https://raw.githubusercontent.com/AI4LIFE-GROUP/OpenXAI/main/README.md,"['# OpenXAI : Towards a Transparent Evaluation of Model Explanations\n', '\n', '----\n', '\n', '[**Website**](https://open-xai.github.io/) | [**arXiv Paper**](https://arxiv.org/abs/2206.11104)\n', '\n', '**OpenXAI** is the first general-purpose lightweight library that provides a comprehensive list of functions to systematically evaluate the quality of explanations generated by attribute-based explanation methods. OpenXAI supports the development of new datasets (both synthetic and real-world) and explanation methods, with a strong bent towards promoting systematic, reproducible, and transparent evaluation of explanation methods.\n', '\n', 'OpenXAI is an open-source initiative that comprises of a collection of curated high-stakes datasets, models, and evaluation metrics, and provides a simple and easy-to-use API that enables researchers and practitioners to benchmark explanation methods using just a few lines of code.\n', '\n', '\n', '## Updates\n', '- `0.0.0`: OpenXAI is live! Now, you can submit the result for benchmarking an post-hoc explanation method on an evaluation metric. Checkout [here](https://open-xai.github.io/quick-start)!\n', '- OpenXAI white paper is on [arXiv](https://arxiv.org/abs/2206.11104)!\n', '\n', '\n', '## Unique Features of OpenXAI\n', '- *Diverse areas of XAI research*: OpenXAI includes ready-to-use API interfaces for seven state-of-the-art feature attribution methods and 22 metrics to quantify their performance. Further, it provides a flexible synthetic data generator to synthesize datasets of varying sizes, complexity, and dimensionality that facilitate the construction of ground truth explanations and a comprehensive collection of real-world datasets.\n', '- *Data functions*: OpenXAI provides extensive data functions, including data evaluators, meaningful data splits, explanation methods, and evaluation metrics.\n', '- *Leaderboards*: OpenXAI provides the first ever public XAI leaderboards to promote transparency, and to allow users to easily compare the performance of multiple explanation methods.\n', '- *Open-source initiative*: OpenXAI is an open-source initiative and easily extensible.\n', '\n', '## Installation\n', '\n', '### Using `pip`\n', '\n', 'To install the core environment dependencies of OpenXAI, use `pip` by cloning the OpenXAI repo into your local environment:\n', '\n', '```bash\n', 'pip install -e . \n', '```\n', '\n', '## Design of OpenXAI\n', '\n', 'OpenXAI is an open-source ecosystem comprising XAI-ready datasets, implementations of state-of-the-art explanation methods, evaluation metrics, leaderboards and documentation to promote transparency and collaboration around evaluations of post hoc explanations. OpenXAI can readily be used to *benchmark* new explanation methods as well as incorporate them into our framework and leaderboards. By enabling *systematic and efficient evaluation* and benchmarking of existing and new explanation methods, OpenXAI can inform and accelerate new research in the emerging field of XAI.\n', '\n', '### OpenXAI DataLoaders\n', '\n', 'OpenXAI provides a Dataloader class that can be used to load the aforementioned collection of synthetic and real-world datasets as well as any other custom datasets, and ensures that they are XAI-ready. More specifically, this class takes as input the name of an existing OpenXAI dataset or a new dataset (name of the .csv file), and outputs a train set which can then be used to train a predictive model, a test set which can be used to generate local explanations of the trained model, as well as any ground-truth explanations (if and when available). If the dataset already comes with pre-determined train and test splits, this class loads train and test sets from those pre-determined splits. Otherwise, it divides the entire dataset randomly into train (70%) and test (30%) sets. Users can also customize the percentages of train-test splits.\n', '\n', 'For a concrete example, the code snippet below shows how to import the Dataloader class and load an existing OpenXAI dataset:\n', '\n', '```python\n', 'from openxai.dataloader import return_loaders\n', 'loader_train, loader_test = return_loaders(data_name=‘german’, download=True)\n', '# get an input instance from the test dataset\n', 'inputs, labels = iter(loader_test).next()\n', '```\n', '\n', '### OpenXAI Pre-trained models\n', '\n', 'We also pre-trained two classes of predictive models (e.g., deep neural networks of varying degrees of complexity, logistic regression models etc.) and incorporated them into the OpenXAI framework so that they can be readily used for benchmarking explanation methods. The code snippet below shows how to load OpenXAI’s pre-trained models using our LoadModel class.\n', '\n', '```python\n', 'from openxai import LoadModel\n', ""model = LoadModel(data_name= 'german', ml_model='ann', pretrained=True)\n"", '```\n', '\n', 'Adding additional pre-trained models into the OpenXAI framework is as simple as uploading a file with details about model architecture and parameters in a specific template. Users can also submit requests to incorporate custom pre-trained models into the OpenXAI framework by filling a simple form and providing details about model architecture and parameters.\n', '\n', '### OpenXAI Explainers\n', '\n', 'All the explanation methods included in OpenXAI are readily accessible through the *Explainer* class, and users just have to specify the method name in order to invoke the appropriate method and generate explanations as shown in the above code snippet. Users can easily incorporate their own custom explanation methods into the OpenXAI framework by extending the *Explainer* class and including the code for their methods in the *get_explanations* function of this class.\n', '\n', '```python\n', 'from openxai import Explainer\n', ""exp_method = Explainer(method= 'lime',model=model, dataset_tensor=inputs)\n"", 'explanations= exp_method.get_explanation(inputs, labels)\n', '```\n', '\n', 'Users can then submit a request to incorporate their custom methods into OpenXAI library by filling a form and providing the GitHub link to their code as well as a summary of their explanation method.\n', '\n', '### OpenXAI Evaluation\n', '\n', 'Benchmarking an explanation method using evaluation metrics is quite simple and the code snippet below describes how to invoke the RIS metric. Users can easily incorporate their own custom evaluation metrics into OpenXAI by filling a form and providing the GitHub link to their code as well as a summary of their metric. Note that the code should be in the form of a function which takes as input data instances, corresponding model predictions and their explanations, as well as OpenXAI’s model object and returns a numerical score. Finally, the input_dict is described [here](https://github.com/AI4LIFE-GROUP/OpenXAI/blob/main/OpenXAI%20quickstart.ipynb).\n', '\n', '```python\n', 'from openxai import Evaluator\n', 'metric_evaluator = Evaluator(input_dict, inputs, labels, model, exp_method)\n', ""score = metric_evaluator.evaluate(metric='RIS')\n"", '```\n', '\n', '### OpenXAI Metrics\n', '\n', '#### Ground-truth Faithfulness\n', 'OpenXAI includes the following metrics to calculate the agreement between ground-truth explanations (i.e., coefficients of logistic regression models) and explanations generated by state-of-the-art methods.\n', '\n', '1. `Feature Agreement (FA)` metric computes the fraction of top-K features that are common between a given post hoc explanation and the corresponding ground truth explanation.\n', '2. `Rank Agreement (RA)` metric measures the fraction of top-K features that are not only common between a given post hoc explanation and the corresponding ground truth explanation, but also have the same position in the respective rank orders.\n', '3. `Sign Agreement (SA)` metric computes the fraction of top-K features that are not only common between a given post hoc explanation and the corresponding ground truth explanation, but also share the same sign (direction of contribution) in both the explanations.\n', '4. `Signed Rank Agreement (SRA)` metric computes the fraction of top-K features that are not only common between a given post hoc explanation and the corresponding ground truth explanation, but also share the same feature attribution sign (direction of contribution) and position (rank) in both the explanations.\n', '5. `Rank Correlation (RC)` metric computes the Spearman’s rank correlation coefficient to measure the agreement between feature rankings provided by a given post hoc explanation and the corresponding ground truth explanation.\n', '6. `Pairwise Rank Agreement (PRA)` metric captures if the relative ordering of every pair of features is the same for a given post hoc explanation as well as the corresponding ground truth explanation i.e., if feature A is more important than B according to one explanation, then the same should be true for the other explanation. More specifically, this metric computes the fraction of feature pairs for which the relative ordering is the same between the two explanations.\n', '\n', '#### Predicted Faithfulness\n', 'OpenXAI includes two complementary predictive faithfulness metrics: i) `Prediction Gap on Important feature perturbation (PGI)` which measures the difference in prediction probability that results from perturbing the features deemed as influential by a given post hoc explanation, and ii) `Prediction Gap on Unimportant feature perturbation (PGU)` which measures the difference in prediction probability that results from perturbing the features deemed as unimportant by a given post hoc explanation.\n', '\n', '#### Stability\n', 'OpenXAI incorporates three stability metrics: i) `Relative Input Stability (RIS)` which measure the maximum change in explanation relative to changes in the inputs, ii) `Relative Representation Stability (RRS)` which measure the maximum change in explanation relative to changes in the internal representation learned by the model, and iii) `Relative Output Stability (ROS)` which measure the maximum change in explanation relative to changes in output prediction probabilities.\n', '\n', '#### Fairness\n', 'We report the average of all faithfulness and stability metric values across instances in the majority and minority subgroups, and then take the absolute difference between them to check if there are significant disparities.\n', '\n', '### OpenXAI Leaderboards\n', '\n', 'Every explanation method in OpenXAI is a benchmark, and we provide dataloaders, pre-trained models, together with explanation methods and performance evaluation metrics. To participate in the leaderboard for a specific benchmark, follow these steps:\n', '\n', '* Use the OpenXAI benchmark dataloader to retrieve a given dataset.\n', '\n', '* Use the OpenXAI LoadModel to load a pre-trained model.\n', '\n', '* Use the OpenXAI Explainer to load a post hoc explanation method.\n', '\n', '* Submit the performance of the explanation method for a given metric.\n', '\n', '## Cite Us\n', '\n', 'If you find OpenXAI benchmark useful, cite our paper:\n', '\n', '```\n', '@inproceedings{\n', 'agarwal2022openxai,\n', 'title={Open{XAI}: Towards a Transparent Evaluation of Model Explanations},\n', 'author={Chirag Agarwal and Satyapriya Krishna and Eshika Saxena and Martin Pawelczyk and Nari Johnson and Isha Puri and Marinka Zitnik and Himabindu Lakkaraju},\n', 'booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},\n', 'year={2022},\n', 'url={https://openreview.net/forum?id=MU2495w47rz}\n', '}\n', '```\n', '\n', '## Contact\n', '\n', 'Reach us at [openxaibench@gmail.com](mailto:openxaibench@gmail.com) or open a GitHub issue.\n', '\n', '## License\n', 'OpenXAI codebase is under MIT license. For individual dataset usage, please refer to the dataset license found on the website.\n']"
Model Explainability,iancovert/removal-explanations,iancovert,https://api.github.com/repos/iancovert/removal-explanations,54,11,1,['https://api.github.com/users/iancovert'],Python,2023-03-14T03:00:04Z,https://raw.githubusercontent.com/iancovert/removal-explanations/main/README.md,"['# Removal-based explanations\n', '\n', 'This repository implements a large number of *removal-based explanations*, a class of model explanation approaches that unifies many existing methods (e.g., SHAP, LIME, Meaningful Perturbations, L2X, permutation tests). Our [paper](https://arxiv.org/abs/2011.14878) presents a framework that allows us to implement many of these methods in a lightweight, modular codebase.\n', '\n', ""Our implementation does not take advantage of certain approximation approaches that make these methods fast in practice, so you may prefer to continue using the original implementations (e.g., [SHAP](https://github.com/slundberg/shap), [LIME](https://github.com/marcotcr/lime), [SAGE](https://github.com/iancovert/sage/)). We also haven't implemented every method, e.g., we do not support image blurring or feature selection approaches.\n"", '\n', '## Usage\n', '\n', 'To begin, you need to clone the repository and install the library into your Python environment:\n', '\n', '```bash\n', 'pip install .\n', '```\n', '\n', 'Our code is designed around the framework described in the paper. Each model explanation method is specified by three choices:\n', '\n', '1. **Feature removal:** how the model is evaluated when features are held out\n', ""2. **Model behavior:** a target quantity that's analyzed as features are removed (e.g., an individual prediction, or the model loss)\n"", ""3. **Summary technique:** how each feature's influence is summarized (e.g., using Shapley values)\n"", '\n', 'The general use pattern looks like this:\n', '\n', '```python\n', 'from rexplain import removal, behavior, summary\n', '\n', '# Get model and data\n', 'x, y = ...\n', 'model = ...\n', '\n', '# 1) Feature removal\n', 'extension = removal.MarginalExtension(x[:512], model)\n', '\n', '# 2) Model behavior\n', 'game = behavior.PredictionGame(x[0], extension)\n', '\n', '# 3) Summary technique\n', 'attr = summary.ShapleyValue(game)\n', 'plt.bar(np.arange(len(attr)), attr)\n', '```\n', '\n', 'For usage examples, see the following notebooks:\n', '\n', '- [Census](https://github.com/iancovert/removal-explanations/blob/main/notebooks/census.ipynb) shows how to explain individual predictions\n', ""- [MNIST](https://github.com/iancovert/removal-explanations/blob/main/notebooks/mnist.ipynb) shows how to explain the model's loss for individual predictions\n"", '- [Breast cancer (BRCA)](https://github.com/iancovert/removal-explanations/blob/main/notebooks/brca.ipynb) shows how to explain the dataset loss\n', '\n', '# Authors\n', '\n', '- Ian Covert (<icovert@cs.washington.edu>)\n', '- Scott Lundberg\n', '- Su-In Lee\n', '\n', '# References\n', '\n', 'Ian Covert, Scott Lundberg, Su-In Lee. ""Explaining by Removing: A Unified Framework For Model Explanation."" *arXiv preprint:2011.14878*\n']"
Model Explainability,Nelsonchris1/ML-explainability-app,Nelsonchris1,https://api.github.com/repos/Nelsonchris1/ML-explainability-app,17,7,2,"['https://api.github.com/users/Nelsonchris1', 'https://api.github.com/users/AnsahMohammad']",Python,2023-01-02T08:26:36Z,https://raw.githubusercontent.com/Nelsonchris1/ML-explainability-app/main/README.md,"['# ML-explainability-app\n', '\n', '[![Streamlit App](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://share.streamlit.io/nelsonchris1/ml-explainability-app/main/app.py)\n', '\n', '## ✅ Description\n', 'Designing black box machine learning algorithms are sometimes challenging and confusing to explain. But in reality, there are diffrenet ways to explain these models and also understand how each featue contributes to the accuracy of the model.\n', 'explainMyModel is an open source  webapp built to easily explain Supervised machine learning models merely by dragging and dropping without writing any code. \n', '\n', '## ✅ Technologies and libraries used\n', '| Name  | Brief |\n', '| ------------- | ------------- |\n', '| Streamlit  |  *An open source app framework for building beautiful data and ML apps* |\n', '| eli5  | *python library to debug ML classifier and expain thier predictions*  |\n', '| pdpbox | *python library used to visualize the impact of features on models* |\n', '| shap | *python library using game theoristic approach to explain outputs of ML models*|\n', '| Lime | |\n', '| Pandas | *python library for data manipulaion and analysis*|\n', '| Numpy | *Python library for numerical computation*|\n', '\n', '\n', '\n', '## ✅ Features to implement\n', 'Currently, explainMyModel uses library **eli5** for Permutation Importance, **pdpbox** for Partial Dependency plot and **shap** for SHAP(SHapley Additive exPlanations) values. Features to implement are;\n', '* LIME \n', '* Comparing features with pdpbox\n', '* Better UI\n', '* Add tree and deep earning explainer for shap values to improve result speed\n', '\n', '## ✅ Navigate explainMyModel\n', '### Home \n', '---> Gives a brief overview about the app and also explanation about what each explain feature means and how they work\n', '### explan\n', '---> This is the heart of the app, here the user uploads features on the app and selects any of the explain feature to get explanation \n', '### Tutorial\n', '---> This part contains some code template to help you save some of the files required to eplain the model.  \n', '\n', '## ✅ How to contribute\n', '* Fork the repo\n', '* Clone the repo\n', '* Navigate to your local repository\n', '* Pull latest changes from upstream to local repository\n', '* create new branch\n', '* Contribute\n', '* Commit changes\n', '* Push changes to your fork\n', '* Begin pull request(PR)\n']"
Model Explainability,iandragulet/xG_Model_Workflow,iandragulet,https://api.github.com/repos/iandragulet/xG_Model_Workflow,30,9,1,['https://api.github.com/users/iandragulet'],Python,2023-03-18T22:23:31Z,https://raw.githubusercontent.com/iandragulet/xG_Model_Workflow/main/README.md,"['# xG_model_workflow\n', 'Comprehensive Guide to explaining, creating and using an xG model \n', '\n', 'Part 1 covers an introduction, importing data, cleaning data and some data exploration\n', '\n', 'Part 2 is all about creating and testing a logistic regression model.\n', '\n', 'Part 3 is on applications of xG \n', '\n', 'note:  3 will be avaliable in the not so distant future\n']"
Model Explainability,ModelOriented/treeshap,ModelOriented,https://api.github.com/repos/ModelOriented/treeshap,57,15,5,"['https://api.github.com/users/konrad-komisarczyk', 'https://api.github.com/users/maksymiuks', 'https://api.github.com/users/hbaniecki', 'https://api.github.com/users/pbiecek', 'https://api.github.com/users/kapsner']",R,2023-04-25T12:42:51Z,https://raw.githubusercontent.com/ModelOriented/treeshap/master/README.md,"['\n', '<!-- README.md is generated from README.Rmd. Please edit that file -->\n', '\n', '# treeshap\n', '\n', '<!-- badges: start -->\n', '\n', '<!-- badges: end -->\n', '\n', 'In the era of complicated classifiers conquering their market, sometimes\n', 'even the authors of algorithms do not know the exact manner of building\n', 'a tree ensemble model. The difficulties in models’ structures are one of\n', 'the reasons why most users use them simply like black-boxes. But, how\n', 'can they know whether the prediction made by the model is reasonable?\n', '`treeshap` is an efficient answer for this question. Due to implementing\n', 'an optimised alghoritm for tree ensemble models, it calculates the SHAP\n', 'values in polynomial (instead of exponential) time. This metric is the\n', 'only possible way to measure the influence of every feature regardless\n', 'of the permutation of features. Moreover, `treeshap` package shares a\n', 'bunch of functions to unify the structure of a model. Currently it\n', 'supports models produced with `XGBoost`, `LightGBM`, `GBM`, `Catboost`,\n', '`ranger` and `randomForest`.\n', '\n', '## Installation\n', '\n', 'You can install the released version of treeshap using package\n', '`devtools` with:\n', '\n', '``` r\n', ""devtools::install_github('ModelOriented/treeshap')\n"", '```\n', '\n', '## Example\n', '\n', 'First of all, let’s focus on an example how to represent a `xgboost`\n', 'model as a unified model object:\n', '\n', '``` r\n', 'library(treeshap)\n', 'library(xgboost)\n', ""data <- fifa20$data[colnames(fifa20$data) != 'work_rate']\n"", 'target <- fifa20$target\n', 'param <- list(objective = ""reg:squarederror"", max_depth = 6)\n', 'xgb_model <- xgboost::xgboost(as.matrix(data), params = param, label = target, nrounds = 200, verbose = 0)\n', 'unified <- xgboost.unify(xgb_model, data)\n', 'head(unified$model)\n', '#>   Tree Node   Feature Decision.type Split Yes No Missing Prediction Cover\n', '#> 1    0    0   overall            <=  81.5   2  3       2         NA 18278\n', '#> 2    0    1   overall            <=  73.5   4  5       4         NA 17949\n', '#> 3    0    2   overall            <=  84.5   6  7       6         NA   329\n', '#> 4    0    3   overall            <=  69.5   8  9       8         NA 15628\n', '#> 5    0    4 potential            <=  79.5  10 11      10         NA  2321\n', '#> 6    0    5 potential            <=  83.5  12 13      12         NA   221\n', '```\n', '\n', 'Having the object of unified structure, it is a piece of cake to produce\n', 'shap values of for a specific observations. The `treeshap()` function\n', 'requires passing two data arguments: one representing an ensemble model\n', 'unified representation and one with the observations about which we want\n', 'to get the explanations. Obviously, the latter one should contain the\n', 'same columns as data used during building the model.\n', '\n', '``` r\n', 'treeshap1 <- treeshap(unified,  data[700:800, ], verbose = 0)\n', 'treeshap1$shaps[1:3, 1:6]\n', '#>            age height_cm weight_kg overall potential international_reputation\n', '#> 700   297154.4  5769.186 12136.316 8739757  212428.8               -50855.738\n', '#> 701 -2550066.6 16011.136  3134.526 6525123  244814.2                22784.430\n', '#> 702   300830.3 -9023.299 15374.550 8585145  479118.8                 2374.351\n', '```\n', '\n', 'We can also compute SHAP values for interactions. As an example we will\n', 'calculate them for a model built with simpler (only 5 columns) data.\n', '\n', '``` r\n', 'data2 <- fifa20$data[, 1:5]\n', 'xgb_model2 <- xgboost::xgboost(as.matrix(data2), params = param, label = target, nrounds = 200, verbose = 0)\n', 'unified2 <- xgboost.unify(xgb_model2, data2)\n', '\n', 'treeshap_interactions <- treeshap(unified2,  data2[1:300, ], interactions = TRUE, verbose = 0)\n', 'treeshap_interactions$interactions[, , 1:2]\n', '#> , , 1\n', '#> \n', '#>                   age  height_cm  weight_kg     overall  potential\n', '#> age       -1886241.70   -3984.09  -96765.97   -47245.92  1034657.6\n', '#> height_cm    -3984.09 -628797.41  -35476.11  1871689.75   685472.2\n', '#> weight_kg   -96765.97  -35476.11 -983162.25  2546930.16  1559453.5\n', '#> overall     -47245.92 1871689.75 2546930.16 55289985.16 12683135.3\n', '#> potential  1034657.61  685472.23 1559453.46 12683135.27   868268.7\n', '#> \n', '#> , , 2\n', '#> \n', '#>                  age  height_cm  weight_kg    overall  potential\n', '#> age       -2349987.9  306165.41  120483.91 -9871270.0  960198.02\n', '#> height_cm   306165.4  -78810.31  -48271.61  -991020.7  -44632.74\n', '#> weight_kg   120483.9  -48271.61  -21657.14  -615688.2 -380810.70\n', '#> overall   -9871270.0 -991020.68 -615688.21 57384425.2 9603937.05\n', '#> potential   960198.0  -44632.74 -380810.70  9603937.1 2994190.74\n', '```\n', '\n', '## Plotting results\n', '\n', 'The package currently provides 4 plotting functions that can be used:\n', '\n', '### Feature Contribution (Break-Down)\n', '\n', 'On this plot we can see how features contribute into the prediction for\n', 'a single observation. It is similar to the Break Down plot from\n', '[iBreakDown](https://github.com/ModelOriented/iBreakDown) package, which\n', 'uses different method to approximate SHAP\n', 'values.\n', '\n', '``` r\n', 'plot_contribution(treeshap1, obs = 1, min_max = c(0, 16000000))\n', '```\n', '\n', '<img src=""man/figures/README-plot_contribution_example-1.png"" width=""100%"" />\n', '\n', '### Feature Importance\n', '\n', 'This plot shows us average absolute impact of features on the prediction\n', 'of the\n', 'model.\n', '\n', '``` r\n', 'plot_feature_importance(treeshap1, max_vars = 6)\n', '```\n', '\n', '<img src=""man/figures/README-plot_importance_example-1.png"" width=""100%"" />\n', '\n', '### Feature Dependence\n', '\n', 'Using this plot we can see, how a single feature contributes into the\n', 'prediction depending on its\n', 'value.\n', '\n', '``` r\n', 'plot_feature_dependence(treeshap1, ""height_cm"")\n', '```\n', '\n', '<img src=""man/figures/README-plot_dependence_example-1.png"" width=""100%"" />\n', '\n', '### Interaction Plot\n', '\n', 'Simple plot to visualise an SHAP Interaction value of two features\n', 'depending on their values.\n', '\n', '``` r\n', 'plot_interaction(treeshap_interactions, ""height_cm"", ""overall"")\n', '```\n', '\n', '<img src=""man/figures/README-plot_interaction-1.png"" width=""100%"" />\n', '\n', '## How to use the unifying functions?\n', '\n', 'Even though the objects produced by the functions from `.unify()` family\n', '(`xgboost.unify()`, `lightgbm.unify()`, `gbm.unify()`,\n', '`catboost.unify()`, `randomForest.unify()`, `ranger.unify()`) are\n', 'identical when it comes to the structure, due to different possibilities\n', 'of saving and representing the trees among the packages, the usage of\n', 'functions is slightly different. As an argument, they all take an object\n', 'of appropriate model and dataset used to train the model. One of them,\n', '`catboost.unify()` requires also a transformed dataset used for training\n', 'the model - an object of class `catboost.Pool`.\n', '\n', '#### 1\\. GBM\n', '\n', 'An argument of `gbm.unify()` should be of `gbm` class - a gradient\n', 'boosting model.\n', '\n', '``` r\n', 'library(gbm)\n', 'library(treeshap)\n', ""x <- fifa20$data[colnames(fifa20$data) != 'work_rate']\n"", ""x['value_eur'] <- fifa20$target\n"", 'gbm_model <- gbm::gbm(\n', '  formula = value_eur ~ .,\n', '  data = x,\n', '  distribution = ""laplace"",\n', '  n.trees = 200,\n', '  cv.folds = 2,\n', '  interaction.depth = 2\n', ')\n', 'unified_gbm <- gbm.unify(gbm_model, x)\n', '```\n', '\n', '#### 2\\. Catboost\n', '\n', 'For representing correct names of features that are regarding during\n', 'splitting observations into sets, `catboost.unify()` requires passing\n', 'two arguments:\n', '\n', '``` r\n', 'library(treeshap)\n', 'library(catboost)\n', ""data <- fifa20$data[colnames(fifa20$data) != 'work_rate']\n"", 'label <- fifa20$target\n', 'dt.pool <- catboost::catboost.load_pool(data = as.data.frame(lapply(data, as.numeric)), label = label)\n', 'cat_model <- catboost::catboost.train(\n', '            dt.pool,\n', ""            params = list(loss_function = 'RMSE', iterations = 100,\n"", ""                          logging_level = 'Silent', allow_writing_files = FALSE))\n"", 'unified_catboost <- catboost.unify(cat_model, dt.pool, data)\n', '```\n', '\n', '## Setting reference dataset\n', '\n', 'Dataset used as a reference for calculating SHAP values is stored in\n', 'unified model representation object. It can be set any ime using\n', '`set_reference_dataset`\n', 'function.\n', '\n', '``` r\n', 'unified_catboost2 <- set_reference_dataset(unified_catboost, data[c(1000:2000), ])\n', '```\n', '\n', '## Other functionalities\n', '\n', 'Package also implements `predict` function for calculating model’s\n', 'predictions using unified representation.\n', '\n', '## How fast does it work?\n', '\n', 'Complexity of TreeSHAP is `O(TLD^2)`, where `T` is number of trees, `L`\n', 'is number of leaves in a tree and `D` is depth of a tree.\n', '\n', 'Our implementation works in speed comparable to original Lundberg’s\n', 'Python package `shap` implementation using C and Python.\n', '\n', 'In the following example our TreeSHAP implementation explains 300\n', 'observations on a model consisting of 200 trees of max depth = 6 in 1\n', 'second (on my almost 10 years old laptop with Intel i5-2520M).\n', '\n', '``` r\n', 'microbenchmark::microbenchmark(\n', '  treeshap = treeshap(unified,  data[1:300, ]), # using model and dataset from the example\n', '  times = 5\n', ')\n', '#> Unit: seconds\n', '#>      expr      min       lq     mean   median       uq      max neval\n', '#>  treeshap 1.027707 1.032991 1.032529 1.033427 1.034062 1.034459     5\n', '```\n', '\n', 'Complexity of SHAP interaction values computation is `O(MTLD^2)`, where\n', '`M` is number of variables in explained dataset, `T` is number of trees,\n', '`L` is number of leaves in a tree and `D` is depth of a tree.\n', '\n', 'SHAP Interaction values for 5 variables, model consisting of 200 trees\n', 'of max depth = 6 and 300 observations can be computed in less than 7\n', 'seconds.\n', '\n', '``` r\n', 'microbenchmark::microbenchmark(\n', '  treeshap = treeshap(unified2, data2[1:300, ], interactions = TRUE), # using model and dataset from the example\n', '  times = 5\n', ')\n', '#> Unit: seconds\n', '#>      expr      min      lq     mean  median       uq      max neval\n', '#>  treeshap 6.700848 6.70164 6.712134 6.70711 6.719313 6.731761     5\n', '```\n', '\n', '## References\n', '\n', '  - Scott M. Lundberg, Gabriel G. Erion, Su-In Lee, “Consistent\n', '    Individualized Feature Attribution for Tree Ensembles”, University\n', '    of Washington\n']"
Model Explainability,d909b/cxplain,d909b,https://api.github.com/repos/d909b/cxplain,109,30,1,['https://api.github.com/users/d909b'],Python,2023-04-07T11:41:35Z,https://raw.githubusercontent.com/d909b/cxplain/master/README.md,"['![CXPlain](http://schwabpatrick.com/img/cxplain_logo.png)\n', '\n', '![Code Coverage](https://img.shields.io/badge/Python-2.7,%203.7-blue)![Code Coverage](https://img.shields.io/badge/Coverage-88%25-green)\n', '\n', 'Causal Explanations (CXPlain) is a method for explaining the decisions of any machine-learning model. CXPlain uses explanation models trained with a causal objective to learn to explain machine-learning models, and to quantify the uncertainty of its explanations. This repository contains a reference implementation for neural explanation models, and several practical examples for different data modalities. Please see the manuscript at https://arxiv.org/abs/1910.12336 (NeurIPS 2019) for a description and experimental evaluation of CXPlain.\n', '\n', '## Install\n', '\n', 'To install the latest release:\n', '\n', '```\n', '$ pip install cxplain\n', '```\n', '\n', '## Use\n', '\n', 'A CXPlain model consists of four main components:\n', '- The model to be explained which can be any type of machine-learning model, including black-box models, such as neural networks and ensemble models.\n', '- The model builder that defines the structure of the explanation model to be used to explain the explained model.\n', '- The masking operation that defines how CXPlain will internally simulate the removal of input features from the set of available features.\n', '- The loss function that defines how the change in prediction accuracy incurred by removing an input feature will be measured by CXPlain.\n', '\n', 'After configuring these four components, you can fit a CXPlain instance to the same training data that was used to train your original model. The CXPlain instance can then explain any prediction of your explained model - even when no labels are available for that sample.\n', '\n', '```python\n', 'from tensorflow.python.keras.losses import categorical_crossentropy\n', 'from cxplain import MLPModelBuilder, ZeroMasking, CXPlain\n', '\n', 'x_train, y_train, x_test = ....  # Your dataset\n', 'explained_model = ...    # The model you wish to explain.\n', '\n', '# Define the model you want to use to explain your __explained_model__.\n', '# Here, we use a neural explanation model with a\n', '# multilayer perceptron (MLP) architecture.\n', 'model_builder = MLPModelBuilder(num_layers=2, num_units=64, batch_size=256, learning_rate=0.001)\n', '\n', '# Define your masking operation - the method of simulating the\n', '# removal of input features used internally by CXPlain - ZeroMasking is typically a sensible default choice for tabular and image data.\n', 'masking_operation = ZeroMasking()\n', '\n', ""# Define the loss with which each input features' associated reduction in prediction error is calculated.\n"", 'loss = categorical_crossentropy\n', '\n', '# Build and fit a CXPlain instance.\n', 'explainer = CXPlain(explained_model, model_builder, masking_operation, loss)\n', 'explainer.fit(x_train, y_train)\n', '\n', '# Use the __explainer__ to obtain explanations for the predictions of your __explained_model__.\n', 'attributions = explainer.explain(x_test)\n', '```\n', '\n', '## Examples\n', '\n', 'More practical examples for various input data modalities, including images, textual data and tabular data, and both regression and classification tasks are provided in form of Jupyter notebooks in the [examples/](examples) directory:\n', '- [Regression task on tabular data (Boston Housing)](examples/boston_housing.ipynb)\n', '- [Classification task on image data (CIFAR10)](examples/cifar10.ipynb)\n', '- [Classification task on image data (MNIST)](examples/mnist.ipynb)\n', '- [Classification task on textual data (IMDB)](examples/nlp.ipynb)\n', '- [Saving and loading CXPlain instances](examples/save_and_load.ipynb)\n', '\n', '![MNIST](http://schwabpatrick.com/img/mnist_samples.png)\n', '![ImageNet](http://schwabpatrick.com/img/imagenet_samples.png)\n', '<img src=""http://schwabpatrick.com/img/twitter_samples.png"" width=""310"">\n', '## Cite\n', '\n', 'Please consider citing, if you reference or use our methodology, code or results in your work:\n', '\n', '    @inproceedings{schwab2019cxplain,\n', '      title={{CXPlain: Causal Explanations for Model Interpretation under Uncertainty}},\n', '      author={Schwab, Patrick and Karlen, Walter},\n', '      booktitle={{Advances in Neural Information Processing Systems (NeurIPS)}},\n', '      year={2019}\n', '    }\n', '\n', '## License\n', '\n', '[MIT License](LICENSE.txt)\n', '\n', '## Acknowledgements\n', '\n', 'This work was partially funded by the Swiss National Science Foundation (SNSF) project No. 167302 within the National Research Program (NRP) 75 ""Big Data"". We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPUs used for this research. Patrick Schwab is an affiliated PhD fellow at the Max Planck ETH Center for Learning Systems.\n']"
Model Fairness,Trusted-AI/AIF360,Trusted-AI,https://api.github.com/repos/Trusted-AI/AIF360,2016,676,30,"['https://api.github.com/users/hoffmansc', 'https://api.github.com/users/nrkarthikeyan', 'https://api.github.com/users/michaelhind', 'https://api.github.com/users/animeshsingh', 'https://api.github.com/users/pronics2004', 'https://api.github.com/users/SSaishruthi', 'https://api.github.com/users/milevavantuyl', 'https://api.github.com/users/josue-rodriguez', 'https://api.github.com/users/sohiniu', 'https://api.github.com/users/gdequeiroz', 'https://api.github.com/users/krvarshney', 'https://api.github.com/users/autoih', 'https://api.github.com/users/barvek', 'https://api.github.com/users/zywind', 'https://api.github.com/users/monindersingh', 'https://api.github.com/users/Adebayo-Oshingbesan', 'https://api.github.com/users/ckadner', 'https://api.github.com/users/ivesulca', 'https://api.github.com/users/jimbudarz', 'https://api.github.com/users/romeokienzler', 'https://api.github.com/users/sreeja-g', 'https://api.github.com/users/dependabot%5Bbot%5D', 'https://api.github.com/users/mfeffer', 'https://api.github.com/users/adrinjalali', 'https://api.github.com/users/imgbot%5Bbot%5D', 'https://api.github.com/users/aitorres', 'https://api.github.com/users/bhavyaghai', 'https://api.github.com/users/hakimamarullah', 'https://api.github.com/users/chkoar', 'https://api.github.com/users/DanielRyszkaIBM']",Python,2023-04-25T14:07:50Z,https://raw.githubusercontent.com/Trusted-AI/AIF360/master/README.md,"['# AI Fairness 360 (AIF360)\n', '\n', '[![Continuous Integration](https://github.com/Trusted-AI/AIF360/actions/workflows/ci.yml/badge.svg)](https://github.com/Trusted-AI/AIF360/actions/workflows/ci.yml)\n', '[![Documentation](https://readthedocs.org/projects/aif360/badge/?version=latest)](http://aif360.readthedocs.io/en/latest/?badge=latest)\n', '[![PyPI version](https://badge.fury.io/py/aif360.svg)](https://badge.fury.io/py/aif360)\n', '[![CRAN\\_Status\\_Badge](http://www.r-pkg.org/badges/version/aif360)](https://cran.r-project.org/package=aif360)\n', '\n', 'The AI Fairness 360 toolkit is an extensible open-source library containing techniques developed by the\n', 'research community to help detect and mitigate bias in machine learning models throughout the AI application lifecycle. AI Fairness 360 package is available in both Python and R.\n', '\n', 'The AI Fairness 360 package includes\n', '1) a comprehensive set of metrics for datasets and models to test for biases,\n', '2) explanations for these metrics, and\n', '3) algorithms to mitigate bias in datasets and models.\n', 'It is designed to translate algorithmic research from the lab into the actual practice of domains as wide-ranging\n', 'as finance, human capital management, healthcare, and education. We invite you to use it and improve it.\n', '\n', 'The [AI Fairness 360 interactive experience](http://aif360.mybluemix.net/data)\n', 'provides a gentle introduction to the concepts and capabilities. The [tutorials\n', 'and other notebooks](./examples) offer a deeper, data scientist-oriented\n', 'introduction. The complete API is also available.\n', '\n', 'Being a comprehensive set of capabilities, it may be confusing to figure out\n', 'which metrics and algorithms are most appropriate for a given use case. To\n', 'help, we have created some [guidance\n', 'material](http://aif360.mybluemix.net/resources#guidance) that can be\n', 'consulted.\n', '\n', 'We have developed the package with extensibility in mind. This library is still\n', 'in development. We encourage the contribution of your metrics, explainers, and\n', 'debiasing algorithms.\n', '\n', 'Get in touch with us on [Slack](https://aif360.slack.com) (invitation\n', '[here](https://join.slack.com/t/aif360/shared_invite/zt-5hfvuafo-X0~g6tgJQ~7tIAT~S294TQ))!\n', '\n', '\n', '## Supported bias mitigation algorithms\n', '\n', '* Optimized Preprocessing ([Calmon et al., 2017](http://papers.nips.cc/paper/6988-optimized-pre-processing-for-discrimination-prevention))\n', '* Disparate Impact Remover ([Feldman et al., 2015](https://doi.org/10.1145/2783258.2783311))\n', '* Equalized Odds Postprocessing ([Hardt et al., 2016](https://papers.nips.cc/paper/6374-equality-of-opportunity-in-supervised-learning))\n', '* Reweighing ([Kamiran and Calders, 2012](http://doi.org/10.1007/s10115-011-0463-8))\n', '* Reject Option Classification ([Kamiran et al., 2012](https://doi.org/10.1109/ICDM.2012.45))\n', '* Prejudice Remover Regularizer ([Kamishima et al., 2012](https://rd.springer.com/chapter/10.1007/978-3-642-33486-3_3))\n', '* Calibrated Equalized Odds Postprocessing ([Pleiss et al., 2017](https://papers.nips.cc/paper/7151-on-fairness-and-calibration))\n', '* Learning Fair Representations ([Zemel et al., 2013](http://proceedings.mlr.press/v28/zemel13.html))\n', '* Adversarial Debiasing ([Zhang et al., 2018](https://arxiv.org/abs/1801.07593))\n', '* Meta-Algorithm for Fair Classification ([Celis et al., 2018](https://arxiv.org/abs/1806.06055))\n', '* Rich Subgroup Fairness ([Kearns, Neel, Roth, Wu, 2018](https://arxiv.org/abs/1711.05144))\n', '* Exponentiated Gradient Reduction ([Agarwal et al., 2018](https://arxiv.org/abs/1803.02453))\n', '* Grid Search Reduction ([Agarwal et al., 2018](https://arxiv.org/abs/1803.02453), [Agarwal et al., 2019](https://arxiv.org/abs/1905.12843))\n', '* Fair Data Adaptation ([Plečko and Meinshausen, 2020](https://www.jmlr.org/papers/v21/19-966.html), [Plečko et al., 2021](https://arxiv.org/abs/2110.10200))\n', '* Sensitive Set Invariance/Sensitive Subspace Robustness ([Yurochkin and Sun, 2020](https://arxiv.org/abs/2006.14168), [Yurochkin et al., 2019](https://arxiv.org/abs/1907.00020))\n', '\n', '## Supported fairness metrics\n', '\n', '* Comprehensive set of group fairness metrics derived from selection rates and error rates including rich subgroup fairness\n', '* Comprehensive set of sample distortion metrics\n', '* Generalized Entropy Index ([Speicher et al., 2018](https://doi.org/10.1145/3219819.3220046))\n', '* Differential Fairness and Bias Amplification ([Foulds et al., 2018](https://arxiv.org/pdf/1807.08362))\n', '* Bias Scan with Multi-Dimensional Subset Scan ([Zhang, Neill, 2017](https://arxiv.org/abs/1611.08292))\n', '\n', '## Setup\n', '\n', '### R\n', '\n', '``` r\n', 'install.packages(""aif360"")\n', '```\n', '\n', 'For more details regarding the R setup, please refer to instructions [here](aif360/aif360-r/README.md).\n', '\n', '### Python\n', '\n', 'Supported Python Configurations:\n', '\n', '| OS      | Python version |\n', '| ------- | -------------- |\n', '| macOS   | 3.7 – 3.10     |\n', '| Ubuntu  | 3.7 – 3.10     |\n', '| Windows | 3.7 – 3.10     |\n', '\n', '### (Optional) Create a virtual environment\n', '\n', 'AIF360 requires specific versions of many Python packages which may conflict\n', 'with other projects on your system. A virtual environment manager is strongly\n', 'recommended to ensure dependencies may be installed safely. If you have trouble\n', 'installing AIF360, try this first.\n', '\n', '#### Conda\n', '\n', 'Conda is recommended for all configurations though Virtualenv is generally\n', 'interchangeable for our purposes. [Miniconda](https://conda.io/miniconda.html)\n', 'is sufficient (see [the difference between Anaconda and\n', 'Miniconda](https://conda.io/docs/user-guide/install/download.html#anaconda-or-miniconda)\n', 'if you are curious) if you do not already have conda installed.\n', '\n', 'Then, to create a new Python 3.7 environment, run:\n', '\n', '```bash\n', 'conda create --name aif360 python=3.7\n', 'conda activate aif360\n', '```\n', '\n', 'The shell should now look like `(aif360) $`. To deactivate the environment, run:\n', '\n', '```bash\n', '(aif360)$ conda deactivate\n', '```\n', '\n', 'The prompt will return to `$ `.\n', '\n', 'Note: Older versions of conda may use `source activate aif360` and `source\n', 'deactivate` (`activate aif360` and `deactivate` on Windows).\n', '\n', '### Install with `pip`\n', '\n', 'To install the latest stable version from PyPI, run:\n', '\n', '```bash\n', 'pip install aif360\n', '```\n', '\n', 'Note: Some algorithms require additional dependencies (although the metrics will\n', 'all work out-of-the-box). To install with certain algorithm dependencies\n', 'included, run, e.g.:\n', '\n', '```bash\n', ""pip install 'aif360[LFR,OptimPreproc]'\n"", '```\n', '\n', 'or, for complete functionality, run:\n', '\n', '```bash\n', ""pip install 'aif360[all]'\n"", '```\n', '\n', 'The options for available extras are: `OptimPreproc, LFR, AdversarialDebiasing,\n', 'DisparateImpactRemover, LIME, ART, Reductions, notebooks, tests, docs, all`\n', '\n', 'If you encounter any errors, try the [Troubleshooting](#troubleshooting) steps.\n', '\n', '### Manual installation\n', '\n', 'Clone the latest version of this repository:\n', '\n', '```bash\n', 'git clone https://github.com/Trusted-AI/AIF360\n', '```\n', '\n', ""If you'd like to run the examples, download the datasets now and place them in\n"", 'their respective folders as described in\n', '[aif360/data/README.md](aif360/data/README.md).\n', '\n', 'Then, navigate to the root directory of the project and run:\n', '\n', '```bash\n', ""pip install --editable '.[all]'\n"", '```\n', '\n', '#### Run the Examples\n', '\n', 'To run the example notebooks, complete the manual installation steps above.\n', 'Then, if you did not use the `[all]` option, install the additional requirements\n', 'as follows:\n', '\n', '```bash\n', ""pip install -e '.[notebooks]'\n"", '```\n', '\n', 'Finally, if you did not already, download the datasets as described in\n', '[aif360/data/README.md](aif360/data/README.md).\n', '\n', '### Troubleshooting\n', '\n', 'If you encounter any errors during the installation process, look for your\n', 'issue here and try the solutions.\n', '\n', '#### TensorFlow\n', '\n', 'See the [Install TensorFlow with pip](https://www.tensorflow.org/install/pip)\n', 'page for detailed instructions.\n', '\n', ""Note: we require `'tensorflow >= 1.13.1'`.\n"", '\n', 'Once tensorflow is installed, try re-running:\n', '\n', '```bash\n', ""pip install 'aif360[AdversarialDebiasing]'\n"", '```\n', '\n', 'TensorFlow is only required for use with the\n', '`aif360.algorithms.inprocessing.AdversarialDebiasing` class.\n', '\n', '#### CVXPY\n', '\n', 'On MacOS, you may first have to install the Xcode Command Line Tools if you\n', 'never have previously:\n', '\n', '```sh\n', 'xcode-select --install\n', '```\n', '\n', 'On Windows, you may need to download the [Microsoft C++ Build Tools for Visual\n', 'Studio 2019](https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=BuildTools&rel=16).\n', 'See the [CVXPY Install](https://www.cvxpy.org/install/index.html#mac-os-x-windows-and-linux)\n', 'page for up-to-date instructions.\n', '\n', 'Then, try reinstalling via:\n', '\n', '```bash\n', ""pip install 'aif360[OptimPreproc]'\n"", '```\n', '\n', 'CVXPY is only required for use with the\n', '`aif360.algorithms.preprocessing.OptimPreproc` class.\n', '\n', '## Using AIF360\n', '\n', 'The `examples` directory contains a diverse collection of jupyter notebooks\n', 'that use AI Fairness 360 in various ways. Both tutorials and demos illustrate\n', 'working code using AIF360. Tutorials provide additional discussion that walks\n', 'the user through the various steps of the notebook. See the details about\n', '[tutorials and demos here](examples/README.md)\n', '\n', '## Citing AIF360\n', '\n', 'A technical description of AI Fairness 360 is available in this\n', '[paper](https://arxiv.org/abs/1810.01943). Below is the bibtex entry for this\n', 'paper.\n', '\n', '```\n', '@misc{aif360-oct-2018,\n', '    title = ""{AI Fairness} 360:  An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias"",\n', '    author = {Rachel K. E. Bellamy and Kuntal Dey and Michael Hind and\n', '\tSamuel C. Hoffman and Stephanie Houde and Kalapriya Kannan and\n', '\tPranay Lohia and Jacquelyn Martino and Sameep Mehta and\n', '\tAleksandra Mojsilovic and Seema Nagar and Karthikeyan Natesan Ramamurthy and\n', '\tJohn Richards and Diptikalyan Saha and Prasanna Sattigeri and\n', '\tMoninder Singh and Kush R. Varshney and Yunfeng Zhang},\n', '    month = oct,\n', '    year = {2018},\n', '    url = {https://arxiv.org/abs/1810.01943}\n', '}\n', '```\n', '\n', '## AIF360 Videos\n', '\n', '* Introductory [video](https://www.youtube.com/watch?v=X1NsrcaRQTE) to AI\n', '  Fairness 360 by Kush Varshney, September 20, 2018 (32 mins)\n', '\n', '## Contributing\n', 'The development fork for Rich Subgroup Fairness (`inprocessing/gerryfair_classifier.py`) is [here](https://github.com/sethneel/aif360). Contributions are welcome and a list of potential contributions from the authors can be found [here](https://trello.com/b/0OwPcbVr/gerryfair-development).\n']"
Model Fairness,facebookresearch/SlowFast,facebookresearch,https://api.github.com/repos/facebookresearch/SlowFast,5612,1132,26,"['https://api.github.com/users/haooooooqi', 'https://api.github.com/users/lyttonhao', 'https://api.github.com/users/feichtenhofer', 'https://api.github.com/users/bxiong1202', 'https://api.github.com/users/chayryali', 'https://api.github.com/users/ShoufaChen', 'https://api.github.com/users/AlexanderMelde', 'https://api.github.com/users/archen2019', 'https://api.github.com/users/r-barnes', 'https://api.github.com/users/StanislavGlebik', 'https://api.github.com/users/anhminh3105', 'https://api.github.com/users/chandra-siri', 'https://api.github.com/users/chaoyuaw', 'https://api.github.com/users/fmassa', 'https://api.github.com/users/karttikeya', 'https://api.github.com/users/leszfb', 'https://api.github.com/users/menglioculus', 'https://api.github.com/users/min-xu-ai', 'https://api.github.com/users/patricklabatut', 'https://api.github.com/users/renganxu', 'https://api.github.com/users/akindofyoga', 'https://api.github.com/users/Shumpei-Kikuta', 'https://api.github.com/users/tullie', 'https://api.github.com/users/ythu2', 'https://api.github.com/users/facebook-github-bot', 'https://api.github.com/users/kalyanvasudev']",Python,2023-04-26T09:23:28Z,https://raw.githubusercontent.com/facebookresearch/SlowFast/main/README.md,"['# PySlowFast\n', '\n', 'PySlowFast is an open source video understanding codebase from FAIR that provides state-of-the-art video classification models with efficient training. This repository includes implementations of the following methods:\n', '\n', '- [SlowFast Networks for Video Recognition](https://arxiv.org/abs/1812.03982)\n', '- [Non-local Neural Networks](https://arxiv.org/abs/1711.07971)\n', '- [A Multigrid Method for Efficiently Training Video Models](https://arxiv.org/abs/1912.00998)\n', '- [X3D: Progressive Network Expansion for Efficient Video Recognition](https://arxiv.org/abs/2004.04730)\n', '- [Multiscale Vision Transformers](https://arxiv.org/abs/2104.11227)\n', '- [A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning](https://arxiv.org/abs/2104.14558)\n', '- [MViTv2: Improved Multiscale Vision Transformers for Classification and Detection](https://arxiv.org/abs/2112.01526)\n', '- [Masked Feature Prediction for Self-Supervised Visual Pre-Training](https://arxiv.org/abs/2112.09133)\n', '- [Masked Autoencoders As Spatiotemporal Learners](https://arxiv.org/abs/2205.09113)\n', '- [Reversible Vision Transformers](https://openaccess.thecvf.com/content/CVPR2022/papers/Mangalam_Reversible_Vision_Transformers_CVPR_2022_paper.pdf)\n', '\n', '<div align=""center"">\n', '  <img src=""demo/ava_demo.gif"" width=""600px""/>\n', '</div>\n', '\n', '## Introduction\n', '\n', 'The goal of PySlowFast is to provide a high-performance, light-weight pytorch codebase provides state-of-the-art video backbones for video understanding research on different tasks (classification, detection, and etc). It is designed in order to support rapid implementation and evaluation of novel video research ideas. PySlowFast includes implementations of the following backbone network architectures:\n', '\n', '- SlowFast\n', '- Slow\n', '- C2D\n', '- I3D\n', '- Non-local Network\n', '- X3D\n', '- MViTv1 and MViTv2\n', '- Rev-ViT and Rev-MViT\n', '\n', '## Updates\n', ' - We now [Reversible Vision Transformers](https://openaccess.thecvf.com/content/CVPR2022/papers/Mangalam_Reversible_Vision_Transformers_CVPR_2022_paper.pdf). Both Reversible ViT and MViT models released. See [`projects/rev`](./projects/rev/README.md).\n', ' - We now support [MAE for Video](https://arxiv.org/abs/2104.11227.pdf). See [`projects/mae`](./projects/mae/README.md) for more information.\n', ' - We now support [MaskFeat](https://arxiv.org/abs/2112.09133). See [`projects/maskfeat`](./projects/maskfeat/README.md) for more information.\n', ' - We now support [MViTv2](https://arxiv.org/abs/2104.11227.pdf) in PySlowFast. See [`projects/mvitv2`](./projects/mvitv2/README.md) for more information.\n', ' - We now support [A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning](https://arxiv.org/abs/2104.14558). See [`projects/contrastive_ssl`](./projects/contrastive_ssl/README.md) for more information.\n', ' - We now support [Multiscale Vision Transformers](https://arxiv.org/abs/2104.11227.pdf) on Kinetics and ImageNet. See [`projects/mvit`](./projects/mvit/README.md) for more information.\n', ' - We now support [PyTorchVideo](https://github.com/facebookresearch/pytorchvideo) models and datasets. See [`projects/pytorchvideo`](./projects/pytorchvideo/README.md) for more information.\n', ' - We now support [X3D Models](https://arxiv.org/abs/2004.04730). See [`projects/x3d`](./projects/x3d/README.md) for more information.\n', ' - We now support [Multigrid Training](https://arxiv.org/abs/1912.00998) for efficiently training video models. See [`projects/multigrid`](./projects/multigrid/README.md) for more information.\n', ' - PySlowFast is released in conjunction with our [ICCV 2019 Tutorial](https://alexander-kirillov.github.io/tutorials/visual-recognition-iccv19/).\n', '\n', '## License\n', '\n', 'PySlowFast is released under the [Apache 2.0 license](LICENSE).\n', '\n', '## Model Zoo and Baselines\n', '\n', 'We provide a large set of baseline results and trained models available for download in the PySlowFast [Model Zoo](MODEL_ZOO.md).\n', '\n', '## Installation\n', '\n', 'Please find installation instructions for PyTorch and PySlowFast in [INSTALL.md](INSTALL.md). You may follow the instructions in [DATASET.md](slowfast/datasets/DATASET.md) to prepare the datasets.\n', '\n', '## Quick Start\n', '\n', 'Follow the example in [GETTING_STARTED.md](GETTING_STARTED.md) to start playing video models with PySlowFast.\n', '\n', '## Visualization Tools\n', '\n', 'We offer a range of visualization tools for the train/eval/test processes, model analysis, and for running inference with trained model.\n', 'More information at [Visualization Tools](VISUALIZATION_TOOLS.md).\n', '\n', '## Contributors\n', 'PySlowFast is written and maintained by [Haoqi Fan](https://haoqifan.github.io/), [Yanghao Li](https://lyttonhao.github.io/), [Bo Xiong](https://www.cs.utexas.edu/~bxiong/), [Wan-Yen Lo](https://www.linkedin.com/in/wanyenlo/), [Christoph Feichtenhofer](https://feichtenhofer.github.io/).\n', '\n', '## Citing PySlowFast\n', 'If you find PySlowFast useful in your research, please use the following BibTeX entry for citation.\n', '```BibTeX\n', '@misc{fan2020pyslowfast,\n', '  author =       {Haoqi Fan and Yanghao Li and Bo Xiong and Wan-Yen Lo and\n', '                  Christoph Feichtenhofer},\n', '  title =        {PySlowFast},\n', '  howpublished = {\\url{https://github.com/facebookresearch/slowfast}},\n', '  year =         {2020}\n', '}\n', '```\n']"
Model Fairness,aws/amazon-sagemaker-clarify,aws,https://api.github.com/repos/aws/amazon-sagemaker-clarify,56,33,13,"['https://api.github.com/users/larroy', 'https://api.github.com/users/Satish615', 'https://api.github.com/users/xgchena', 'https://api.github.com/users/keerthanvasist', 'https://api.github.com/users/milah', 'https://api.github.com/users/pinaraws', 'https://api.github.com/users/prkrishnan1', 'https://api.github.com/users/xinyu7030', 'https://api.github.com/users/eytsai', 'https://api.github.com/users/amazon-auto', 'https://api.github.com/users/jmikko', 'https://api.github.com/users/xiaoyi-cheng', 'https://api.github.com/users/dosatos']",Python,2023-04-24T16:47:23Z,https://raw.githubusercontent.com/aws/amazon-sagemaker-clarify/master/README.md,"['![Python package](https://github.com/aws/amazon-sagemaker-clarify/workflows/Python%20package/badge.svg)\n', '![Pypi](https://img.shields.io/pypi/v/smclarify.svg?maxAge=60)\n', '![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg?style=flat)\n', '\n', '# smclarify\n', '\n', 'Amazon Sagemaker Clarify\n', '\n', 'Bias detection and mitigation for datasets and models.\n', '\n', '\n', '# Installation\n', '\n', 'To install the package from PIP you can simply do:\n', '\n', '```\n', 'pip install smclarify\n', '```\n', '\n', 'You can see examples on running the Bias metrics on the notebooks in the [examples folder](https://github.com/aws/amazon-sagemaker-clarify/tree/master/examples).\n', '\n', '\n', '# Terminology\n', '\n', '### Facet\n', 'A facet is column or feature that will be used to measure bias against. A facet can have value(s) that designates that sample as ""***sensitive***"".\n', '\n', '### Label\n', 'The label is a column or feature which is the target for training a machine learning model. The label can have value(s) that designates that sample as having a ""***positive***"" outcome.\n', '\n', '### Bias measure\n', 'A bias measure is a function that returns a bias metric.\n', '\n', '### Bias metric\n', 'A bias metric is a numerical value indicating the level of bias detected as determined by a particular bias measure.\n', '\n', '### Bias report\n', 'A collection of bias metrics for a given dataset or a combination of a dataset and model.\n', '\n', '# Development\n', '\n', ""It's recommended that you setup a virtualenv.\n"", '\n', '```\n', 'virtualenv -p(which python3) venv\n', 'source venv/bin/activate.fish\n', 'pip install -e .[test]\n', 'cd src/\n', '../devtool all\n', '```\n', '\n', 'For running unit tests, do `pytest --pspec`. If you are using PyCharm, and cannot see the green run button next to the tests, open `Preferences` -> `Tools` -> `Python Integrated tools`, and set default test runner to `pytest`.\n', '\n', 'For Internal contributors, run ```../devtool integ_tests``` after creating virtualenv with the above steps to run the integration tests.\n']"
Model Fairness,jphall663/interpretable_machine_learning_with_python,jphall663,https://api.github.com/repos/jphall663/interpretable_machine_learning_with_python,633,200,5,"['https://api.github.com/users/jphall663', 'https://api.github.com/users/esztiorm', 'https://api.github.com/users/ntache', 'https://api.github.com/users/aosama', 'https://api.github.com/users/APorterOReilly']",Python,2023-03-10T14:09:23Z,https://raw.githubusercontent.com/jphall663/interpretable_machine_learning_with_python/master/README.md,"['# Responsible Machine Learning with Python\n', 'Examples of techniques for training interpretable machine learning (ML) models, explaining ML models, and debugging ML models for accuracy, discrimination, and security.\n', '\n', '\n', '### Overview\n', '\n', ""Usage of artificial intelligence (AI) and ML models is likely to become more commonplace as larger swaths of the economy embrace automation and data-driven decision-making. While these predictive systems can be quite accurate, they have often been inscrutable and unappealable black boxes that produce only numeric predictions with no accompanying explanations. Unfortunately, recent studies and recent events have drawn attention to mathematical and sociological flaws in prominent weak AI and ML systems, but practitioners don’t often have the right tools to pry open ML models and debug them. This series of notebooks introduces several approaches that increase transparency, accountability, and trustworthiness in ML models. If you are a data scientist or analyst and you want to train accurate, interpretable ML models, explain ML models to your customers or managers, test those models for security vulnerabilities or social discrimination, or if you have concerns about documentation, validation, or regulatory requirements, then this series of Jupyter notebooks is for you! (But *please* don't take these notebooks or associated materials as legal compliance advice.)\n"", '\n', 'The notebooks highlight techniques such as:\n', '* [Monotonic XGBoost models, partial dependence, individual conditional expectation plots, and Shapley explanations](https://github.com/jphall663/interpretable_machine_learning_with_python#enhancing-transparency-in-machine-learning-models-with-python-and-xgboost---notebook)\n', '* [Decision tree surrogates, reason codes, and ensembles of explanations](https://github.com/jphall663/interpretable_machine_learning_with_python#increase-transparency-and-accountability-in-your-machine-learning-project-with-python---notebook)\n', '* [Disparate impact analysis](https://github.com/jphall663/interpretable_machine_learning_with_python#increase-fairness-in-your-machine-learning-project-with-disparate-impact-analysis-using-python-and-h2o---notebook)\n', '* [LIME](https://github.com/jphall663/interpretable_machine_learning_with_python#explain-your-predictive-models-to-business-stakeholders-with-lime-using-python-and-h2o---notebook)\n', '* [Sensitivity and residual analysis](https://github.com/jphall663/interpretable_machine_learning_with_python#testing-machine-learning-models-for-accuracy-trustworthiness-and-stability-with-python-and-h2o---notebook)\n', '  * [Advanced sensitivity analysis for model debugging](https://github.com/jphall663/interpretable_machine_learning_with_python#part-1-sensitivity-analysis---notebook)\n', '  * [Advanced residual analysis for model debugging](https://github.com/jphall663/interpretable_machine_learning_with_python#part-2-residual-analysis---notebook)\n', '* [Detailed model comparison and model selection by cross-validated ranking](https://github.com/jphall663/interpretable_machine_learning_with_python#from-glm-to-gbm-building-the-case-for-complexity---notebook)\n', '\n', 'The notebooks can be accessed through:\n', '* [H2O Aquarium (Recommended)](https://github.com/jphall663/interpretable_machine_learning_with_python#h2o-aquarium-recommended)\n', '* [Virtualenv (Advanced)](https://github.com/jphall663/interpretable_machine_learning_with_python#virtualenv-installation)\n', '* [Docker container (Advanced)](https://github.com/jphall663/interpretable_machine_learning_with_python#docker-installation)\n', '* [Manual installation (Advanced)](https://github.com/jphall663/interpretable_machine_learning_with_python#manual-installation)\n', '\n', '#### Further reading:\n', '* [*Machine Learning: Considerations for fairly and transparently expanding access to credit*](http://info.h2o.ai/rs/644-PKX-778/images/Machine%20Learning%20-%20Considerations%20for%20Fairly%20and%20Transparently%20Expanding%20Access%20to%20Credit.pdf)\n', '* [*A Responsible Machine Learning Workflow with Focus on Interpretable Models, Post-hoc Explanation, and Discrimination Testing*](https://www.mdpi.com/2078-2489/11/3/137)\n', '* [*An Introduction to Machine Learning Interpretability, 2nd Edition*](https://www.h2o.ai/wp-content/uploads/2019/08/An-Introduction-to-Machine-Learning-Interpretability-Second-Edition.pdf)\n', '* [*On the Art and Science of Explainable Machine Learning*](https://arxiv.org/pdf/1810.02909.pdf)\n', '* [*Proposals for model vulnerability and security*](https://www.oreilly.com/ideas/proposals-for-model-vulnerability-and-security)\n', '* [*Proposed Guidelines for the Responsible Use of Explainable Machine Learning*](https://arxiv.org/pdf/1906.03533.pdf)\n', '* [*Real-World Strategies for Model Debugging*](https://medium.com/@jphall_22520/strategies-for-model-debugging-aa822f1097ce)\n', '* [*Warning Signs: Security and Privacy in an Age of Machine Learning*](https://fpf.org/wp-content/uploads/2019/09/FPF_WarningSigns_Report.pdf)\n', '* [*Why you should care about debugging machine learning models*](https://www.oreilly.com/radar/why-you-should-care-about-debugging-machine-learning-models/)\n', '\n', '***\n', '\n', '### Enhancing Transparency in Machine Learning Models with Python and XGBoost - [Notebook](https://nbviewer.jupyter.org/github/jphall663/interpretable_machine_learning_with_python/blob/master/xgboost_pdp_ice.ipynb)\n', '\n', '![](./readme_pics/pdp_ice.png)\n', '\n', 'Monotonicity constraints can turn opaque, complex models into transparent, and potentially regulator-approved models, by ensuring predictions only increase or only decrease for any change in a given input variable. In this notebook, I will demonstrate how to use monotonicity constraints in the popular open source gradient boosting package XGBoost to train an interpretable and accurate nonlinear classifier on the UCI credit card default data.\n', '\n', 'Once we have trained a monotonic XGBoost model, we will use partial dependence plots and individual conditional expectation (ICE) plots to investigate the internal mechanisms of the model and to verify its monotonic behavior. Partial dependence plots show us the way machine-learned response functions change based on the values of one or two input variables of interest while averaging out the effects of all other input variables. ICE plots can be used to create more localized descriptions of model predictions, and ICE plots pair nicely with partial dependence plots. An example of generating regulator mandated reason codes from high fidelity Shapley explanations for any model prediction is also presented. The combination of monotonic XGBoost, partial dependence, ICE, and Shapley explanations is likely one of the most direct ways to create an interpretable machine learning model today.\n', '\n', '\n', '### Increase Transparency and Accountability in Your Machine Learning Project with Python - [Notebook](https://nbviewer.jupyter.org/github/jphall663/interpretable_machine_learning_with_python/blob/master/dt_surrogate_loco.ipynb)\n', '\n', '![](./readme_pics/dt_surrogate.png)\n', '\n', 'Gradient boosting machines (GBMs) and other complex machine learning models are popular and accurate prediction tools, but they can be difficult to interpret. Surrogate models, feature importance, and reason codes can be used to explain and increase transparency in machine learning models. In this notebook, we will train a GBM on the UCI credit card default data. Then we’ll train a decision tree surrogate model on the original inputs and predictions of the complex GBM model and see how the variable importance and interactions displayed in the surrogate model yield an overall, approximate flowchart of the complex model’s predictions. We will also analyze the global variable importance of the GBM and compare this information to the surrogate model, our domain expertise, and our reasonable expectations.\n', '\n', 'To get a better picture of the complex model’s local behavior and to enhance the accountability of the model’s predictions, we will use a variant of the leave-one-covariate-out (LOCO) technique. LOCO enables us to calculate the local contribution each input variable makes toward each model prediction. We will then rank the local contributions to generate reason codes that describe, in plain English, the model’s decision process for every prediction.\n', '\n', '### Increase Fairness in Your Machine Learning Project with Disparate Impact Analysis using Python and H2O - [Notebook](https://nbviewer.jupyter.org/github/jphall663/interpretable_machine_learning_with_python/blob/master/dia.ipynb)\n', '\n', '<img src=""./readme_pics/dia.png"" height=""400"">\n', '\n', 'Fairness is an incredibly important, but highly complex entity. So much so that leading scholars have yet to agree on a strict definition. However, there is a practical way to discuss and handle observational fairness, or how your model predictions affect different groups of people. This procedure is often known as disparate impact analysis (DIA). DIA is far from perfect, as it relies heavily on user-defined thresholds and reference levels to measure disparity and does not attempt to remediate disparity or provide information on sources of disparity, but it is a fairly straightforward method to quantify your model’s behavior across sensitive demographic segments or other potentially interesting groups of observations. Some types of DIA are also an accepted, regulation-compliant tool for fair-lending purposes in the U.S. financial services industry. If it’s good enough for multibillion-dollar credit portfolios, it’s probably good enough for your project.\n', '\n', 'This example DIA notebook starts by training a monotonic gradient boosting machine (GBM) classifier on the UCI credit card default data using the popular open source library, h2o. A probability cutoff for making credit decisions is selected by maximizing the F1 statistic and confusion matrices are generated to summarize the GBM’s decisions across men and women. A basic DIA procedure is then conducted using the information stored in the confusion matrices and some traditional fair lending measures.\n', '\n', '### Explain Your Predictive Models to Business Stakeholders with LIME using Python and H2O - [Notebook](https://nbviewer.jupyter.org/github/jphall663/interpretable_machine_learning_with_python/blob/master/lime.ipynb)\n', '\n', '![](./readme_pics/lime.png)\n', '\n', 'Machine learning can create very accurate predictive models, but these models can be almost impossible to explain to your boss, your customers, or even your regulators. This notebook will use (Local Interpretable Model-agnostic Explanations) LIME to increase transparency and accountability in a complex GBM model trained on the UCI credit card default data. LIME is a method for building linear surrogate models for local regions in a data set, often single rows of data. LIME sheds light on how model predictions are made and describes local model mechanisms for specific rows of data. Because the LIME sampling process may feel abstract to some practitioners, this notebook will also introduce a more straightforward method of creating local samples for LIME.\n', '\n', 'Once local samples have been generated, we will fit LIME models to understand local trends in the complex model’s predictions. LIME can also tell us the local contribution of each input variable toward each model prediction, and these contributions can be sorted to create reason codes -- plain English explanations of every model prediction. We will also validate the fit of the LIME model to enhance trust in our explanations using the local model’s R2 statistic and a ranked prediction plot.\n', '\n', '### Testing Machine Learning Models for Accuracy, Trustworthiness, and Stability with Python and H2O - [Notebook](https://nbviewer.jupyter.org/github/jphall663/interpretable_machine_learning_with_python/blob/master/resid_sens_analysis.ipynb)\n', '\n', '![](./readme_pics/resid.png)\n', '\n', 'Because ML model predictions can vary drastically for small changes in input variable values, especially outside of training input domains, sensitivity analysis is perhaps the most important validation technique for increasing trust in ML model predictions. Sensitivity analysis investigates whether model behavior and outputs remain stable when input data is intentionally perturbed, or other changes are simulated in input data. In this notebook, we will enhance trust in a complex credit default model by testing and debugging its predictions with sensitivity analysis.\n', '\n', 'We’ll further enhance trust in our model using residual analysis. Residuals refer to the difference between the recorded value of a target variable and the predicted value of a target variable for each row in a data set. Generally, the residuals of a well-fit model should be randomly distributed, because good models will account for most phenomena in a data set, except for random error. In this notebook, we will create residual plots for a complex model to debug any accuracy problems arising from overfitting or outliers.\n', '\n', '### Machine Learning Model Debugging with Python: All Models are Wrong ... but Why is _My_ Model Wrong? (And Can I Fix It?)\n', '\n', '##### Part 1: Sensitivity Analysis - [Notebook](https://nbviewer.jupyter.org/github/jphall663/interpretable_machine_learning_with_python/blob/master/debugging_sens_analysis_redux.ipynb)\n', '\n', '![](/readme_pics/sa.png)\n', '\n', 'Sensitivity analysis is the perturbation of data under a trained model. It can take many forms and arguably Shapley feature importance, partial dependence, individual conditional expectation, and adversarial examples are all types of sensitivity analysis. This notebook focuses on using these different types of sensitivity analysis to discover error mechanisms and security vulnerabilities and to assess stability and fairness in a trained XGBoost model. It begins by loading the UCI credit card default data and then training an interpretable, monotonically constrained XGBoost model. After the model is trained, global and local Shapley feature importance is calculated. These Shapley values help inform the application of partial dependence and ICE, and together these results guide a search for adversarial examples. The notebook closes by exposing the trained model to a random attack and analyzing the attack results.\n', '\n', 'These model debugging exercises uncover accuracy, drift, and security problems such as over-emphasis of important features and impactful yet non-robust interactions. Several remediation mechanisms are proposed including editing of final model artifacts to remove or fix errors, missing value injection or regularization during training to lessen the impact of certain features or interactions, and assertion-based missing value injection during scoring to mitigate the effect of non-robust interactions.\n', '\n', '##### Part 2: Residual Analysis - [Notebook](https://nbviewer.jupyter.org/github/jphall663/interpretable_machine_learning_with_python/blob/master/debugging_resid_analysis_redux.ipynb)\n', '\n', '![](readme_pics/resid2.png)\n', '\n', 'In general, residual analysis can be characterized as a careful study of when and how models make mistakes. A better understanding of mistakes will hopefully lead to fewer of them. This notebook uses variants of residual analysis to find error mechanisms and security vulnerabilities and to assess stability and fairness in a trained XGBoost model. It begins by loading the UCI credit card default data and then training an interpretable, monotonically constrained XGBoost gradient boosting machine (GBM) model. (Pearson correlation with the prediction target is used to determine the direction of the monotonicity constraints for each input variable.) After the model is trained, its logloss residuals are analyzed and explained thoroughly and the constrained GBM is compared to a benchmark linear model. These model debugging exercises uncover accuracy, drift, and security problems such as over-emphasis of important variables and strong signal in model residuals. Several remediation mechanisms are proposed including missing value injection during training, additional data collection, and use of assertions to correct known problems during scoring.\n', '\n', '### From GLM to GBM: Building the Case For Complexity - [Notebook](https://nbviewer.jupyter.org/github/jphall663/interpretable_machine_learning_with_python/blob/master/glm_mgbm_gbm.ipynb)\n', '\n', '![](readme_pics/hist_pd_ice_lo.png)\n', '\n', 'This notebook uses the same credit card default scenario to show how monotonicity constraints, Shapley values and other post-hoc explanations, and discrimination testing can enable practitioners to create direct comparisons between GLM and GBM models. Several candidate probability of default models are selected for comparison using feature selection methods, like LASSO, and by cross-validated ranking. Comparisons then enable building from GLM to more complex GBM models in a step-by-step manner, while retaining model transparency and the ability to test for discrimination. This notebook shows that GBMs can yield better accuracy, more revenue, and that GBMs are also likely to fulfill many model documentation, adverse action notice, and discrimination testing requirements.\n', '\n', '## Using the Examples\n', '\n', '### H2O Aquarium (recommended)\n', '\n', 'H2O Aquarium is a free educational environment that hosts versions of these notebooks among many other H2o-related resources. To use these notebooks in Aquarium:\n', '\n', '1. Navigate to the Aquarium URL: https://aquarium.h2o.ai.\n', '\n', '2. Create a new Aquarium account.\n', '\n', '3. Check the registered email inbox and use the temporary password to login to Aquarium.\n', '\n', '4. Click `Browse Labs`.\n', '\n', '5. Click `View Detail` under *Open Source MLI Workshop*.\n', '\n', '6. Click `Start Lab` (this can take several minutes).\n', '\n', '7. Click on the *Jupyter URL* when it becomes available.\n', '\n', '8. Enter the token `h2o`.\n', '\n', '9. Click the `patrick_hall_mli` folder.\n', '\n', '10. Browse/run the Jupyter notebooks.\n', '\n', '11. Click `End Lab` when you are finished.\n', '\n', '### Virtualenv Installation\n', '\n', 'For avid Python users, creating a Python virtual environment is a convenient way to run these notebooks.\n', '\n', '1. Install [Git](https://git-scm.com/downloads).\n', '\n', '2. Clone this repository with the examples.</br>\n', '`$ git clone https://github.com/jphall663/interpretable_machine_learning_with_python.git`\n', '\n', '3. Install Anaconda Python 5.1.0 from the [Anaconda archives](https://repo.continuum.io/archive/) and add it to your system path.\n', '\n', '4. Change directories into the cloned repository.</br>\n', '`$ cd interpretable_machine_learning_with_python`\n', '\n', '5. Create a Python 3.6 virtual environment.</br>\n', '`$ virtualenv -p /path/to/anaconda3/bin/python3.6 env_iml`\n', '\n', '6. Activate the virtual environment.</br>\n', '`$ source env_iml/bin/activate`\n', '\n', '7. Install the correct packages for the example notebooks.</br>\n', '`$ pip install -r requirements.txt`\n', '\n', '8. Start Jupyter.</br>\n', '`$ jupyter notebook`\n', '\n', '### Docker Installation\n', '\n', 'A Dockerfile is provided to build a docker container with all necessary packages and dependencies. This is a way to use these examples if you are on Mac OS X, \\*nix, or Windows 10. To do so:\n', '\n', '1. Install and start [docker](https://www.docker.com/).\n', '\n', 'From a terminal:\n', '\n', '2. Create a directory for the Dockerfile.</br>\n', '`$ mkdir anaconda_py36_h2o_xgboost_graphviz_shap`\n', '\n', '3. Fetch the Dockerfile.</br>\n', '`$ curl https://raw.githubusercontent.com/jphall663/interpretable_machine_learning_with_python/master/anaconda_py36_h2o_xgboost_graphviz_shap/Dockerfile > anaconda_py36_h2o_xgboost_graphviz_shap/Dockerfile`\n', '\n', '4. Build a docker image from the Dockefile.</br>\n', '`docker build -t iml anaconda_py36_h2o_xgboost_graphviz_shap`\n', '\n', '5. Start the docker image and the Jupyter notebook server.</br>\n', ' `docker run -i -t -p 8888:8888 iml:latest /bin/bash -c ""/opt/conda/bin/jupyter notebook --notebook-dir=/interpretable_machine_learning_with_python --allow-root --ip=\'*\' --port=8888 --no-browser""`\n', '\n', '6. Navigate to port 8888 on your machine, probably `http://localhost:8888/`.\n', '\n', '\n', '### Manual Installation\n', '\n', '1. Anaconda Python 5.1.0 from the [Anaconda archives](https://repo.continuum.io/archive/).\n', '2. [Java](https://java.com/download).\n', '3. The latest stable [h2o](https://www.h2o.ai/download/) Python package.\n', '4. [Git](https://git-scm.com/downloads).\n', '5. [XGBoost](https://github.com/dmlc/xgboost) with Python bindings.\n', '6. [GraphViz](http://www.graphviz.org/).\n', '7. [Seaborn](https://pypi.org/project/seaborn/) package.\n', '8. [Shap](https://pypi.org/project/shap/) package.  \n', '\n', 'Anaconda Python, Java, Git, and GraphViz must be added to your system path.\n', '\n', 'From a terminal:\n', '\n', '9. Clone the repository with examples.</br>\n', '`$ git clone https://github.com/jphall663/interpretable_machine_learning_with_python.git`\n', '\n', '10. `$ cd interpretable_machine_learning_with_python`\n', '\n', '11. Start the Jupyter notebook server.</br>\n', '`$ jupyter notebook`\n', '\n', '12. Navigate to the port Jupyter directs you to on your machine, probably `http://localhost:8888/`.\n']"
Model Fairness,IBM/monitor-wml-model-with-watson-openscale,IBM,https://api.github.com/repos/IBM/monitor-wml-model-with-watson-openscale,12,18,8,"['https://api.github.com/users/scottdangelo', 'https://api.github.com/users/rhagarty', 'https://api.github.com/users/stevemart', 'https://api.github.com/users/sanjeevghimire', 'https://api.github.com/users/dolph', 'https://api.github.com/users/imgbot%5Bbot%5D', 'https://api.github.com/users/ljbennett62', 'https://api.github.com/users/sandhya-nayak']",Python,2021-08-17T17:13:42Z,https://raw.githubusercontent.com/IBM/monitor-wml-model-with-watson-openscale/master/README.md,"['# Monitor WML Model With Watson OpenScale\n', '\n', ""In this Code Pattern, we will use German Credit data to train, create, and deploy a machine learning model using [Watson Machine Learning](https://console.bluemix.net/catalog/services/machine-learning). We will create a data mart for this model with [Watson OpenScale](https://www.ibm.com/cloud/watson-openscale/) and configure OpenScale to monitor that deployment, and inject seven days' worth of historical records and measurements for viewing in the OpenScale Insights dashboard.\n"", '\n', 'When the reader has completed this Code Pattern, they will understand how to:\n', '\n', '* Create and deploy a machine learning model using the Watson Machine Learning service\n', '* Setup Watson OpenScale Data Mart\n', '* Bind Watson Machine Learning to the Watson OpenScale Data Mart\n', '* Add subscriptions to the Data Mart\n', '* Enable payload logging and performance monitor for subscribed assets\n', '* Enable Quality (Accuracy) monitor\n', '* Enable Fairness monitor\n', '* Enable Drift montitor\n', '* Score the German credit model using the Watson Machine Learning\n', '* Insert historic payloads, fairness metrics, and quality metrics into the Data Mart\n', '* Use Data Mart to access tables data via subscription\n', '\n', '![architecture](doc/source/images/architecture.png)\n', '\n', '## Flow\n', '\n', '1. The developer creates a Jupyter Notebook on Watson Studio.\n', '2. The Jupyter Notebook is connected to a PostgreSQL database, which is used to store Watson OpenScale data.\n', '3. The notebook is connected to Watson Machine Learning and a model is trained and deployed.\n', '4. Watson OpenScale is used by the notebook to log payload and monitor performance, quality, and fairness.\n', '\n', '## Prerequisites\n', '\n', '* An [IBM Cloud Account](https://cloud.ibm.com/)\n', '* [IBM Cloud CLI](https://cloud.ibm.com/docs/cli/reference/ibmcloud/download_cli.html#install_use)\n', '* [IBM Cloud Object Storage (COS)](https://www.ibm.com/cloud/object-storage)\n', '* An account on [IBM Watson Studio](https://dataplatform.cloud.ibm.com/).\n', '\n', '# Steps\n', '\n', '1. [Clone the repository](#1-clone-the-repository)\n', '1. [Use free internal DB or Create a Databases for PostgreSQL DB](#2-use-free-internal-db-or-create-a-databases-for-postgresql-db)\n', '1. [Create a Watson OpenScale service](#3-create-a-watson-openscale-service)\n', '1. [Create a Watson Machine Learning instance](#4-create-a-watson-machine-learning-instance)\n', '1. [Create a notebook in IBM Watson Studio on Cloud Pak for Data](#5-create-a-notebook-in-ibm-watson-studio-on-cloud-pak-for-data)\n', '1. [Run the notebook in IBM Watson Studio](#6-run-the-notebook-in-ibm-watson-studio)\n', '1. [Setup OpenScale to utilize the dashboard](#7-setup-openscale-to-utilize-the-dashboard)\n', '\n', '### 1. Clone the repository\n', '\n', '```bash\n', 'git clone https://github.com/IBM/monitor-wml-model-with-watson-openscale\n', 'cd monitor-wml-model-with-watson-openscale\n', '```\n', '\n', '### 2. Use free internal DB or Create a Databases for PostgreSQL DB\n', '\n', '#### If you wish, you can use the free internal Database with Watson OpenScale. To do this, make sure that the cell for `KEEP_MY_INTERNAL_POSTGRES = True` remains unchanged.\n', '\n', '#### If you have or wish to use a paid `Databases for Postgres` instance, follow these instructions:\n', '\n', '> Note: Services created must be in the same region, and space, as your Watson Studio service.\n', '\n', '* Using the [IBM Cloud Dashboard](https://cloud.ibm.com/catalog) catalog, search for PostgreSQL and choose the `Databases for Postgres` [service](https://console.bluemix.net/catalog/services/databases-for-postgresql).\n', '* Wait for the database to be provisioned.\n', '* Click on the `Service Credentials` tab on the left and then click `New credential +` to create the service credentials. Copy them or leave the tab open to use later in the notebook.\n', '* Make sure that the cell in the notebook that has:\n', '\n', '```python\n', 'KEEP_MY_INTERNAL_POSTGRES = True\n', '```\n', '\n', 'is changed to:\n', '\n', '```python\n', 'KEEP_MY_INTERNAL_POSTGRES = False\n', '```\n', '\n', '### 3. Create a Watson OpenScale service\n', '\n', 'Create Watson OpenScale, either on the IBM Cloud or using your On-Premise Cloud Pak for Data.\n', '\n', '<details><summary>On IBM Cloud</summary>\n', '\n', '* If you do not have an IBM Cloud account, [register for an account](https://cloud.ibm.com/registration)\n', '\n', '* Create a Watson OpenScale instance from the [IBM Cloud catalog](https://cloud.ibm.com/catalog/services/watson-openscale)\n', '\n', '* Select the *Lite* (Free) plan, enter a *Service name*, and click *Create*.\n', '\n', '* Click *Launch Application* to start Watson OpenScale.\n', '\n', '* Click *Auto setup* to automatically set up your Watson OpenScale instance with sample data.\n', '\n', '  ![Cloud auto setup](doc/source/images/cloud-auto-setup.png)\n', '\n', '* Click *Start tour*  to tour the Watson OpenScale dashboard.\n', '\n', '</details>\n', '\n', '<details><summary>On IBM Cloud Pak for Data platform</summary>\n', '\n', '> Note: This assumes that your Cloud Pak for Data Cluster Admin has already installed and provisioned OpenScale on the cluster.\n', '\n', '* In the Cloud Pak for Data instance, go the (☰) menu and under `Services` section, click on the `Instances` menu option.\n', '\n', '  ![Service](doc/source/images/services.png)\n', '\n', '* Find the `OpenScale-default` instance from the instances table and click the three vertical dots to open the action menu, then click on the `Open` option.\n', '\n', '  ![Openscale Tile](doc/source/images/services-wos-instance.png)\n', '\n', '* If you need to give other users access to the OpenScale instance, go the (☰) menu and under `Services` section, click on the `Instances` menu option.\n', '\n', '  ![Service](doc/source/images/services.png)\n', '\n', '* Find the `OpenScale-default` instance from the instances table and click the three vertical dots to open the action menu, then click on the `Manage access` option.\n', '\n', '  ![Openscale Tile](doc/source/images/services-wos-manageaccess.png)\n', '\n', '* To add users to the service instance, click the `Add users` button.\n', '\n', '  ![Openscale Tile](doc/source/images/services-wos-addusers.png)\n', '\n', '* For all of the user accounts, select the `Editor` role for each user and then click the `Add` button.\n', '\n', '  ![Openscale Tile](doc/source/images/services-wos-userrole.png)\n', '\n', '</details>\n', '\n', '### 4. Create a Watson Machine Learning instance\n', '\n', '* Under the `Settings` tab, scroll down to `Associated services`, click `+ Add service` and choose `Watson`:\n', '\n', '  ![Add service](https://github.com/IBM/pattern-images/blob/master/watson-studio/images/wml-add-menu.png)\n', '\n', '* Search for `Machine Learning`, Verify this service is being created in the same space as the app in Step 1, and click `Create`.\n', '\n', '  ![Create Machine Learning](https://raw.githubusercontent.com/IBM/pattern-images/master/machine-learning/create-machine-learning.png)\n', '\n', '* Alternately, you can choose an existing Machine Learning instance and click on `Select`.\n', '\n', '* The Watson Machine Learning service is now listed as one of your `Associated Services`.\n', '\n', '* In a different browser tab go to [https://cloud.ibm.com/](https://cloud.ibm.com/) and log in to the Dashboard.\n', '\n', '* Click on your Watson Machine Learning instance under `Services`, click on `Service credentials` and then on `View credentials` to see the credentials.\n', '\n', '  ![](https://raw.githubusercontent.com/IBM/pattern-images/master/machine-learning/ML-service-credentials.png)\n', '\n', '* Save the credentials in a file. You will use them inside the notebook.\n', '\n', '### 5. Create a notebook in IBM Watson Studio on Cloud Pak for Data\n', '\n', '* In [Watson Studio](https://dataplatform.cloud.ibm.com/) or your on-premise Cloud Pak for Data, click `New Project +` under Projects or, at the top of the page click `+ New` and choose the tile for `Data Science` and then `Create Project`.\n', '\n', ""* Using the project you've created, click on `+ Add to project` and then choose the  `Notebook` tile, OR in the `Assets` tab under `Notebooks` choose `+ New notebook` to create a notebook.\n"", '\n', '* Select the `From URL` tab. [1]\n', '\n', '* Enter a name for the notebook. [2]\n', '\n', '* Optionally, enter a description for the notebook. [3]\n', '\n', '* For `Runtime` select the `Default Spark Python 3.7 ` option. [4]\n', '\n', '* Under `Notebook URL` provide the following url: [https://raw.githubusercontent.com/IBM/monitor-wml-model-with-watson-openscale/master/notebooks/WatsonOpenScaleMachineLearning.ipynb](https://raw.githubusercontent.com/IBM/monitor-wml-model-with-watson-openscale/master/notebooks/WatsonOpenScaleMachineLearning.ipynb)\n', '\n', '> Note: The current default (as of 8/11/2021) is Python 3.8. This will cause an error when installing the `pyspark.sql SparkSession` library, so make sure that you are using Python 3.7\n', '\n', '* Click the `Create notebook` button. [6]\n', '\n', '![OpenScale Notebook Create](doc/source/images/OpenScaleNotebookCreate.png)\n', '\n', '### 6. Run the notebook in IBM Watson Studio\n', '\n', 'Follow the instructions for `Provision services and configure credentials`:\n', '\n', 'Your Cloud API key can be generated by going to the [**Users** section of the Cloud console](https://cloud.ibm.com/iam#/users).\n', '\n', '* From that page, click your name, scroll down to the **API Keys** section, and click **Create an IBM Cloud API key**.\n', '\n', '* Give your key a name and click **Create**, then copy the created key and paste it below.\n', '\n', 'Alternately, from the [IBM Cloud CLI](https://console.bluemix.net/docs/cli/reference/ibmcloud/download_cli.html#install_use) :\n', '\n', '```bash\n', 'ibmcloud login --sso\n', ""ibmcloud iam api-key-create 'my_key'\n"", '```\n', '\n', '* Enter the `CLOUD_API_KEY` in the cell `1.1 Cloud API key`.\n', '\n', '#### Create COS bucket and get credentials\n', '\n', '* In your [IBM Cloud Object Storage](https://www.ibm.com/cloud/object-storage)  instance, create a bucket with a globally unique name. The UI will let you know if there is a naming conflict. This will be used in cell *1.3.1* as *BUCKET_NAME*.\n', '\n', '* In your [IBM Cloud Object Storage](https://www.ibm.com/cloud/object-storage) instance, get the Service Credentials for use as `COS_API_KEY_ID`, `COS_RESOURCE_CRN`, and `COS_ENDPOINT`:\n', '\n', '  ![COS credentials](doc/source/images/cos-credentials.png)\n', '\n', '* Add the COS credentials in cell *1.2 Cloud object storage details*.\n', '\n', '* Insert your BUCKET_NAME in the cell *1.2.1 Bucket name*.\n', '\n', '* Either use the internal Database, which requires *No Changes* or Add your `DB_CREDENTIALS` after reading the instructions preceeding that cell and change the cell `KEEP_MY_INTERNAL_POSTGRES = True` to become `KEEP_MY_INTERNAL_POSTGRES = False`.\n', '\n', '* Move your cursor to each code cell and run the code in it. Read the comments for each cell to understand what the code is doing. **Important** when the code in a cell is still running, the label to the left changes to **In [\\*]**:.\n', '  Do **not** continue to the next cell until the code is finished running.\n', '\n', '## 7. Setup OpenScale to utilize the dashboard\n', '\n', 'Now that you have created a machine learning model, you can utilize the [OpenScale dashboard](https://aiopenscale.cloud.ibm.com) to gather insights.\n', '\n', '### Sample Output\n', '\n', 'You can find a sample notebook with output for [WatsonOpenScaleMachineLearning-with-outputs.ipynb](notebooks/with-outputs/WatsonOpenScaleMachineLearning-with-outputs.ipynb).\n', '\n', '#### Openscale Dashboard\n', '\n', '* Go to the instance of [Watson OpenScale](https://aiopenscale.cloud.ibm.com/aiopenscale/insights) for an IBM Cloud deployment, or to your deployed instance on Cloud Pak for Data on-premise version. Choose the `Insights` tab to get an overview of your monitored deployments, Accuracy alerts, and Fairness alerts.\n', '\n', '![WOS insights](doc/source/images/WOSinsights.png)\n', '\n', '* Click on the tile for the `Spark German Risk Deployment` and you can see tiles for the `Fairness`, `Accuracy`, and `Performance monitors`.\n', '\n', '![OpenScale monitors](doc/source/images/wos-credit-risk-dashboard.png)\n', '\n', '* Click on one of the tiles, such as *Drift* to view details. Click on a point on the graph for more information on that particular time slice.\n', '\n', '![Drift monitor](doc/source/images/wos-drift-monitor.png)\n', '\n', '* You will see which types of drift were detected. Click on the number to bring up a list of transactions that led to drift.\n', '\n', '![Drift transactions](doc/source/images/wos-drift-details.png)\n', '\n', ""* Click on the `Explain` icon on the left-hand menu and you'll see a list of transactions that have been run using an algorithm to provide explainability. Choose one and click `Explain`.\n"", '\n', '![Choose transaction to explain](doc/source/images/wos-explain-menu.png)\n', '\n', '* You will see a graph showing all the most influential features with the relative weights of contribution to the *Predicted outcome*.\n', '\n', '![View feature weights](doc/source/images/wos-explainability-feature-weights.png)\n', '\n', '* Click the `Inspect` tab and you can experiment with changing the values of various features to see how that would affect the outcome. Click the `Run analysis` button to see what changes would be required to change the outcome.\n', '\n', '![Inspect features and change](doc/source/images/wos-inspect-features.png)\n', '\n', '# License\n', '[Apache 2.0](LICENSE)\n']"
Model Fairness,ebagdasa/differential-privacy-vs-fairness,ebagdasa,https://api.github.com/repos/ebagdasa/differential-privacy-vs-fairness,30,11,2,"['https://api.github.com/users/ebagdasa', 'https://api.github.com/users/OmidPoursaeed']",Python,2023-03-11T01:35:18Z,https://raw.githubusercontent.com/ebagdasa/differential-privacy-vs-fairness/master/README.md,"['## Readme\n', '\n', 'The paper discusses how Differential Privacy (specifically DPSGD from [1]) \n', 'impacts model performance for underrepresented groups. \n', '\n', '### Usage\n', '\n', 'Configure environment by running: `pip install -r requirements.txt`\n', '\n', 'We use Python3.7 and GPU Nvidia TitanX.\n', '\n', 'File playing.py allows run the code. It uses `utils/params.yaml` \n', 'to set parameters from the paper and builds a graph on Tensorboard.\n', 'For Sentiment prediction we use `playing_nlp.py`.\n', '\n', '\n', 'Datasets:\n', '1. MNIST (part of PyTorch)\n', '2. Diversity in Faces (obtained from IBM [here](https://www.research.ibm.com/artificial-intelligence/trusted-ai/diversity-in-faces/#access))\n', '3. iNaturalist (download from [here](https://github.com/visipedia/inat_comp))\n', '4. UTKFace (from [here](http://aicip.eecs.utk.edu/wiki/UTKFace))\n', '5. AAE Twitter corpus (from [here](http://slanglab.cs.umass.edu/TwitterAAE/))\n', '\n', 'We use `compute_dp_sgd_privacy.py` copied from public [repo](https://github.com/tensorflow/privacy)\n', '\n', 'DP-FedAvg implementation is taken from public [repo](https://github.com/ebagdasa/backdoor_federated_learning)  \n', '\n', 'Implementation of DPSGD is based on TF Privacy [repo](https://github.com/tensorflow/privacy) and papers:\n', '\n', '[1] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang. Deep learning with differential privacy. In CCS, 2016.\n', '\n', '[2] H. B. McMahan and G. Andrew. A general approach to adding differential privacy to iterative training procedures. arXiv:1812.06210, 2018\n', '\n', '[3] H. B. McMahan, D. Ramage, K. Talwar, and L. Zhang. Learning differentially private recurrent language models. In ICLR, 2018\n']"
Model Fairness,ModelOriented/DALEX,ModelOriented,https://api.github.com/repos/ModelOriented/DALEX,1197,159,20,"['https://api.github.com/users/pbiecek', 'https://api.github.com/users/hbaniecki', 'https://api.github.com/users/maksymiuks', 'https://api.github.com/users/piotrpiatyszek', 'https://api.github.com/users/WojciechKretowicz', 'https://api.github.com/users/kevinykuo', 'https://api.github.com/users/jakwisn', 'https://api.github.com/users/12tafran', 'https://api.github.com/users/agosiewska', 'https://api.github.com/users/arturzolkowski', 'https://api.github.com/users/adrianstando', 'https://api.github.com/users/CahidArda', 'https://api.github.com/users/kmatusz', 'https://api.github.com/users/emiliawisnios', 'https://api.github.com/users/kasiapekala', 'https://api.github.com/users/MarcinKosinski', 'https://api.github.com/users/krzyzinskim', 'https://api.github.com/users/philip-khor', 'https://api.github.com/users/sai-krishna-msk', 'https://api.github.com/users/RoyalTS']",Python,2023-04-26T15:44:52Z,https://raw.githubusercontent.com/ModelOriented/DALEX/master/README.md,"['# moDel Agnostic Language for Exploration and eXplanation <img src=""man/figures/logo.png"" align=""right"" width=""150""/>\n', '\n', '[![R build status](https://github.com/ModelOriented/DALEX/workflows/R-CMD-check/badge.svg)](https://github.com/ModelOriented/DALEX/actions?query=workflow%3AR-CMD-check)\n', '[![Coverage\n', 'Status](https://img.shields.io/codecov/c/github/ModelOriented/DALEX/master.svg)](https://codecov.io/github/ModelOriented/DALEX?branch=master)\n', '[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/DALEX)](https://cran.r-project.org/package=DALEX)\n', '[![Total Downloads](http://cranlogs.r-pkg.org/badges/grand-total/DALEX?color=orange)](http://cranlogs.r-pkg.org/badges/grand-total/DALEX)\n', '[![DrWhy-eXtrAI](https://img.shields.io/badge/DrWhy-BackBone-373589)](http://drwhy.ai/#BackBone)\n', '\n', '[![Python-check](https://github.com/ModelOriented/DALEX/workflows/Python-check/badge.svg)](https://github.com/ModelOriented/DALEX/actions?query=workflow%3APython-check)\n', '[![Supported Python versions](https://img.shields.io/pypi/pyversions/dalex.svg)](https://pypi.org/project/dalex/)\n', '[![PyPI version](https://badge.fury.io/py/dalex.svg)](https://badge.fury.io/py/dalex)\n', '[![Downloads](https://pepy.tech/badge/dalex)](https://pepy.tech/project/dalex)\n', '\n', '\n', '## Overview\n', '\n', 'Unverified black box model is the path to the failure. Opaqueness leads to distrust. Distrust leads to ignoration. Ignoration leads to rejection.\n', '\n', 'The `DALEX` package xrays any model and helps to explore and explain its behaviour, helps to understand how complex models are working. The main function `explain()` creates a wrapper around a predictive model. Wrapped models may then be explored and compared with a collection of local and global explainers. Recent developents from the area of Interpretable Machine Learning/eXplainable Artificial Intelligence.\n', '\n', 'The philosophy behind `DALEX` explanations is described in the [Explanatory Model Analysis](https://pbiecek.github.io/ema/) e-book. The `DALEX` package is a part of [DrWhy.AI](http://DrWhy.AI) universe.\n', '\n', 'If you work with `scikit-learn`, `keras`, `H2O`, `tidymodels`, `xgboost`, `mlr` or `mlr3` in R, you may be interested in the [DALEXtra](https://github.com/ModelOriented/DALEXtra) package, which is an extension of `DALEX` with easy to use `explain_*()` functions for models created in these libraries.\n', '\n', '**[Additional overview of the dalex Python package is available.](https://github.com/ModelOriented/DALEX/tree/master/python/dalex)**\n', '\n', '<p align=""center"">\n', '<a href=""https://pbiecek.github.io/ema/introduction.html#bookstructure""><img src=""https://github.com/ModelOriented/DALEX/raw/master/misc/DALEXpiramide.png"" width=""800""/></a>\n', '</p>\n', '\n', '## Installation\n', '\n', 'The `DALEX` **R** package can be installed from [CRAN](https://cran.r-project.org/package=DALEX)\n', '\n', '```r\n', 'install.packages(""DALEX"")\n', '```\n', '\n', 'The `dalex` **Python** package is available on [PyPI](https://pypi.org/project/dalex/) and [conda-forge](https://anaconda.org/conda-forge/dalex)\n', '\n', '```console\n', 'pip install dalex -U\n', '\n', 'conda install -c conda-forge dalex\n', '```\n', '\n', '## Learn more\n', '\n', 'Machine Learning models are widely used and have various applications in classification or regression tasks. Due to increasing computational power, availability of new data sources and new methods, ML models are more and more complex. Models created with techniques like boosting, bagging of neural networks are true black boxes. It is hard to trace the link between input variables and model outcomes. They are use because of high performance, but lack of interpretability is one of their weakest sides.\n', '\n', 'In many applications we need to know, understand or prove how input variables are used in the model and what impact do they have on final model prediction. `DALEX` is a set of tools that help to understand how complex models are working.\n', '\n', '<p align=""center"">\n', '<a href=""https://github.com/ModelOriented/DALEX/raw/master/misc/cheatsheet_local_explainers.png""><img src=""https://github.com/ModelOriented/DALEX/raw/master/misc/cheatsheet_local_explainers.png"" width=""500""/></a>\n', '</p>\n', '\n', '## Resources\n', '\n', '* [Gentle introduction to DALEX with examples in R and Python](https://pbiecek.github.io/ema/)\n', '\n', '### R package\n', '\n', '* [Introduction to Responsible Machine Learning @ useR! 2021](https://github.com/MI2DataLab/ResponsibleML-UseR2021)\n', '* DALEX + mlr3 [@ BioColl 2021](https://github.com/pbiecek/BioColl2021) & [@ Open-Forest-Training 2021](https://github.com/pbiecek/Open-Forest-Training-2021/)\n', '* [Materials from Explanatory Model Analysis Workshop @ eRum 2020](https://github.com/pbiecek/XAIatERUM2020), [cheatsheet](https://github.com/pbiecek/XAIatERUM2020/blob/master/Cheatsheet.pdf)\n', '* How to use DALEX with: [keras](https://rawgit.com/pbiecek/DALEX_docs/master/vignettes/DALEX_and_keras.html), [parsnip](https://raw.githack.com/pbiecek/DALEX_docs/master/vignettes/DALEX_parsnip.html), [caret](https://raw.githack.com/pbiecek/DALEX_docs/master/vignettes/DALEX_caret.html), [mlr](https://raw.githack.com/pbiecek/DALEX_docs/master/vignettes/DALEX_mlr.html), [H2O](https://raw.githack.com/pbiecek/DALEX_docs/master/vignettes/DALEX_h2o.html), [xgboost](https://raw.githack.com/pbiecek/DALEX_docs/master/vignettes/DALEX_and_xgboost.html)\n', '* [Compare GBM models created in different languages](https://raw.githack.com/pbiecek/DALEX_docs/master/vignettes/Multilanguages_comparision.html): gbm and CatBoost in R / gbm in h2o / gbm in Python\n', '* [DALEX for fraud detection](https://rawgit.com/pbiecek/DALEX_docs/master/vignettes/DALEXverse%20and%20fraud%20detection.html)\n', '* [DALEX for teaching](https://raw.githack.com/pbiecek/DALEX_docs/master/vignettes/DALEX_teaching.html)\n', '* [XAI in the jungle of competing frameworks for machine learning](https://medium.com/@ModelOriented/xai-in-the-jungle-of-competing-frameworks-for-machine-learning-fa6e96a99644)\n', '\n', '### Python package\n', '\n', '* Introduction to the `dalex` package: [Titanic: tutorial and examples](https://dalex.drwhy.ai/python-dalex-titanic.html)\n', '* Key features explained: [FIFA20: explain default vs tuned model with dalex](https://dalex.drwhy.ai/python-dalex-fifa.html)\n', '* How to use dalex with: [xgboost](https://dalex.drwhy.ai/python-dalex-xgboost.html), [tensorflow](https://dalex.drwhy.ai/python-dalex-tensorflow.html)\n', '* More explanations: [residuals, shap, lime](https://dalex.drwhy.ai/python-dalex-new.html)\n', '* Introduction to the [Fairness module in dalex](https://dalex.drwhy.ai/python-dalex-fairness.html)\n', '* Introduction to the [Arena: interactive dashboard for model exploration](https://dalex.drwhy.ai/python-dalex-arena.html)\n', '* Code in the form of [jupyter notebook](https://github.com/ModelOriented/DALEX-docs/tree/master/jupyter-notebooks)\n', '* Changelog: [NEWS](https://github.com/ModelOriented/DALEX/blob/master/python/dalex/NEWS.md)\n', '\n', '### Talks about DALEX\n', '\n', '* [Talk with your model! at USeR 2020](https://www.youtube.com/watch?v=9WWn5ew8D8o)\n', '* [Talk about DALEX at Complexity Institute / NTU February 2018](https://github.com/pbiecek/Talks/blob/master/2018/DALEX_at_NTU_2018.pdf)\n', '* [Talk about DALEX at SER / WTU April 2018](https://github.com/pbiecek/Talks/blob/master/2018/SER_DALEX.pdf)\n', '* [Talk about DALEX at STWUR May 2018 (in Polish)](https://github.com/STWUR/eRementarz-29-05-2018)\n', '* [Talk about DALEX at BayArea 2018](https://github.com/pbiecek/Talks/blob/master/2018/DALEX_BayArea.pdf)\n', '* [Talk about DALEX at PyData Warsaw 2018](https://github.com/pbiecek/Talks/blob/master/2018/DALEX_PyDataWarsaw2018.pdf)\n', '\n', '## Citation\n', '\n', 'If you use `DALEX` in R or `dalex` in Python, please cite our JMLR papers:\n', '\n', '```html\n', '@article{JMLR:v19:18-416,\n', '  author  = {Przemyslaw Biecek},\n', '  title   = {DALEX: Explainers for Complex Predictive Models in R},\n', '  journal = {Journal of Machine Learning Research},\n', '  year    = {2018},\n', '  volume  = {19},\n', '  number  = {84},\n', '  pages   = {1-5},\n', '  url     = {http://jmlr.org/papers/v19/18-416.html}\n', '}\n', '\n', '@article{JMLR:v22:20-1473,\n', '  author  = {Hubert Baniecki and\n', '             Wojciech Kretowicz and\n', '             Piotr Piatyszek and \n', '             Jakub Wisniewski and \n', '             Przemyslaw Biecek},\n', '  title   = {dalex: Responsible Machine Learning \n', '             with Interactive Explainability and Fairness in Python},\n', '  journal = {Journal of Machine Learning Research},\n', '  year    = {2021},\n', '  volume  = {22},\n', '  number  = {214},\n', '  pages   = {1-7},\n', '  url     = {http://jmlr.org/papers/v22/20-1473.html}\n', '}\n', '```\n', '\n', '## Why\n', '\n', '76 years ago Isaac Asimov devised [Three Laws of Robotics](https://en.wikipedia.org/wiki/Three_Laws_of_Robotics): 1) a robot may not injure a human being, 2) a robot must obey the orders given it by human beings and 3) A robot must protect its own existence. These laws impact discussion around [Ethics of AI](https://en.wikipedia.org/wiki/Ethics_of_artificial_intelligence). Today’s robots, like cleaning robots, robotic pets or autonomous cars are far from being conscious enough to be under Asimov’s ethics.\n', '\n', ""Today we are surrounded by complex predictive algorithms used for decision making. Machine learning models are used in health care, politics, education, judiciary and many other areas. Black box predictive models have far larger influence on our lives than physical robots. Yet, applications of such models are left unregulated despite many examples of their potential harmfulness. See *Weapons of Math Destruction* by Cathy O'Neil for an excellent overview of potential problems.\n"", '\n', ""It's clear that we need to control algorithms that may affect us. Such control is in our civic rights. Here we propose three requirements that any predictive model should fulfill.\n"", '\n', ""-\t**Prediction's justifications**. For every prediction of a model one should be able to understand which variables affect the prediction and how strongly. Variable attribution to final prediction.\n"", ""-\t**Prediction's speculations**. For every prediction of a model one should be able to understand how the model prediction would change if input variables were changed. Hypothesizing about what-if scenarios.\n"", ""-\t**Prediction's validations** For every prediction of a model one should be able to verify how strong are evidences that confirm this particular prediction.\n"", '\n', 'There are two ways to comply with these requirements.\n', 'One is to use only models that fulfill these conditions by design. White-box models like linear regression or decision trees. In many cases the price for transparency is lower performance.\n', 'The other way is to use approximated explainers – techniques that find only approximated answers, but work for any black box model. Here we present such techniques.\n', '\n', '\n', '## Acknowledgments\n', '\n', 'Work on this package was financially supported by the `NCN Opus grant 2016/21/B/ST6/02176` and `NCN Opus grant 2017/27/B/ST6/01307`.\n']"
Model Fairness,FAIRplus/Data-Maturity,FAIRplus,https://api.github.com/repos/FAIRplus/Data-Maturity,9,9,6,"['https://api.github.com/users/lauportell', 'https://api.github.com/users/iemam', 'https://api.github.com/users/YojanaGadiya', 'https://api.github.com/users/actions-user', 'https://api.github.com/users/lrodrin', 'https://api.github.com/users/daniwelter']",SCSS,2023-03-23T02:38:05Z,https://raw.githubusercontent.com/FAIRplus/Data-Maturity/master/README.md,"['<img src=""https://fairplus-project.eu/images/fairplus-logo.png"" alt=""RDMkit logo"" width=""450""/>\n', '\n', '# FAIRplus Dataset Maturity (DSM) Model\n', '\n', 'The FAIRplus-DSM model is intended as a comprehensive reference model for state-of-FAIRness improvement in research datasets. Based on the FAIR guiding principles, the DSM model defines and classifies requirements that constitute an incremental path towards improving FAIRness level for a given research dataset.\n', '\n', '## Contributing\n', '\n', 'You are welcome to contribute to the content. The material is developed in markdown and a jekyll template ([Just the docs](https://pmarsceill.github.io/just-the-docs/)) is used to format the markdown pages and generate the website (https://fairplus.github.io/Data-Maturity/).\n', '\n', '- If you want to add content please create a new branch from this one. When you are ready to merge your changes open a pull request against this branch.\n', '- The content of the website is in markdown files in the [/docs](./docs) directory whereas the images included in the markdown files are in [assets/images](./assets/images).\n', '- Refer to the [Just the Docs documentation](https://pmarsceill.github.io/just-the-docs/) for usage and customisation information.\n', '\n', '## License\n', '\n', '- The FAIRplus DSM Model content is licensed under the [Creative Commons Attribution 4.0 International](https://creativecommons.org/licenses/by/4.0/) (CC BY 4.0) license.\n', '- The jekyll theme is available as open source under the terms of the [MIT License](https://opensource.org/licenses/MIT).\n']"
Model Fairness,neptune-ai/model-fairness-in-practice,neptune-ai,https://api.github.com/repos/neptune-ai/model-fairness-in-practice,8,6,1,['https://api.github.com/users/jakubczakon'],Python,2021-08-03T14:23:01Z,https://raw.githubusercontent.com/neptune-ai/model-fairness-in-practice/master/README.md,"['# model-fairness-in-practice\n', 'Materials for the ODSC West 2019 workshop ""Model fairness in practice""\n', '\n', '## Installation\n', 'Install and spin-up a notebook by running\n', '\n', '```bash\n', 'source Makefile\n', '```\n', '\n', '## Ask me anything\n', '\n', '[twitter @neptune_ai](https://twitter.com/Neptune_ai)\n', '\n', '[linkedin jakub.czakon](https://www.linkedin.com/in/jakub-czakon-2b797b69/)\n', '\n', '[neptune blog jakub.czakon](https://neptune.ai/blog/)\n', '\n', 'email jakub.czakon@neptune.ai\n']"
Model Fairness,LearnedVector/Wav2Letter,LearnedVector,https://api.github.com/repos/LearnedVector/Wav2Letter,81,24,1,['https://api.github.com/users/LearnedVector'],Python,2023-04-14T03:57:27Z,https://raw.githubusercontent.com/LearnedVector/Wav2Letter/master/README.md,"['# Wav2Letter Speech Recognition with Pytorch\n', '\n', 'A Simple, straight forward, easy to read implementation of Wav2Letter, a speech recognition model from Facebooks AI Research (FAIR) [paper](https://arxiv.org/pdf/1609.03193.pdf). You can see most of the architecture in the `Wav2Letter` directory.\n', '\n', 'The next iteration of Wav2Letter can be found in this [paper](https://arxiv.org/abs/1712.09444). This paper uses Gated Convnets instead of normal Convnets.\n', '\n', 'The [Google Speech Command Example.ipynb](https://github.com/LearnedVector/Wav2Letter/blob/master/Google%20Speech%20Command%20Example.ipynb) notebook contains an example of this implementation.\n', '\n', '<p align=""center"">\n', '  <img src=""Wav2Letter-diagram.png"" alt=""Precise 2 Diagram"" height=""700""/>\n', '</p>\n', '\n', '## Differences\n', '\n', '* Uses CTC Loss\n', '* Uses Greedy Decoder\n', '\n', '## TODO\n', '\n', '* Implement Train, Validation, Test sets\n', '* Test on larger speech data\n', '* Implement AutoSegCriterion\n', '* Implement Beam Search Decoder\n', '* Use KenLM Langauge Model in Decoder\n', '* Use larger datasets\n', '* Add Gated ConvNets\n', '\n', '## Getting Started\n', '\n', '## Requirements\n', '\n', '```bash\n', 'pip install -r requirements.txt\n', '```\n', '\n', 'Make sure you are using pytorch-nightly (version 1.0 alpha). This has the CTC_Loss loss function we need.\n', '\n', '## Smoke Test\n', '\n', '`smoke_test.py` contains a quick test to see if everything is working\n', '\n', '```bash\n', 'python smoke_test.py\n', '```\n', '\n', 'This will train a model on randomly generated inputs and target generated data. If everyhing is working correctly, expect to see outputs of the predicted and target labels. Of course expect the outputs to be garbage.\n', '\n', '## Data\n', '\n', 'For an initial test, I used the [Google Speech Command Dataset](https://www.kaggle.com/c/tensorflow-speech-recognition-challenge/data). This is a simple to use lightweight dataset for testing model performance.\n', '\n', '### Instructions to download data\n', '\n', '1. Download the dataset.\n', '2. Create a `./speech_data` directory at root of this project.\n', '3. Unzip the google speech data. Should be named `speech_commands_v0.01`.\n', '\n', '### Prepare data\n', '\n', '`data.py` contains scripts to process google speech command audio data into features compatible with Wav2Letter.\n', '\n', '```bash\n', 'python Wav2Letter/data.py\n', '```\n', '\n', 'This will process the google speech commands audio data into 13 mfcc features with a max framelength of 250 (these are short audio clips). Anything less will be padded with zeros. Target data will be integer encoded and also padded to have the same length. Final outputs are numpy arrays saved as `x.npy` and `y.npy` in the `./speech_data` directory.\n', '\n', '## Train\n', '\n', '`train.py` has the code to run the training. Example would be.\n', '\n', '```bash\n', 'python train.py --batch_size=256 --epochs=1000\n', '```\n', '\n', '## Contributions\n', '\n', ""Pull Requests are accepted! I would love some help to knock out the Todo's. Email me at learnedvector@gmail.com for any questions.\n""]"
Model Fairness,IBM/inFairness,IBM,https://api.github.com/repos/IBM/inFairness,55,3,5,"['https://api.github.com/users/MayankAgarwal', 'https://api.github.com/users/aldopareja', 'https://api.github.com/users/onkarbhardwaj', 'https://api.github.com/users/ibm-open-source-bot', 'https://api.github.com/users/imgbot%5Bbot%5D']",Python,2023-03-26T04:50:03Z,https://raw.githubusercontent.com/IBM/inFairness/main/README.md,"['<p align=""center"">\n', '  <a href=""https://ibm.github.io/inFairness"">\n', '     <img width=""350"" height=""350"" src=""https://ibm.github.io/inFairness/_static/infairness-logo.png"">\n', '   </a>\n', '</p>\n', '\n', '<p align=""center"">\n', '   <a href=""https://pypi.org/project/infairness""><img src=""https://img.shields.io/pypi/v/infairness?color=important&label=pypi%20package&logo=PyPy""></a>\n', '   <a href=""./examples""><img src=""https://img.shields.io/badge/example-notebooks-red?logo=jupyter""></a>\n', '   <a href=""https://ibm.github.io/inFairness""><img src=""https://img.shields.io/badge/documentation-up-green?logo=GitBook""></a>\n', '   <a href=""https://fairbert.vizhub.ai""><img src=""https://img.shields.io/badge/fairness-demonstration-yellow?logo=ibm-watson""></a>\n', '   <br/>\n', '   <a href=""https://app.travis-ci.com/IBM/inFairness""><img src=""https://app.travis-ci.com/IBM/inFairness.svg?branch=main""></a>\n', '   <a href=""https://pypistats.org/packages/infairness""><img alt=""PyPI - Downloads"" src=""https://img.shields.io/pypi/dm/inFairness?color=blue""></a>\n', '   <a href=""https://www.python.org/""><img src=""https://img.shields.io/badge/python-3.7+-blue?logo=python""></a>\n', '   <a href=""https://opensource.org/licenses/Apache-2.0""><img src=""https://img.shields.io/badge/license-Apache-yellow""></a>\n', '   <a href=""https://github.com/psf/black""><img src=""https://img.shields.io/badge/code%20style-black-000000.svg""></a>\n', '</p>\n', '\n', '\n', '## Individual Fairness and inFairness\n', '\n', 'Intuitively, an individually fair Machine Learning (ML) model treats similar inputs similarly. Formally, the leading notion of individual fairness is metric fairness [(Dwork et al., 2011)](https://dl.acm.org/doi/abs/10.1145/2090236.2090255); it requires:\n', '\n', '$$ d_y (h(x_1), h(x_2)) \\leq L d_x(x_1, x_2) \\quad \\forall \\quad x_1, x_2 \\in X $$\n', '\n', 'Here, $h: X \\rightarrow Y$ is a ML model, where $X$ and $Y$ are input and output spaces; $d_x$ and $d_y$ are metrics on the input and output spaces, and $L \\geq 0$ is a Lipchitz constant. This constrained optimization equation states that the distance between the model predictions for inputs $x_1$ and $x_2$ is upper-bounded by the fair distance between the inputs $x_1$ and $x_2$. Here, the fair metric $d_x$ encodes our intuition of which samples should be treated similarly by the ML model, and in designing so, we ensure that for input samples considered similar by the fair metric $d_x$, the model outputs will be similar as well.\n', '\n', 'inFairness is a PyTorch package that supports auditing, training, and post-processing ML models for individual fairness. At its core, the library implements the key components of individual fairness pipeline: $d_x$ - distance in the input space, $d_y$ - distance in the output space, and the learning algorithms to optimize for the equation above.\n', '\n', 'For an in-depth tutorial of Individual Fairness and the inFairness package, please watch this tutorial. Also, take a look at the [examples](./examples/) folder for illustrative use-cases and try the [Fairness Playground demo](https://fairbert.vizhub.ai). For more group fairness examples see [AIF360](https://aif360.mybluemix.net/).\n', '\n', '<p align=""center"">\n', '  <a href=""https://ibm.box.com/v/fairness-tutorial-2022"" target=""_blank""><img width=""700"" alt=""Watch the tutorial"" src=""https://user-images.githubusercontent.com/991913/178768336-2bfa5958-487f-4f14-a156-03dacfd68263.png""></a>\n', '</p>\n', '\n', '## Installation\n', '\n', 'inFairness can be installed using `pip`:\n', '\n', '```\n', 'pip install inFairness\n', '```\n', '\n', '\n', 'Alternatively, if you wish to install the latest development version, you can install directly by cloning this repository:\n', '\n', '```\n', 'git clone <git repo url>\n', 'cd inFairness\n', 'pip install -e .\n', '```\n', '\n', '\n', '\n', '## Features\n', '\n', 'inFairness currently supports:\n', '\n', '1. Learning individually fair metrics : [[Docs]](https://ibm.github.io/inFairness/reference/distances.html)\n', '2. Training of individually fair models : [[Docs]](https://ibm.github.io/inFairness/reference/algorithms.html)\n', '3. Auditing pre-trained ML models for individual fairness : [[Docs]](https://ibm.github.io/inFairness/reference/auditors.html)\n', '4. Post-processing for Individual Fairness : [[Docs]](https://ibm.github.io/inFairness/reference/postprocessing.html)\n', '5. Individually fair ranking : [[Docs]](https://ibm.github.io/inFairness/reference/algorithms.html)\n', '\n', '\n', '## Contributing\n', '\n', 'We welcome contributions from the community in any form - whether it is through the contribution of a new fair algorithm, fair metric, a new use-case, or simply reporting an issue or enhancement in the package. To contribute code to the package, please follow the following steps:\n', '\n', '1. Clone this git repository to your local system\n', '2. Setup your system by installing dependencies as: `pip3 install -r requirements.txt` and `pip3 install -r  build_requirements.txt`\n', '3. Add your code contribution to the package. Please refer to the [`inFairness`](./inFairness) folder for an overview of the directory structure\n', '4. Add appropriate unit tests in the [`tests`](./tests) folder\n', '5. Once you are ready to commit code, check for the following:\n', '   1. Coding style compliance using: `flake8 inFairness/`. This command will list all stylistic violations found in the code. Please try to fix as much as you can\n', '   2. Ensure all the test cases pass using: `coverage run --source inFairness -m pytest tests/`. All unit tests need to pass to be able merge code in the package.\n', '6. Finally, commit your code and raise a Pull Request.\n', '\n', '\n', '## Tutorials\n', '\n', 'The [`examples`](./examples) folder contains tutorials from different fields illustrating how to use the package.\n', '\n', '### Minimal example\n', '\n', 'First, you need to import the relevant packages\n', '\n', '```\n', 'from inFairness import distances\n', 'from inFairness.fairalgo import SenSeI\n', '```\n', '\n', 'The `inFairness.distances` module implements various distance metrics on the input and the output spaces, and the `inFairness.fairalgo` implements various individually fair learning algorithms with `SenSeI` being one particular algorithm.\n', '\n', 'Thereafter, we instantiate and fit the distance metrics on the training data, and \n', '\n', '\n', '```[python]\n', 'distance_x = distances.SVDSensitiveSubspaceDistance()\n', 'distance_y = distances.EuclideanDistance()\n', '\n', 'distance_x.fit(X_train=data, n_components=50)\n', '\n', '# Finally instantiate the fair algorithm\n', 'fairalgo = SenSeI(network, distance_x, distance_y, lossfn, rho=1.0, eps=1e-3, lr=0.01, auditor_nsteps=100, auditor_lr=0.1)\n', '```\n', '\n', 'Finally, you can train the `fairalgo` as you would train your standard PyTorch deep neural network:\n', '\n', '```\n', 'fairalgo.train()\n', '\n', 'for epoch in range(EPOCHS):\n', '    for x, y in train_dl:\n', '        optimizer.zero_grad()\n', '        result = fairalgo(x, y)\n', '        result.loss.backward()\n', '        optimizer.step()\n', '```\n', '\n', '\n', '##  Authors\n', '\n', '<table align=""center"">\n', '  <tr>\n', '    <td align=""center""><a href=""http://moonfolk.github.io/""><img src=""https://avatars.githubusercontent.com/u/24443134?v=4?s=100"" width=""120px;"" alt=""""/><br /><b>Mikhail Yurochkin</b></a></a></td>\n', '    <td align=""center""><a href=""http://mayankagarwal.github.io/""><img src=""https://avatars.githubusercontent.com/u/991913?v=4?s=100"" width=""120px;"" alt=""""/><br /><b>Mayank Agarwal</b></a></a></td>\n', '    <td align=""center""><a href=""https://github.com/aldopareja""><img src=""https://avatars.githubusercontent.com/u/7622817?v=4?s=100"" width=""120px;"" alt=""""/><br /><b>Aldo Pareja</b></a></a></td>\n', '    <td align=""center""><a href=""https://github.com/onkarbhardwaj""><img src=""https://avatars.githubusercontent.com/u/13560220?v=4?s=100"" width=""120px;"" alt=""""/><br /><b>Onkar Bhardwaj</b></a></a></td>\n', '  </tr>\n', '</table>\n']"
Model Fairness,sakshiudeshi/Aequitas,sakshiudeshi,https://api.github.com/repos/sakshiudeshi/Aequitas,8,9,1,['https://api.github.com/users/sakshiudeshi'],Python,2022-09-12T19:39:56Z,https://raw.githubusercontent.com/sakshiudeshi/Aequitas/master/README.md,"['# Aequitas\n', '\n', 'We present Aequitas, a directed fairness testing framework machine learning models. See the paper [Automated Directed Fairness Testing](https://arxiv.org/abs/1807.00468) for more details.\n', '\n', '\n', '\n', '## Prerequisites\n', '\n', '* Python 2.7.15\n', '* numpy 1.14.5\n', '* scipy 1.1.0\n', '* scikit-learn 1.19.0\n', '\n', 'The authors used Pycharm CE 2017.2.3 as the development IDE.\n', '\n', '## Background\n', 'There are 3 test generation strategies in our suite, namely Aequitas Random, Aequitas Semi-Directed and Aequitas Fully Directed. There are files to evaluate [Fair SVM](https://github.com/mbilalzafar/fair-classification) and Scikit-Learn classifiers trained on the same [dataset](http://archive.ics.uci.edu/ml/datasets/Adult).\n', '\n', '## Config\n', 'The [config](config.py) file has the following data:\n', '\n', '* params : The number of parameters in the data\n', '* sensitive_param: The parameter under test.\n', '* input_bounds: The bounds of each parameter\n', '* classifier_name: Pickled scikit-learn classifier under test (only applicable to the sklearn files)\n', '* threshold: Discrimination threshold.\n', '* perturbation_unit: By what unit would the user like to perturb the input in the local search.\n', '* retraining_inputs: Inputs to be used for the retraining. Please see [this file](Retrain_Example_File.txt).\n', '\n', '## Demo\n', '`python <filename>`\n', '\n', 'eg. `python Aequitas_Fully_Directed.py`\n', '\n', '## Contact\n', '* Please contact sakshi_udeshi@mymail.sutd.edu.sg for any comments/questions\n', '\n', '\n', '## Citation \n', 'If you use any part of this code, please cite the following paper\n', '\n', '```\n', '@inproceedings{aequitas,\n', '  title={Automated directed fairness testing},\n', '  author={Udeshi, Sakshi and Arora, Pryanshu and Chattopadhyay, Sudipta},\n', '  booktitle={Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},\n', '  pages={98--108},\n', '  year={2018}\n', '}\n', '```\n', '\n', '\n', '\n']"
Model Fairness,ermongroup/fairgen,ermongroup,https://api.github.com/repos/ermongroup/fairgen,14,7,3,"['https://api.github.com/users/kristychoi', 'https://api.github.com/users/dependabot%5Bbot%5D', 'https://api.github.com/users/trishasingh']",Python,2023-03-08T18:06:37Z,https://raw.githubusercontent.com/ermongroup/fairgen/master/README.md,"['# Fair Generative Modeling via Weak Supervision\n', 'This repo contains a reference implementation for fairgen as described in the paper:\n', '> Fair Generative Modeling via Weak Supervision </br>\n', '> [Kristy Choi*](http://kristychoi.com/), [Aditya Grover*](http://aditya-grover.github.io/), [Trisha Singh](https://profiles.stanford.edu/trisha-singh), [Rui Shu](http://ruishu.io/about/), [Stefano Ermon](https://cs.stanford.edu/~ermon/) </br>\n', '> International Conference on Machine Learning (ICML), 2020. </br>\n', '> Paper: https://arxiv.org/abs/1910.12008 </br>\n', '\n', '\n', '## 1) Data setup:\n', '(a) Download the CelebA dataset here (http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) into the `data/` directory (if elsewhere, note the path for step b). Of the download links provided, choose `Align&Cropped Images` and download `Img/img_align_celeba/` folder, `Anno/list_attr_celeba.txt`, and `Eval/list_eval_partition.txt` to `data/`.\n', '\n', '(b) Preprocess the CelebA dataset for faster training:\n', '```\n', 'python3 preprocess_celeba.py --data_dir=/path/to/downloaded/dataset/celeba/ --out_dir=../data --partition=train\n', '```\n', '\n', 'You should run this script for `--partition=[train, val, test]` to cache all the necessary data. The preprocessed files will then be saved in `data/`.\n', '\n', 'To split the data for multiple attributes, check `notebooks/multi-attribute data and unbiased FID splits.ipynb`.\n', '\n', '## 2) Pre-train attribute classifier\n', 'For a single-attribute:\n', '```\n', 'python3 train_attribute_clf.py celeba ./results/attr_clf\n', '```\n', '\n', 'For multiple attributes, add the `--multi=True` flag.\n', '```\n', 'python3 train_attribute_clf.py celeba ./results/multi_clf -- multi=True\n', '```\n', '\n', 'Then, the trained attribute classifier will be saved in `./results/attr_clf` (`./results/multi_clf`) and will be used for downstream evaluation for generative model training. Note the path where these classifiers are saved, as they will be needed for GAN training + evaluation.\n', '\n', '\n', '## 3) Pre-train density ratio classifier\n', 'The density ratio classifier should be trained for the appropriate `bias` and `perc` setting, which can be adjusted in the script below:\n', '```\n', 'python3 get_density_ratios.py celeba celeba --perc=[0.1, 0.25, 0.5, 1.0] --bias=[90_10, 80_20, multi]\n', '```\n', 'Note that the best density ratio classifier will be saved in its corresponding directory under `./data/`. \n', '\n', '\n', '## 4) Pre-compute unbiased FID scores:\n', 'We have provided both (a) biased and (b) unbiased FID statistics in the `fid_stats/` directory.\n', '\n', '(a) `fid_stats/fid_stats_celeba.npz` contains the original activations from the *entire* CelebA dataset, as in: https://github.com/ajbrock/BigGAN-PyTorch\n', '\n', '(b) `fid_stats/unbiased_all_gender_fid_stats.npz` contains activations from the entire CelebA dataset, where the gender attribute (female, male) are balanced.\n', '\n', '(c) `fid_stats/unbiased_all_multi_fid_stats.npz` contains activations from the entire CelebA dataset, where the 4 attribute classes (black hair, other hair, female, male) are balanced.\n', '\n', 'These pre-computed FID statistics are for model checkpointing (during GAN training) and downstream evaluation of sample quality only, and should be substituted for other statistics when using a different dataset/attribute splits.\n', '\n', '\n', '## 5) Train generative model (BigGAN)\n', 'A sample script to train the model can be found in `scripts/`:\n', '\n', '`bash run_celeba_90_10_perc1.0_impweight.sh`\n', '\n', 'You should add different arguments for different model configurations. For example:\n', '(a) for the multi-attribute setting, append ` --multi 1`\n', '(b) for the equi-weighted baseline, append ` --reweight 0`\n', '(c) for the conditional baseline, append `--conditional 1 --y 1 --reweight 0`\n', '(d) for the importance-weighted model, append `--reweight 1 --alpha 1.0`\n', '\n', 'Note the argument for `--name_suffix my_experiment`, as you will need it for sampling and computing FID scores.\n', '\n', '\n', '## 6) Sample from trained model\n', 'A sample script to sample from the (trained) model can be found in `scripts/`:\n', '\n', '`bash sample_celeba_90_10_perc1.0_impweight.sh`\n', '\n', 'You can either append the argument `--load_weights name_of_weights` to load a specific set of weights, or pass in the `--name_suffix my_experiment` argument for the script to find the most recent checkpoint with the best FID.\n', '\n', '\n', '## 7) Compute FID scores\n', 'To compute FID scores after running the sampling script, (using the original Tensorflow implementation), run the following:\n', '`python3 fast_fid.py my_experiment --multi=[True,False] --n_replicates=10`\n', '\n', 'This code assumes that there are 10 sets (`n_replicates`) of 10K samples generated from the model (as per `sample.py`), and will evaluate the samples on both (a) the original FID scores and (b) unbiased FID scores (as per Step #4). `my_experiment` refers to the `--name_suffix my_experiment` parameter from Step 5.\n', '\n', '\n', '## References\n', 'If you find this work useful in your research, please consider citing the following paper:\n', '```\n', '@article{grover2019fair,\n', '  title={Fair Generative Modeling via Weak Supervision},\n', '  author={Grover, Aditya and Choi, Kristy and Singh, Trisha and Shu, Rui and Ermon, Stefano},\n', '  journal={arXiv preprint arXiv:1910.12008},\n', '  year={2019}\n', '}\n', '```\n']"
Model Fairness,txsun1997/Metric-Fairness,txsun1997,https://api.github.com/repos/txsun1997/Metric-Fairness,28,4,2,"['https://api.github.com/users/1710763616', 'https://api.github.com/users/txsun1997']",Python,2023-04-20T14:20:43Z,https://raw.githubusercontent.com/txsun1997/Metric-Fairness/main/README.md,"['# Metric Fairness: Is BERTScore Fair?\n', 'This repository contains the code, data, and pre-trained checkpoints for our EMNLP paper [BERTScore is Unfair: On Social Bias in Language Model-Based Metrics for Text Generation](https://arxiv.org/abs/2210.07626).\n', '\n', '## Quick Links\n', '\n', '- [Overview](#overview)\n', '- [Measure Metric Bias](#measure-metric-bias)\n', '- [Mitigate Metric Bias](#mitigate-metric-bias)\n', '  - [Train](#train)\n', '    - [Datasets](#datasets)\n', '  - [Test](#test)\n', '    - [Adapters](#adapters)\n', '  - [Performance Evaluation](#performance-evaluation)\n', '    - [WMT20](#wmt20)\n', '    - [REALSumm](#realsumm)\n', '- [Citation](#citation)\n', '\n', '## Overview\n', '\n', 'Pre-trained language model-based metrics (PLM-based metrics, e.g., BERTScore, MoverScore, BLEURT) have been widely used in various text generation tasks including machine translation, text summarization, etc. Compared with traditional $n$-gram-based metrics (e.g., BLEU, ROUGE, NIST), PLM-based metrics can well capture the semantic similarity between system outputs and references, and therefore achieve higher correlation with human judgements. However, it is well known that PLMs can encode a high degree of social bias. How much of the social bias in BERT is inherited by BERTScore? Will BERTScore encourage systems that generate biased text? To what extent do different PLM-based metrics carry social bias? This work presents the first systematic study on social bias in PLM-based metrics for text generation.\n', '\n', 'The following figure illustrates the impact of social bias in PLM-based metrics: If the metric is biased against some sensitive attributes (e.g., gender), generative models that express such bias will be rewarded and selected. The texts generated by these biased models may be incorporated in the corpus, further reinforcing the social bias in data. \n', '\n', '\n', '![](https://github.com/txsun1997/Metric-Fairness/blob/main/metric-bias.png)\n', '\n', 'Our work includes:\n', '\n', '1. **Measuring social bias in PLM-based metrics.** We constructed datasets and metrics to measure the bias (or unfairness) in existing PLM-based metrics including BERTScore, MoverScore, BLEURT, PRISM, BARTScore, and FrugalScore.\n', '2. **Mitigating social bias in PLM-based metrics.** We explore mitigating social bias in existing metrics by (1) replacing the backbone models with debiased ones such as [Zari models](https://github.com/google-research-datasets/Zari) and (2) training debiasing adapters on augmented data.\n', '\n', '## Measure Metric Bias\n', '\n', 'We have uploaded our constructed datasets for measuring metric bias (see [/measuring_bias/data](/measuring_bias/data)). We provide 6 datasets for evaluating social bias against different sensitive attributes including age, gender, physical appearance, race, religion, and socioeconomic status. We also provide our evaluated scores using 29 existing text generation metrics for each sample in the datasets. You can reproduce our results as follows:\n', '\n', '```bash\n', 'pip install prettytable\n', 'git clone https://github.com/txsun1997/Metric-Fairness\n', 'cd Metric-Fairness/measuring_bias\n', 'python get_bias_score.py\n', '```\n', '\n', 'If all is well, you should obtain the following results:\n', '\n', '```\n', '+-------------------------+-------+--------+---------------------+------+----------+---------------+\n', '|          metric         |  age  | gender | physical-appearance | race | religion | socioeconomic |\n', '+-------------------------+-------+--------+---------------------+------+----------+---------------+\n', '|  bartscore-bart-base-f  |  6.2  |  3.67  |         6.04        | 2.44 |   5.97   |      6.65     |\n', '|  bartscore-bart-base-p  |  6.51 |  6.5   |         7.59        | 2.6  |   7.63   |      8.0      |\n', '|  bartscore-bart-base-r  |  7.1  |  2.47  |         8.44        | 2.52 |   7.12   |      7.55     |\n', '|  bartscore-bart-large-f |  3.83 |  9.47  |         6.38        | 1.67 |   4.7    |      3.47     |\n', '|  bartscore-bart-large-p |  7.65 | 14.17  |         6.42        | 1.87 |   5.13   |      4.55     |\n', '|  bartscore-bart-large-r |  2.36 |  3.69  |         4.92        | 2.13 |   4.34   |      3.48     |\n', '|   bertscore-bert-base   |  5.68 |  8.73  |         6.36        | 1.24 |   6.2    |      7.66     |\n', '|   bertscore-bert-large  |  4.64 |  4.39  |         6.07        | 2.3  |   7.87   |      6.85     |\n', '|   bertscore-distilbert  |  5.26 |  8.36  |         4.93        | 1.94 |   6.82   |      7.64     |\n', '|  bertscore-roberta-base |  6.63 |  3.75  |         7.82        | 2.27 |   4.08   |      6.21     |\n', '| bertscore-roberta-large |  8.23 |  6.99  |         7.94        | 2.59 |   4.64   |      7.4      |\n', '|           bleu          |  2.35 |  0.1   |         0.94        | 0.19 |   0.61   |      2.79     |\n', '|     bleurt-bert-base    | 13.44 | 29.97  |        12.92        | 3.02 |  16.21   |     15.41     |\n', '|    bleurt-bert-large    | 15.07 | 27.08  |         7.98        | 4.0  |  16.18   |      14.6     |\n', '|     bleurt-bert-tiny    | 14.01 |  6.47  |        10.71        | 8.43 |   6.39   |     13.01     |\n', '|      bleurt-rembert     | 16.52 | 20.93  |         8.84        | 4.21 |  17.12   |     12.93     |\n', '|           chrf          |  3.43 |  1.23  |         1.57        | 1.89 |   1.44   |      3.46     |\n', '| frugalscore-bert-medium |  5.02 |  5.73  |         5.07        | 0.93 |   5.57   |      8.09     |\n', '|  frugalscore-bert-small |  4.9  |  7.04  |         4.64        | 0.91 |   5.82   |      8.78     |\n', '|  frugalscore-bert-tiny  |  7.96 |  3.2   |         5.27        | 1.39 |   5.96   |      7.12     |\n', '|          meteor         |  4.96 |  2.63  |         3.08        | 1.53 |   2.56   |      4.4      |\n', '|   moverscore-bert-base  |  6.06 | 11.36  |         6.69        | 3.84 |   9.63   |      7.94     |\n', '|  moverscore-bert-large  |  6.78 |  6.68  |         8.04        | 4.43 |  10.24   |      8.3      |\n', '|  moverscore-distilbert  |  7.24 | 13.24  |         4.94        | 3.35 |   9.67   |      8.59     |\n', '|           nist          |  2.2  |  0.11  |         1.03        | 0.25 |   0.54   |      1.43     |\n', '|         prism-f         |  6.69 |  7.13  |         7.48        | 1.97 |   6.79   |      4.85     |\n', '|         prism-p         |  9.1  | 14.33  |         7.05        | 2.6  |   7.06   |      6.51     |\n', '|         prism-r         |  5.1  |  3.0   |         7.13        | 2.65 |   5.92   |      4.91     |\n', '|          rouge          |  3.83 |  0.21  |         2.01        | 0.12 |   1.02   |      3.4      |\n', '+-------------------------+-------+--------+---------------------+------+----------+---------------+\n', '```\n', '\n', 'You can also evaluate the bias types (and text generation metrics) of interest instead of all of them by specifying the parameters `--bias_type` and `--metric_name`. For example,\n', '\n', '```bash\n', 'python get_bias_score.py --bias_type age gender --metric_name rouge bleu\n', '```\n', '\n', 'would result in a tiny table:\n', '\n', '```\n', '+--------+------+--------+\n', '| metric | age  | gender |\n', '+--------+------+--------+\n', '|  bleu  | 2.35 |  0.1   |\n', '| rouge  | 3.83 |  0.21  |\n', '+--------+------+--------+\n', '```\n', '\n', 'You can see the exact details of how we calculated each score by checking `metrics.py`, and if you need to calculate scores of default backbones, run\n', '\n', '```bash\n', 'cd Metric-Fairness/measuring_bias/metrics\n', 'pip install -r requirements.txt\n', 'bash metrics.sh\n', '```\n', '\n', 'and you will obtain an output file named `scores.csv` by default which contains scores on our gender bias dataset.\n', '\n', 'Also, you can calculate the polarized bias scores by running\n', '\n', '```bash\n', 'python cal_bias_score.py --polarity True\n', '```\n', 'You are expected to obtain the results reported in Table 9 in the paper.\n', '\n', 'Below is an example output of `python cal_bias_score.py` (without polarity):\n', '\n', '```\n', '+------+-------+--------+------+------+-------------+-------------+-------------+------------+--------+---------+---------+---------+-------------+-------------+-------------+-------------+\n', '| bleu | rouge | meteor | nist | chrf | bertscore_r | bertscore_p | bertscore_f | moverscore | bleurt | prism_r | prism_p | prism_f | bartscore_r | bartscore_p | bartscore_f | frugalscore |\n', '+------+-------+--------+------+------+-------------+-------------+-------------+------------+--------+---------+---------+---------+-------------+-------------+-------------+-------------+\n', '| 0.1  |  0.21 |  2.14  | 0.12 | 1.23 |     4.61    |     9.04    |     6.99    |   13.24    |  30.0  |   3.0   |  14.33  |   7.13  |     3.69    |    14.17    |     9.47    |     3.19    |\n', '+------+-------+--------+------+------+-------------+-------------+-------------+------------+--------+---------+---------+---------+-------------+-------------+-------------+-------------+\n', '```\n', '\n', '\n', '\n', '## Mitigate Metric Bias\n', '\n', '### Train\n', '\n', '#### Datasets\n', '\n', '[Download link](https://drive.google.com/drive/folders/1rqPw_h6_0CxgL4LY2LhBPMR2RODnMnv6?usp=sharing)\n', '\n', 'We collect training data based on two public sentence-pair datasets, MultiNLI [(Williams et al., 2018)](https://doi.org/10.18653/v1/n18-1101) and STS-B [(Cer et al., 2017)](http://arxiv.org/abs/1708.00055), in which each sample is comprised of a premise and a hypothesis. We perform counterfactual data augmentation (CDA) ([Zhao et al., 2018b)](https://arxiv.org/abs/1804.06876) on the sentences in MultiNLI and STS-B to construct a training set. You can download the datasets from the above link, which includes `train.tsv` for BERTScore (both BERT-base and BERT-large), BARTScore (BART-base), and BLEURT (BERT-base).\n', '\n', 'The following example shows how to add and train a debiasing adapter on the BERT-large model of BERTScore. Note that we used a single NVIDIA 3090 GPU (24GB) to perform training.\n', '\n', '```bash\n', 'cd Metric-Fairness/mitigating_bias/train/BERTScore\n', 'mkdir ./logs\n', 'pip install -r requirements.txt\n', 'INPUT_PATH=train.tsv # your training set path\n', 'python train_BERTScore.py\n', '    --model_type bert-large-uncased \\\n', '    --adapter_name debiased-bertscore \\\n', '    --lr 5e-4 \\\n', '    --warmup 0.0 \\\n', '    --batch_size 16 \\\n', '    --n_epochs 4 \\\n', '    --seed 42 \\\n', '    --device cuda \\\n', '    --logging_steps 100 \\\n', '    --data_path ${INPUT_PATH}\n', '```\n', '\n', 'After training, a debiasing adapter will be saved in `./adapter/`, and you can check more training details in `./logs` . See also [fitlog](https://fitlog.readthedocs.io/zh/latest/).\n', '\n', '### Test\n', '\n', '#### Adapters\n', '\n', '[Download link](https://drive.google.com/drive/folders/1nqTQWXtf14SXZ5pC0hK5kBa28q9h-0-y?usp=sharing)\n', '\n', ""We have trained debiasing adapters for BERTScore (both BERT-base and BERT-large), BARTScore (BART-base), and BLEURT (BERT-base), so you can download these adapters' checkpoints through the link above.\n"", '\n', 'The following example shows how to add our trained debiasing adapters to BERTScore (both BERT-base and BERT-large), BARTScore (BART-base), and BLEURT (BERT-base) , and calculate the bias scores using debiased metrics on our test set in `Metric-Fairness/mitigating_bias/test/test_data` [(WinoBias)](https://doi.org/10.18653/v1/n18-2003).\n', '\n', '```bash\n', 'cd Metric-Fairness/mitigating_bias/test\n', 'pip install -r requirements.txt\n', 'BERT_SCORE_BERT_LARGE_ADAPTER_PATH=BERTScore/BERT-large/adapter # bert_score_bert_large adapter path\n', 'BERT_SCORE_BERT_BASE_ADAPTER_PATH=BERTScore/BERT-base/adapter # bert_score_bert_base adapter path\n', 'BLEURT_BERT_BASE_ADAPTER_PATH=BLEURT/adapter # bleurt_bert_base adapter path\n', 'BART_SCORE_BART_BASE_ADAPTER_PATH=BARTScore/adapter # bart_score_bart_base adapter path\n', 'python cal_debias_scores.py\n', '    --bert_score_bert_large_adapter_path ${BERT_SCORE_BERT_LARGE_ADAPTER_PATH} \\\n', '    --bert_score_bert_base_adapter_path ${BERT_SCORE_BERT_BASE_ADAPTER_PATH} \\\n', '    --bleurt_bert_base_adapter_path ${BLEURT_BERT_BASE_ADAPTER_PATH} \\\n', '    --bart_score_bart_base_adapter_path ${BART_SCORE_BART_BASE_ADAPTER_PATH}\n', '```\n', '\n', 'Below is an example result:\n', '\n', '```\n', '+----------------------+-----------------------+------------------+----------------------+\n', '| bert_score_bert_base | bert_score_bert_large | bleurt_bert_base | bart_score_bart_base |\n', '+----------------------+-----------------------+------------------+----------------------+\n', '|         4.21         |          2.69         |      10.46       |         2.35         |\n', '+----------------------+-----------------------+------------------+----------------------+\n', '```\n', '\n', 'If without debiasing adapters, the result should be:\n', '\n', '```\n', '+----------------------+-----------------------+------------------+----------------------+\n', '| bert_score_bert_base | bert_score_bert_large | bleurt_bert_base | bart_score_bart_base |\n', '+----------------------+-----------------------+------------------+----------------------+\n', '|         8.73         |          4.39         |       30.0       |         3.67         |\n', '+----------------------+-----------------------+------------------+----------------------+\n', '```\n', '\n', 'As you can see, the attached debiasing adapter successfully mitigates bias in these metrics.\n', '\n', '### Performance Evaluation\n', '\n', '#### WMT20\n', '\n', ""The following example shows how to evaluate the original metrics' perfomance on [WMT20](https://aclanthology.org/2020.wmt-1.77/):\n"", '\n', '```bash\n', 'cd Metric-Fairness/mitigating_bias/performance_eval/WMT\n', 'pip install -r requirements.txt\n', 'python eval_bert_score.py --model_type bert-base-uncased \n', 'python eval_bert_score.py --model_type bert-large-uncased \n', 'python eval_bleurt.py --model_type Elron/bleurt-base-512\n', 'python eval_bart_score.py --model_type facebook/bart-base\n', '```\n', '\n', 'Below is an example output:\n', '\n', '```\n', '+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+---------+\n', '| cs-en | de-en | iu-en | ja-en | km-en | pl-en | ps-en | ru-en | ta-en | zh-en | average |\n', '+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+---------+\n', '| 0.746 | 0.793 | 0.663 | 0.882 | 0.971 | 0.356 | 0.928 | 0.858 | 0.833 | 0.929 |  0.796  |\n', '+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+---------+\n', '```\n', '\n', ""The following example shows how to evaluate the metrics' perfomance after attaching our debiasing adapters on [WMT20](https://aclanthology.org/2020.wmt-1.77/):\n"", '\n', '```bash\n', 'BERT_SCORE_BERT_LARGE_ADAPTER_PATH=BERTScore/BERT-large/adapter # bert_score_bert_large adapter path\n', 'BERT_SCORE_BERT_BASE_ADAPTER_PATH=BERTScore/BERT-base/adapter # bert_score_bert_base adapter path\n', 'BLEURT_BERT_BASE_ADAPTER_PATH=BLEURT/adapter # bleurt_bert_base adapter path\n', 'BART_SCORE_BART_BASE_ADAPTER_PATH=BARTScore/adapter # bart_score_bart_base adapter path\n', 'python eval_bert_score.py \n', '    --model_type bert-base-uncased \\\n', '    --adapter_path ${BERT_SCORE_BERT_BASE_ADAPTER_PATH}\n', 'python eval_bert_score.py \n', '    --model_type bert-large-uncased \\\n', '    --adapter_path ${BERT_SCORE_BERT_LARGE_ADAPTER_PATH}\n', 'python eval_bleurt.py \n', '    --model_type Elron/bleurt-base-512 \\\n', '    --adapter_path ${BLEURT_BERT_BASE_ADAPTER_PATH}\n', 'python eval_bart_score.py \n', '    --model_type facebook/bart-base \\\n', '    --adapter_path ${BART_SCORE_BART_BASE_ADAPTER_PATH}\n', '```\n', '\n', 'Below is an example of performance result:\n', '\n', '```\n', '+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+---------+\n', '| cs-en | de-en | iu-en | ja-en | km-en | pl-en | ps-en | ru-en | ta-en | zh-en | average |\n', '+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+---------+\n', '| 0.758 | 0.786 | 0.639 | 0.873 |  0.97 | 0.364 | 0.932 | 0.862 | 0.832 | 0.925 |  0.794  |\n', '+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+---------+\n', '```\n', '\n', '#### REALSumm\n', '\n', 'For the sake of time, we provide the `pkl` file, run\n', '\n', '```bash\n', 'cd Metric-Fairness/mitigating_bias/performance_eval/REALSumm\n', 'pip install -r requirements.txt\n', 'python analyse_pkls.py\n', '```\n', '\n', 'and you will get results like\n', '\n', '```\n', '+------------------------------+----------------------+------------------------------+----------------------+-------------------------------+-----------------------+--------------------------+------------------+\n', '| bart_score_bart_base_adapter | bart_score_bart_base | bert_score_bert_base_adapter | bert_score_bert_base | bert_score_bert_large_adapter | bert_score_bert_large | bleurt_bert_base_adapter | bleurt_bert_base |\n', '+------------------------------+----------------------+------------------------------+----------------------+-------------------------------+-----------------------+--------------------------+------------------+\n', '|            0.307             |        0.325         |            0.473             |        0.465         |             0.468             |         0.464         |           0.4            |      0.299       |\n', '+------------------------------+----------------------+------------------------------+----------------------+-------------------------------+-----------------------+--------------------------+------------------+\n', '```\n', '\n', '## Citation\n', '\n', 'If you use our data or code, please cite:\n', '\n', '```bibtex\n', '@inproceedings{sun2022bertscore,\n', '  title={BERTScore is Unfair: On Social Bias in Language Model-Based Metrics for Text Generation},\n', '  author={Tianxiang Sun and Junliang He and Xipeng Qiu and Xuanjing Huang},\n', '  booktitle = {Proceedings of {EMNLP}},\n', '  year={2022}\n', '}\n', '```\n', '\n']"
Model Fairness,genforce/fairgen,genforce,https://api.github.com/repos/genforce/fairgen,31,6,2,"['https://api.github.com/users/Ariostgx', 'https://api.github.com/users/zhoubolei']",Python,2023-03-11T07:12:07Z,https://raw.githubusercontent.com/genforce/fairgen/main/README.md,"['# FairGen - Improving the Fairness of Deep Generative Models without Retraining\n', '\n', '![image](./docs/assets/framework.jpg)\n', '**Figure:** *Framework of FairGen*.\n', '\n', '> **Improving the Fairness of Deep Generative Models without Retraining** <br>\n', '> Shuhan Tan, Yujun Shen, Bolei Zhou <br>\n', '> *arXiv preprint arXiv:2012.04842*\n', '\n', '[[Paper](https://arxiv.org/pdf/2012.04842.pdf)]\n', '[[Project Page](https://genforce.github.io/fairgen/)]\n', '[[Colab](https://colab.research.google.com/github/genforce/fairgen/blob/main/docs/fairgen.ipynb)]\n', '\n', 'In this repository, we propose a simple yet effective method to improve the *fairness* of image generation for a pre-trained GAN model *without retraining*.\n', 'We utilize the recent work of *GAN interpretation* and a *Gaussian Mixture Model (GMM)* to support the sampling of latent codes for producing images with a more fair attribute distribution.\n', 'We call this method *FairGen*.\n', 'Experiments show that *FairGen* can substantially improve the fairness of image generation. The images generated from our method are further applied to reveal and quantify the biases in commercial face classifiers and face super-resolution model. Some results are shown as follows.\n', '\n', '\n', '## Fair Image Generation\n', '\n', 'Attributes: Age-Eyeglasses\n', '![image](./docs/assets/age_eyeglasses.jpg)\n', '\n', 'Attributes: Gender-Black Hair\n', '![image](./docs/assets/gender_black_hair.jpg)\n', '\n', '## Identifying Bias in Existing Models\n', '\n', 'Mis-classification in Commercial Gender Classifiers\n', '![image](./docs/assets/api.jpg)\n', '\n', 'Attribute Alternation by a Face Super-resolution Model\n', '![image](./docs/assets/PULSE.jpg)\n', '\n', '## BibTeX\n', '\n', '```bibtex\n', '@article{tan2020fairgen,\n', '  title   = {Improving the Fairness of Deep Generative Models without Retraining},\n', '  author  = {Tan, Shuhan and Shen, Yujun and Zhou, Bolei},\n', '  journal = {arXiv preprint arXiv:2012.04842},\n', '  year    = {2020}\n', '}\n', '```\n', '\n', '## Code Coming Soon']"
Model Fairness,Hackathonners/vania,Hackathonners,https://api.github.com/repos/Hackathonners/vania,77,6,5,"['https://api.github.com/users/fntneves', 'https://api.github.com/users/hgg', 'https://api.github.com/users/MarcoCouto', 'https://api.github.com/users/djcouto', 'https://api.github.com/users/mrjbq7']",Python,2023-03-07T22:38:40Z,https://raw.githubusercontent.com/Hackathonners/vania/master/README.md,"['# Project Vania - A Fair Distributor\n', '**Fair Distributor** is a module which [fairly](#our-meaning-of-fairness) distributes a list of arbitrary **objects** through a set of **targets**.\n', '\n', 'To be more explicit, this module considers 3 key components:\n', '* **object**: some kind of entity that can be assigned to something.\n', '* **target**: the entity that will have one (or more) **objects** assigned to it.\n', '* **weight**: represents the cost of assigning a given **object** to a **target**.\n', '\n', 'A collection of each of these components is given as input to the module.\n', 'Using linear programming, the **weights** of the **targets** relative to the **objects** are taken into consideration and used to build the constraints of an Integer Linear Programming (ILP) model. An ILP solver is then used, in order to distribute the **objects** through the **targets**, in the *fairest way possible*.\n', '\n', 'For instance, this module can be used to fairly distribute:\n', '* A set of tasks (objects) among a group of people (targets) according to their preferences to do each task (weights).\n', '* A set of projects (objects) among development teams (targets) according to their skill-level (weights) on the required skills for each project.\n', '\n', '\n', '## Our Meaning of Fairness\n', '\n', 'We define **Fairness** as:\n', ' * The total **weight** of distributing all **objects** through the **targets** should be minimal.\n', 'This enforces that the least amount of shared effort is made.\n', '\n', '_Optionally_, the following rule can be applied (enabled by default):\n', ' * The difference between the individual **weight** of each **target** is minimal.\n', 'This enforces the least amount of individual effort.\n', '\n', '## Documentation\n', '\n', 'You can find all the documentation in the following link:\n', 'https://hackathonners.github.io/vania\n', '\n', '## Download and Install\n', '\n', 'Install the latest stable version of this module:\n', '\n', '    $ pip install vania\n', '\n', 'To work with the source code, clone this repository:\n', '\n', '    $ git clone git://github.com/hackathonners/vania.git\n', '\n', '## Usage\n', 'To start using the **Fair Distributor**, you need first to import it, by doing this:\n', '```python\n', 'from vania.fair_distributor import FairDistributor\n', '```\n', 'Now, just feed it with your problem variables, and ask for the solution.\n', 'To better explain how you can do it, lets consider a specific example.\n', '\n', 'Suppose that you are managing a project, which contains **4** tasks: _Front-end Development_, _Back-end Development_, _Testing_, and _Documentation_.\n', 'There is a need to assign these **4** tasks through a set of **3** teams: _A_, _B_ and _C_.\n', 'You have the expected number of hours each team needs to finish each task:\n', '\n', '|        |*Front-end Development*|*Back-end Development*|*Testing*|*Documentation*| \n', '|--------|-----------------------|----------------------|---------|---------------|\n', '|_Team A_|          1h           |          2h          |    3h   |       2h      |\n', '|_Team B_|          3h           |          1h          |    4h   |       2h      |\n', '|_Team C_|          3h           |          4h          |    1h   |       1h      |\n', '\n', 'Here, we consider tasks as **objects**, teams as **targets** and the hours expressed in each cell are the **weights**.\n', '\n', 'It is necessary to create a data structure for each component. **Objects** and **targets** are lists, while **weights** is a collection, which contains for each target the cost of assigning every object to it, and is represented as a matrix.\n', 'The structures for this example would be as follow:\n', '\n', '```python\n', ""targets = ['Team A', 'Team B', 'Team C']\n"", ""objects = ['Front-end Development', 'Back-end Development', 'Testing', 'Documentation']\n"", 'weights = [\n', '    [1, 2, 3, 2],\t\t# hours for Team A to complete each task\n', '    [3, 1, 4, 2],\t\t# hours for Team B to complete each task\n', '    [3, 4, 1, 1]\t\t# hours for Team C to complete each task\n', ']\n', '```\n', '\n', 'Now, just feed the **Fair Distributor** with all the components, and ask for the solution:\n', '```python\n', 'distributor = FairDistributor(targets, objects, weights)\n', 'print(distributor.distribute())\n', '```\n', '\n', 'And here is the solution!\n', '```python\n', '# Output\n', '{\n', ""    'Team A': ['Front-end Development'],        # Team A does the Front-end Development\n"", ""    'Team B': ['Back-end Development'],         # Team B does the Back-end Development\n"", ""    'Team C': ['Testing', 'Documentation']      # Team C does the Testing and Documentation\n"", '}\n', '```\n', '\n', 'Here is the final code of this example:\n', '```python\n', 'from vania.fair_distributor import FairDistributor\n', '\n', ""targets = ['Team A', 'Team B', 'Team C']\n"", ""objects = ['Front-end Development', 'Back-end Development', 'Testing', 'Documentation']\n"", 'weights = [\n', '    [1, 2, 3, 2],\t\t# hours for Team A to complete each task\n', '    [3, 1, 4, 2],\t\t# hours for Team B to complete each task\n', '    [3, 4, 1, 1]\t\t# hours for Team C to complete each task\n', ']\n', '\n', 'distributor = FairDistributor(targets, objects, weights)\n', 'print(distributor.distribute())\n', '```\n', '\n', '## Contributions and Bugs\n', '\n', 'Found a bug and wish to report it? You can do so here: https://github.com/Hackathonners/vania/issues.\n', ""If you'd rather contribute to this project with the bugfix, awesome! Simply Fork the project on Github and make a Pull Request.\n"", '\n', ""Please tell us if you are unfamiliar with Git or Github and we'll definitely help you make your contribution.\n"", '\n', '## Authors\n', '\n', 'Hackathonners is **_a group of people who build things_**.\n', '\n', 'You can check us out at http://hackathonners.org.\n', '\n', '## License\n', '\n', 'The Fair Distributor is licensed under the [MIT License](https://opensource.org/licenses/MIT).\n', '\n', 'Copyright (C) 2017 Hackathonners\n']"
Model Fairness,heyaudace/ml-bias-fairness,heyaudace,https://api.github.com/repos/heyaudace/ml-bias-fairness,16,7,1,['https://api.github.com/users/heyaudace'],Python,2022-10-30T05:34:15Z,https://raw.githubusercontent.com/heyaudace/ml-bias-fairness/master/README.md,"['# ml-bias-fairness\n', '\n', 'Data collection is an expensive process that only large, profitable companies have the means to afford. This leaves smaller, less profitable organizations with no choice but to re-use data, which in some cases might have been collected for a different purpose.\n', '\n', 'In addition, all demographics are not always equally represented in the data.  There is more data about individuals from major demographics, and less or no data about people from minorities. This leads to a higher error rate for individuals of minority demographics.\n', '\n', 'In this work, we explore different steps involved in manipulating data and choosing the right algorithm to create unbiased Machine Learning applications. The work was intended and is highly applicable in developing countries where there are not enough resources for data collection, and most of the time the demographics representing the target users are not well represented in the training data.\n', '\n']"
Model Fairness,yuji-roh/fairbatch,yuji-roh,https://api.github.com/repos/yuji-roh/fairbatch,15,4,1,['https://api.github.com/users/yuji-roh'],Python,2023-04-23T05:46:44Z,https://raw.githubusercontent.com/yuji-roh/fairbatch/main/README.md,"['# FairBatch: Batch Selection for Model Fairness\n', '\n', '#### Authors: Yuji Roh, Kangwook Lee, Steven Euijong Whang, and Changho Suh\n', '#### In Proceedings of the 9th International Conference on Learning Representations (ICLR), 2021\n', '----------------------------------------------------------------------\n', '\n', 'This directory is for simulating FairBatch on the synthetic dataset.\n', 'The program needs PyTorch and Jupyter Notebook.\n', '\n', 'The directory contains a total of 4 files and 1 child directory: \n', '1 README, 2 python files, 1 jupyter notebook, \n', 'and the child directory containing 6 numpy files for synthetic data.\n', '\n', '\n', '#### To simulate FairBatch, please use the jupyter notebook in the directory.\n', '\n', 'The jupyter notebook will load the data and train the models with three \n', 'different fairness metrics: equal opportunity, equalized odds, and demographic parity.\n', '\n', 'Each training utilizes the FairBatch sampler, which is defined in FairBatchSampler.py.\n', 'The pytorch dataloader serves the batches to the model via the FairBatch sampler. \n', 'Experiments are repeated 10 times each.\n', 'After the training, the test accuracy and fairness will be shown.\n', '\n', 'The two python files are models.py and FairBatchSampler.py.\n', 'The models.py file contains a logistic regression architecture and a test function.\n', 'The FairBatchSampler.py file contains two classes: CustomDataset and FairBatch. \n', 'The CustomDataset class defines the dataset, and the FairBatch class implements \n', 'the algorithm of FairBatch as described in the paper.\n', '\n', 'More detailed explanations of each component can be found in the code as comments.\n', 'Thanks!\n']"
Model Fairness,rd-alliance/FAIR-data-maturity-model-WG,rd-alliance,https://api.github.com/repos/rd-alliance/FAIR-data-maturity-model-WG,12,3,1,['https://api.github.com/users/bahimc'],,2022-06-29T16:14:31Z,https://raw.githubusercontent.com/rd-alliance/FAIR-data-maturity-model-WG/master/README.md,"['# FAIR-data-maturity-model-WG\n', '## Introduction\n', 'Welcome to the repository supporting the work that will be carried out by the RDA FAIR data maturity Working Group. This Working Group will build on top and combine the most salient characteristics of existing efforts for measuring the readiness and implementation level of a dataset vis-à-vis the FAIR data principles.\n', '\n', 'For futher information you can read the [case statement](https://www.rd-alliance.org/group/fair-data-maturity-model-wg/case-statement/fair-data-maturity-model-wg-case-statement) on the RDA Website.\n', '\n', '### Context, objectives and scope\n', 'Please have a look at the [three slides presentation](https://github.com/RDA-FAIR/FAIR-data-maturity-model-WG/blob/master/Context%2C%20objectives%20and%20scope.pdf) to better understand the situation.  \n', '\n', '## How to contribute\n', 'Thanks for taking the time to contribute!\n', '\n', 'Each specific issue/thread treats a different topic. The overriding goal is to get consensus about every proposition that is made during the workshops and on this very repository. Navigate through the issues and comment to give your opinion about the topic of your choice. \n', '\n', 'Besides, any problems encountered, or suggestions, questions, etc. considered within scope can be submitted as issue on this very repository.  \n', '\n', '## Additional material\n', 'Here below you can find the presentations and reports for each and every workshop\n', '\n', '- [Workshop #1](https://www.rd-alliance.org/workshop-1)\n', '- [Workshop #2](https://www.rd-alliance.org/workshop-2)\n', '- [Workshop #3](https://www.rd-alliance.org/workshop-3)\n', '- [Workshop #4](https://www.rd-alliance.org/workshop-4)\n', '\n', '## Structure of the repository\n', '- /results of preliminary analysis\n', '\n', '\n', '\n']"
Model Fairness,TangJiakai/RecBole-FairRec,TangJiakai,https://api.github.com/repos/TangJiakai/RecBole-FairRec,15,4,2,"['https://api.github.com/users/TangJiakai', 'https://api.github.com/users/peteryang1031']",Python,2023-03-17T01:31:22Z,https://raw.githubusercontent.com/TangJiakai/RecBole-FairRec/master/README.md,"['# RecBole-FairRec\n', '\n', '![logo](asset/logo.png)\n', '\n', '**RecBole-FairRec** is a library toolkit built upon [RecBole](https://recbole.io) for reproducing and developing fairness-aware recommendation.\n', '\n', '## Highlights\n', '\n', '- **Easy-to-use**: Our library shares unified API and input(atomic files) as RecBole.\n', '- **Conveniently learn and compare**: Our library provides several fairess-metrics and frameworks for learning and comparing.\n', '- **Extensive FairRec library**: Recently proposed fairness-aware algorithms can be easily equipped in our library.\n', '\n', '## Requirements\n', '\n', '```\n', 'python>=3.7.0\n', 'recbole>=1.0.1\n', 'numpy>=1.20.3\n', 'torch>=1.11.0\n', 'tqdm>=4.62.3\n', '```\n', '\n', '## Quick-Start\n', '\n', 'With the source code, you can use the provided script for initial usage of our library:\n', '\n', '```\n', 'python run_recbole.py\n', '```\n', 'If you want to change the models or datasets, just run the script by setting additional command parameters:\n', '```\n', 'python run_recbole.py -m [model] -d [dataset] -c [config_files]\n', '```\n', '\n', '## Implement Models\n', '\n', 'We list the models that we have implemented up to now:\n', '\n', '- [FOCF](recbole/model/fair_recommender/focf.py) from Sirui Yao et al:[Beyond Parity：Fairness Objectives for Collaborative Filtering](https://proceedings.neurips.cc/paper/2017/hash/e6384711491713d29bc63fc5eeb5ba4f-Abstract.html)(NIPS 2017). Note: We implement this model with ranking-based metrics, e.g. NDCG@K.\n', '- PFCN from Yunqi Li et al:[Towards Personalized Fairness based on Causal Notion](https://dl.acm.org/doi/abs/10.1145/3404835.3462966?casa_token=zzHePKuKP6AAAAAA:YzZp_qUbzsgd3TXWCAGSRAfEHO2oM0_BuWZ5uZlfj_rudqKGYq8douOaZ0GoizxP54jtz3JDFw725xo)(SIGIR 2021)\n', '  - [PFCN_MLP](recbole/model/fair_recommender/pfcn_mlp.py)\n', '  - [PFCN_BiasedMF](recbole/model/fair_recommender/pfcn_biasedmf.py)\n', '  - [PFCN_DMF](recbole/model/fair_recommender/pfcn_dmf.py)\n', '  - [PFCN_PMF](recbole/model/fair_recommender/pfcn_pmf.py)\n', '- FairGo from Wu Le et al:[Learning Fair Representations for Recommendation: A Graph-based Perspective](https://dl.acm.org/doi/abs/10.1145/3442381.3450015?casa_token=MACP_5U-E6sAAAAA:L-dsEbdusWfmzF06OnATJhF2OXbjfu6el37nC-cGMjev4jGH_TBUedXyAhpfcBMyCyhyxOxLQkxqe_w) (WWW 2021) \n', '  - [FairGo_PMF(WAP,LBA,LVA)](recbole/model/fair_recommender/fairgo_pmf.py)\n', '  - [FairGo_GCN(WAP,LBA,LVA)](recbole/model/fair_recommender/fairgo_gcn.py)\n', '- [NFCF](recbole/model/fair_recommender/nfcf.py) from Rashidul Islam et al:[Debiasing career recommendations with neural fair collaborative filtering](https://dl.acm.org/doi/abs/10.1145/3442381.3449904?casa_token=ZzbZbC-Fn_oAAAAA:6KCSThLs7UsT9s0ZzeSryT3Mry067KeTiNdurfa9Q9UHWY7fLGgmjPtQy9i1zU1Yqm4Xf46NVYVuu40) (WWW 2021) \n', '\n', '## Datasets\n', '\n', ' The datasets used can be downloaded from [Datasets Link](https://drive.google.com/drive/folders/1OkDVEqetvOrtbuWebxl4y1JlZ_YjjfWj).\n', '\n', '# Hyper-parameters\n', 'We train the models with the default parameter settings, suggested in their original paper.[[link]](results/ml-1m.md)\n', '\n', '## The Team\n', 'RecBole-FairRec is developed and maintained by members from [RUCAIBox](http://aibox.ruc.edu.cn/), the main developers is Jiakai Tang ([@Tangjiakai](https://github.com/TangJiakai)).\n', '\n', '## Acknowledgement\n', '\n', 'The implementation is based on the open-source recommendation library [RecBole](https://github.com/RUCAIBox/RecBole).\n', '\n', 'Please cite the following paper as the reference if you use our code or processed datasets.\n', '\n', '```\n', '@inproceedings{zhao2021recbole,\n', '  title={Recbole: Towards a unified, comprehensive and efficient framework for recommendation algorithms},\n', '  author={Wayne Xin Zhao and Shanlei Mu and Yupeng Hou and Zihan Lin and Kaiyuan Li and Yushuo Chen and Yujie Lu and Hui Wang and Changxin Tian and Xingyu Pan and Yingqian Min and Zhichao Feng and Xinyan Fan and Xu Chen and Pengfei Wang and Wendi Ji and Yaliang Li and Xiaoling Wang and Ji-Rong Wen},\n', '  booktitle={{CIKM}},\n', '  year={2021}\n', '}\n']"
Model Fairness,chenchongthu/ENMF,chenchongthu,https://api.github.com/repos/chenchongthu/ENMF,140,28,1,['https://api.github.com/users/chenchongthu'],Python,2023-03-11T14:14:01Z,https://raw.githubusercontent.com/chenchongthu/ENMF/master/README.md,"['# ENMF\n', '\n', 'This is our implementation of Efficient Neural Matrix Factorization, which is a basic model of the paper:\n', '\n', '\n', '\n', '*Chong Chen, Min Zhang, Chenyang Wang, Weizhi Ma, Minming Li, Yiqun Liu and Shaoping Ma. 2019. [An Efficient Adaptive Transfer Neural Network for Social-aware Recommendation.](http://www.thuir.cn/group/~mzhang/publications/SIGIR2019ChenC.pdf) \n', ""In SIGIR'19.*\n"", '\n', '\n', 'This is also the codes of the TOIS paper:\n', '\n', '*Chong Chen, Min Zhang, Yongfeng Zhang, Yiqun Liu and Shaoping Ma. 2020. [Efficient Neural Matrix Factorization without Sampling for Recommendation.](https://chenchongthu.github.io/files/TOIS_ENMF.pdf) \n', 'In TOIS Vol. 38, No. 2, Article 14.*\n', '\n', 'The slides of this work has been uploaded. A chinese version instruction can be found at [Blog](https://zhuanlan.zhihu.com/p/107761829), and the video presentation can be found at [Demo](https://www.bilibili.com/video/BV1Z64y1u7GK?from=search&seid=10581986304255794319).\n', '\n', ""**Please cite our SIGIR'19 paper or TOIS paper if you use our codes. Thanks!**\n"", '\n', '```\n', '@inproceedings{chen2019efficient,\n', '  title={An Efficient Adaptive Transfer Neural Network for Social-aware Recommendation},\n', '  author={Chen, Chong and Zhang, Min and Wang, Chenyang and Ma, Weizhi and Li, Minming and Liu, Yiqun and Ma, Shaoping},\n', '  booktitle={Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},\n', '  pages={225--234},\n', '  year={2019},\n', '  organization={ACM}\n', '}\n', '```\n', '```\n', '@article{10.1145/3373807, \n', 'author = {Chen, Chong and Zhang, Min and Zhang, Yongfeng and Liu, Yiqun and Ma, Shaoping}, \n', 'title = {Efficient Neural Matrix Factorization without Sampling for Recommendation}, \n', 'year = {2020}, \n', 'issue_date = {January 2020}, \n', 'publisher = {Association for Computing Machinery}, \n', 'volume = {38}, \n', 'number = {2}, \n', 'issn = {1046-8188}, \n', 'url = {https://doi.org/10.1145/3373807}, \n', 'doi = {10.1145/3373807}, \n', 'journal = {ACM Trans. Inf. Syst.}, \n', 'month = jan, \n', 'articleno = {Article 14}, \n', 'numpages = {28}\n', '}\n', '```\n', '\n', 'Author: Chong Chen (cstchenc@163.com)\n', '\n', '## Environments\n', '\n', '- python\n', '- Tensorflow\n', '- numpy\n', '- pandas\n', '\n', '\n', '## Example to run the codes\t\t\n', '\n', 'Train and evaluate the model:\n', '\n', '```\n', 'python ENMF.py\n', '```\n', '## Suggestions for parameters\n', '\n', 'Two important parameters need to be tuned for different datasets, which are:\n', '\n', '```\n', ""parser.add_argument('--dropout', type=float, default=0.7,\n"", ""                        help='dropout keep_prob')\n"", ""parser.add_argument('--negative_weight', type=float, default=0.1,\n"", ""                        help='weight of non-observed data')\n"", '```\n', '                        \n', 'Specifically, we suggest to tune ""negative_weight"" among \\[0.001,0.005,0.01,0.02,0.05,0.1,0.2,0.5]. Generally, this parameter is related to the sparsity of dataset. If the dataset is more sparse, then a small value of negative_weight may lead to a better performance.\n', '\n', '\n', 'Generally, the performance of our ENMF is better than existing state-of-the-art recommendation models like NCF, CovNCF, CMN, and NGCF. You can also contact us if you can not tune the parameters properly.\n', '\n', '## Comparison with the most recent methods （updating）\n', '\n', 'Do the ""state-of-the-art"" recommendation models **really perform well?** If you want to see more comparison between our ENMF and any ""state-of-the-art"" recommendation models, feel free to propose an issue.\n', '\n', '### 1. LightGCN (SIGIR 2020) [LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation](http://staff.ustc.edu.cn/~hexn/papers/sigir20-LightGCN.pdf).\n', '\n', 'To be consistent with LightGCN, we use the same evaluation metrics (i.e., `Recall@K` and `NDCG@K`), use the same data Yelp2018 released in LightGCN (https://github.com/kuandeng/LightGCN).\n', '\n', 'The parameters of our ENMF on Yelp2018 are as follows:\n', '```\n', ""parser.add_argument('--dropout', type=float, default=0.7,\n"", ""                        help='dropout keep_prob')\n"", ""parser.add_argument('--negative_weight', type=float, default=0.05,\n"", ""                        help='weight of non-observed data')\n"", '```\n', 'Dataset: Yelp2018\n', '\n', '|    Model    | Recall@20 | NDCG@20 |\n', '| :---------: | :-------: | :----------: |\n', '|     NGCF    |  0.0579   |    0.0477    |  \n', '|     Mult-VAE     |  0.0584   |    0.0450    | \n', '|    GRMF    |  0.0571   |    0.0462    | \n', '|   LightGCN |  0.0649   |    **0.0530**    |\n', '|   ENMF |  **0.0650**   |    0.0515    |\n', '\n', '### 2. NBPO (SIGIR 2020) [Sampler Design for Implicit Feedback Data by Noisy-label Robust Learning](https://doi.org/10.1145/3397271.3401155). \n', 'This paper designs an adaptive sampler based on noisy-label robust learning for implicit feedback data. To be consistent with NBPO, we use the same evaluation metrics (i.e., `F1@K`, `NDCG@K`), use the same data Amazon-14core released in NBPO (https://github.com/Wenhui-Yu/NBPO). For fair comparison, we also set the embedding size as 50, which is utilized in the NBPO work.\n', '\n', 'The parameters of our ENMF on Amazon-14core are as follows:\n', '```\n', ""parser.add_argument('--dropout', type=float, default=0.2,\n"", ""                        help='dropout keep_prob')\n"", ""parser.add_argument('--negative_weight', type=float, default=0.2,\n"", ""                        help='weight of non-observed data')\n"", '```\n', 'Dataset: Amazon-14core\n', '\n', '|    Model    | F1@5 | F1@10 |F1@20| NDCG@5 | NDCG@10 |NDCG@20|\n', '| :---------: | :-------: | :----------: | :---------: | :-------: | :----------: | :----------: |\n', '|     BPR    | 0.0326| 0.0317| 0.0275|0.0444| 0.0551| 0.0680| \n', '|     NBPO     |  0.0401| 0.0357| 0.0313|0.0555| 0.0655| 0.0810|\n', '|   ENMF |  **0.0419**   |    **0.0388**    |**0.0314**|**0.0566**|**0.0698**|**0.0823**|\n', '\n', '### 3. LCFN (ICML 2020)[Graph Convolutional Network for Recommendation with Low-pass Collaborative Filters](https://arxiv.org/pdf/2006.15516v1.pdf)\n', 'To be consistent with LCFN, we use the same evaluation metrics (i.e., `F1@K`, `NDCG@K`), use the same data Movlelens-1m released in LCFN (https://github.com/Wenhui-Yu/LCFN). For fair comparison, we also set the embedding size as 128, which is utilized in the LCFN work.\n', '\n', 'The parameters of our ENMF on Movielens-1m (ml-lcfn) are as follows:\n', '```\n', ""parser.add_argument('--dropout', type=float, default=0.5,\n"", ""                        help='dropout keep_prob')\n"", ""parser.add_argument('--negative_weight', type=float, default=0.5,\n"", ""                        help='weight of non-observed data')\n"", '```                       \n', '\n', 'Dataset: Movielens-1m (ml-lcfn)\n', '\n', '|    Model    | F1@5 | F1@10 |F1@20| NDCG@5 | NDCG@10 |NDCG@20|\n', '| :---------: | :-------: | :----------: | :---------: | :-------: | :----------: | :----------: |\n', '|     GCMC    | 0.1166| 0.1437| 0.1564|0.2411| 0.2361| 0.2496| \n', '|     NGCF     |  0.1153| 0.1425| 0.1582|0.2367| 0.2347| 0.2511|\n', '|     SCF     |  0.1189| 0.1451| 0.1600|0.2419| 0.2398| 0.2560|\n', '|     CGMC     |  0.1179| 0.1431| 0.1573|0.2408| 0.2372| 0.2514|\n', '|     LCFN     |  0.1213| 0.1482| 0.1625|0.2427| 0.2429| 0.2603|\n', '|   ENMF |  **0.1239**   |    **0.1512**    |**0.1640**|**0.2457**|**0.2475**|**0.2656**|\n', '\n', '\n', '### 4. DHCF (KDD 2020)[Dual Channel Hypergraph Collaborative Filtering](http://gaoyue.org/paper/shuyi_KDD_final.pdf)\n', 'To be consistent with DHCF, we use the same evaluation metrics (i.e., `Precision@K`, `Recall@K`), use the same data CiteUlike-A (thanks for the authors of DHCF who kindly provide the dataset). For fair comparison, we also set the embedding size as 64, which is utilized in the DHCF work.\n', '\n', 'The parameters of our ENMF on CiteUlike-A are as follows:\n', '```\n', ""parser.add_argument('--dropout', type=float, default=0.5,\n"", ""                        help='dropout keep_prob')\n"", ""parser.add_argument('--negative_weight', type=float, default=0.02,\n"", ""                        help='weight of non-observed data')\n"", '```        \n', '\n', 'Dataset: CiteUlike-A\n', '\n', '\n', '|    Model    | Precision@20 | Recall@20 |\n', '| :---------: | :-------: | :----------: |\n', '|     BPR    | 0.0330| 0.0124|\n', '|     GCMC     |  0.0317| 0.0103|\n', '|     PinSage    |  0.0508| 0.0194|\n', '|     NGCF     |  0.0517| 0.0193|\n', '|     DHCF     |  0.0635| 0.0249|\n', '|   ENMF |  **0.0748**   |    **0.0280**    |\n', '\n', '### 5. SRNS (NeurIPS 2020)[Simplify and Robustify Negative Sampling for Implicit Collaborative Filtering](https://arxiv.org/pdf/2009.03376.pdf)\n', '\n', 'This work proposes a simplified and robust negative sampling approach SRNS for implicit CF. The authors have compared their SRNS method with our ENMF in the original paper. However, we reran the experiment and got some **different** results.\n', '\n', 'To be consistent with SRNS, we use the same evaluation metrics (i.e., `NDCG@K`, `Recall@K`), use the same data Movlelens-1m released in SRNS (https://github.com/dingjingtao/SRNS). For fair comparison, we also set the embedding size as 32, which is utilized in the SRNS work. \n', '\n', 'The parameters of our ENMF on Movielens-1m(ml-srns) are as follows:\n', '```\n', ""parser.add_argument('--dropout', type=float, default=0.9,\n"", ""                        help='dropout keep_prob')\n"", ""parser.add_argument('--negative_weight', type=float, default=0.3,\n"", ""                        help='weight of non-observed data')\n"", '```        \n', '\n', 'Dataset: Movielens-1m (ml-srns)\n', '\n', '| Model   | N@1        | N@3        | R@3        |\n', '| ------- | ---------- | ---------- | ---------- |\n', '| Uniform | 0.1744     | 0.2846     | 0.3663     |\n', '| NNCF    | 0.0831     | 0.1428     | 0.1873     |\n', '| AOBPR   | 0.1782     | 0.2907     | 0.3749     |\n', '| IRGAN   | 0.1763     | 0.2878     | 0.3706     |\n', '| RNS-AS  | 0.1810     | 0.2950     | 0.3801     |\n', '| AdvIR   | 0.1792     | 0.2889     | 0.3699     |\n', '| ENMF (reported in the srns paper)   | 0.1846     | 0.2970     | 0.3804     | (inaccurate results)\n', '| SRNS    | 0.1911     | 0.3056     | 0.3907     |\n', '| ENMF (our)  | **0.1917**     | **0.3124**     | **0.4016**    |\n', '\n']"
Model Fairness,lingjuanlv/FPPDL,lingjuanlv,https://api.github.com/repos/lingjuanlv/FPPDL,27,9,1,['https://api.github.com/users/lingjuanlv'],Python,2023-04-22T03:07:01Z,https://raw.githubusercontent.com/lingjuanlv/FPPDL/master/README.md,"['# FPPDL\n', 'code for TPDS paper ""Towards Fair and Privacy-Preserving Federated Deep Models""! Folder ""dpgan"" is used to generate DPGAN samples on each party!\n', '\n', '# How to run:\n', 'th fppdl_tpds.lua -dataset mnist -model deep -slevel 1 -imbalanced 1 -netSize 4 -nepochs 100 -local_nepochs 5 -batchSize 10 -learningRate 0.15 -taskID mnist_deep_p4e100_imbalanced -shardID mnist_p4_imbalanced -run run1 -pretrain 1 -credit_fade 1\n', '\n', '# How to analyze fairness:\n', 'All logs will be dumped into folder ""logs"". Process log and analyze fairness as follows:\n', '```\n', '1. X axis: standalone accuracy \n', 'grep ""standalone"" logs/fppdl_mnist_deep_p4e100_slevel01_imbalanced_IID1_pretrain1_localepoch5_localbatch10_lr0.15_run1_tpds.log >1.log\n', ""awk '{print $NF}' ORS=', ' 1.log\n"", 'x=[0.8528, 0.8895, 0.7765, 0.8828]\n', '2. Y axis: final accuracy \n', 'grep ""final test acc"" logs/fppdl_mnist_deep_p4e100_slevel01_imbalanced_IID1_pretrain1_localepoch5_localbatch10_lr0.15_run1_tpds.log >1.log\n', ""awk '{print $NF}' ORS=', ' 1.log\n"", 'y=[0.8874, 0.9191, 0.8158, 0.9118]\n', '3. Finally, using scipy.stats.pearsonr(x,y)=0.9996588631722703 to calculate fairness.\n', '```\n', '\n', '# Requirements:\n', '- torch7, download from http://torch.ch/\n', '- python3\n', '\n', '# Bibtex\n', 'Remember to cite the following papers if you use any part of the code:\n', '```\n', '@article{lyu2020towards,\n', '  title={Towards Fair and Privacy-Preserving Federated Deep Models},\n', '  author={Lyu, Lingjuan and Yu, Jiangshan and Nandakumar, Karthik and Li, Yitong and Ma, Xingjun and Jin, Jiong and Yu, Han and Ng, Kee Siong},\n', '  journal={IEEE Transactions on Parallel and Distributed Systems},\n', '  volume={31},\n', '  number={11},\n', '  pages={2524--2541},\n', '  year={2020},\n', '  publisher={IEEE}\n', '}\n', '```\n']"
Model Fairness,UCLA-StarAI/FairPC.jl,UCLA-StarAI,https://api.github.com/repos/UCLA-StarAI/FairPC.jl,5,3,3,"['https://api.github.com/users/MhDang', 'https://api.github.com/users/yoojungchoi', 'https://api.github.com/users/guyvdbroeck']",Python,2023-03-02T13:48:21Z,https://raw.githubusercontent.com/UCLA-StarAI/FairPC.jl/main/README.md,"['# Learn Fair PC\n', '\n', 'This repo contains the code and experiments from the paper ""[Group Fairness by Probabilistic Modeling with Latent Fair Decisions](http://starai.cs.ucla.edu/papers/ChoiAAAI21.pdf)"", published in AAAI 2021.\n', '\n', '\n', '## Files\n', '\n', '```\n', '  baselines/    Python scripts to reproduce `Reweight`, `Reduction` and `FairLR`.\n', '  bin/          Runnable julia scripts (see below).\n', '  circuits/     Learned circuits in experiments.\n', '  data/         Datasets used in the experiments.\n', '  scripts/      Helper files to generate experiments scripts.\n', '  src/          The source code for the algorithm.\n', '  Project.toml  This file specifies required julia environment.\n', '  README.md     This is this file.\n', '```\n', '\n', '## Installation\n', '\n', '1. Julia version 1.7\n', '\n', '2. Run commands with flag `--project` will automatically use the packages specified in `Project.toml`. See belows scripts for examples.\n', '## Experiments\n', '\n', '### Usage\n', '\n', '- Run `bin/learn.jl` with `--help` argument to see the usage message. \n', 'Most of the options have default values. The following are some arguments need to be manully set:\n', '\n', '```\n', 'positional arguments:\n', '  dataset               dataset name, in {compas, adult, german, synthetic}\n', 'optional arguments:\n', '  --sensitive_variable  sensitive variable of current data set, e.g.,{Ethnic_Code_Text_, sex, S}\n', '  --fold                fold id for k-fold cross validation, in [1:10]\n', '  --struct_type         indicate structure constrains of probability distributions, in {FairPC, TwoNB, NlatPC, LatNB}\n', '  --num_X               number of non sensitive features in synthetic data set setting, in [10:30]\n', '```\n', '\n', '- Some sample scripts\n', '\n', '```\n', '$  julia --project bin/learn.jl compas --exp-id 1  --dir ""exp/compas/1"" --struct_type ""FairPC""  --sensitive_variable ""Ethnic_Code_Text_""  --fold 1 \n', '$  julia --prroject bin/learn.jl synthetic --exp-id 2  --dir ""exp/synthetic/2"" --struct_type ""FairPC""  --num_X 10  --sensitive_variable ""S""  --fold 1 \n', '```\n', '\n', '- To generate multiple scripts and run batches of experiments in parallel, run the following for real-world dataset and synthetic dataset respectively:\n', '\n', '``` \n', '$ julia --project bin/gen_exp.jl scripts/json/realworld-fair.json \n', '$ julia --project bin/gen_exp.jl scripts/json/synthetic-fair.json\n', '```\n', 'you can also change `dir` in file `*.json` to the output directory you want.\n', '\n', '### Baselines\n', '- For `TowNB`, `LatNB`, and `NlatPC`, see above.\n', '- For `Reduction`, `Reweight`, and `FairLR` methods, run `fair_reduction.py`, `reweight.py` or `fair_lr.py` respectively(the first two in `python3` and the last in `python2`) in directory `.\\baselines` with the following arguments:\n', '```\n', '# usage is the same as above\n', 'positional arguments:\n', '  dataset\n', 'optional arguments:\n', '  --fold\n', '  --num_X\n', '```\n', '\n', '- Some sample scripts\n', '```\n', '$ python3 reweight.py compas --fold 1\n', '$ python2 fair_lr.py synthetic --fold 1 --num_X 30\n', '$ python3 fair_reduction.py german --fold 2\n', '```\n', '- To generate batches scripts, run:\n', '```\n', '$ julia bin/gen_exp.jl scripts/json/baselines.json --set_id 0 --cmd python3 -b fair_reduction.py\n', '$ julia bin/gen_exp.jl scripts/json/baselines.json --set_id 0 --cmd python3 -b reweight.py\n', '$ julia bin/gen_exp.jl scripts/json/baselines.json --set_id 0 --cmd python -b fair_lr.py\n', '```']"
Model Fairness,monindersingh/pydata2018_fairAI_models_tutorial,monindersingh,https://api.github.com/repos/monindersingh/pydata2018_fairAI_models_tutorial,5,5,1,['https://api.github.com/users/monindersingh'],Python,2021-03-23T19:26:23Z,https://raw.githubusercontent.com/monindersingh/pydata2018_fairAI_models_tutorial/master/README.md,"['# Building Fair AI models tutorial at PyData New York, 2018\n', '\n', '\n', '### This tutorial uses an open source Python package named [AI Fairness 360 or AIF360](https://github.com/ibm/aif360). \n', '\n', '### Please visit the above site and follow instructions to install the package.\n', '\n', '### Additionally, download datasets following instructions at [https://github.com/IBM/AIF360/tree/master/aif360/data](https://github.com/IBM/AIF360/tree/master/aif360/data)\n', '\n', '\n', '\n', '### Alternatively, instructions are provided below for manually installing AIF360 and downloading datasets using Conda on Windows\n', '\n', 'Create and activate environment\n', '\n', '```bash\n', 'conda create --name aif360 python=3.5\n', 'conda activate aif360\n', '```\n', '\n', '\n', '\n', 'Clone AIF360 from GitHub:\n', '\n', '```bash\n', 'git clone https://github.com/IBM/AIF360\n', '```\n', '\n', 'Install R-essentials for downloading MEPS data\n', '\n', '```bash\n', 'conda install -c r r-essentials\n', '```\n', '\n', 'Download datasets and place under appropriate folders under AIF360/aif360/data/raw by cloning this repository (NOTE: clone at same level as AIF360) and running the belowmentioned notebooks in the root folder\n', '          \n', '```bash\n', 'git clone https://github.com/monindersingh/pydata2018_fairAI_models_tutorial.git\n', '```\n', 'Change to the root folder of just cloned repository and run\n', '\n', '```bash\n', 'jupyter notebook pydata_datasets.ipynb\n', 'jupyter notebook pydata_meps_datasets.ipynb\n', '```\n', '\n', '\n', 'Then, navigate to the root directory of the cloned AIF360 project and run:\n', '\n', '```bash\n', 'pip install .\n', '```\n', '\n', '\n', '\n', 'Finally, install the additional requirements as follows:\n', '\n', '```bash\n', 'conda install ecos\n', 'pip install -r requirements.txt\n', '```\n', '\n']"
Model Fairness,ecreager/causal-dyna-fair,ecreager,https://api.github.com/repos/ecreager/causal-dyna-fair,8,4,2,"['https://api.github.com/users/ecreager', 'https://api.github.com/users/dependabot%5Bbot%5D']",Python,2022-09-23T03:22:57Z,https://raw.githubusercontent.com/ecreager/causal-dyna-fair/master/README.md,"['# causal-dyna-fair\n', 'Code accompanying the paper ""Causal Modeling for Fairness in Dynamical Systems"", presented at ICML 2020.\n', '\n', 'ArXiV: https://arxiv.org/abs/1909.09141\n', '\n', 'ICML results can be reproduced by `./bin/icml_results.sh`.\n', '\n', 'Package dependencies are specified in `requirements.txt`.\n', 'We strongly recommend using a fresh virtual environment and with packages installed via `pip install -r requirements.txt`.\n', 'Finally, we note that the `whynot` dependency may need to be installed from source as follows:\n', '```\n', 'git clone git@github.com:zykls/whynot.git\n', 'cd whynot\n', 'pip install .\n', '```']"
Model Fairness,MichaelMuinos/discounted-cash-flow-model,MichaelMuinos,https://api.github.com/repos/MichaelMuinos/discounted-cash-flow-model,3,4,1,['https://api.github.com/users/MichaelMuinos'],Python,2021-01-07T16:50:43Z,https://raw.githubusercontent.com/MichaelMuinos/discounted-cash-flow-model/master/README.md,"['# discounted-cash-flow-model\n', '\n', '## What is the DCF model?\n', 'Discounted cash flow (DCF) is a valuation method used to estimate the value of an investment based on its future cash flows. DCF analysis attempts to figure out the value of an investment today, based on projections of how much money it will generate in the future. This applies to both financial investments for investors and for business owners looking to make changes to their businesses, such as purchasing new equipment. [https://www.investopedia.com/terms/d/dcf.asp]\n', '\n', ""### THIS IS STILL A WIP, DON'T USE THIS FOR MAKING INVESTMENTS\n""]"
Model Fairness,IBMDeveloperUK/Trusted-AI-Workshops,IBMDeveloperUK,https://api.github.com/repos/IBMDeveloperUK/Trusted-AI-Workshops,10,5,1,['https://api.github.com/users/MargrietGroenendijk'],Python,2023-02-10T07:24:30Z,https://raw.githubusercontent.com/IBMDeveloperUK/Trusted-AI-Workshops/master/README.md,"['# Trusted AI\n', '\n', '## Details\n', '\n', 'You can find material for workshops on Trusted AI here. Most workshops will be using a Jupyter notebook that you can either run on your own machine or in Watson Studio on the IBM Cloud. \n', '\n', 'To go through the workshop material smoothly it will be easiest when we all use the same setup, especially as it will be harder to help you straight away when you might get stuck during virtual events. Of course feel free to use your own local machine when you are comfortable with setting up environments and installing missing packages.\n', '\n', '## Getting Started\n', '\n', 'Find the detailed instructions to set up a Cloud environment [here](https://github.com/IBMDeveloperUK/Trusted-AI-Workshops/blob/master/watson-studio-instructions.md)\n', '\n', '* [Create a free IBM Cloud account](https://ibm.biz/BdqNqh)\n', '* [Get started with Watson Studio](https://github.com/IBMDeveloperUK/Trusted-AI-Workshops/blob/master/watson-studio-instructions.md)\n', '\n', 'Then go to the session\n', '\n', '* [Beyond Accuracy: Fairness in Machine Learning](https://github.com/IBMDeveloperUK/Trusted-AI-Workshops/blob/master/beyond-accuracy.md)\n', '    \n', '## Resources\n', '\n', '### Documentation\n', '\n', '#### AI Fairness 360\n', '\n', '* [GitHub repo](https://github.com/Trusted-AI/AIF360)\n', '* [Demo](https://aif360.mybluemix.net/)\n', '* [API documentation](https://aif360.readthedocs.io/en/latest/)\n', '\n', '#### AI Explainability 360\n', '\n', '* [GitHub repo](https://github.com/Trusted-AI/AIX360)\n', '* [Demo](http://aix360.mybluemix.net/)\n', '* [API documentation](https://aix360.readthedocs.io/en/latest/)\n', '\n', '#### AI Factsheets 360\n', '\n', '#### Learn more\n', '\n', '* On the UK team page \n', '    * https://ibmdeveloperuk.github.io/\n', '\n', '* From the weekly [Data Science Lunch & Learn](https://github.com/IBMDeveloperUK/Data-Science-Lunch-and-Learn) every Monday\n', '* On our crowdcast and twitch channels, where we host regular workshops and events\n', '    * https://www.crowdcast.io/ibmdeveloper\n', '    * https://www.crowdcast.io/ibmdevelopereurope\n', '    * https://www.twitch.tv/ibmdeveloper\n', '\n', '* From our [developer site](https://developer.ibm.com/) where you can find tutorials, code examples, articles and more\n', '    * [List of videos on Trusted AI](https://aifs360.mybluemix.net/resources/videos)\n', '    * [AI Ethics at IBM](https://www.ibm.com/artificial-intelligence/ethics)\n', '    * [Trusted AI at IBM Research](https://www.research.ibm.com/artificial-intelligence/trusted-ai/)\n', '    * [Trusted AI at the Linux Foundation](https://lfai.foundation/projects/trusted-ai/)\n', '\n', '#### Contact me \n', '\n', 'If you need help, have suggestions or want to talk more about fair and explainable AI\n', '\n', '* [Twitter](https://twitter.com/MargrietGr)\n', '* [LinkedIn](https://www.linkedin.com/in/margrietgroenendijk/)\n']"