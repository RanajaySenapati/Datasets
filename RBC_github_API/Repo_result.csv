tag_name,git_name,owner_name,git_url,star_count,forks_count,contributor_count,contributors_list,code_language,last_updated_date,readme_url,readme_text
Responsible AI,microsoft/responsible-ai-toolbox,microsoft,https://api.github.com/repos/microsoft/responsible-ai-toolbox,685,179,30,"['https://api.github.com/users/imatiach-msft', 'https://api.github.com/users/gaugup', 'https://api.github.com/users/xuke444', 'https://api.github.com/users/zhb000', 'https://api.github.com/users/romanlutz', 'https://api.github.com/users/vinuthakaranth', 'https://api.github.com/users/tongyu-microsoft', 'https://api.github.com/users/gregorybchris', 'https://api.github.com/users/dependabot%5Bbot%5D', 'https://api.github.com/users/riedgar-ms', 'https://api.github.com/users/RubyZ10', 'https://api.github.com/users/csigs', 'https://api.github.com/users/rihorn2', 'https://api.github.com/users/ms-kashyap', 'https://api.github.com/users/mesameki', 'https://api.github.com/users/jamesbchao', 'https://api.github.com/users/ilmarinen', 'https://api.github.com/users/microsoftopensource', 'https://api.github.com/users/janjagusch', 'https://api.github.com/users/JarvisG495', 'https://api.github.com/users/LeJit', 'https://api.github.com/users/yongjiaaaa', 'https://api.github.com/users/alexquach', 'https://api.github.com/users/aminadibi', 'https://api.github.com/users/zhb789', 'https://api.github.com/users/jlema', 'https://api.github.com/users/michaelamoako', 'https://api.github.com/users/sawravchy', 'https://api.github.com/users/ShabadVaswani', 'https://api.github.com/users/hawestra']",TypeScript,2023-02-23T09:10:43Z,https://raw.githubusercontent.com/microsoft/responsible-ai-toolbox/main/README.md,"['![MIT license](https://img.shields.io/badge/License-MIT-blue.svg)\n', '\n', '![Responsible AI Widgets Python Build](https://img.shields.io/github/actions/workflow/status/microsoft/responsible-ai-toolbox/CI-raiwidgets-pytest.yml?branch=main&label=Responsible%20AI%20Widgets%20Python%20Build)\n', '![UI deployment to test environment](https://img.shields.io/github/actions/workflow/status/microsoft/responsible-ai-toolbox/CD.yml?branch=main&label=UI%20deployment%20to%20test%20environment)\n', '\n', '![PyPI raiwidgets](https://img.shields.io/pypi/v/raiwidgets?label=PyPI%20raiwidgets)\n', '![PyPI responsibleai](https://img.shields.io/pypi/v/responsibleai?label=PyPI%20responsibleai)\n', '![PyPI erroranalysis](https://img.shields.io/pypi/v/erroranalysis?label=PyPI%20erroranalysis)\n', '![PyPI raiutils](https://img.shields.io/pypi/v/raiutils?label=PyPI%20raiutils)\n', '![PyPI rai_test_utils](https://img.shields.io/pypi/v/rai_test_utils?label=PyPI%20rai_test_utils)\n', '\n', '![npm model-assessment](https://img.shields.io/npm/v/@responsible-ai/model-assessment?label=npm%20%40responsible-ai%2Fmodel-assessment)\n', '\n', '# Responsible AI Toolbox\n', 'Responsible AI is an approach to assessing, developing, and deploying AI systems in a safe, trustworthy, and ethical manner, and take responsible decisions and actions.\n', '\n', 'Responsible AI Toolbox is a suite of tools providing a collection of model and data exploration and assessment user interfaces and libraries that enable a better understanding of AI systems. These interfaces and libraries empower developers and stakeholders of AI systems to develop and monitor AI more responsibly, and take better data-driven actions.\n', '\n', '\n', '<p align=""center"">\n', '<img src=""https://raw.githubusercontent.com/microsoft/responsible-ai-widgets/main/img/responsible-ai-toolbox.png"" alt=""ResponsibleAIToolboxOverview"" width=""750""/>\n', '\n', '\n', 'The Toolbox consists of three repositories: \n', '\n', '\xa0\n', '| Repository| Tools Covered  |\n', '|--|--|\n', '| [Responsible-AI-Toolbox Repository](https://github.com/microsoft/responsible-ai-toolbox) (Here) |This repository contains four visualization widgets for model assessment and decision making:<br>1. [Responsible AI dashboard](https://github.com/microsoft/responsible-ai-toolbox#introducing-responsible-ai-dashboard), a single pane of glass bringing together several mature Responsible AI tools from the toolbox for a holistic responsible assessment and debugging of models and making informed business decisions. With this dashboard, you can identify model errors, diagnose why those errors are happening, and mitigate them. Moreover, the causal decision-making capabilities provide actionable insights to your stakeholders and customers.<br>2. [Error Analysis dashboard](https://github.com/microsoft/responsible-ai-toolbox/blob/main/docs/erroranalysis-dashboard-README.md), for identifying model errors and discovering cohorts of data for which the model underperforms. \t<br>3. [Interpretability dashboard](https://github.com/microsoft/responsible-ai-toolbox/blob/main/docs/explanation-dashboard-README.md), for understanding model predictions. This dashboard is powered by InterpretML.<br>4. [Fairness dashboard](https://github.com/microsoft/responsible-ai-toolbox/blob/main/docs/fairness-dashboard-README.md), for understanding model’s fairness issues using various group-fairness metrics across sensitive features and cohorts. This dashboard is powered by Fairlearn. \n', '| [Responsible-AI-Toolbox-Mitigations Repository](https://github.com/microsoft/responsible-ai-toolbox-mitigations) | The Responsible AI Mitigations Library helps AI practitioners explore different measurements and mitigation steps that may be most appropriate when the model underperforms for a given data cohort. The library currently has two modules: <br>1. DataProcessing, which offers mitigation techniques for improving model performance for specific cohorts. <br>2. DataBalanceAnalysis, which provides metrics for diagnosing errors that originate from data imbalance either on class labels or feature values. <br> 3. Cohort: provides classes for handling and managing cohorts, which allows the creation of custom pipelines for each cohort in an easy and intuitive interface. The module also provides techniques for learning different decoupled estimators (models) for different cohorts and combining them in a way that optimizes different definitions of group fairness.|  \n', '[Responsible-AI-Tracker Repository](https://github.com/microsoft/responsible-ai-toolbox-tracker) |Responsible AI Toolbox Tracker is a JupyterLab extension for managing, tracking, and comparing results of machine learning experiments for model improvement. Using this extension, users can view models, code, and visualization artifacts within the same framework enabling therefore fast model iteration and evaluation processes. Main functionalities include: <br>1. Managing and linking model improvement artifacts<br> 2. Disaggregated model evaluation and comparisons<br>3. Integration with the Responsible AI Mitigations library<br>4. Integration with mlflow|\n', ' [Responsible-AI-Toolbox-GenBit Repository](https://github.com/microsoft/responsible-ai-toolbox-genbit) | The Responsible AI Gender Bias (GenBit) Library helps AI practitioners measure gender bias in Natural Language Processing (NLP) datasets. The main goal of GenBit is to analyze your text corpora and compute metrics that give insights into the gender bias present in a corpus.|\n', '\n', '  \n', '\n', '\n', '## Introducing Responsible AI dashboard\n', '\n', '[Responsible AI dashboard](https://github.com/microsoft/responsible-ai-toolbox/blob/main/notebooks/responsibleaidashboard/tour.ipynb) is a single pane of glass, enabling you to easily flow through different stages of model debugging and decision-making. This customizable experience can be taken in a multitude of directions, from analyzing the model or data holistically, to conducting a deep dive or comparison on cohorts of interest, to explaining and perturbing model predictions for individual instances, and to informing users on business decisions and actions.\n', '\n', '\n', '<p align=""center"">\n', '<img src=""https://raw.githubusercontent.com/microsoft/responsible-ai-widgets/main/img/responsible-ai-dashboard.png"" alt=""ResponsibleAIDashboard"" width=""750""/>\n', '\n', '\n', '\n', '\n', 'In order to achieve these capabilities, the dashboard integrates together ideas and technologies from several open-source toolkits in the areas of\n', '\n', '\n', '\n', '- <b>Error Analysis</b> powered by [Error Analysis](https://github.com/microsoft/responsible-ai-widgets/blob/main/docs/erroranalysis-dashboard-README.md), which identifies cohorts of data with higher error rate than the overall benchmark. These discrepancies might occur when the system or model underperforms for specific demographic groups or infrequently observed input conditions in the training data.\n', '- <b>Fairness Assessment</b> powered by [Fairlearn](https://github.com/fairlearn/fairlearn), which identifies which groups of people may be disproportionately negatively impacted by an AI system and in what ways.\n', '\n', ""- <b>Model Interpretability</b> powered by [InterpretML](https://github.com/interpretml/interpret-community), which explains blackbox models, helping users understand their model's global behavior, or the reasons behind individual predictions.\n"", '\n', ""- <b>Counterfactual Analysis</b> powered by [DiCE](https://github.com/interpretml/DiCE), which shows feature-perturbed versions of the same datapoint who would have received a different prediction outcome, e.g., Taylor's loan has been rejected by the model. But they would have received the loan if their income was higher by $10,000.\n"", '\n', '- <b>Causal Analysis</b> powered by [EconML](https://github.com/microsoft/EconML), which focuses on answering What If-style questions to apply data-driven decision-making – how would revenue be affected if a corporation pursues a new pricing strategy? Would a new medication improve a patient’s condition, all else equal?\n', '\n', '- <b>Data Balance</b> powered by [Responsible AI](https://github.com/microsoft/responsible-ai-toolbox/blob/main/docs/databalance-README.md), which helps users gain an overall understanding of their data, identify features receiving the positive outcome more than others, and visualize feature distributions.\n', '\n', 'Responsible AI dashboard is designed to achieve the following goals:\n', '\n', '- To help further accelerate engineering processes in machine learning by enabling practitioners to design customizable workflows and tailor Responsible AI dashboards that best fit with their model assessment and data-driven decision making scenarios.\n', '- To help model developers create end to end and fluid debugging experiences and navigate seamlessly through error identification and diagnosis by using interactive visualizations that identify errors, inspect the data, generate global and local explanations models, and potentially inspect problematic examples.\n', '- To help business stakeholders explore causal relationships in the data and take informed decisions in the real world.\n', '\n', 'This repository contains the Jupyter notebooks with examples to showcase how to use this widget. Get started [here](https://github.com/microsoft/responsible-ai-toolbox/blob/main/notebooks/responsibleaidashboard/getting-started.ipynb).\n', '\n', '\n', '### Installation\n', '\n', 'Use the following pip command to install the Responsible AI Toolbox.\n', '\n', 'If running in jupyter, please make sure to restart the jupyter kernel after installing.\n', '\n', '```\n', 'pip install raiwidgets\n', '```\n', '\n', '\n', '### Responsible AI dashboard Customization\n', '\n', 'The Responsible AI Toolbox’s strength lies in its customizability. It empowers users to design tailored, end-to-end model debugging and decision-making workflows that address their particular needs. Need some inspiration? Here are some examples of how Toolbox components can be put together to analyze scenarios in different ways:\n', '\n', 'Please note that model overview (including fairness analysis) and data explorer components are activated by default!\n', '\xa0\n', '| Responsible AI Dashboard Flow| Use Case  |\n', '|--|--|\n', '| Model Overview -> Error Analysis -> Data Explorer | To identify model errors and diagnose them by understanding the underlying data distribution\n', '| Model Overview -> Fairness Assessment -> Data Explorer | To identify model fairness issues and diagnose them by understanding the underlying data distribution\n', '| Model Overview -> Error Analysis -> Counterfactuals Analysis and What-If | To diagnose errors in individual instances with counterfactual analysis (minimum change to lead to a different model prediction)\n', '| Model Overview -> Data Explorer -> Data Balance | To understand the root cause of errors and fairness issues introduced via data imbalances or lack of representation of a particular data cohort\n', ' | Model Overview -> Interpretability | To diagnose model errors through understanding how the model has made its predictions\n', ' | Data Explorer -> Causal Inference | To distinguish between correlations and causations in the data or decide the best treatments to apply to see a positive outcome\n', '  | Interpretability -> Causal Inference | To learn whether the factors that model has used for decision making has any causal effect on the real-world outcome.\n', ' | Data Explorer -> Counterfactuals Analysis and What-If | To address customer questions about what they can do next time to get a different outcome from an AI.\n', '  | Data Explorer -> Data Balance | To gain an overall understanding of the data, identify features receiving the positive outcome more than others, and visualize feature distributions\n', '\n', '\n', '### Useful Links\n', '\n', '- [Take a tour of Responsible AI Dashboard](https://github.com/microsoft/responsible-ai-toolbox/blob/main/notebooks/responsibleaidashboard/tour.ipynb)\n', '- [Get started](https://github.com/microsoft/responsible-ai-toolbox/blob/main/notebooks/responsibleaidashboard/getting-started.ipynb)\n', '\n', 'Model Debugging Examples:\n', '- [Try the tool: model debugging of a census income prediction model (classification)](https://github.com/microsoft/responsible-ai-toolbox/tree/main/notebooks/responsibleaidashboard/responsibleaidashboard-census-classification-model-debugging.ipynb)\n', '- [Try the tool: model debugging of a housing price prediction model (classification)](https://github.com/microsoft/responsible-ai-toolbox/tree/main/notebooks/responsibleaidashboard/responsibleaidashboard-housing-classification-model-debugging.ipynb)\n', '- [Try the tool: model debugging of a diabetes progression prediction model (regression)](https://github.com/microsoft/responsible-ai-toolbox/tree/main/notebooks/responsibleaidashboard/responsibleaidashboard-diabetes-regression-model-debugging.ipynb)\n', '\n', ' Responsible Decision Making Examples:\n', '- [Try the tool: make decisions for house improvements](https://github.com/microsoft/responsible-ai-toolbox/tree/main/notebooks/responsibleaidashboard/responsibleaidashboard-housing-decision-making.ipynb)\n', '- [Try the tool: provide recommendations to patients using diabetes data](https://github.com/microsoft/responsible-ai-toolbox/tree/main/notebooks/responsibleaidashboard/responsibleaidashboard-diabetes-decision-making.ipynb)\n', '\n', '\n', '\n', '## Supported Models\n', '\n', 'This Responsible AI Toolbox API supports models that are trained on datasets in Python `numpy.ndarray`, `pandas.DataFrame`, `iml.datatypes.DenseData`, or `scipy.sparse.csr_matrix` format.\n', '\n', ""The explanation functions of [Interpret-Community](https://github.com/interpretml/interpret-community) accept both models and pipelines as input as long as the model or pipeline implements a `predict` or `predict_proba` function that conforms to the Scikit convention. If not compatible, you can wrap your model's prediction function into a wrapper function that transforms the output into the format that is supported (predict or predict_proba of Scikit), and pass that wrapper function to your selected interpretability techniques.\n"", '\n', 'If a pipeline script is provided, the explanation function assumes that the running pipeline script returns a prediction. The repository also supports models trained via **PyTorch**, **TensorFlow**, and **Keras** deep learning frameworks.\n', '\n', '## Other Use Cases\n', '\n', 'Tools within the Responsible AI Toolbox can also be used with AI models offered as APIs by providers such as [Azure Cognitive Services](https://azure.microsoft.com/en-us/services/cognitive-services/). To see example use cases, see the folders below:\n', '\n', '- [Cognitive Services Speech to Text Fairness testing](https://github.com/microsoft/responsible-ai-toolbox/tree/main/notebooks/cognitive-services-examples/speech-to-text)\n', '- [Cognitive Services Face Verification Fairness testing](https://github.com/microsoft/responsible-ai-toolbox/tree/main/notebooks/cognitive-services-examples/face-verification)\n', '\n', '## Maintainers\n', '\n', '- [Ke Xu](https://github.com/KeXu444)\n', '- [Roman Lutz](https://github.com/romanlutz)\n', '- [Ilya Matiach](https://github.com/imatiach-msft)\n', '- [Gaurav Gupta](https://github.com/gaugup)\n', '- [Vinutha Karanth](https://github.com/vinuthakaranth)\n', '- [Tong Yu](https://github.com/tongyu-microsoft)\n', '- [Ruby Zhu](https://github.com/RubyZ10)\n', '- [Mehrnoosh Sameki](https://github.com/mesameki)\n']"
Responsible AI,alexandrainst/responsible-ai,alexandrainst,https://api.github.com/repos/alexandrainst/responsible-ai,56,13,6,"['https://api.github.com/users/nkasenburg', 'https://api.github.com/users/AmaliePauli', 'https://api.github.com/users/KrydenZ', 'https://api.github.com/users/kasperbaynoer', 'https://api.github.com/users/KatrineHJ', 'https://api.github.com/users/agnethe-gron']",,2023-02-15T19:34:58Z,https://raw.githubusercontent.com/alexandrainst/responsible-ai/main/README.md,"['# Responsible AI Knowledge-base\n', '\n', 'This repository is a knowledge-base of different areas of using and developing AI in a responsible way:heart:. Responsible AI includes both the field of explainable and interpretable machine learning, fairness and bias in machine learning, law regulations as well as the aspect of user experience and human centralized AI.  Hence, it is a cross-disciplinary field which includes both the field of computer science and social science. The aim is to achieve systems that are trustworthy, accountable and fair. Therefore, responsible AI should hopefully both interest researchers and practitioners, which includes both developers, system owners/buyers and users :family:.\n', '\n', 'This repo is a collection of links to **research papers, blog post, tools, tutorials, videos and books**. The references are divide into different areas as listed in the table of contents.\n', '\n', '#### Table of contents :open_file_folder:\n', '\n', '|        | | |\n', '| ------------- |:-------------:| -----:|\n', '| [Explainable AI](#explainable-ai)      | [Fairness](#fairness) | [Guidelines & principles](#guide-princip)\n', '| [People & Tech](#people-tech)  | [Policy & Regulation](#pol-reg)      | [User Experience](#ux) |\n', '\n', '<a name=""explainable-ai""></a>\n', '\n', '  ####  Contributions  :raising_hand:\n', '\n', 'We really welcome and appreciates :pray:contributions to make sure this knowledge-base stays relevant. So if you have a link or reference you think should be included then pleas create a pull request. You can also open an issue if you find it easier.\n', '\n', '\n', '\n', '#### Who is behind :construction_worker:\n', '\n', 'The Responsible AI repository is maintained by the [Alexandra Institute](https://alexandra.dk/uk) which is a Danish non-profit company with a mission to create value, growth and welfare in society. The Alexandra Institute is a member of [GTS](https://gts-net.dk/), a network of independent Danish research and technology organisations.\n', '\n', 'The initial work on this repository is conducted under a performance contract allocated to the Alexandra Insitute by the [Danish Ministry of Higher Education and Science](https://ufm.dk/en?set_language=en&cl=en). The project ran in the two years in 2019 and 2020.``\n', '\n', '\n', '\n', '# Explainable AI (XAI)\n', '## Frameworks and Github repos\n', '1. [InterpretML](https://interpret.ml/) - Open source Python framework that combines local and global explanation methods,\n', 'as well as, transparent models, like decision trees, rule based models, and GAMs (Generalized Additive Models), into\n', 'a common API and dashboard.\n', '2. [AI Explainability 360](http://aix360.mybluemix.net/) - Open source Python XAI framework devloped by IBM researchers\n', 'combining different data, local and global explanation methods. Also see there [github page](https://github.com/Trusted-AI/AIX360).\n', '3. [explainX.ai](https://github.com/explainX/explainx) - Open source Python framework that launches an\n', 'interactive dashboard for a model in a single line of code in which a model can be investigated using\n', 'different XAI methods.\n', '4. [Alibi Explain](https://github.com/SeldonIO/alibi) - Open source Pyton XAI framework combining different methods.\n', 'Main focus on counterfactual explanations and SHAP for classification tasks on tabular data or images.\n', '5. [SHAP](https://github.com/slundberg/shap) - THe open source Python framework for generating SHAP explanations. Focused\n', 'on tree based models, but contains the model agnostic KernelSHAP and an implementation for deep neural networks.\n', '6. [Lucid](https://github.com/tensorflow/lucid) - Open source Python framework to explain deep convolutional\n', 'neural networks used on image data (currently only supports Tensforflow 1). Focuses on understanding the\n', 'representations the network has learned.\n', '7. [DeepLIFT](https://github.com/kundajelab/deeplift) - Open source implementation of the DeepLIFT methods for generating\n', 'local feature attributions for deep neural networks.\n', '8. [iNNvestigate](https://github.com/albermax/innvestigate) - Github repository collecting implementations of different\n', 'feature attribution and gradient based explanation methods for deep neural networks.\n', '9. [Skope-rules](https://github.com/scikit-learn-contrib/skope-rules) - Open source Python framework for building rule\n', 'based models.\n', '10. [Yellowbrick](https://www.scikit-yb.org/en/latest/) - Open source Python framework to create different visualizations\n', 'of data and ML models.\n', '11. [Captum](https://captum.ai/) - Open source framework to explain deep learning models created with PyTorch. Includes\n', 'many known XAI algorithms for deep neural networks.\n', '12. [What-If Tool](https://pair-code.github.io/what-if-tool/) - Open source framework from Google to probe the behaviour\n', 'of a trained model.\n', '13. [AllenNLP Interpret](https://allennlp.org/interpret) - Python framework for explaining deep neural networks\n', 'for language processing developed by the Allen Institute for AI.\n', '14. [Dalex](http://dalex.drwhy.ai/) - Part of the DrWhy.AI universe of packages for interpretable and responsible ML.\n', '15. [RuleFit](https://github.com/christophM/rulefit) - Open source python implementation of an interpretable rule ensemble model.\n', '16. [SkopeRules](https://github.com/scikit-learn-contrib/skope-rules) - Open source python package for fitting a rule based model.\n', '17. [ELI5](https://eli5.readthedocs.io/en/latest/index.html) - Open source python package that implements LIME local explanations\n', '    and permutation explanations.\n', '18. [tf-explain](https://github.com/sicara/tf-explain) - Open source framework that implements interpretability methods as Tensorflow 2.x callbacks. Includes\n', 'several known XAI algorithms for deep neural networks.\n', '19. [PAIR - Saliency methods](https://github.com/PAIR-code/saliency) - Framework that collects different gradient based, saliency methods for deep learning model for Tensorflow created by the Google People+AI Research (PAIR) Initiative.\n', '20. [Quantus](https://github.com/understandable-machine-intelligence-lab/quantus) - Toolkit to evaluate XAI methods for neural networks.\n', '21. [Xplique](https://github.com/deel-ai/xplique) - Python library that gathers state of the art of XAI methods for deep neural networks (currently for Tensorflow).\n', '22. [PiML](https://github.com/SelfExplainML/PiML-Toolbox) - Python toolbox for developing interpretable models through low-code interfaces and high-code APIs.\n', '23. [VL-InterpreT](https://github.com/IntelLabs/VL-InterpreT) - Python toolbox for interactive visualizations of the attentions and hidden representations in vision-language transformers (**Note:** currently only link to the paper and live demo available, but no code)\n', '\n', '## Reading material\n', '1. [Ansvarlig AI](https://medium.com/ansvarlig-ai) - Cross-disciplinary medium blog about XAI,\n', 'fairness and responsible AI (in Danish)\n', '2. [Introducing the Model Card Toolkit](https://ai.googleblog.com/2020/07/introducing-model-card-toolkit-for.html) -\n', 'Google blogpost about the Model Card Toolkit that is a framework for reporting about a ML model.\n', '3. [Interpreting Decision Trees and Random Forests](https://engineering.pivotal.io/post/interpreting-decision-trees-and-random-forests/) -\n', 'Blog post about how to interpret and visualize tree based models.\n', '4. [Introducing PDPbox](https://towardsdatascience.com/introducing-pdpbox-2aa820afd312) - Blog post about a python\n', 'package for generating partial dependence plots.\n', '5. [Use SHAP loss values to debug/monitor your model](https://towardsdatascience.com/use-shap-loss-values-to-debug-monitor-your-model-83f7808af40f) -\n', ' Blog post about how to use SHAP explanations to debug and monitoring.\n', '6. [Be careful what you SHAP for…](https://medium.com/@pauldossantos/be-careful-what-you-shap-for-aeccabf3655c) - Blog\n', ' post about the assumption for how and when to use SHAP explanations.\n', '7. [Awesome Interpretable Machine Learning](https://github.com/lopusz/awesome-interpretable-machine-learning) - Collection\n', ' of resources (articles, conferences, frameworks, software, etc.) about interpretable ML.\n', '8. [http://heatmapping.org/](http://heatmapping.org/) - Homepage of the lab behind the LRP (layerwise propagation relevance)\n', ' method with links to tutorials and research articles.\n', '9. [Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/) - E-book by Christoph Molnar\n', 'describing and explaining different XAI methods and ways to build intepretable models or methods to interpret them, including\n', 'examples on open available datasets.\n', '10. [Can A.I. Be Taught to Explain Itself?](https://www.nytimes.com/2017/11/21/magazine/can-ai-be-taught-to-explain-itself.html) -\n', 'The New York Times Magazine article about the need of explainable models.\n', '11. [Deconstructing BERT, Part 2: Visualizing the Inner Workings of Attention](https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1) -\n', 'Blog post about how to interprete a BERT model.\n', '12. [AI Explanations Whitepaper](https://storage.googleapis.com/cloud-ai-whitepapers/AI%20Explainability%20Whitepaper.pdf) -\n', ""Google's whitepaper about Explainable AI.\n"", '13. [Robust-and-Explainable-machine-learning](https://github.com/dongyp13/Robust-and-Explainable-Machine-Learning) -\n', '    Collection of links and articles with respect to robust and explainable machine learning,\n', '    containing mostly deep learning related resources.\n', '14. [Explaining the decisions of XGBoost models using counterfactual examples](https://towardsdatascience.com/explaining-the-decisions-of-xgboost-models-using-counterfactual-examples-fd9c57c83062) - Blog post describing an algorithm of how to compute counterfactual explanations for decision tree ensemble models.\n', '15. [Interpretable K-Means: Clusters Feature Importances](https://towardsdatascience.com/interpretable-k-means-clusters-feature-importances-7e516eeb8d3c) - Blog post describing methods to compute feature importance for K-means clustering, i.e. which feature mostly contributes for a datapoint belonging to a cluster.\n', '16. [Explainable Graph Neural Networks](https://towardsdatascience.com/explainable-graph-neural-networks-cb009c2bc8ea) - Blog post that provides a brief overview of XAI methods for graph neural networks (GNNs).\n', '\n', '## Videos and presentations\n', '1. [ICML 2019 session - Robust statistics and interpretability](https://slideslive.com/38917641/robust-statistics-and-interpretability)\n', '\n', '## Courses\n', '1. [Kaggle - Machine Learning Explainability](https://www.kaggle.com/learn/machine-learning-explainability) -\n', 'Kaggle course about the basics of XAI with example notebooks and exercises.\n', '\n', '## Research articles\n', 'In this section we list research articles related to interpretable ML and explainable AI.\n', '\n', '### Definitions of interpretability\n', '1. A. Weller, ""Transparency: Motivations and Challenges"", [arXiv:1708.01870](https://arxiv.org/abs/1708.01870)\n', '[cs.CY]\n', '2. J. Chang et al., ""[Reading Tea Leaves: How Humans Interpret Topic Models](http://papers.neurips.cc/paper/3700-reading-tea-leaves-how-humans-interpret-topic-models.pdf)"",\n', 'NIPS 2009\n', '3. Z. C. Lipton, ""The Mythos of Model Interpretability"", [arXiv:1606.03490](https://arxiv.org/abs/1606.03490)\n', '[cs.LG]\n', '4. F. Doshi-Velez and B. Kim, ""Towards A Rigorous Science of Interpretable Machine Learning"",\n', '[arXiv:1702.08608](https://arxiv.org/abs/1702.08608) [stat.ML]\n', '\n', '### Review, survey and overview papers\n', '1. G. Vilone and L. Longo, ""Explainable Artificial Intelligence: a Systematic Review"",\n', '[arXiv:2006.00093](https://arxiv.org/abs/2006.00093) [cs.AI]\n', '2. U. Bhatt et al., ""[Explainable Machine Learning in Deployment](https://dl.acm.org/doi/abs/10.1145/3351095.3375624)"",\n', 'FAT*20 648-657, 2020 - Survey about how XAI is used in practice.  The key results are:\n', '    1. XAI methods are mainly used by ML engineers / designers for debugging.\n', '    2. Limitations of the methods are often unclear to those using it.\n', '    3. The goal og why XAI is used in the first place is often unclear or not well defined, which could potentially lead to using the wrong method.\n', '3. L. H. Gilpin, ""[Explaining Explanations: An Overview of Interpretability of Machine Learning](https://doi.org/10.1109/DSAA.2018.00018)"",\n', 'IEEE 5th DSAA 80-89, 2019\n', '4. S. T. Mueller,\n', '""Explanation in Human-AI Systems: A Literature Meta-Review, Synopsis of Key Ideas and Publications, and Bibliography for Explainable AI"",\n', '[arXiv:1902.01876](https://arxiv.org/abs/1902.01876) [cs.AI]\n', '5. R. Guidotti et al., ""[A Survey of Methods for Explaining Black Box Models](https://dl.acm.org/doi/abs/10.1145/3236009)"",\n', 'ACM Computing Surveys, 2018 - Overview of different interpretability methods grouping them after type of method,\n', 'model they explain and type of explanation.\n', '6. M. Du et al., ""[Techniques for interpretable machine learning](https://dl.acm.org/doi/10.1145/3359786)"",\n', 'Communications of the ACM, 2019\n', '7. I. C. Covert et al., Explaining by Removing:A Unified Framework for Model Explanation,\n', '[arXiv:2011.14878](https://arxiv.org/abs/2011.14878) [cs.LG] -\n', '(Mathematical) framework that summarizes 25 feature influence methods.\n', '8. A. Adadi and M. Berrada, ""[Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)](https://doi.org/10.1109/ACCESS.2018.2870052)"",\n', 'IEEE Access (6) 52138-52160, 2018\n', '9. A. Abdul et al.,\n', '""[Trends and Trajectories for Explainable, Accountable and Intelligible Systems: An HCI Research Agenda](https://dl.acm.org/doi/10.1145/3173574.3174156)"",\n', ""CHI'18 582 1-18, 2018\n"", '10. A. Preece, ""[Asking ‘Why’ in AI: Explainability of intelligent systems – perspectives and challenges](https://onlinelibrary.wiley.com/doi/abs/10.1002/isaf.1422)"",\n', 'Intell Sys Acc Fin Mgmt (25) 63-72, 2018\n', '11. Q. Zhang and S.-C. Zhu,\n', '    ""[Visual Interpretability for Deep Learning: a Survey](https://link.springer.com/article/10.1631/FITEE.1700808)"",\n', '    Technol. Electronic Eng. (19) 27–39, 2018\n', '12. B. Mittelstadt et al.,\n', '    ""[Explaining Explanations in AI](https://dl.acm.org/doi/10.1145/3287560.3287574)"",\n', ""    FAT*'19 279–288, 2019\n"", '13. T. Rojat et al., ""Explainable Artificial Intelligence (XAI) on TimeSeries Data: A Survey"",\n', '    [arXiv:2104.00950](https://arxiv.org/abs/2104.00950) [cs.LG] - Survey paper about XAI methods for models predicting on time series data. \n', '\n', '### Evaluation of XAI\n', 'This section contains articles that describe ways to evaluate explanations and explainable models.\n', '1. S. Mohseni et al., ""A Human-Grounded Evaluation Benchmark for Local Explanations of Machine Learning"",\n', '[arXiv:1801.05075](https://arxiv.org/abs/1801.05075) [cs.HC]\n', '2. J. Huysmans et al.,\n', '""[An empirical evaluation of the comprehensibility of decision table, tree and rule based predictive models](https://www.sciencedirect.com/science/article/abs/pii/S0167923610002368)"",\n', 'Decision Support Systems (51:1) 141-154, 2011\n', '3. F. Poursabzi-Sangdeh et al., ""Manipulating and Measuring Model Interpretability"",\n', '[arXiv:1802.07810](https://arxiv.org/abs/1802.07810) [cs.AI]\n', '4. C. J. Cai et al.,\n', '""[The Effects of Example-Based Explanations in a Machine Learning Interface](https://dl.acm.org/doi/abs/10.1145/3301275.3302289)"",\n', "" IUI'19 258-262, 2019\n"", '5. L. Sixt et al., ""When Explanations Lie: Why Many Modified BP Attributions Fail"",\n', '[arXiv:1912.09818](https://arxiv.org/abs/1912.09818) [cs.LG]\n', '6. Y. Zhang et al.,\n', '""[Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making](https://dl.acm.org/doi/abs/10.1145/3351095.3372852)"",\n', ""FAT*'20 295-305, 2020 - Analyses the effect of LIME explanation and confidence score as explanation on trust and human decision performance.\n"", '7. K. Sokol and P. Flach,\n', '""[Explainability fact sheets: a framework for systematic assessment of explainable approaches](https://dl.acm.org/doi/abs/10.1145/3351095.3372870)"",\n', ""FAT*'20 56-67, 2020 - Framework (essentially a list of questions or checklist) to evaluate and document XAI methods.\n"", 'Also includes question that are relevant to the context in which the XAI methods should be employed, i.e. changing the outcome of the assessment based on the context.\n', '8. E. S. Jo and T. Gebru,\n', '""[Lessons from archives: strategies for collecting sociocultural data in machine learning](https://dl.acm.org/doi/abs/10.1145/3351095.3372829)"",\n', ""FAT*'20 306-316, 2020 - Use archives as inspiration of how to collect, curate and annotate data.\n"", '9. J. Adebayo et al., ""Sanity Checks for Saliency Maps"", [arXiv:1810.03292](https://arxiv.org/abs/1810.03292) [cs.CV] - Comparing different saliency map XAI methods for their sensitivity to the input image and weights of the network.\n', '10. H. Kaur et al.,\n', '""[Interpreting Interpretability: Understanding Data Scientists’ Use of Interpretability Tools for Machine Learning](https://dl.acm.org/doi/fullHtml/10.1145/3313831.3376219)"",\n', ""CHI'20 1-14, 2020\n"", '11. P. Hase and M. Bansal, ""Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?"", [arXiv:2005.01831](https://arxiv.org/abs/2005.01831) [cs.CL]\n', '12. J. V. Jeyakumar et al.,\n', '    ""[How Can I Explain This to You? An Empirical Study of Deep Neural Network Explanation Methods](https://proceedings.neurips.cc/paper/2020/hash/2c29d89cc56cdb191c60db2f0bae796b-Abstract.html)"",\n', '    33rd NeurIPS, 2020 - The authors evaluate different methods for explaining deep neural networks for end-user preference. Code can be found on [github](https://github.com/nesl/Explainability-Study),\n', '    as well as, their [implementation of an example based explainer](https://github.com/nesl/ExMatchina).\n', '13. S. Jesus et al.,\n', '    ""How can I choose an explainer? An Application-grounded Evaluation of Post-hoc Explanations"", [arXiv:2101.08758](https://arxiv.org/abs/2101.08758) [cs.AI] - Evaluating XAI methods based on an application-grounded approach measuring decision time and accuracy of end-users.\n', '14. M. Nauta et al.,\n', '    ""From Anecdotal Evidence to Quantitative Evaluation Methods: A Systematic Review on Evaluating Explainable AI"", [arXiv:2201.08164](https://arxiv.org/abs/2201.08164) [cs.AI] - Lietrature survey of XAI methods and how they where evaluated in the presented paper.\n', '\n', '### Method to explain data\n', 'This section contains articles that explain datasets, for example by finding representative examples.\n', '1. B. Kim et al.,\n', '   ""[Examples are not Enough, Learn to Criticize! Criticism for Interpretability](https://papers.nips.cc/paper/2016/hash/5680522b8e2bb01943234bce7bf84534-Abstract.html)"",\n', '   NIPS, 2016 - Code can we found on [github](https://github.com/BeenKim/MMD-critic).\n', '\n', '### Explainable models\n', 'This section contains articles that describe models that are explainable or transparent by design.\n', '1. X. Zhang et al.,\n', '   ""[Axiomatic Interpretability for Multiclass Additive Models](https://dl.acm.org/doi/abs/10.1145/3292500.3330898)"",\n', ""   KDD'19 226–234, 2019\n"", '2. T. Kulesza et al.,\n', '   ""[Principles of Explanatory Debugging to Personalize Interactive Machine Learning](https://dl.acm.org/doi/10.1145/2678025.2701399)"",\n', ""   IUI'15 126–137, 2015 - Framework showing how a Naive Bayes method can be trained with user interaction and\n"", '   how to generate explanations for these kinds of models.\n', '3. M. Hind et al.,\n', '   ""[TED: Teaching AI to Explain its Decisions](https://dl.acm.org/doi/abs/10.1145/3306618.3314273)"",\n', ""   AIES'19 123–129, 2019\n"", '4. Y. Lou et al.,\n', '   ""[Accurate Intelligible Models with Pairwise Interactions](https://dl.acm.org/doi/10.1145/2487575.2487579)"",\n', ""   KDD'13 623–631, 2013\n"", '5. C. Chen et al., ""An Interpretable Model with Globally Consistent Explanations for Credit Risk"",\n', '   [arXiv:1811.12615](https://arxiv.org/abs/1811.12615) [cs.LG]\n', '6. C. Chen and C. Rudin,\n', '   ""[An Optimization Approach to Learning Falling Rule Lists](http://proceedings.mlr.press/v84/chen18a.html)"",\n', '   PMLR (84) 604-612, 2018\n', '7. F. Wang and C. Rudin,  ""Falling Rule Lists"",\n', '   [arXiv:1411.5899](https://arxiv.org/abs/1411.5899) [cs.AI]\n', '8. B. Ustun and C. Rudin, ""Supersparse Linear Integer Models for Optimized Medical Scoring Systems"",\n', '   [arXiv:1502.04269](https://arxiv.org/abs/1502.04269) [stat.ML]\n', '8. E. Angelino et al.,\n', '   ""[Learning Certifiably Optimal Rule Lists for Categorical Data](https://dl.acm.org/doi/abs/10.5555/3122009.3290419)"",\n', '   JMLR (18:234) 1-78, 2018\n', '9. H. Lakkaraju et al.,\n', '   ""[Interpretable Decision Sets: A Joint Framework for Description and Prediction](https://dl.acm.org/doi/10.1145/2939672.2939874)"",\n', ""   KDD'16 1675–1684, 2016\n"", '10. K. Shu et al.,\n', '      ""[dEFEND: Explainable Fake News Detection](https://dl.acm.org/doi/10.1145/3292500.3330935)"",\n', ""      KDD'19 395–405, 2019\n"", '11. J. Jung et al., ""Simple Rules for Complex Decisions"",\n', '    [arXiv:1702.04690](https://arxiv.org/abs/1702.04690) [stat.AP]\n', '\n', '### XAI methods to visualize / explain a model\n', 'This section contains articles that are describing methods to globally explain a model.\n', 'Typically, this is done by generating visualizations in one form or the other.\n', '1. B. Ustun et al.,\n', '   ""[Actionable Recourse in Linear Classification](https://dl.acm.org/doi/10.1145/3287560.3287566)"",\n', ""   FAT*'19 Pages 10–19, 2019 - Article describing a method to evaluate actionable variables,\n"", '   i.e. variables a person can impact to change the outcome af a model, of a linear\n', '   classification model.\n', '2. A Datta et al.,\n', '   ""[Algorithmic Transparency via Quantitative Input Influence: Theory and Experiments with Learning Systems](https://ieeexplore.ieee.org/document/7546525)"",\n', '    IEEE SP 598-617, 2016\n', '3. P.Adler et al.,\n', '   ""[Auditing black-box models for indirect influence](https://link.springer.com/article/10.1007/s10115-017-1116-3)"",\n', '   Knowl. Inf. Syst. (54) 95–122, 2018\n', '4. A. Lucic et al.,\n', '   ""[Why Does My Model Fail? Contrastive Local Explanations for Retail Forecasting](https://dl.acm.org/doi/abs/10.1145/3351095.3372824)"",\n', ""   FAT*'20 90–98, 2020 - Presents an explanation to explain failure cases of an ML/AI model.\n"", '   The explanation is presented in form of a feasible range of feature values in which the model works and a trend\n', '   for each feature. Code for the method is available on [github](https://github.com/a-lucic/mc-brp).\n', '5. J. Krause et al.,\n', '   ""[Interacting with Predictions: Visual Inspection of Black-box Machine Learning Models](https://dl.acm.org/doi/10.1145/2858036.2858529)"",\n', ""   CHI'16 5686–5697, 2016\n"", '6. B. Kim et al.,\n', '   ""[Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)](http://proceedings.mlr.press/v80/kim18d.html)"",\n', '   ICML, PMLR (80) 2668-2677, 2018 - Code for the method can be found on [github](https://github.com/tensorflow/tcav).\n', '7. A. Goldstein et al.,\n', '   ""[Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation](https://doi.org/10.1080/10618600.2014.907095)"",\n', '   Journal of Computational and Graphical Statistics (24:1) 44-65, 2015\n', '8. J. Wang et al., ""Shapley Flow: A Graph-based Approach to Interpreting Model Predictions"",\n', '   [arXiv:2010.14592](https://arxiv.org/abs/2010.14592) [cs.LG]\n', '\n', '### XAI methods that explain a model through construction of mimicking models\n', 'This section contains articles that are describing methods to explain a model by constructing an inherent\n', 'transparent model that mimics the behaviour of the black-box model.\n', '1. S. Tan et al.,  \n', '   ""[Distill-and-Compare: Auditing Black-Box Models Using Transparent Model Distillation](https://dl.acm.org/doi/abs/10.1145/3278721.3278725)"",\n', ""   AIES'18 303–310, 2018\n"", '2. L. Chu et al., ""Exact and Consistent Interpretation for Piecewise Linear Neural Networks: A Closed Form Solution"",\n', '   [arXiv:1802.06259](https://arxiv.org/abs/1802.06259) [cs.CV]\n', '3. C. Yang et al., ""Global Model Interpretation via Recursive Partitioning"",\n', '   [arXiv:1802.04253](https://arxiv.org/abs/1802.04253) [cs.LG]\n', '4. H. Lakkaraju et al., ""Interpretable & Explorable Approximations of Black Box Models"",\n', '   [arXiv:1707.01154](https://arxiv.org/abs/1707.01154) [cs.AI]\n', '5. Y. Hayashi,\n', '   ""[Synergy effects between grafting and subdivision in Re-RX with J48graft for the diagnosis of thyroid disease](https://www.sciencedirect.com/science/article/abs/pii/S095070511730285X)"",\n', '   Knowledge-Based Systems (131) 170-182, 2017\n', '6. H. F. Tan et al., ""Tree Space Prototypes: Another Look at Making Tree Ensembles Interpretable"",\n', '   [arXiv:1611.07115](https://arxiv.org/abs/1611.07115) [stat.ML]\n', '7. O. Sagi and L. Rokach, \n', '   ""[Approximating XGBoost with an interpretable decision tree](https://www.sciencedirect.com/science/article/abs/pii/S0020025521005272)"", Information Sciences (572) 522-542, 2021 \n', '\n', '### Local XAI methods\n', 'This section contains articles that describe local explanation methods, i.e. methods that generate an explanation\n', 'for a specific outcome of a model.\n', '1. M. T. Ribeiro et al.,\n', '   ""[Anchors: High-Precision Model-Agnostic Explanations](https://homes.cs.washington.edu/~marcotcr/aaai18.pdf)"",\n', '   AAAI Conference on Artificial Intelligence, 2018 -\n', '   The implementation of the method can be found on [github](https://github.com/marcotcr/anchor).\n', '2. A. Shrikumar et al.,\n', '   ""[Learning Important Features Through Propagating Activation Differences](https://dl.acm.org/doi/10.5555/3305890.3306006)"",\n', ""   ICML'17 3145–3153, 2017 - DeepLIFT method for local explanations of deep neural networks.\n"", '3. S. M. Lundberg et al., ""Explainable AI for Trees: From Local Explanations to Global Understanding"",\n', '   [arXiv:1905.04610](https://arxiv.org/abs/1905.04610) [stat.ML]\n', '4. S. M. Lundberg et al.,\n', '   ""[From local explanations to global understanding with explainable AI for trees](https://www.nature.com/articles/s42256-019-0138-9)"",\n', '   Nat. Mach. Intell. (2) 56–67, 2020\n', '5. M. T. Ribeiro et al.,\n', '   [“Why Should I Trust You?” Explaining the Predictions of Any Classifier](https://dl.acm.org/doi/10.1145/2939672.2939778),\n', ""   KDD'16 1135–1144, 2016\n"", '6. D. Slack et al., ""How Much Should I Trust You? Modeling Uncertainty of Black Box Explanations"",\n', '   [arXiv:2008.05030](https://arxiv.org/abs/2008.05030) [cs.LG]\n', '7. S. M. Lundberg and S.-I. Lee,\n', '   ""[A Unified Approach to Interpreting Model Predictions](https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html)"",\n', '   NIPS, 2017\n', '8. M. Sundararajan and A. Najmi,\n', '   ""[The Many Shapley Values for Model Explanation](http://proceedings.mlr.press/v119/sundararajan20b.html)"",\n', '   ICML (119) 9269-9278, 2020\n', '9. I. E. Kumar et al., ""Problems with Shapley-value-based explanations as feature importance measures"",\n', '   [arXiv:2002.11097](https://arxiv.org/abs/2002.11097) [cs.AI]\n', '10. P. W. Koh and P. Liang, ""Understanding Black-box Predictions via Influence Functions"",\n', '      [arXiv:1703.04730](https://arxiv.org/abs/1703.04730) [stat.ML]\n', '\n', '### Counterfactual explanations\n', 'This section contains articles that describe methods for counterfactual explanations.\n', '1. S. Sharma et al.,\n', '   ""[CERTIFAI: A Common Framework to Provide Explanations and Analyse the Fairness and Robustness of Black-box Models](https://dl.acm.org/doi/10.1145/3375627.3375812)"",\n', ""   AIES'20 166–172, 2020\n"", '2. C. Russell, ""[Efficient Search for Diverse Coherent Explanations](https://dl.acm.org/doi/10.1145/3287560.3287569)"",\n', ""   FAT*'19  20–28, 2019\n"", '3. R. K. Mothilal et al.,\n', '   ""[Explaining Machine Learning Classifiers through Diverse Counterfactual Explanations](https://dl.acm.org/doi/abs/10.1145/3351095.3372850)"",\n', ""   FAT*'20 607–617, 2020 - Code for the method is available on [github](https://github.com/interpretml/DiCE).\n"", '4. S. Barocas et al.,\n', '   ""[The Hidden Assumptions Behind Counterfactual Explanations and Principal Reasons](https://dl.acm.org/doi/abs/10.1145/3351095.3372830)"",\n', ""   FAT*'20  80–89, 2020 - Raises some questions with respect to the use of counterfactual examples as a form of explanation:\n"", '   * Are the changes proposed by the counterfactual example feasible (actionable) for a person to change their outcome?\n', '   * If the changes are performed, what do they affect otherwise, i.e. they might not be favorable in other contexts?\n', '   * Changing one factor might inherently change another factor that actually negatively affects the outcome\n', '     (counterfactual examples can not describe complex relationships between variables)?\n', '\n', '### XAI and user interaction\n', 'This section contains research articles that are looking at the interaction of users with explanations or\n', 'interpretable models.\n', '1. B. Y. Lim and A. K. Dey,\n', '   ""[Assessing Demand for Intelligibility in Context-Aware Applications](https://dl.acm.org/doi/10.1145/1620545.1620576)"",\n', ""   UbiComp'09 195–204, 2009\n"", '2. D. Wang et al.,\n', '   ""[Designing Theory-Driven User-Centric Explainable AI](https://dl.acm.org/doi/10.1145/3290605.3300831)"",\n', ""   CHI'19 (601)  1–15, 2019\n"", '3. M. Narayanan et al.,\n', '   ""How do Humans Understand Explanations from Machine Learning Systems? An Evaluation of the Human-Interpretability of Explanation"",\n', '   [arXiv:1802.00682](https://arxiv.org/abs/1802.00682) [cs.AI]\n', '4. U. Bhatt et al., ""Machine Learning Explainability for External Stakeholders"",\n', '   [arXiv:2007.05408](https://arxiv.org/abs/2007.05408) [cs.CY]\n', '5. V. Lai and C. Tan,\n', '   ""[On Human Predictions with Explanations and Predictions of Machine Learning Models: A Case Study on Deception Detection](https://dl.acm.org/doi/abs/10.1145/3287560.3287590)"",\n', ""   FAT*'19 29–38, 2019\n"", '6. C. Molnar et al., ""Pitfalls to Avoid when Interpreting Machine Learning Models"",\n', '   [arXiv:2007.04131](https://arxiv.org/abs/2007.04131) [stat.ML]\n', '7. A. Preece et al., ""Stakeholders in Explainable AI"",\n', '   [arXiv:1810.00184](https://arxiv.org/abs/1810.00184) [cs.AI]\n', '8. M. Katell et al.,\n', '   ""[Toward Situated Interventions for Algorithmic Equity: Lessons from the Field](https://dl.acm.org/doi/abs/10.1145/3351095.3372874)"",\n', ""   FAT*'20 45–55, 2020 - Presenting a framework for designing ML/AI solutions based on participatory design and co-design methods,\n"", '   which especially focuses on solutions that effect communities, i.e. models employed by municipalities. The framework is applied\n', '   to an example case in which a surveillance tool with an automatic decision system is designed.\n', '9. M. Eiband et al.,\n', '   ""[Bringing Transparency Design into Practice](https://dl.acm.org/doi/10.1145/3172944.3172961)"",\n', ""   IUI'18 211–223, 2018\n"", '\n', '### XAI used in practice\n', 'This section contains research articles where XAI was used as part of an application or used for validation on a system\n', 'deployed in practice.\n', '1. S. Coppers et al.,\n', '   ""[Intellingo: An Intelligible Translation Environment](https://dl.acm.org/doi/10.1145/3173574.3174098)"",\n', ""   CHI'18 (524) 1–13, 2018\n"", '2. H. Tang and P. Eratuuli,\n', '   ""[Package and Classify Wireless Product Features to Their Sales Items and Categories Automatically](https://link.springer.com/chapter/10.1007/978-3-030-29726-8_20)"",\n', '   Machine Learning and Knowledge Extraction. CD-MAKE 2019. LNCS (11713), 2019\n', '\n', '### XAI for deep neural networks\n', 'This section focuses on explainability with respect to deep neural networks (DNNs). This can be methods to explain\n', 'DNNs or methods to build DNNs that can explain themselves.\n', '1. Y. Goyal et al.,\n', '   ""[Counterfactual Visual Explanations](http://pr"
Responsible AI,romanlutz/ResponsibleAI,romanlutz,https://api.github.com/repos/romanlutz/ResponsibleAI,49,9,3,"['https://api.github.com/users/romanlutz', 'https://api.github.com/users/riedgar-ms', 'https://api.github.com/users/benbyford']",,2022-12-09T12:53:53Z,https://raw.githubusercontent.com/romanlutz/ResponsibleAI/main/README.md,"['The following collection is meant to serve as a reference for engineers, data scientists, and others making decisions about building technological solutions for real-world problems. Hopefully, this will help us avoid repeating mistakes of the past by informing the design of new systems or the decision not to build a technological solution at all.\n', '\n', 'This is a living document, so please send suggestions for additions through ""Issues"" or feel free to send pull requests. If you find any other problems with links or the articles themselves, please also open an ""Issue"".\n', '\n', '# Fairness\n', '\n', '## Lending & Credit approval\n', '\n', '- [Gender Bias Complaints against Apple Card Signal a Dark Side to Fintech](https://hbswk.hbs.edu/item/gender-bias-complaints-against-apple-card-signal-a-dark-side-to-fintech)\n', '- [Exploring Racial Discrimination in Mortgage Lending: A Call for Greater Transparency](https://listwithclever.com/real-estate-blog/racial-discrimination-in-mortgage-lending/)\n', '- [DFS Issues Guidance to Life Insurers on Use of “External Data” in Underwriting Decisions](https://www.jdsupra.com/legalnews/dfs-issues-guidance-to-life-insurers-on-45997/)\n', '\n', '## Hiring\n', '\n', '- [Amazon scraps secret AI recruiting tool that showed bias against women](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G)\n', '- [Automated Employment Discrimination](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3437631)\n', '- [Help wanted: an examination of hiring algorithms, equity, and bias](https://apo.org.au/node/210071)\n', '- [All the Ways Hiring Algorithms Can Introduce Bias](https://hbr.org/2019/05/all-the-ways-hiring-algorithms-can-introduce-bias)\n', '- [Mitigating Bias in Algorithmic Hiring: Evaluating Claims and Practices](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3408010)\n', '- [Help Wanted - An Examination of Hiring Algorithms, Equity, and Bias](https://www.upturn.org/static/reports/2018/hiring-algorithms/files/Upturn%20--%20Help%20Wanted%20-%20An%20Exploration%20of%20Hiring%20Algorithms,%20Equity%20and%20Bias.pdf)\n', '- [Wanted: The ‘perfect babysitter.’ Must pass AI scan for respect and attitude.](https://www.washingtonpost.com/technology/2018/11/16/wanted-perfect-babysitter-must-pass-ai-scan-respect-attitude/)\n', '- [Job Screening Service Halts Facial Analysis of Applicants](https://www.wired.com/story/job-screening-service-halts-facial-analysis-applicants/)\n', '\n', '## Employee evaluation\n', '\n', '- [Houston Schools Must Face Teacher Evaluation Lawsuit](https://www.courthousenews.com/houston-schools-must-face-teacher-evaluation-lawsuit/)\n', '- [How Amazon automatically tracks and fires warehouse workers for ‘productivity’](https://www.theverge.com/2019/4/25/18516004/amazon-warehouse-fulfillment-centers-productivity-firing-terminations)\n', ""- [Court Rules Deliveroo Used 'Discriminatory' Algorithm](https://www.vice.com/en/article/7k9e4e/court-rules-deliveroo-used-discriminatory-algorithm)\n"", '\n', '## Pre-trial risk assessment and criminal sentencing\n', '\n', '- [Machine Bias](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)\n', '- [How We Analyzed the COMPAS Recidivism Algorithm](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm)\n', '- [GitHub repository for COMPAS analysis](https://github.com/propublica/compas-analysis)\n', '- [Can you make AI fairer than a judge? Play our courtroom algorithm game](https://www.technologyreview.com/s/613508/ai-fairer-than-judge-criminal-risk-assessment-algorithm/)\n', '    \n', '## Predictive Policing & Other Law Enforcement Use Cases\n', '\n', '- [Dirty Data, Bad Predictions: How Civil Rights Violations Impact Police Data, Predictive Policing Systems, and Justice](https://www.nyulawreview.org/online-features/dirty-data-bad-predictions-how-civil-rights-violations-impact-police-data-predictive-policing-systems-and-justice/)\n', '- [Amazon’s Face Recognition Falsely Matched 28 Members of Congress With Mugshots](https://www.aclu.org/blog/privacy-technology/surveillance-technologies/amazons-face-recognition-falsely-matched-28)\n', '- [The Perpetual Line-Up - Unregulated police face recognition in America](https://www.perpetuallineup.org/)\n', '- [Stuck in a Pattern: Early evidence on ""predictive policing"" and civil rights](https://www.upturn.org/reports/2016/stuck-in-a-pattern/)\n', '- [Crime-prediction tool PredPol amplifies racially biased policing, study shows](https://www.mic.com/articles/156286/crime-prediction-tool-pred-pol-only-amplifies-racially-biased-policing-study-shows#.DZeqQ4LYs)\n', '- [Criminal machine learning](https://callingbullshit.org/case_studies/case_study_criminal_machine_learning.html)\n', '- [The Liar’s Walk - Detecting Deception with Gait and Gesture](http://gamma.cs.unc.edu/GAIT/files/Deception_LSTM.pdf)\n', '- [Federal study confirms racial bias of many facial-recognition systems, casts doubt on their expanding use](https://www.washingtonpost.com/technology/2019/12/19/federal-study-confirms-racial-bias-many-facial-recognition-systems-casts-doubt-their-expanding-use/)\n', '- [Return of physiognomy? Facial recognition study says it can identify criminals from looks alone](https://www.rt.com/news/368307-facial-recognition-criminal-china/)\n', '- [Live facial recognition is tracking kids suspected of being criminals](https://www.technologyreview.com/2020/10/09/1009992/live-facial-recognition-is-tracking-kids-suspected-of-crime/)\n', '\n', '## Admissions\n', '\n', '- [British Medical Journal: A blot on the profession](https://www.bmj.com/content/296/6623/657)\n', '\n', '## School Choice\n', '\n', '- [Custom Software Helps Cities Manage School Choice](https://www.edweek.org/ew/articles/2013/12/04/13algorithm_ep.h33.html)\n', '\n', '## Speech Detection\n', '\n', '- [Oh dear... AI models used to flag hate speech online are, er, racist against black people](https://www.theregister.co.uk/2019/10/11/ai_black_people/)\n', '- [The Risk of Racial Bias in Hate Speech Detection](https://homes.cs.washington.edu/~msap/pdfs/sap2019risk.pdf)\n', '- [Toxicity and Tone Are Not The Same Thing: analyzing the new Google API on toxicity, PerspectiveAPI.](https://medium.com/@carolinesinders/toxicity-and-tone-are-not-the-same-thing-analyzing-the-new-google-api-on-toxicity-perspectiveapi-14abe4e728b3)\n', '- [Voice Is the Next Big Platform, Unless You Have an Accent](https://www.wired.com/2017/03/voice-is-the-next-big-platform-unless-you-have-an-accent/)\n', '- [Google’s speech recognition has a gender bias](https://makingnoiseandhearingthings.com/2016/07/12/googles-speech-recognition-has-a-gender-bias/)\n', '- [Fair Speech report by Stanford Computational Policy Lab](https://fairspeech.stanford.edu/), also covered in [Speech recognition algorithms may also have racial bias](https://arstechnica.com/science/2020/03/speech-recognition-algorithms-may-also-have-racial-bias/)\n', '- [Automated moderation tool from Google rates People of Color and gays as “toxic”](https://algorithmwatch.org/en/story/automated-moderation-perspective-bias/)\n', '- [Someone made an AI that predicted gender from email addresses, usernames. It went about as well as expected](https://www.theregister.com/2020/07/30/genderify_shuts_down/)\n', '\n', '## Image Labelling & Face Recognition\n', '\n', ""- [Google Photos identified two black people as 'gorillas'](https://mashable.com/2015/07/01/google-photos-black-people-gorillas/)\n"", '- [When It Comes to Gorillas, Google Photos Remains Blind](https://www.wired.com/story/when-it-comes-to-gorillas-google-photos-remains-blind/)\n', '- [The viral selfie app ImageNet Roulette seemed fun – until it called me a racist slur](https://www.theguardian.com/technology/2019/sep/17/imagenet-roulette-asian-racist-slur-selfie)\n', ""- [Google Is Investigating Why it Trained Facial Recognition on 'Dark Skinned' Homeless People](https://www.vice.com/en_uk/article/43k7yd/google-is-investigating-why-it-trained-facial-recognition-on-dark-skinned-homeless-people)\n"", '- [Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification](http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf)\n', '- [Machines Taught by Photos Learn a Sexist View of Women](https://www.wired.com/story/machines-taught-by-photos-learn-a-sexist-view-of-women/)\n', '- [Tenants sounded the alarm on facial recognition in their buildings. Lawmakers are listening.](https://www.msn.com/en-us/news/politics/tenants-sounded-the-alarm-on-facial-recognition-in-their-buildings-lawmakers-are-listening/ar-BBYnaqB)\n', '- [Google apologizes after its Vision AI produced racist results](https://algorithmwatch.org/en/story/google-vision-racism/)\n', ""- [When AI Sees a Man, It Thinks 'Official.' A Woman? 'Smile'](https://www.wired.com/story/ai-sees-man-thinks-official-woman-smile/)\n"", '\n', '## Public Benefits & Health\n', '\n', '- [A health care algorithm affecting millions is biased against black patients](https://www.theverge.com/2019/10/24/20929337/care-algorithm-study-race-bias-health)\n', '- [What happens when an algorithm cuts your health care](https://www.theverge.com/2018/3/21/17144260/healthcare-medicaid-algorithm-arkansas-cerebral-palsy)\n', '- [China Knows How to Take Away Your Health Insurance](https://www.bloomberg.com/opinion/articles/2019-06-14/china-knows-how-to-take-away-your-health-insurance)\n', '- [Foretelling the Future: A Critical Perspective on the Use of Predictive Analytics in Child Welfare](http://kirwaninstitute.osu.edu/wp-content/uploads/2017/05/ki-predictive-analytics.pdf)\n', '- [There’s no quick fix to find racial bias in health care algorithms](https://www.theverge.com/2019/12/4/20995178/racial-bias-health-care-algorithms-cory-booker-senator-wyden)\n', '- [Health algorithms discriminate against Black patients, also in Switzerland](https://algorithmwatch.ch/en/racial-health-bias/)\n', '\n', '## Ads\n', '\n', '- [Discrimination in Online Ad Delivery](https://arxiv.org/abs/1301.6822)\n', '- [Probing the Dark Side of Google’s Ad-Targeting System](https://www.technologyreview.com/s/539021/probing-the-dark-side-of-googles-ad-targeting-system/)\n', '- [Facebook Engages in Housing Discrimination With Its Ad Practices, U.S. Says](https://www.nytimes.com/2019/03/28/us/politics/facebook-housing-discrimination.html)\n', '- [Facebook Job Ads Raise Concerns About Age Discrimination](https://www.outtengolden.com/facebook-job-ads-raise-concerns-about-age-discrimination-nyt)\n', '- [Facebook Ads Can Still Discriminate Against Women and Older Workers, Despite a Civil Rights Settlement](https://www.propublica.org/article/facebook-ads-can-still-discriminate-against-women-and-older-workers-despite-a-civil-rights-settlement)\n', '- [Women less likely to be shown ads for high-paid jobs on Google, study shows](https://www.theguardian.com/technology/2015/jul/08/women-less-likely-ads-high-paid-jobs-google-study)\n', '- [Algorithms That “Don’t See Color”: Comparing Biases in Lookalike and Special Ad Audiences](https://sapiezynski.com/papers/sapiezynski2019algorithms.pdf)\n', '- [Facebook is letting job advertisers target only men](https://www.propublica.org/article/facebook-is-letting-job-advertisers-target-only-men)\n', '- [Facebook (Still) Letting Housing Advertisers Exclude Users by Race](https://www.propublica.org/article/facebook-advertising-discrimination-housing-race-sex-national-origin)\n', '\n', '## Search\n', '\n', '- [Algorithms of Oppression: How Search Engines reinforce racism](http://algorithmsofoppression.com/)\n', '- [Bias already exists in search engine results, and it’s only going to get worse](https://www.technologyreview.com/s/610275/meet-the-woman-who-searches-out-search-engines-bias-against-women-and-minorities/)\n', '- [Truth in pictures: What Google image searches tell us about inequality at work](https://www.diversityemployers.com/blog/2017/05/truth-in-pictures-what-google-image-searches-tell-us-about-inequality-at-work/)\n', '\n', '## Translations\n', '\n', '- [Google Translate might have a gender problem](https://mashable.com/2017/11/30/google-translate-sexism/)\n', '\n', '## Jury Selection\n', '\n', '- [The Big Data Jury](https://scholarship.law.nd.edu/ndlr/vol91/iss3/2/)\n', '\n', '## Dating\n', '\n', '- [Coffee Meets Bagel: The Online Dating Site That Helps You Weed Out The Creeps](https://www.laweekly.com/coffee-meets-bagel-the-online-dating-site-that-helps-you-weed-out-the-creeps/)\n', '- [The Biases we feed to Tinder algorithms](https://www.diggitmagazine.com/articles/biases-we-feed-tinder-algorithms)\n', '- [Redesign dating apps to lessen racial bias, study recommends](http://news.cornell.edu/stories/2018/09/redesign-dating-apps-lessen-racial-bias-study-recommends)\n', '\n', '## Word Embeddings\n', '\n', 'Word Embeddings may affect many of the categories above through applications that use them.\n', '- [Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings](https://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf)\n', '\n', '## Gerrymandering\n', '\n', '- [When drawing a line is hard](https://medium.com/equal-future/when-drawing-a-line-is-hard-8d92d30c9044)\n', '\n', '## Recommender systems\n', '\n', '- [Why is TikTok creating filter bubbles based on your race?](https://www.wired.co.uk/article/tiktok-filter-bubbles)\n', '\n', '## Picking areas for improved service\n', '\n', '- [Amazon Doesn’t Consider the Race of Its Customers. Should It?](https://www.bloomberg.com/graphics/2016-amazon-same-day/)\n', '\n', '# Safety\n', '\n', '## Self-driving cars\n', '\n', '- [Remember the Uber self-driving car that killed a woman crossing the street? The AI had no clue about jaywalkers](https://www.theregister.co.uk/2019/11/06/uber_self_driving_car_death/)\n', '- [Franken-algorithms: The Deadly Consequences of Unpredictable Code](https://getpocket.com/explore/item/franken-algorithms-the-deadly-consequences-of-unpredictable-code)\n', '\n', '## Weaponized AI\n', '\n', '- [Google employee protest: Now Google backs off Pentagon drone AI project](https://www.zdnet.com/article/google-employee-protests-now-google-backs-off-pentagon-drone-ai-project/)\n', '- [Google Wants to Do Business With the Military—Many of Its Employees Don’t](https://www.bloomberg.com/features/2019-google-military-contract-dilemma/)\n', '\n', '## Health\n', '\n', '- Model interpretability in Medicine\n', '  - [Intelligible Models for HealthCare: Predicting Pneumonia Risk and Hospital 30-day Readmission](http://people.dbmi.columbia.edu/noemie/papers/15kdd.pdf) shows importance of model interpretability for such critical decisions.\n', ""  - [Rich Caruana--Friends Don't Let Friends Release Black Box Models in Medicine](https://www.youtube.com/watch?v=iyGh46NA8tk)\n"", '- [IBM pitched its Watson supercomputer as a revolution in cancer care. It’s nowhere close](https://www.statnews.com/2017/09/05/watson-ibm-cancer/)\n', '- [International evaluation of an AI system for breast cancer screening](https://deepmind.com/research/publications/International-evaluation-of-an-artificial-intelligence-system-to-identify-breast-cancer-in-screening-mammography) - [This thread examines the issues with the problem setting.](https://twitter.com/VPrasadMDMPH/status/1212840987363442689?s=20)\n', '\n', '# Privacy\n', '\n', '## Machine Learning-based privacy attacks\n', '\n', '- [Privacy Attacks on Machine Learning Models](https://www.infoq.com/articles/privacy-attacks-machine-learning-models/)\n', '\n', '## Lending\n', '\n', '- [The new lending game, post-demonetisation](https://tech.economictimes.indiatimes.com/news/technology/the-new-lending-game-post-demonetisation/56367457)\n', '- [Perpetual Debt in the Silicon Savannah](http://bostonreview.net/class-inequality-global-justice/kevin-p-donovan-emma-park-perpetual-debt-silicon-savannah)\n', '\n', '## Work\n', '\n', '- [Woman fired after disabling work app that tracked her movements 24/7](https://www.theverge.com/2015/5/13/8597081/worker-gps-fired-myrna-arias-xora)\n', '- [Limitless Worker Surveillance](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2746211)\n', '\n', '## Prison tech\n', '\n', '- [Prison tech company is questioned for retaining ‘voice prints’ of people presumed innocent](https://theappeal.org/jails-across-the-u-s-are-extracting-the-voice-prints-of-people-presumed-innocent/)\n', '\n', '## Location data\n', '\n', '- [Twelve Million Phones, One Dataset, Zero Privacy](https://www.nytimes.com/interactive/2019/12/19/opinion/location-tracking-cell-phone.html) shines a light on data privacy (or the lack thereof). That same data may be used to for ML as well.\n', '- [Tenants sounded the alarm on facial recognition in their buildings. Lawmakers are listening.](https://www.msn.com/en-us/news/politics/tenants-sounded-the-alarm-on-facial-recognition-in-their-buildings-lawmakers-are-listening/ar-BBYnaqB)\n', '- [Face for sale: Leaks and lawsuits blight Russia facial recognition](https://www.reuters.com/article/us-russia-privacy-lawsuit-feature-trfn/face-for-sale-leaks-and-lawsuits-blight-russia-facial-recognition-idUSKBN27P10U)\n', '\n', '## Social media & dating\n', '\n', '- [OkCupid Study Reveals the Perils of Big-Data Science](https://www.wired.com/2016/05/okcupid-study-reveals-perils-big-data-science/)\n', '- [“We Are the Product”: Public Reactions to Online Data Sharing and Privacy Controversies in the Media](https://cmci.colorado.edu/~cafi5706/CHI2018_FieslerHallinan.pdf)\n', '\n', '## Basic anonymization as an insufficient measure\n', '\n', '- [“Anonymized” data really isn’t—and here’s why not](https://arstechnica.com/tech-policy/2009/09/your-secrets-live-online-in-databases-of-ruin/)\n', '\n', '## Health\n', '\n', '- [Health Insurers Are Vacuuming Up Details About You — And It Could Raise Your Rates](https://www.npr.org/sections/health-shots/2018/07/17/629441555/health-insurers-are-vacuuming-up-details-about-you-and-it-could-raise-your-rates)\n', '- [How Your Medical Data Fuels a Hidden Multi-Billion Dollar Industry](https://time.com/4588104/medical-data-industry/)\n', ""- [23andMe's Pharma Deals Have Been the Plan All Along](https://www.wired.com/story/23andme-glaxosmithkline-pharma-deal/)\n"", '- [If You Want Life Insurance, Think Twice Before Getting A Genetic Test](https://www.fastcompany.com/3055710/if-you-want-life-insurance-think-twice-before-getting-genetic-testing)\n', '- [Medical Start-up Invited Millions Of Patients To Write Reviews They May Not Realize Are Public. Some Are Explicit.](https://www.forbes.com/sites/kashmirhill/2013/10/21/practice-fusion-patient-privacy-explicit-reviews/#2918de354ae3)\n', '- [Help Desk: Can your medical records become marketing? We investigate a reader’s suspicious ‘patient portal.’](https://www.washingtonpost.com/technology/2019/10/22/help-desk-can-your-medical-records-become-marketing-we-investigate-readers-suspicious-patient-portal/)\n', '- [Is your pregnancy app sharing your intimate data with your boss?](https://www.washingtonpost.com/technology/2019/04/10/tracking-your-pregnancy-an-app-may-be-more-public-than-you-think/)\n', '- [Data Crisis: Who Owns Your Medical Records?](https://www.sandiegomagazine.com/San-Diego-Magazine/October-2016/Top-Doctors-2016-Innovation-in-Health-and-Medicine/Data-Crisis-Who-Owns-Your-Medical-Records/)\n', ""- [This Bluetooth Tampon Is the Smartest Thing You Can Put In Your Vagina](https://gizmodo.com/this-bluetooth-tampon-is-the-smartest-thing-you-can-put-1777044090) didn't mention the privacy concerns of such a device. [This Twitter comment adds the necessary comment.](https://twitter.com/DrRanjanaDas/status/1213940445245509671?s=20) \n"", '\n', '## Face Recognition\n', '\n', '- [Clearview AI: We Are ‘Working to Acquire All U.S. Mugshots’ From Past 15 Years](https://onezero.medium.com/clearview-ai-we-are-working-to-acquire-all-u-s-mugshots-from-past-15-years-645d92319f33)\n', '- [Face for sale: Leaks and lawsuits blight Russia facial recognition](https://www.reuters.com/article/us-russia-privacy-lawsuit-feature-trfn/face-for-sale-leaks-and-lawsuits-blight-russia-facial-recognition-idUSKBN27P10U)\n', '\n', '## Supply of goods\n', '\n', '- [Grocers Stopped Stockpiling Food. Then Came Coronavirus.](https://www.wsj.com/articles/grocers-stopped-stockpiling-food-then-came-coronavirus-11584982605)\n', '\n', '\n', '# Anti-Money Laundering\n', '\n', '- [Trusting Machine Learning in Anti-Money Laundering: A Risk-Based Approach](http://www.caspian.co.uk/rba/RBA.pdf)\n', '\n', '\n', '# General resources about Responsible AI\n', '\n', 'Many of the books and articles in this area cover a wide range of topics. Below is a list of a few of them, sorted alphabetically by title:\n', '\n', '- [A Hippocratic Oath for artificial intelligence practitioners](https://techcrunch.com/2018/03/14/a-hippocratic-oath-for-artificial-intelligence-practitioners/) by [Oren Etzioni](https://allenai.org/team/orene/)\n', '- [Algorithms, Correcting Biases](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3300171) by [Cass Sunstein](https://hls.harvard.edu/faculty/directory/10871/Sunstein)\n', '- [Algorithms of Oppression - How Search Engines Reinforce Racism](http://algorithmsofoppression.com/) by [Safiya Umoja Noble](https://safiyaunoble.com/)\n', '- [Artificial Unintelligence - How Computers Misunderstand the World](https://mitpress.mit.edu/books/artificial-unintelligence) by [Meredith Broussard](https://merbroussard.github.io/)\n', '- [Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor](https://us.macmillan.com/books/9781250074317) by [Virginia Eubanks](https://virginia-eubanks.com/)\n', ""- [Big Data's Disparate Impact](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2477899) by [Solon Barocas](http://solon.barocas.org/) and [Andrew D. Selbst](https://andrewselbst.com/)\n"", '- [Datasheets for Datasets](https://arxiv.org/abs/1803.09010) by [Timnit Gebru](http://ai.stanford.edu/~tgebru/) et al.\n', '- [Design Justice](https://design-justice.pubpub.org/) by [Sasha Costanza-Chock](https://www.schock.cc/)\n', '- [Fairness and Abstraction in Sociotechnical Systems](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3265913) by [Andrew D. Selbst](https://andrewselbst.com/), [danah boyd](https://www.danah.org/), [Sorelle Friedler](http://sorelle.friedler.net/), [Suresh Venkatasubramanian](http://www.cs.utah.edu/~suresh/), [Janet Vertesi](https://janet.vertesi.com/)\n', '- [Fairness and machine learning - Limitations and Opportunities](https://fairmlbook.org/) by [Solon Barocas](http://solon.barocas.org/), [Moritz Hardt](https://mrtz.org/), [Arvind Narayanan](http://randomwalker.info/)\n', ""- [How I'm fighting bias in algorithms](https://www.ted.com/talks/joy_buolamwini_how_i_m_fighting_bias_in_algorithms?language=en) by [Joy Buolamwini](https://www.poetofcode.com/)\n"", '- [Race after Technology](https://www.ruhabenjamin.com/race-after-technology) by [Ruha Benjamin](https://www.ruhabenjamin.com)\n', '- [Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/) by [Christoph Molnar](https://christophm.github.io/)\n', '- [Tech Ethics Curriculum](https://docs.google.com/spreadsheets/d/1jWIrA8jHz5fYAW4h9CkUD8gKS5V98PDJDymRf8d9vKI/edit#gid=1174187227) by [Casey Fiesler](https://caseyfiesler.com/)\n', '- [The Measure and Mismeasure of Fairness: A Critical Review of Machine Learning](https://5harad.com/papers/fair-ml.pdf) by [Sam Corbett-Davis](https://samcorbettdavies.com/) and [Sharad Goel](https://5harad.com/)\n', ""- [Weapons of Math Destruction](https://weaponsofmathdestructionbook.com/) by [Cathy O'Neil](https://mathbabe.org/)\n""]"
Responsible AI,Azure/Azureml-ResponsibleAI-Preview,Azure,https://api.github.com/repos/Azure/Azureml-ResponsibleAI-Preview,17,6,3,"['https://api.github.com/users/minthigpen', 'https://api.github.com/users/gaugup', 'https://api.github.com/users/microsoftopensource']",Jupyter Notebook,2022-11-22T19:35:35Z,https://raw.githubusercontent.com/Azure/Azureml-ResponsibleAI-Preview/main/README.md,"['# [Deprecated] Azure Machine Learning Responsible AI Toolbox - Private Preview\n', '\n', '❗ **DO NOT USE THESE DOCS:* This repo has been deprecated, for the updated private preview docs please see here: https://github.com/Azure/RAI-vNext-Preview\n', '\n', 'Welcome to the private preview for the new Responsible AI toolbox in Azure Machine Learning (AzureML) SDK and studio. The following is a guide for you to onboard to the new capabilities. For questions, please contact mithigpe@microsoft.com.\n', '\n', '## What is this new feature?\n', '\n', 'AzureML currently supports both [model explanations](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-interpretability-aml) and [model fairness](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-fairness-aml) in public preview. As we expand our offerings under Responsible AI tools for AzureML users, this new feature brings pre-existing features and brand new offerings under one-stop-shop SDK package and studio UI dashboard:\n', '- Error Analysis (new): view and understand the error distributions of your model over your dataset via a decision tree map or heat map visualization.\n', '- Data Explorer: explore your dataset by feature sets and other metrics such as predicted Y or true Y\n', '- Model Statistics: explore the distribution of your model outcomes and performance metrics\n', '- Interpretability: view the aggregate and individual feature importances across your model and dataset\n', ""- Counterfactual Example What-If's (new): create automatically generated diverse sets of counterfactual examples for each datapoint that is minimally perturbed in order to switch its predicted class or output. Also create your own counterfactual datapoint by perturbing feature values manually to observe the new outcome of your model prediction.\n"", '- Causal Analysis (new): view the aggregate and individual causal effects of *treatment features* (features which you are interested in controlling to affect the outcome) on the outcome in order to make informed real-life business decisions. See recommended treatment policies for segmentations of your population for features in your dataset to see its effect on your real-life outcome. \n', '\n', 'This new feature offers users a new powerful and robust toolkit for understanding your model and data in order to develop your machine learning models responsibly, now all in one place and integrated with your AzureML workspace.\n', '\n', '❗ **Please note:** This initial version of the Responsible AI toolbox currently does not support the integration of fairness metrics. For fairness metrics, please refer to our existing offering [here.](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-fairness-aml)\n', '\n', '## Supported scenarios, models and datasets\n', '\n', ""`azureml-responsibleai` supports computation of Responsible AI insights for `scikit-learn` models that are trained on `pandas.DataFrame`. The `azureml-responsibleai` accept both models and pipelines as input as long as the model or pipeline implements a `predict` or `predict_proba` function that conforms to the `scikit-learn` convention. If not compatible, you can wrap your model's prediction function into a wrapper class that transforms the output into the format that is supported (`predict` or `predict_proba` of `scikit-learn`), and pass that wrapper class to modules in `azureml-responsibleai`.\n"", '\n', 'Currently, we support datasets having numerical and categorical features. The following table provides the scenarios supported for each of the four responsible AI insights:-\n', '\n', '| RAI insight | Binary classification | Multi-class classification | Multilabel classification | Regression | Timeseries forecasting | Categorical features | Text features | Image Features | Recommender Systems | Reinforcement Learning |\n', '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | -- |\n', '| Explainability | Yes | Yes | No | Yes | No | Yes | No | No | No | No |\n', '| Error Analysis | Yes | Yes | No | Yes | No | Yes | No | No | No | No |\n', '| Causal Analysis | Yes | No | No | Yes | No | Yes (max 5 features due to expensiveness) | No | No | No | No |\n', '| Counterfactual | Yes | Yes | No | Yes | No | Yes | No | No | No | No |\n', '\n', '\n', '\n', '## Set Up\n', 'In this section, we will go over the basic setup steps that you need in order to generate Responsible AI insights for your models from SDK and visualize the generated Responsible AI insights in [AML studio](https://ml.azure.com/).\n', '\n', '### Installing `azureml-responsibleai` SDK\n', 'In order to install `azureml-responsibleai` package you will need a python virtual environment. You can create a python virtual environment using `conda`.\n', '```c\n', 'conda create -n azureml_env python=3.7 nb_conda -y\n', '```\n', '\n', 'We support python versions `>= 3.6` and `< 3.9`. Once the `conda` environment `azureml_env` is created, you can install `azureml-responsibleai` using `pip`.\n', '\n', '```c\n', 'activate azureml_env\n', 'pip install azureml-responsibleai\n', 'pip install liac-arff\n', '```\n', '\n', '### Create an Azure subscription\n', 'Create an Azure workspace by using the [configuration notebook](https://github.com/Azure/MachineLearningNotebooks/blob/master/configuration.ipynb)\n', '\n', '### Generating Responsibleai AI Toolbox insights\n', 'Once you have installed `azureml-responsibleai` and created an Azure workspace, you can execute the responsibleai notebooks in the `notebooks` [folder](notebooks/model-analysis) in this repo.\n', '\n', '### Viewing your Responsible AI Toolbox Dashboard in the AzureML studio portal\n', 'After generating the Responsible AI insights via SDK, you can view them in your associated workspace in AzureML studio, under your model registry.\n', '\n', '![01](images/01_model_registry.png)\n', '1. Go to your model registry in your AzureML studio workspace\n', ""2. Click on the model you've uploaded your Responsible AI insights for\n"", '\n', '![02](images/02_model_details.png)\n', '3. Click on the tab for `Responsible AI toolbox (preview)` under your model details page\n', '\n', '![03](images/03_responsibleaitoolbox.png)\n', '4. Under the `Responsible AI toolbox (preview)` tab of your model details, you will see a list of your uploaded Responsible AI insights. You can upload more than one Responsible AI toolbox dashboards for each model. Each row represents one dashboard, with information on which components were uploaded to each dashboard (i.e. explanations, counterfactuals, etc).\n', '\n', '![04](images/04_dashboard.png)\n', '5. At anytime viewing the dashboard, if you wish to return to the model details page, click on `Back to model details`\n', '<ol type=""A"">\n', '  <li>You can view the dashboard insights for each component filtered down on a global cohort you specify. Hovering over the global cohort name will show the number of datapoints and filters in that cohort as a tooltip.</li>\n', '  <li>Switch which global cohort you are applying to the dashboard.</li>\n', '  <li>Create a new global cohort based on filters you can apply in a flyout panel.</li>\n', '  <li>View a list of all global cohorts created and duplicate, edit or delete them.</li>\n', ""  <li>View a list of all Responsible AI components you've uploaded to this dashboard as well as deleting components. The layout of the dashboard will reflect the order of the components in this list.</li>\n"", '</ol>\n', '\n', '❗ **Please note:** Error Analysis, if generated, will always be at the top of the component list in your dashboard. Selecting on the nodes of the error tree or tiles of the error heatmap will automatically generate a temporary cohort that will be populated in the components below so that you can easily experiment with looking at insights for different areas of your error distribution.\n', '\n', '![05](images/05_add_dashboard.png)\n', '6. In between each component you can add components by clicking the blue circular button with a plus sign. This will pop up a tooltip that will give you an option of adding whichever Responsible AI toolbox component you enabled with your SDK.\n', '\n', '#### Known limitations of viewing dashboard in AzureML studio\n', 'Due to current lack of active compute and backend storing and recomputing your Responsible AI toolbox insights in real time, the dashboard in AzureML studio is much less robust than the dashboard generated with the open source package. To generate the full dashboard in a Jupyter python notebook, please download and use our [open source Responsible AI Toolbox SDK](https://github.com/microsoft/responsible-ai-widgets). \n', '\n', 'Some limitations in AzureML studio include:\n', '- Retraining of the Error analysis tree on different features is disabled\n', '- Switching the Error analysis heat map to different features is disabled\n', '- Viewing the Error analysis tree or heatmap on a smaller subset of your full dataset passed into the dashboard (requires retraining of the tree) is disabled\n', '- ICE (Individual Conditional Expectation) plots in the feature importance tab for explanations is disabled\n', '- Manually creating a What-If datapoint is disabled; you can only view the counterfactual examples already pre-generated by the SDK\n', '- Causal analysis individual what-if is disabled; you can only view the individual causal effects of each individual datapoint\n', '\n', '## Responsible AI Toolbox walkthrough and sample notebooks\n', 'Please read through our [sample notebooks for both regression and classification](notebooks/model-analysis) to see if this feature supports your use case. For more details about each individual component, please read through our brief [tour guide of the new Responsible AI toolbox capabilities.](https://github.com/microsoft/responsible-ai-widgets/blob/main/notebooks/responsibleaitoolbox-dashboard/tour.ipynb) \n', '\n', '## What Next?: How to join Private Preview 👀\n', 'We are super excited for you to try this new feature in AzureML! \n', '- Reach out to mithigpe@microsoft.com to enable your Azure subscription for this Private Preview feature.\n', '- Fill out this form - Private Preview sign up for [Responsible AI Dashboard in AzureML](https://forms.office.com/r/R6PmBCkyWb)\n', '\n', '## Contributing\n', '\n', 'This project welcomes contributions and suggestions.  Most contributions require you to agree to a\n', 'Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\n', 'the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n', '\n', 'When you submit a pull request, a CLA bot will automatically determine whether you need to provide\n', 'a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\n', 'provided by the bot. You will only need to do this once across all repos using our CLA.\n', '\n', 'This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\n', 'For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\n', 'contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n', '\n', '## Trademarks\n', '\n', 'This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \n', 'trademarks or logos is subject to and must follow \n', ""[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\n"", 'Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\n', ""Any use of third-party trademarks or logos are subject to those third-party's policies.\n""]"
Responsible AI,understandable-machine-intelligence-lab/Quantus,understandable-machine-intelligence-lab,https://api.github.com/repos/understandable-machine-intelligence-lab/Quantus,305,47,14,"['https://api.github.com/users/annahedstroem', 'https://api.github.com/users/dkrako', 'https://api.github.com/users/dilyabareeva', 'https://api.github.com/users/leanderweber', 'https://api.github.com/users/aaarrti', 'https://api.github.com/users/3-nan', 'https://api.github.com/users/FerranPares', 'https://api.github.com/users/annariasdu', 'https://api.github.com/users/Wickstrom', 'https://api.github.com/users/rodrigobdz', 'https://api.github.com/users/sebastian-lapuschkin', 'https://api.github.com/users/vedal', 'https://api.github.com/users/p-wojciechowski', 'https://api.github.com/users/sltzgs']",Jupyter Notebook,2023-02-23T11:15:09Z,https://raw.githubusercontent.com/understandable-machine-intelligence-lab/Quantus/main/README.md,"['<p align=""center"">\n', '  <img width=""350"" src=""https://raw.githubusercontent.com/understandable-machine-intelligence-lab/Quantus/main/quantus_logo.png"">\n', '</p>\n', '<!--<h1 align=""center""><b>Quantus</b></h1>-->\n', '<h3 align=""center""><b>A toolkit to evaluate neural network explanations</b></h3>\n', '<p align=""center"">\n', '  PyTorch and TensorFlow\n', '\n', '[![Getting started!](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/understandable-machine-intelligence-lab/Quantus/blob/main/tutorials/Tutorial_ImageNet_Example_All_Metrics.ipynb)\n', '[![Launch Tutorials](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/understandable-machine-intelligence-lab/Quantus/HEAD?labpath=tutorials)\n', '[![Python package](https://github.com/understandable-machine-intelligence-lab/Quantus/actions/workflows/python-package.yml/badge.svg)](https://github.com/understandable-machine-intelligence-lab/Quantus/actions/workflows/python-package.yml)\n', '[![Code coverage](https://github.com/understandable-machine-intelligence-lab/Quantus/actions/workflows/codecov.yml/badge.svg)](https://github.com/understandable-machine-intelligence-lab/Quantus/actions/workflows/codecov.yml)\n', '![Python version](https://img.shields.io/badge/python-3.7%20%7C%203.8%20%7C%203.9-blue.svg)\n', '[![PyPI version](https://badge.fury.io/py/quantus.svg)](https://badge.fury.io/py/quantus)\n', '[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n', '[![Documentation Status](https://readthedocs.org/projects/quantus/badge/?version=latest)](https://quantus.readthedocs.io/en/latest/?badge=latest)\n', '[![codecov.io](https://codecov.io/github/understandable-machine-intelligence-lab/Quantus/coverage.svg?branch=master)](https://codecov.io/github/understandable-machine-intelligence-lab/Quantus?branch=master)\n', '\n', '_Quantus is currently under active development so carefully note the Quantus release version to ensure reproducibility of your work._\n', '\n', '[📑 Shortcut to paper!](https://jmlr.org/papers/volume24/22-0142/22-0142.pdf)\n', '        \n', '## News and Highlights! :rocket:\n', '\n', '- Accepted to Journal of Machine Learning Research (MLOSS) ([paper](https://jmlr.org/papers/v24/22-0142.html))!\n', '- Offers more than **30+ metrics in 6 categories** for XAI evaluation\n', '- Supports different data types (image, time-series, tabular, NLP next up!) and models (PyTorch, TensorFlow)\n', '- Extended built-in support for explanation methods ([captum](https://captum.ai/) and [tf-explain](https://tf-explain.readthedocs.io/en/latest/))\n', '- New optimisations to help speed up computation, see API reference [here](https://quantus.readthedocs.io/en/latest/docs_api/quantus.metrics.base_batched.html)!\n', '- Latest metrics additions:\n', '    - <b>Relative Input Stability</b><a href=""https://arxiv.org/pdf/2203.06877.pdf""> (Chirag Agarwal, et. al., 2022)</a>\n', '    - <b>Relative Output Stability</b><a href=""https://arxiv.org/pdf/2203.06877.pdf""> (Chirag Agarwal, et. al., 2022)</a>\n', '    - <b>Relative Representation Stability</b><a href=""https://arxiv.org/pdf/2203.06877.pdf""> (Chirag Agarwal, et. al., 2022)</a>\n', '\n', 'See [here](https://github.com/understandable-machine-intelligence-lab/Quantus/releases) for the latest release(s).\n', '\n', '## Citation\n', '\n', 'If you find this toolkit or its companion paper\n', '[**Quantus: An Explainable AI Toolkit for Responsible Evaluation of Neural Network Explanations and Beyond**](https://jmlr.org/papers/v24/22-0142.html)\n', 'interesting or useful in your research, use the following Bibtex annotation to cite us:\n', '\n', '```bibtex\n', '@article{hedstrom2023quantus,\n', '  author  = {Anna Hedstr{\\""{o}}m and Leander Weber and Daniel Krakowczyk and Dilyara Bareeva and Franz Motzkus and Wojciech Samek and Sebastian Lapuschkin and Marina Marina M.{-}C. H{\\""{o}}hne},\n', '  title   = {Quantus: An Explainable AI Toolkit for Responsible Evaluation of Neural Network Explanations and Beyond},\n', '  journal = {Journal of Machine Learning Research},\n', '  year    = {2023},\n', '  volume  = {24},\n', '  number  = {34},\n', '  pages   = {1--11},\n', '  url     = {http://jmlr.org/papers/v24/22-0142.html}\n', '}\n', '```\n', '\n', 'When applying the individual metrics of Quantus, please make sure to also properly cite the work of the original authors (as linked below).\n', '\n', '## Table of contents\n', '\n', '* [Library overview](#library-overview)\n', '* [Installation](#installation)\n', '* [Getting started](#getting-started)\n', '* [Tutorials](#tutorials)\n', '* [Contributing](#contributing)\n', '<!--* [Citation](#citation)-->\n', '\n', '## Library overview \n', '\n', 'A simple visual comparison of eXplainable Artificial Intelligence (XAI) methods is often not sufficient to decide which explanation method works best as shown exemplarily in Figure a) for four gradient-based methods — Saliency ([Mørch et al., 1995](https://ieeexplore.ieee.org/document/488997); [Baehrens et al., 2010](https://www.jmlr.org/papers/volume11/baehrens10a/baehrens10a.pdf)), Integrated Gradients ([Sundararajan et al., 2017](http://proceedings.mlr.press/v70/sundararajan17a/sundararajan17a.pdf)), GradientShap ([Lundberg and Lee, 2017](https://arxiv.org/abs/1705.07874)) or FusionGrad ([Bykov et al., 2021](https://arxiv.org/abs/2106.10185)), yet it is a common practice for evaluation XAI methods in absence of ground truth data. Therefore, we developed Quantus, an easy-to-use yet comprehensive toolbox for quantitative evaluation of explanations — including 30+ different metrics. \n', '\n', '</p>\n', '<p align=""center"">\n', '  <img width=""800"" src=""https://raw.githubusercontent.com/understandable-machine-intelligence-lab/Quantus/main/viz.png"">\n', '</p>\n', '\n', 'With Quantus, we can obtain richer insights on how the methods compare e.g., b) by holistic quantification on several evaluation criteria and c) by providing sensitivity analysis of how a single parameter e.g. the pixel replacement strategy of a faithfulness test influences the ranking of the XAI methods.\n', ' \n', '### Metrics\n', '\n', 'This project started with the goal of collecting existing evaluation metrics that have been introduced in the context of XAI research — to help automate the task of _XAI quantification_. Along the way of implementation, it became clear that XAI metrics most often belong to one out of six categories i.e., 1) faithfulness, 2) robustness, 3) localisation 4) complexity 5) randomisation or 6) axiomatic metrics. The library contains implementations of the following evaluation metrics:\n', '\n', '<details>\n', '  <summary><b>Faithfulness</b></summary>\n', 'quantifies to what extent explanations follow the predictive behaviour of the model (asserting that more important features play a larger role in model outcomes)\n', ' <br><br>\n', '  <ul>\n', '    <li><b>Faithfulness Correlation </b><a href=""https://www.ijcai.org/Proceedings/2020/0417.pdf"">(Bhatt et al., 2020)</a>: iteratively replaces a random subset of given attributions with a baseline value and then measuring the correlation between the sum of this attribution subset and the difference in function output \n', '    <li><b>Faithfulness Estimate </b><a href=""https://arxiv.org/pdf/1806.07538.pdf"">(Alvarez-Melis et al., 2018)</a>: computes the correlation between probability drops and attribution scores on various points\n', '    <li><b>Monotonicity Metric </b><a href=""https://arxiv.org/abs/1909.03012"">(Arya et al. 2019)</a>: starts from a reference baseline to then incrementally replace each feature in a sorted attribution vector, measuring the effect on model performance\n', '    <li><b>Monotonicity Metric </b><a href=""https://arxiv.org/pdf/2007.07584.pdf""> (Nguyen et al, 2020)</a>: measures the spearman rank correlation between the absolute values of the attribution and the uncertainty in the probability estimation\n', '    <li><b>Pixel Flipping </b><a href=""https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140"">(Bach et al., 2015)</a>: captures the impact of perturbing pixels in descending order according to the attributed value on the classification score\n', '    <li><b>Region Perturbation </b><a href=""https://arxiv.org/pdf/1509.06321.pdf"">(Samek et al., 2015)</a>: is an extension of Pixel-Flipping to flip an area rather than a single pixel\n', '    <li><b>Selectivity </b><a href=""https://arxiv.org/pdf/1706.07979.pdf"">(Montavon et al., 2018)</a>: measures how quickly an evaluated prediction function starts to drop when removing features with the highest attributed values\n', '    <li><b>SensitivityN </b><a href=""https://arxiv.org/pdf/1711.06104.pdf"">(Ancona et al., 2019)</a>: computes the correlation between the sum of the attributions and the variation in the target output while varying the fraction of the total number of features, averaged over several test samples\n', '    <li><b>IROF </b><a href=""https://arxiv.org/pdf/2003.08747.pdf"">(Rieger at el., 2020)</a>: computes the area over the curve per class for sorted mean importances of feature segments (superpixels) as they are iteratively removed (and prediction scores are collected), averaged over several test samples\n', '    <li><b>Infidelity </b><a href=""https://arxiv.org/pdf/1901.09392.pdf"">(Chih-Kuan, Yeh, et al., 2019)</a>: represents the expected mean square error between 1) a dot product of an attribution and input perturbation and 2) difference in model output after significant perturbation \n', '    <li><b>ROAD </b><a href=""https://arxiv.org/pdf/2202.00449.pdf"">(Rong, Leemann, et al., 2022)</a>: measures the accuracy of the model on the test set in an iterative process of removing k most important pixels, at each step k most relevant pixels (MoRF order) are replaced with noisy linear imputations\n', '    <li><b>Sufficiency </b><a href=""https://arxiv.org/abs/2202.00734"">(Dasgupta et al., 2022)</a>: measures the extent to which similar explanations have the same prediction label\n', '</ul>\n', '</details>\n', '\n', '<details>\n', '<summary><b>Robustness</b></summary>\n', 'measures to what extent explanations are stable when subject to slight perturbations of the input, assuming that model output approximately stayed the same\n', '     <br><br>\n', '<ul>\n', '    <li><b>Local Lipschitz Estimate </b><a href=""https://arxiv.org/pdf/1806.08049.pdf"">(Alvarez-Melis et al., 2018)</a>: tests the consistency in the explanation between adjacent examples\n', '    <li><b>Max-Sensitivity </b><a href=""https://arxiv.org/pdf/1901.09392.pdf"">(Yeh et al., 2019)</a>: measures the maximum sensitivity of an explanation using a Monte Carlo sampling-based approximation\n', '    <li><b>Avg-Sensitivity </b><a href=""https://arxiv.org/pdf/1901.09392.pdf"">(Yeh et al., 2019)</a>: measures the average sensitivity of an explanation using a Monte Carlo sampling-based approximation\n', '    <li><b>Continuity </b><a href=""https://arxiv.org/pdf/1706.07979.pdf"">(Montavon et al., 2018)</a>: captures the strongest variation in explanation of an input and its perturbed version\n', '    <li><b>Consistency </b><a href=""https://arxiv.org/abs/2202.00734"">(Dasgupta et al., 2022)</a>: measures the probability that the inputs with the same explanation have the same prediction label\n', '    <li><b>Relative Input Stability (RIS)</b><a href=""https://arxiv.org/pdf/2203.06877.pdf""> (Chirag Agarwal, et. al., 2022)</a>: measures the relative distance between explanations e_x and e_x\' with respect to the distance between the two inputs x and x\'\n', '    <li><b>Relative Representation Stability (RRS)</b><a href=""https://arxiv.org/pdf/2203.06877.pdf""> (Chirag Agarwal, et. al., 2022)</a>: measures the relative distance between explanations e_x and e_x\' with respect to the distance between internal models representations L_x and L_x\' for x and x\' respectively\n', '    <li><b>Relative Output Stability (ROS)</b><a href=""https://arxiv.org/pdf/2203.06877.pdf""> (Chirag Agarwal, et. al., 2022)</a>: measures the relative distance between explanations e_x and e_x\' with respect to the distance between output logits h(x) and h(x\') for x and x\' respectively\n', '</ul>\n', '</details>\n', '\n', '<details>\n', '<summary><b>Localisation</b></summary>\n', 'tests if the explainable evidence is centred around a region of interest (RoI) which may be defined around an object by a bounding box, a segmentation mask or, a cell within a grid\n', '     <br><br>\n', '<ul>\n', '    <li><b>Pointing Game </b><a href=""https://arxiv.org/abs/1608.00507"">(Zhang et al., 2018)</a>: checks whether attribution with the highest score is located within the targeted object\n', '    <li><b>Attribution Localization </b><a href=""https://arxiv.org/abs/1910.09840"">(Kohlbrenner et al., 2020)</a>: measures the ratio of positive attributions within the targeted object towards the total positive attributions\n', '    <li><b>Top-K Intersection </b><a href=""https://arxiv.org/abs/2104.14995"">(Theiner et al., 2021)</a>: computes the intersection between a ground truth mask and the binarized explanation at the top k feature locations\n', '    <li><b>Relevance Rank Accuracy </b><a href=""https://arxiv.org/abs/2003.07258"">(Arras et al., 2021)</a>: measures the ratio of highly attributed pixels within a ground-truth mask towards the size of the ground truth mask\n', '    <li><b>Relevance Mass Accuracy </b><a href=""https://arxiv.org/abs/2003.07258"">(Arras et al., 2021)</a>: measures the ratio of positively attributed attributions inside the ground-truth mask towards the overall positive attributions\n', '    <li><b>AUC </b><a href=""https://doi.org/10.1016/j.patrec.2005.10.010"">(Fawcett et al., 2006)</a>: compares the ranking between attributions and a given ground-truth mask\n', '    <li><b>Focus </b><a href=""https://arxiv.org/abs/2109.15035"">(Arias et al., 2022)</a>: quantifies the precision of the explanation by creating mosaics of data instances from different classes\n', '</ul>\n', '</details>\n', '\n', '<details>\n', '<summary><b>Complexity</b></summary>\n', 'captures to what extent explanations are concise i.e., that few features are used to explain a model prediction\n', '     <br><br>\n', '<ul>\n', '    <li><b>Sparseness </b><a href=""https://arxiv.org/abs/1810.06583"">(Chalasani et al., 2020)</a>: uses the Gini Index for measuring, if only highly attributed features are truly predictive of the model output\n', '    <li><b>Complexity </b><a href=""https://arxiv.org/abs/2005.00631"">(Bhatt et al., 2020)</a>: computes the entropy of the fractional contribution of all features to the total magnitude of the attribution individually\n', '    <li><b>Effective Complexity </b><a href=""https://arxiv.org/abs/2007.07584"">(Nguyen at el., 2020)</a>: measures how many attributions in absolute values are exceeding a certain threshold\n', '</ul>\n', '</details>\n', '\n', '<details>\n', '<summary><b>Randomisation</b></summary>\n', 'tests to what extent explanations deteriorate as inputs to the evaluation problem e.g., model parameters are increasingly randomised\n', '     <br><br>\n', '<ul>\n', '    <li><b>Model Parameter Randomisation </b><a href=""https://arxiv.org/abs/1810.03292"">(Adebayo et. al., 2018)</a>: randomises the parameters of single model layers in a cascading or independent way and measures the distance of the respective explanation to the original explanation\n', '    <li><b>Random Logit Test </b><a href=""https://arxiv.org/abs/1912.09818"">(Sixt et al., 2020)</a>: computes for the distance between the original explanation and the explanation for a random other class\n', '</ul>\n', '</details>\n', '\n', '<details>\n', '<summary><b>Axiomatic</b></summary>\n', '  assesses if explanations fulfil certain axiomatic properties\n', '     <br><br>\n', '<ul>\n', '    <li><b>Completeness </b><a href=""https://arxiv.org/abs/1703.01365"">(Sundararajan et al., 2017)</a>: evaluates whether the sum of attributions is equal to the difference between the function values at the input x and baseline x\' (and referred to as Summation to Delta (Shrikumar et al., 2017), Sensitivity-n (slight variation, Ancona et al., 2018) and Conservation (Montavon et al., 2018))\n', '    <li><b>Non-Sensitivity </b><a href=""https://arxiv.org/abs/2007.07584"">(Nguyen at el., 2020)</a>: measures whether the total attribution is proportional to the explainable evidence at the model output\n', '    <li><b>Input Invariance </b><a href=""https://arxiv.org/abs/1711.00867"">(Kindermans et al., 2017)</a>: adds a shift to input, asking that attributions should not change in response (assuming the model does not)\n', '</ul>\n', '</details>\n', '\n', 'Additional metrics will be included in future releases. Please [open an issue](https://github.com/understandable-machine-intelligence-lab/Quantus/issues/new/choose) if you have a metric you believe should be apart of Quantus.\n', '\n', '**Disclaimers.** It is worth noting that the implementations of the metrics in this library have not been verified by the original authors. Thus any metric implementation in this library may differ from the original authors. Further, bear in mind that evaluation metrics for XAI methods are often empirical interpretations (or translations) of qualities that some researcher(s) claimed were important for explanations to fulfil, so it may be a discrepancy between what the author claims to measure by the proposed metric and what is actually measured e.g., using entropy as an operationalisation of explanation complexity. Please read the [user guidelines](https://quantus.readthedocs.io/en/latest/guidelines/guidelines_and_disclaimers.html) for further guidance on how to best use the library. \n', '\n', '## Installation\n', '\n', 'If you already have [PyTorch](https://pytorch.org/) or [TensorFlow](https://www.TensorFlow.org) installed on your machine, \n', 'the most light-weight version of Quantus can be obtained from [PyPI](https://pypi.org/project/quantus/) as follows (no additional explainability functionality or deep learning framework will be included):\n', '\n', '```setup\n', 'pip install quantus\n', '```\n', 'Alternatively, you can simply add the desired deep learning framework (in brackets) to have the package installed together with Quantus.\n', 'To install Quantus with PyTorch, please run:\n', '```setup\n', 'pip install ""quantus[torch]""\n', '```\n', '\n', 'For TensorFlow, please run:\n', '\n', '```setup\n', 'pip install ""quantus[tensorflow]""\n', '```\n', '\n', 'Alternatively, you can simply install Quantus with [requirements.txt](https://github.com/understandable-machine-intelligence-lab/Quantus/blob/main/requirements.txt).\n', 'Note that this installation requires that either [PyTorch](https://pytorch.org/) or [TensorFlow](https://www.TensorFlow.org) are already installed on your machine.\n', '\n', '```setup\n', 'pip install -r requirements.txt\n', '```\n', '\n', 'For a more in-depth guide on how to install Quantus, please read more [here](https://quantus.readthedocs.io/en/latest/getting_started/installation.html). This includes instructions for how to install a desired deep learning framework such as PyTorch or TensorFlow together with Quantus.\n', '\n', '### Package requirements\n', '\n', 'The package requirements are as follows:\n', '```\n', 'python>=3.7.0\n', 'pytorch>=1.10.1\n', 'TensorFlow==2.6.2\n', '```\n', '\n', '## Getting started\n', '\n', 'The following will give a short introduction to how to get started with Quantus. Note that this example is based on the [PyTorch](https://pytorch.org/) framework, but we also support \n', '[TensorFlow](https://www.tensorflow.org), which would differ only in the loading of the model, data and explanations. To get started with Quantus, you need:\n', '* A model (`model`), inputs (`x_batch`) and labels (`y_batch`)\n', '* Some explanations you want to evaluate (`a_batch`)\n', '\n', '\n', '<details>\n', '<summary><b><big>Step 1. Load data and model</big></b></summary>\n', '\n', ""Let's first load the data and model. In this example, a pre-trained LeNet available from Quantus \n"", ""for the purpose of this tutorial is loaded, but generally, you might use any Pytorch (or TensorFlow) model instead. To follow this example, one needs to have quantus and torch installed, by e.g., `pip install 'quantus[torch]'`.\n"", '\n', '```python\n', 'import quantus\n', 'from quantus.helpers.model.models import LeNet\n', 'import torch\n', 'import torchvision\n', 'from torchvision import transforms\n', '  \n', '# Enable GPU.\n', 'device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n', '\n', '# Load a pre-trained LeNet classification model (architecture at quantus/helpers/models).\n', 'model = LeNet()\n', 'if device.type == ""cpu"":\n', '    model.load_state_dict(torch.load(""tests/assets/mnist"", map_location=torch.device(\'cpu\')))\n', 'else: \n', '    model.load_state_dict(torch.load(""tests/assets/mnist""))\n', '\n', '# Load datasets and make loaders.\n', ""test_set = torchvision.datasets.MNIST(root='./sample_data', download=True, transforms=transforms.Compose([transforms.ToTensor()]))\n"", 'test_loader = torch.utils.data.DataLoader(test_set, batch_size=24)\n', '\n', '# Load a batch of inputs and outputs to use for XAI evaluation.\n', 'x_batch, y_batch = iter(test_loader).next()\n', 'x_batch, y_batch = x_batch.cpu().numpy(), y_batch.cpu().numpy()\n', '```\n', '</details>\n', '\n', '<details>\n', '<summary><b><big>Step 2. Load explanations</big></b></summary>\n', '\n', 'We still need some explanations to evaluate. \n', 'For this, there are two possibilities in Quantus. You can provide either:\n', '1. a set of re-computed attributions (`np.ndarray`)\n', '2. any arbitrary explanation function (`callable`), e.g., the built-in method `quantus.explain` or your own customised function\n', '\n', 'We show the different options below.\n', '\n', '#### Using pre-computed explanations\n', '\n', 'Quantus allows you to evaluate explanations that you have pre-computed, \n', ""assuming that they match the data you provide in `x_batch`. Let's say you have explanations \n"", 'for [Saliency](https://arxiv.org/abs/1312.6034) and [Integrated Gradients](https://arxiv.org/abs/1703.01365)\n', 'already pre-computed.\n', '\n', 'In that case, you can simply load these into corresponding variables `a_batch_saliency` \n', 'and `a_batch_intgrad`:\n', '\n', '```python\n', 'a_batch_saliency = load(""path/to/precomputed/saliency/explanations"")\n', 'a_batch_saliency = load(""path/to/precomputed/intgrad/explanations"")\n', '```\n', '\n', 'Another option is to simply obtain the attributions using one of many XAI frameworks out there, \n', 'such as [Captum](https://captum.ai/), \n', '[Zennit](https://github.com/chr5tphr/zennit), \n', '[tf.explain](https://github.com/sicara/tf-explain),\n', 'or [iNNvestigate](https://github.com/albermax/innvestigate). The following code example shows how to obtain explanations ([Saliency](https://arxiv.org/abs/1312.6034) \n', 'and [Integrated Gradients](https://arxiv.org/abs/1703.01365), to be specific) \n', 'using [Captum](https://captum.ai/):\n', '\n', '```python\n', 'import captum\n', 'from captum.attr import Saliency, IntegratedGradients\n', '\n', '# Generate Integrated Gradients attributions of the first batch of the test set.\n', 'a_batch_saliency = Saliency(model).attribute(inputs=x_batch, target=y_batch, abs=True).sum(axis=1).cpu().numpy()\n', 'a_batch_intgrad = IntegratedGradients(model).attribute(inputs=x_batch, target=y_batch, baselines=torch.zeros_like(x_batch)).sum(axis=1).cpu().numpy()\n', '\n', '# Save x_batch and y_batch as numpy arrays that will be used to call metric instances.\n', 'x_batch, y_batch = x_batch.cpu().numpy(), y_batch.cpu().numpy()\n', '\n', '# Quick assert.\n', 'assert [isinstance(obj, np.ndarray) for obj in [x_batch, y_batch, a_batch_saliency, a_batch_intgrad]]\n', '```\n', '\n', '#### Passing an explanation function\n', '\n', ""If you don't have a pre-computed set of explanations but rather want to pass an arbitrary explanation function \n"", 'that you wish to evaluate with Quantus, this option exists. \n', '\n', 'For this, you can for example rely on the built-in `quantus.explain` function to get started, which includes some popular explanation methods \n', '(please run `quantus.available_methods()` to see which ones).  Examples of how to use `quantus.explain` \n', 'or your own customised explanation function are included in the next section.\n', '\n', '<img class=""center"" width=""500"" alt=""drawing""  src=""tutorials/assets/mnist_example.png""/>\n', '\n', 'As seen in the above image, the qualitative aspects of explanations \n', 'may look fairly uninterpretable --- since we lack ground truth of what the explanations\n', 'should be looking like, it is hard to draw conclusions about the explainable evidence. To gather quantitative evidence for the quality of the different explanation methods, we can apply Quantus.\n', '</details>\n', '\n', '<details>\n', '<summary><b><big>Step 3. Evaluate with Quantus</big></b></summary> \n', '\n', 'Quantus implements XAI evaluation metrics from different categories, \n', 'e.g., Faithfulness, Localisation and Robustness etc which all inherit from the base `quantus.Metric` class. \n', 'To apply a metric to your setting (e.g., [Max-Sensitivity](https://arxiv.org/abs/1901.09392)) \n', 'it first needs to be instantiated:\n', '\n', '```python\n', 'metric = quantus.MaxSensitivity(nr_samples=10,\n', '                                lower_bound=0.2,\n', '                                norm_numerator=quantus.fro_norm,\n', '                                norm_denominator=quantus.fro_norm,\n', '                                perturb_func=quantus.uniform_noise,\n', '                                similarity_func=quantus.difference)\n', '```\n', '\n', 'and then applied to your model, data, and (pre-computed) explanations:\n', '\n', '```python\n', 'scores = metric(\n', '    model=model,\n', '    x_batch=x_batch,\n', '    y_batch=y_batch,\n', '    a_batch=a_batch_saliency,\n', '    device=device\n', ')\n', '```\n', '\n', '#### Use quantus.explain\n', '\n', 'Alternatively, instead of providing pre-computed explanations, you can employ the `quantus.explain` function,\n', 'which can be specified through a dictionary passed to `explain_func_kwargs`.\n', '\n', '```python\n', 'scores = metric(\n', '    model=model,\n', '    x_batch=x_batch,\n', '    y_batch=y_batch,\n', '    device=device,\n', '    explain_func=quantus.explain,\n', '    explain_func_kwargs={""method"": ""Saliency""}\n', ')\n', '```\n', '\n', '#### Employ customised functions\n', '\n', 'You can alternatively use your own customised explanation function\n', '(assuming it returns an `np.ndarray` in a shape that matches the input `x_batch`). This is done as follows:\n', '\n', '```python\n', 'def your_own_callable(model, models, targets, **kwargs) -> np.ndarray\n', '  """"""Logic goes here to compute the attributions and return an \n', '  explanation  in the same shape as x_batch (np.array), \n', '  (flatten channels if necessary).""""""\n', '  return explanation(model, x_batch, y_batch)\n', '\n', 'scores = metric(\n', '    model=model,\n', '    x_batch=x_batch,\n', '    y_batch=y_batch,\n', '    device=device,\n', '    explain_func=your_own_callable\n', ')\n', '```\n', '#### Run large-scale evaluation\n', '\n', 'Quantus also provides high-level functionality to support large-scale evaluations,\n', 'e.g., multiple XAI methods, multifaceted evaluation through several metrics, or a combination thereof. To utilise `quantus.evaluate()`, you simply need to define two things:\n', '\n', '1. The **Metrics** you would like to use for evaluation (each `__init__` parameter configuration counts as its own metric):\n', '    ```python\n', '    metrics = {\n', '        ""max-sensitivity-10"": quantus.MaxSensitivity(nr_samples=10),\n', '        ""max-sensitivity-20"": quantus.MaxSensitivity(nr_samples=20),\n', '        ""region-perturbation"": quantus.RegionPerturbation(),\n', '    }\n', '    ```\n', '   \n', '2. The **XAI methods** you would like to evaluate, e.g., a `dict` with pre-computed attributions:\n', '    ```python\n', '    xai_methods = {\n', '        ""Saliency"": a_batch_saliency,\n', '        ""IntegratedGradients"": a_batch_intgrad\n', '    }\n', '    ```\n', '\n', 'You can then simply run a large-scale evaluation as follows (this aggregates the result by `np.mean` averaging):\n', '\n', '```python\n', 'import numpy as np\n', 'results = quantus.evaluate(\n', '      metrics=metrics,\n', '      xai_methods=xai_methods,\n', '      agg_func=np.mean,\n', '      model=model,\n', '      x_batch=x_batch,\n', '      y_batch=y_batch,\n', '      **{""softmax"": False,}\n', ')\n', '```\n', '</details>\n', '\n', 'Please see [\n', ""Getting started tutorial](https://github.com/understandable-machine-intelligence-lab/quantus/blob/main/tutorials/Tutorial_Getting_Started.ipynb) to run code similar to this example. For more information on how to customise metrics and extend Quantus' functionality, please see [Getting started guide](https://quantus.readthedocs.io/en/latest/getting_started/getting_started_example.html).\n"", '\n', '\n', '## Tutorials\n', '\n', 'Further tutorials are available that showcase the many types of analysis that can be done using Quantus.\n', 'For this purpose, please see notebooks in the [tutorials](https://github.com/understandable-machine-intelligence-lab/Quantus/blob/main/tutorials/) folder which includes examples such as:\n', '* [All Metrics ImageNet Example](https://github.com/understandable-machine-intelligence-lab/Quantus/blob/main/tutorials/Tutorial_ImageNet_Example_All_Metrics.ipynb): shows how to instantiate the different metrics for ImageNet dataset\n', '* [Metric Parameterisation Analysis](https://github.com/understandable-machine-intelligence-lab/Quantus/blob/main/tutorials/Tutorial_Metric_Parameterisation_Analysis.ipynb): explores how sensitive a metric could be to its hyperparameters\n', '* [Robustness Analysis Model Training](https://github.com/understandable-machine-intelligence-lab/Quantus/blob/main/tutorials/Tutorial_XAI_Sensitivity_Model_Training.ipynb): measures robustness of explanations as model accuracy increases \n', '* [Full Quantification with Quantus](https://github.com/understandable-machine-intelligence-lab/Quantus/blob/main/tutorials/Tutorial_ImageNet_Quantification_with_Quantus.ipynb): example of benchmarking explanation methods\n', '* [Tabular Data Example](https://github.com/understandable-machine-intelligence-lab/Quantus/blob/main/tutorials/Tutorial_Getting_Started_with_Tabular_Data.ipynb): example of how to use Quantus with tabular data\n', '* [Quantus and TensorFlow Data Example](https://github.com/understandable-machine-intelligence-lab/Quantus/blob/main/tutorials/Tutorial_Getting_Started_with_Tensorflow.ipynb): showcases how to use Quantus with TensorFlow\n', '\n', '... and more.\n', '\n', '## Contributing\n', '\n', 'We welcome any sort of contribution to Quantus! For a detailed contribution guide, please refer to [Contributing](https://github.com/understandable-machine-intelligence-lab/Quantus/blob/main/CONTRIBUTING.md) documentation first. \n', '\n', 'If you have any developer-related questions, please [open an issue](https://github.com/understandable-machine-intelligence-lab/Quantus/issues/new/choose)\n', 'or write us at [hedstroem.anna@gmail.com](mailto:hedstroem.anna@gmail.com).\n']"
Responsible AI,microsoft/responsible-ai-workshop,microsoft,https://api.github.com/repos/microsoft/responsible-ai-workshop,13,6,5,"['https://api.github.com/users/philber', 'https://api.github.com/users/microsoftopensource', 'https://api.github.com/users/RiadEtm', 'https://api.github.com/users/alazraq', 'https://api.github.com/users/microsoft-github-operations%5Bbot%5D']",Jupyter Notebook,2023-02-16T03:28:31Z,https://raw.githubusercontent.com/microsoft/responsible-ai-workshop/main/README.md,"['\n', '# Responsible AI Workshop\n', '![Workshop logo](https://github.com/microsoft/responsible-ai-workshop/blob/main/rai-ws-banner.png)\n', '\n', 'Responsible innovation is top of mind. As such, the tech industry as well as a growing number of organizations of all kinds in their digital transformation are being called upon to develop and deploy Artificial Intelligence (AI) technologies and Machine Learning (ML)-powered systems (products or services) and/or features (all referred as to AI systems below) more responsibly. And yet many organizations implementing such AI systems report being unprepared to address AI risks and failures, and struggle with new challenges in terms of governance, security and compliance.\n', '\n', 'Advancements in AI are indeed different than other technologies because of the pace of innovation. There has been hundreds of research papers published every year in the past few years -, but also because of its proximity to human intelligence, impacting us at a personal and societal level.\n', '\n', ""There are a number of challenges and questions raised through the use of AI technologies. We refer to these as socio-technical impacts. All of these have given rise to an industry debate about how the world should/shouldn't use these new capabilities. It isn't because you can do something that you should necessarily do it. \n"", '\n', 'This project is an attempt to introduce and illustrate the use of: \n', '* Resources designed to help you responsibly use AI at every stage of innovation - from concept to development, deployment, and beyond. \n', '* Available toolkits & frameworks that help you integrate relevant Responsible AI features into your AI environment by themes and through the lifecycle stages of your AI system.\n', '* Activities to strengthen gradually the confidence that we can have in this technology and therefore facilitate its adoption in contexts where it would have a great responsibility.\n', '\n', 'It is thus designed to help you or your ""customers"", whoever they are, to put Responsible AI into practice for your AI-powered solutions throughout their lifecycle.\n', '\n', '# Workshop Tutorials/Walkthroughs\n', '\n', '## Work in Progress\n', '\n', 'This project is a work in progress (WIP).\n', '\n', 'This project currently contains the following tutorials:\n', '* [Responsible AI Tooling Tutorials](https://github.com/microsoft/responsible-ai-workshop/tree/master/tooling-tutorials)\n', '* [End-to-End Responsible AI Lifecycle Walkthrough](https://github.com/microsoft/responsible-ai-workshop/tree/main/lifecycle-walkthrough)\n', '* [Towards a (more) trustworthy AI lifecycle](https://github.com/microsoft/responsible-ai-workshop/tree/main/trustworthy-ai-lifecycle)\n', '\n', 'Each of the above tutorials consists of a series of modules for data engineers, data scientists, ML developers, ML engineers, and other AI practitioners, as well as potentially anyone interested considering the wide range of socio-technical aspects involved in the subject.\n', '\n', '## Prerequisites\n', '\n', 'The workshop is meant to be hands-on. Therefore, basic knowledge of any version of Python is a prerequisite. It also assumes that you have prior experience training machine learning (ML) models with Python using open-source frameworks like Scikit-Learn, PyTorch, and TensorFlow.\n', '\n', 'One should also note that this workshop might also be introduced by the following [Microsoft Learn](https://docs.microsoft.com/en-us/learn/) learning paths:\n', '* [Discover ways to foster an AI-ready culture in your business](https://docs.microsoft.com/en-us/learn/paths/foster-ai-ready-culture/).\n', '* [Identify principles and practices for responsible AI](https://docs.microsoft.com/en-us/learn/paths/responsible-ai-business-principles/).\n', '* [Identify guiding principles for responsible AI in government](https://docs.microsoft.com/en-us/learn/paths/responsible-ai-government-principles/).\n', '\n', '## Additional resources\n', '\n', 'From holistically transforming industries to addressing critical issues facing humanity, AI is already solving some of our most complex challenges and redefining how humans and technology interact. \n', '\n', 'You can read the publicly shared [Microsoft Responsible AI Standard](https://blogs.microsoft.com/wp-content/uploads/prod/sites/5/2022/06/Microsoft-Responsible-AI-Standard-v2-General-Requirements-3.pdf), i.e., a framework to guide how Microsoft build AI systems. It is an important step in our journey to develop better, more trustworthy AI systems. We are releasing our latest Responsible AI Standard to share what we have learned, invite feedback from others, and contribute to the discussion about building better norms and practices around AI. \n', '\n', 'For those wanting to dig into our approach further, we have also made available some key resources that support the Responsible AI Standard: notably our [Impact Assessment template](https://blogs.microsoft.com/wp-content/uploads/prod/sites/5/2022/06/Microsoft-RAI-Impact-Assessment-Template.pdf) and [guide](https://blogs.microsoft.com/wp-content/uploads/prod/sites/5/2022/06/Microsoft-RAI-Impact-Assessment-Guide.pdf). Impact Assessments have proven valuable at Microsoft to ensure teams explore the impact of their AI system – including its stakeholders, intended benefits, and potential harms – in depth at the earliest design stages. \n', '\n', 'In addition, you can also visit our [Responsible AI resource center](https://www.microsoft.com/en-us/ai/responsible-ai) where you can find access to tools, guidelines, and additional resources that will help you create a (more) Responsible AI solution:\n', '* [Put Responsible AI into Practice webinar](https://info.microsoft.com/ww-put-responsible-ai-into-practice-On-Demand-Registration.html) (On Demand).\n', '* [Ten Guidelines for Product Leaders to implement AI Responsibly](https://aka.ms/RAITenGuidelines).\n', '* [Establish a responsible AI strategy](https://aka.ms/AIBS).\n', '* [Design, build, and manage your AI-powered solution](http://aka.ms/RAIresources).\n', '  \n', '\n', '# Contributing\n', '\n', 'This project welcomes contributions and suggestions.  Most contributions require you to agree to a\n', 'Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\n', 'the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n', '\n', 'When you submit a pull request, a CLA bot will automatically determine whether you need to provide\n', 'a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\n', 'provided by the bot. You will only need to do this once across all repos using our CLA.\n', '\n', 'This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\n', 'For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\n', 'contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n', '\n', '# Legal Notices\n', '\n', 'Microsoft and any contributors grant you a license to the Microsoft documentation and other content\n', 'in this repository under the [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/legalcode),\n', 'see the [LICENSE](LICENSE) file, and grant you a license to any code in the repository under the [MIT License](https://opensource.org/licenses/MIT), see the\n', '[LICENSE-CODE](LICENSE-CODE) file.\n', '\n', 'Microsoft, Windows, Microsoft Azure and/or other Microsoft products and services referenced in the documentation\n', 'may be either trademarks or registered trademarks of Microsoft in the United States and/or other countries.\n', 'The licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks.\n', ""Microsoft's general trademark guidelines can be found at http://go.microsoft.com/fwlink/?LinkID=254653.\n"", '\n', 'Privacy information can be found at https://privacy.microsoft.com/en-us/\n', '\n', 'Microsoft and any contributors reserve all other rights, whether under their respective copyrights, patents,\n', 'or trademarks, whether by implication, estoppel or otherwise.\n']"
Responsible AI,mit-ll-responsible-ai/responsible-ai-toolbox,mit-ll-responsible-ai,https://api.github.com/repos/mit-ll-responsible-ai/responsible-ai-toolbox,24,5,6,"['https://api.github.com/users/rsokl', 'https://api.github.com/users/jgbos', 'https://api.github.com/users/dependabot%5Bbot%5D', 'https://api.github.com/users/oliviamb', 'https://api.github.com/users/miscpeeps', 'https://api.github.com/users/Jasha10']",Jupyter Notebook,2022-12-22T15:56:44Z,https://raw.githubusercontent.com/mit-ll-responsible-ai/responsible-ai-toolbox/main/README.md,"['# Responsible AI Toolbox\n', '\n', '<p align=""center"">\n', '  <img width=""200"" height=""200"" src=""brand/logo_no_text_small.png"">\n', '</p>\n', '\n', '<p align=""center"">\n', '  <a href=""https://pypi.org/project/rai-toolbox/"">\n', '    <img src=""https://img.shields.io/pypi/v/rai-toolbox.svg"" alt=""PyPI"" />\n', '  </a>\n', '  <a>\n', '    <img src=""https://img.shields.io/badge/python-3.7%20&#8208;%203.10-blue.svg"" alt=""Python version support"" />\n', '  </a>\n', '  <a href=""https://github.com/mit-ll-responsible-ai/responsible-ai-toolbox/actions?query=workflow%3ATests+branch%3Amain"">\n', '    <img src=""https://github.com/mit-ll-responsible-ai/responsible-ai-toolbox/workflows/Tests/badge.svg"" alt=""GitHub Actions"" />\n', '  <a href=""https://hypothesis.readthedocs.io/"">\n', '    <img src=""https://img.shields.io/badge/hypothesis-tested-brightgreen.svg"" alt=""Tested with Hypothesis"" />\n', '  </a>\n', '\n', '  <p align=""center"">\n', '    A library that provides high-quality, PyTorch-centric tools for evaluating and enhancing both the robustness and the explainability of AI models.\n', '  </p>\n', '  <p align=""center"">\n', '    Check out our <a href=""https://mit-ll-responsible-ai.github.io/responsible-ai-toolbox/"">documentation</a> for more information.\n', '  </p>\n', '  <p align=""center"">\n', '    The rAI-toolbox works great with <a href=""https://www.pytorchlightning.ai/"">PyTorch Lightning</a> ⚡ and <a href=""https://hydra.cc/"">Hydra</a> 🐉. Check out <a href=""https://mit-ll-responsible-ai.github.io/responsible-ai-toolbox/ref_mushin.html"">rai_toolbox.mushin</a> to see how we use these frameworks to create efficient, configurable, and reproducible ML workflows with minimal boilerplate code.\n', '  </p>\n', '</p>\n', '\n', '\n', '\n', '## Citation\n', '\n', 'Using `rai_toolbox` for your research? Please cite the following publication:\n', '\n', '```\n', '@article{soklaski2022tools,\n', '  title={Tools and Practices for Responsible AI Engineering},\n', '  author={Soklaski, Ryan and Goodwin, Justin and Brown, Olivia and Yee, Michael and Matterer, Jason},\n', '  journal={arXiv preprint arXiv:2201.05647},\n', '  year={2022}\n', '}\n', '```\n', '\n', '\n', '## Contributing\n', '\n', 'If you would like to contribute to this repo, please refer to our `CONTRIBUTING.md` document.\n', '\n', '\n', '\n', '## Disclaimer\n', '\n', 'DISTRIBUTION STATEMENT A. Approved for public release. Distribution is unlimited.\n', '\n', '© 2023 MASSACHUSETTS INSTITUTE OF TECHNOLOGY\n', '\n', '- Subject to FAR 52.227-11 – Patent Rights – Ownership by the Contractor (May 2014)\n', '- SPDX-License-Identifier: MIT\n', '\n', 'This material is based upon work supported by the Under Secretary of Defense for Research and Engineering under Air Force Contract No. FA8702-15-D-0001. Any opinions, findings, conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Under Secretary of Defense for Research and Engineering.\n', '\n', 'A portion of this research was sponsored by the United States Air Force Research Laboratory and the United States Air Force Artificial Intelligence Accelerator and was accomplished under Cooperative Agreement Number FA8750-19-2-1000. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the United States Air Force or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein.\n', '\n', 'The software/firmware is provided to you on an As-Is basis.\n']"
Responsible AI,jphall663/responsible_xai,jphall663,https://api.github.com/repos/jphall663/responsible_xai,17,6,1,['https://api.github.com/users/jphall663'],Jupyter Notebook,2023-01-30T18:09:13Z,https://raw.githubusercontent.com/jphall663/responsible_xai/master/README.md,"['# responsible_xai\n', 'Guidelines for the responsible use of explainable AI and machine learning. See [responsible_xai.pdf](responsible_xai.pdf).\n', '\n', 'A static version of this document, that may not reflect recent changes, is available on [arXiv](https://arxiv.org/abs/1906.03533).\n', '\n', 'For associated slides see: https://github.com/jphall663/hc_ml.\n']"
Responsible AI,aws-samples/aws-machine-learning-university-responsible-ai,aws-samples,https://api.github.com/repos/aws-samples/aws-machine-learning-university-responsible-ai,28,4,2,"['https://api.github.com/users/mimicarina', 'https://api.github.com/users/amazon-auto']",Jupyter Notebook,2023-02-23T06:54:38Z,https://raw.githubusercontent.com/aws-samples/aws-machine-learning-university-responsible-ai/main/README.md,"['![logo](data/MLU_Logo.png)\n', '## Machine Learning University: Responsible AI\n', '\n', 'This repository contains __slides__, __notebooks__, and __data__ for the __Machine Learning University (MLU) Responsible AI__ class. Our mission is to make Machine Learning accessible to everyone. We have courses available across many topics of machine learning and believe knowledge of ML can be a key enabler for success. This class is designed to help you get started with Responsible AI, learn about widely used Machine Learning techniques, and apply them to real-world problems.\n', '\n', '## YouTube\n', 'Watch all Responsible AI video recordings in this [YouTube playlist](https://www.youtube.com/playlist?list=PL8P_Z6C4GcuVMxhwT9JO_nKuW0QMSJ-cZ) from our [YouTube channel](https://www.youtube.com/channel/UC12LqyqTQYbXatYS9AA7Nuw/playlists).\n', '\n', '## Course Overview\n', 'There are three lectures and one final project for this class.\n', '\n', '__Final Project:__ Practice working with a ""real-world"" dataset for the final project. Final project dataset is in the [data/final_project folder](https://github.com/aws-samples/aws-machine-learning-university-responsible-ai/tree/master/data/final_project). For more details on the final project, check out [this notebook](https://github.com/aws-samples/aws-machine-learning-university-responsible-ai/blob/main/notebooks/day_1/MLA-RESML-DAY1-FINAL-STUDENT-NB.ipynb).\n', '\n', '## Interactives/Visuals\n', 'Interested in visual, interactive explanations of core machine learning concepts? Check out our [MLU-Explain articles](https://mlu-explain.github.io/) to learn at your own pace! \n', '\n', '## Contribute\n', 'If you would like to contribute to the project, see [CONTRIBUTING](CONTRIBUTING.md) for more information.\n', '\n', '## License\n', ""The license for this repository depends on the section.  Data set for the course is being provided to you by permission of Amazon and is subject to the terms of the [Amazon License and Access](https://www.amazon.com/gp/help/customer/display.html?nodeId=201909000). You are expressly prohibited from copying, modifying, selling, exporting or using this data set in any way other than for the purpose of completing this course. The lecture slides are released under the CC-BY-SA-4.0 License.  This project is licensed under the Apache-2.0 License. See each section's LICENSE file for details.\n""]"
Responsible AI,AthenaCore/AwesomeResponsibleAI,AthenaCore,https://api.github.com/repos/AthenaCore/AwesomeResponsibleAI,9,5,2,"['https://api.github.com/users/josepcurto', 'https://api.github.com/users/mariolopezdeavila']",,2022-12-24T18:18:25Z,https://raw.githubusercontent.com/AthenaCore/AwesomeResponsibleAI/main/README.md,"['[![Awesome](awesome.svg)](https://github.com/AthenaCore/AwesomeResponsibleAI)\n', '[![Maintenance](https://img.shields.io/badge/Maintained%3F-YES-green.svg)](https://github.com/AthenaCore/AwesomeResponsibleAI/graphs/commit-activity)\n', '![GitHub](https://img.shields.io/badge/Release-PROD-yellow.svg)\n', '![GitHub](https://img.shields.io/badge/Languages-MULTI-blue.svg)\n', '![GitHub](https://img.shields.io/badge/License-MIT-lightgrey.svg)\n', '[![GitHub](https://img.shields.io/twitter/follow/athenacoreai.svg?label=Follow)](https://twitter.com/athenacoreai)\n', '\n', '# Awesome Responsible AI\n', 'A curated list of awesome academic research, books, code of ethics, newsletters, principles, podcast, reports, tools and regulations related to Responsible AI and Human-Centered AI.\n', '\n', '## Contents\n', '\n', '- [Academic Research](#academic-research)\n', '- [Books](#books)\n', '- [Code of Ethics](#code-of-ethics)\n', '- [Data Sets](#data-sets)\n', '- [Institutes](#institutes)\n', '- [Newsletters](#newsletters)\n', '- [Principles](#principles)\n', '- [Podcasts](#podcasts)\n', '- [Reports](#reports)\n', '- [Tools](#tools)\n', '- [Regulations](#regulations)\n', '\n', '## Academic Research\n', '\n', '### Bias\n', '\n', '- Towards a Standard for Identifying and Managing Bias in Artificial Intelligence ([Schwartz, Reva et al., 2022]()) `NIST`\n', '\n', '### Challenges\n', '\n', ""- Underspecification presents challenges for credibility in modern machine learning. ([D'AMOUR, Alexander, et al., 2020](https://arxiv.org/abs/2011.03395)) `Google`\n"", '\n', '### Drift\n', '\n', '- FreaAI: Automated extraction of data slices to test machine learning models ([Ackerman, S. et al. 2021](https://arxiv.org/pdf/2108.05620.pdf)) `IBM`\n', '- Machine Learning Model Drift Detection Via Weak Data Slices ([Ackerman, S. et al. 2021](https://arxiv.org/pdf/2108.05319.pdf)) `IBM`\n', '\n', '### Explainability\n', '\n', '- Efficient Data Representation by Selecting Prototypes with Importance Weights ([Gurumoorthy et al., 2019](https://arxiv.org/abs/1707.01212)) `Amazon Development Center` `IBM Research`\n', '- Explanations based on the Missing: Towards Contrastive Explanations with Pertinent Negatives ([Dhurandhar et al., 2018](https://papers.nips.cc/paper/7340-explanations-based-on-the-missing-towards-contrastive-explanations-with-pertinent-negatives)) `University of Michigan` `IBM Research`\n', '- Contrastive Explanations Method with Monotonic Attribute Functions ([Luss et al., 2019](https://arxiv.org/abs/1905.12698))\n', '- ""Why Should I Trust You?"": Explaining the Predictions of Any Classifier (LIME) ([Ribeiro et al. 2016](https://arxiv.org/abs/1602.04938),  [Github](https://github.com/marcotcr/lime)) `University of Washington`\n', '- A Unified Approach to Interpreting Model Predictions (SHAP) ([Lundberg, et al. 2017](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions),  [Github](https://github.com/slundberg/shap)) `University of Washington`\n', '- Teaching AI to Explain its Decisions ([Hind et al., 2019](https://doi.org/10.1145/3306618.3314273)) `IBM Research`\n', '- Boolean Decision Rules via Column Generation (Light Edition) ([Dash et al., 2018](https://papers.nips.cc/paper/7716-boolean-decision-rules-via-column-generation)) `IBM Research`\n', '- Generalized Linear Rule Models ([Wei et al., 2019](http://proceedings.mlr.press/v97/wei19a.html)) `IBM Research`\n', '- Improving Simple Models with Confidence Profiles ([Dhurandhar et al., 2018](https://papers.nips.cc/paper/8231-improving-simple-models-with-confidence-profiles)) `IBM Research`\n', '- Towards Robust Interpretability with Self-Explaining Neural Networks ([Alvarez-Melis et al., 2018](https://papers.nips.cc/paper/8003-towards-robust-interpretability-with-self-explaining-neural-networks)) `MIT`\n', '- Leveraging Latent Features for Local Explanations ([Luss et al., 2019](https://arxiv.org/abs/1905.12698)) `IBM Research` `University of Michigan`\n', '\n', '### Fairness\n', '\n', '- [LiFT: A Scalable Framework for Measuring Fairness in ML Applications](https://engineering.linkedin.com/blog/2020/lift-addressing-bias-in-large-scale-ai-applications) ([Vasuvedan et al., 2020](https://arxiv.org/abs/2008.07433)) `LinkedIn`\n', '\n', '### Ethical Data Products\n', '\n', '- [Building Inclusive Products Through A/B Testing](https://engineering.linkedin.com/blog/2020/building-inclusive-products-through-a-b-testing) ([Saint-Jacques et al, 2020](https://arxiv.org/pdf/2002.05819)) `LinkedIn`\n', '\n', '### Sustainability\n', '\n', '- Energy and policy considerations for deep learning in NLP ([Strubell, E. et al. 2019](https://arxiv.org/abs/1906.02243))\n', '- Quantifying the carbon emissions of machine learning. ([Lacoste, A. et al. 2019](https://arxiv.org/abs/1910.09700))\n', '- Carbon emissions and large neural network training. ([Patterson, D. et al. 2021](https://arxiv.org/abs/2104.10350)) \n', '- The Energy and Carbon Footprint of Training End-to-End Speech Recognizers. ([Parcollet, T., & Ravanelli, M. 2021](https://hal.archives-ouvertes.fr/hal-03190119/document))\n', '- Sustainable AI: AI for sustainability and the sustainability of AI ([van Wynsberghe, A. 2021](https://link.springer.com/article/10.1007/s43681-021-00043-6)). AI and Ethics, 1-6\n', '- Green Algorithms: Quantifying the carbon emissions of computation ([Lannelongue, L. et al. 2020](https://arxiv.org/abs/2007.07610))\n', '- Machine Learning: The High Interest Credit Card of Technical Debt ([Sculley, D. et al. 2014](https://research.google/pubs/pub43146/)) `Google`\n', '\n', '### Collections\n', '\n', '- Google Research on Responsible AI: https://research.google/pubs/?collection=responsible-ai `Google`\n', '\n', '## Books\n', '\n', '### Open Access\n', '\n', '- Interpretable Machine Learning ([Molnar, C., 2021](https://christophm.github.io/interpretable-ml-book/)) `Explainability` `Interpretability` `Transparency` `R`\n', '- Explanatory Model Analysis ([Biecek et al., 2020](https://ema.drwhy.ai)) `Explainability` `Interpretability` `Transparency` `R`\n', '\n', '### Commercial / Propietary / Closed Access\n', '\n', '- Trust in Machine Learning ([Varshney, K., 2022](https://www.manning.com/books/trust-in-machine-learning)) `Safety` `Privacy` `Drift` `Fairness` `Interpretability` `Explainability`\n', '- Interpretable AI ([Thampi, A., 2022](https://www.manning.com/books/interpretable-ai)) `Explainability` `Fairness` `Interpretability` \n', '- AI Fairness ([Mahoney, T., Varshney, K.R., Hind, M., 2020](https://learning.oreilly.com/library/view/ai-fairness/9781492077664/) `Report` `Fairness`\n', '- Practical Fairness ([Nielsen, A., 2021](https://learning.oreilly.com/library/view/practical-fairness/9781492075721/)) `Fairness`\n', '- Hands-On Explainable AI (XAI) with Python ([Rothman, D., 2020](https://www.packtpub.com/product/hands-on-explainable-ai-xai-with-python/9781800208131)) `Explainability`\n', '- AI and the Law ([Kilroy, K., 2021](https://learning.oreilly.com/library/view/ai-and-the/9781492091837/)) `Report` `Trust` `Law`\n', '- Responsible Machine Learning ([Hall, P., Gill, N., Cox, B., 2020](https://learning.oreilly.com/library/view/responsible-machine-learning/9781492090878/)) `Report` `Law`  `Compliance` `Safety` `Privacy` \n', '- [Privacy-Preserving Machine Learning](https://www.manning.com/books/privacy-preserving-machine-learning)\n', '- [Human-In-The-Loop Machine Learning: Active Learning and Annotation for Human-Centered AI](https://www.manning.com/books/human-in-the-loop-machine-learning)\n', '- [Interpretable Machine Learning With Python: Learn to Build Interpretable High-Performance Models With Hands-On Real-World Examples](https://www.packtpub.com/product/interpretable-machine-learning-with-python/9781800203907)\n', '- Responsible AI ([Hall, P., Chowdhury, R., 2023](https://learning.oreilly.com/library/view/responsible-ai/9781098102425/)) `Governance` `Safety` `Drift`\n', '\n', '## Code of Ethics\n', '\n', '- [ACS Code of Professional Conduct](https://www.acs.org.au/content/dam/acs/rules-and-regulations/Code-of-Professional-Conduct_v2.1.pdf) by Australian ICT (Information and Communication Technology)\n', '- [AI Standards Hub](https://aistandardshub.org)\n', ""- [Association for Computer Machinery's Code of Ethics and Professional Conduct](https://www.acm.org/code-of-ethics)\n"", '- [IEEE Global Initiative for Ethical Considerations in Artificial Intelligence (AI) and Autonomous Systems (AS)](https://ethicsinaction.ieee.org/)\n', ""- [ISO/IEC's Standards for Artificial Intelligence](https://www.iso.org/committee/6794475/x/catalogue/)\n"", '- [Ethics guidelines for trustworthy AI](https://op.europa.eu/en/publication-detail/-/publication/d3988569-0434-11ea-8c1f-01aa75ed71a1/language-en/format-PDF/source-229277158) - European Commission document prepared by the High-Level Expert Group on Artificial Intelligence (AI HLEG).\n', '- [Google AI Principles](https://ai.google/principles/)\n', '- [Microsoft AI Principles](https://www.microsoft.com/en-us/ai/responsible-ai)\n', '\n', '## Data Sets\n', '\n', '- [An ImageNet replacement for self-supervised pretraining without humans](https://www.robots.ox.ac.uk/~vgg/research/pass/)\n', '- [Huggingface Data Sets](https://huggingface.co/datasets)\n', '\n', '## Institutes\n', '\n', '- [Ada Lovelace Institute](https://www.adalovelaceinstitute.org/)\n', '- [European Centre for Algorithmic Transparency](https://algorithmic-transparency.ec.europa.eu/index_en)\n', '- [Center for Responsible AI](https://airesponsibly.com/)\n', '- [Montreal AI Ethics Institute](https://montrealethics.ai/)\n', '- [Munich Center for Technology in Society (IEAI)](https://ieai.mcts.tum.de/)\n', '- [Open Data Institute](https://theodi.org/)\n', '- [Stanford University Human-Centered Artificial Intelligence (HAI)](https://hai.stanford.edu)\n', '- [The Institute for Ethical AI & Machine Learning](https://ethical.institute/)\n', '- [University of Oxford Institute for Ethics in AI](https://www.oxford-aiethics.ox.ac.uk/)\n', '\n', '## Newsletters\n', '\n', '- [Import AI](https://jack-clark.net)\n', '- [The AI Ethics Brief](https://brief.montrealethics.ai)\n', '- [The Machine Learning Engineer](https://ethical.institute/mle.html) \n', '\n', '## Principles\n', '\n', ""- [European Commission's Guidelines for Trustworthy AI](https://ec.europa.eu/futurium/en/ai-alliance-consultation)\n"", ""- [IEEE's Ethically Aligned Design](https://ethicsinaction.ieee.org/)\n"", '- [The Institute for Ethical AI & Machine Learning: The Responsible Machine Learning Principles](https://ethical.institute/principles.html)\n', '\n', 'Additional:\n', '\n', '- [FAIR Principles](https://www.go-fair.org/fair-principles/) `Findability` `Accessibility` `Interoperability` `Reuse`\n', '\n', '## Podcasts\n', '\n', '- [The Human-Centered AI Podcast](https://podcasts.apple.com/us/podcast/the-human-centered-ai-podcast/id1499839858)\n', '- [Responsible AI Podcast](https://open.spotify.com/show/63Fx70r96P3ghWavisvPEQ)\n', '- [Trustworthy AI](https://marketing.truera.com/trustworthy-ai-podcast)\n', '\n', '## Reports\n', '\n', '- [Inferring Concept Drift Without Labeled Data, 2021](https://concept-drift.fastforwardlabs.com) `Drift`\n', '- [Interpretability, Fast Forward Labs, 2020](https://ff06-2020.fastforwardlabs.com) `Interpretability`\n', '- [State of AI](https://www.stateof.ai) - from 2018 up to now -\n', '\n', '## Tools\n', '\n', '### Bias\n', '\n', '- [balance](https://import-balance.org) `Python` `Facebook`\n', '\n', '### Causal Inference\n', '\n', '- [CausalAI](https://github.com/salesforce/causalai) `Python` `Salesforce`\n', '- [CausalNex](https://causalnex.readthedocs.io) `Python`\n', '- [CausalImpact](https://cran.r-project.org/web/packages/CausalImpact) `R`\n', '- [Causalinference](https://causalinferenceinpython.org) `Python`\n', '- [CIMTx: Causal Inference for Multiple Treatments with a Binary Outcome](https://cran.r-project.org/web/packages/CIMTx) `R`\n', '- [dagitty](https://cran.r-project.org/web/packages/dagitty) `R`\n', '- [DoWhy](https://github.com/Microsoft/dowhy) `Python` `Microsoft`\n', '- [mediation: Causal Mediation Analysis](https://cran.r-project.org/web/packages/mediation) `R`\n', '- [MRPC](https://cran.r-project.org/web/packages/MRPC) `R`\n', '\n', '### Differential Privacy\n', '\n', '- [BackPACK](https://toiaydcdyywlhzvlob.github.io/backpack) `Python`\n', '- [DataSynthesizer: Privacy-Preserving Synthetic Datasets](https://github.com/DataResponsibly/DataSynthesizer) `Python` `Drexel University` `University of Washington`\n', '- [diffpriv](https://github.com/brubinstein/diffpriv) `R`\n', '- [Diffprivlib](https://github.com/IBM/differential-privacy-library) `Python` `IBM`\n', '- [Discrete Gaussian for Differential Privacy](https://github.com/IBM/discrete-gaussian-differential-privacy) `Python` `IBM`\n', '- [Opacus](https://opacus.ai) `Python` `Facebook`\n', '- [PyVacy: Privacy Algorithms for PyTorch](https://github.com/ChrisWaites/pyvacy) `Python`\n', '- [SEAL](https://github.com/Microsoft/SEAL) `Python` `Microsoft`\n', '- [SmartNoise](https://github.com/opendp/smartnoise-core) `Python` `OpenDP`\n', '- [Tensorflow Privacy](https://github.com/tensorflow/privacy) `Python` `Google`\n', '\n', '### Drift\n', '\n', '- [Alibi Detect](https://github.com/SeldonIO/alibi-detect) `Python`\n', '- [Deepchecks](https://github.com/deepchecks/deepchecks) `Python`\n', '- [drifter](https://cran.r-project.org/web/packages/drifter/) `R`\n', '- [Evidently](https://github.com/evidentlyai/evidently) `Python`\n', '- [nannyML](https://github.com/NannyML/nannyml) `Python`\n', '\n', '### Fairness\n', '\n', ""- [Aequitas' Bias & Fairness Audit Toolkit](http://aequitas.dssg.io/) `Python`\n"", '- [AI360 Toolkit](https://github.com/Trusted-AI/AIF360) `Python` `R` `IBM`\n', '- [Fairlearn](https://fairlearn.org) `Python` `Microsoft`\n', '- [Fairmodels](https://fairmodels.drwhy.ai) `R`\n', '- [fairness](https://cran.r-project.org/web/packages/fairness/) `R`\n', '- [FairPAN - Fair Predictive Adversarial Network](https://modeloriented.github.io/FairPAN/) `R`\n', '- [Themis ML](https://github.com/cosmicBboy/themis-ml) `Python`\n', '- [What-If Tool](https://github.com/PAIR-code/what-if-tool) `Python` `Google`\n', '\n', '### Interpretability/Explicability\n', '\n', '- [AI360 Toolkit](https://github.com/Trusted-AI/AIF360) `Python` `R` `IBM`\n', '- [aorsf: Accelerated Oblique Random Survival Forests](https://cran.r-project.org/web/packages/aorsf/index.html) `R`\n', '- [breakDown: Model Agnostic Explainers for Individual Predictions](https://cran.r-project.org/web/packages/breakDown/index.html) `R`\n', '- [ceterisParibus: Ceteris Paribus Profiles](https://cran.r-project.org/web/packages/ceterisParibus/index.html) `R`\n', '- [DALEX: moDel Agnostic Language for Exploration and eXplanation](https://dalex.drwhy.ai) `Python` `R`\n', '- [DALEXtra: extension for DALEX](https://modeloriented.github.io/DALEXtra) `Python` `R`\n', '- [ecco](https://pypi.org/project/ecco/) [article](https://jalammar.github.io/explaining-transformers/) `Python`\n', '- [eli5](https://github.com/TeamHG-Memex/eli5) `Python`\n', '- [eXplainability Toolbox](https://ethical.institute/xai.html) `Python`\n', '- [ExplainerHub](https://explainerdashboard.readthedocs.io/en/latest/index.html) [in github](https://github.com/oegedijk/explainerdashboard) `Python` \n', '- [fasttreeshap](https://github.com/linkedin/fasttreeshap) `Python` `LinkedIn`\n', '- [FAT Forensics](https://fat-forensics.org/) `Python`\n', '- [intepretML](https://interpret.ml) `Python`\n', '- [kernelshap: Kernel SHAP](https://cran.r-project.org/web/packages/kernelshap/index.html) `R`\n', '- [lime: Local Interpretable Model-Agnostic Explanations](https://cran.r-project.org/web/packages/lime/index.html) `R`\n', '- [Shap](https://github.com/slundberg/shap) `Python`\n', '- [Shapash](https://github.com/maif/shapash) `Python`\n', '- [shapviz](https://cran.r-project.org/web/packages/shapviz/index.html) `R`\n', '- [Skater](https://github.com/oracle/Skater) `Python` `Oracle`\n', '- [survex](https://github.com/ModelOriented/survex) `R`\n', '- [pre: Prediction Rule Ensembles](https://cran.r-project.org/web/packages/pre/index.html) `R`\n', '- [interactions: Comprehensive, User-Friendly Toolkit for Probing Interactions](https://cran.r-project.org/web/packages/interactions/index.html) `R`\n', '- [Zennit](https://github.com/chr5tphr/zennit) `Python`\n', '\n', '### Performance (& Automated ML)\n', '\n', '- [automl: Deep Learning with Metaheuristic](https://cran.r-project.org/web/packages/automl/index.html) `R`\n', '- [AutoKeras](https://github.com/keras-team/autokeras) `Python`\n', '- [Auto-Sklearn](https://github.com/automl/auto-sklearn) `Python`\n', '- [deepchecks](https://deepchecks.com) `Python`\n', '- [Featuretools](https://www.featuretools.com) `Python`\n', '- [forester](https://modeloriented.github.io/forester/) `R`\n', '- [metrica: Prediction performance metrics](https://adriancorrendo.github.io/metrica/) `R`\n', '- [NNI: Neural Network Intelligence](https://github.com/microsoft/nni) `Python` `Microsoft`\n', '- [TPOT](http://epistasislab.github.io/tpot/) `Python`\n', '- [Unleash](https://www.getunleash.io)\n', '\n', '### Responsible AI toolkit\n', '\n', '- [Dr. Why](https://github.com/ModelOriented/DrWhy) `R` `Warsaw University of Technology`\n', '- [Responsible AI Widgets](https://github.com/microsoft/responsible-ai-widgets) `R` `Microsoft`\n', '- [The Data Cards Playbook](https://pair-code.github.io/datacardsplaybook/)\n', '\n', '### Sustainability\n', '\n', '- [Code Carbon](https://github.com/mlco2/codecarbon) `Python`\n', '- [Azure Sustainability Calculator](https://appsource.microsoft.com/en-us/product/power-bi/coi-sustainability.sustainability_dashboard) `Microsoft`\n', '- [Computer Progress](https://www.computerprogress.com)\n', '\n', '## Reproducible Research\n', '\n', '- [Papers with Code](https://paperswithcode.com)\n', '- [Papers without Code](https://www.paperswithoutcode.com)\n', '\n', '## Regulations\n', '\n', '- [Data Protection Laws of the Word](https://www.dlapiperdataprotection.com)\n', '\n', '### European Union\n', '\n', '- [General Data Protection Regulation GDPR](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=celex%3A32016R0679) - Legal text for the EU GDPR regulation 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC\n', '- [GDPR.EU Guide](https://gdpr.eu/) - A project co-funded by the Horizon 2020 Framework programme of the EU which provides a resource for organisations and individuals researching GDPR, including a library of straightforward and up-to-date information to help organisations achieve GDPR compliance ([Legal Text](https://www.govinfo.gov/content/pkg/USCODE-2012-title5/pdf/USCODE-2012-title5-partI-chap5-subchapII-sec552a.pdf)).\n', '\n', '### United States\n', '\n', '- State consumer privacy laws: California ([CCPA](https://www.oag.ca.gov/privacy/ccpa) and its amendment, [CPRA](https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202120220AB1490)), Virginia ([VCDPA](https://lis.virginia.gov/cgi-bin/legp604.exe?212+sum+HB2307)), and Colorado ([ColoPA](https://leg.colorado.gov/sites/default/files/documents/2021A/bills/2021a_190_rer.pdf)).\n', '- Specific and limited privacy data laws: [HIPAA](https://www.cdc.gov/phlp/publications/topic/hipaa.html), [FCRA](https://www.ftc.gov/enforcement/statutes/fair-credit-reporting-act), [FERPA](https://www.cdc.gov/phlp/publications/topic/ferpa.html), [GLBA](https://www.ftc.gov/tips-advice/business-center/privacy-and-security/gramm-leach-bliley-act), [ECPA](https://bja.ojp.gov/program/it/privacy-civil-liberties/authorities/statutes/1285), [COPPA](https://www.ftc.gov/enforcement/rules/rulemaking-regulatory-reform-proceedings/childrens-online-privacy-protection-rule), [VPPA](https://www.law.cornell.edu/uscode/text/18/2710) and [FTC](https://www.ftc.gov/enforcement/statutes/federal-trade-commission-act).\n', '- [EU-U.S. and Swiss-U.S. Privacy Shield Frameworks](https://www.privacyshield.gov/welcome) - The EU-U.S. and Swiss-U.S. Privacy Shield Frameworks were designed by the U.S. Department of Commerce and the European Commission and Swiss Administration to provide companies on both sides of the Atlantic with a mechanism to comply with data protection requirements when transferring personal data from the European Union and Switzerland to the United States in support of transatlantic commerce.\n', '- [Executive Order on Maintaining American Leadership in AI](https://www.whitehouse.gov/presidential-actions/executive-order-maintaining-american-leadership-artificial-intelligence/) - Official mandate by the President of the US to \n', '[Privacy Act of 1974](https://www.justice.gov/opcl/privacy-act-1974) - The privacy act of 1974 which establishes a code of fair information practices that governs the collection, maintenance, use and dissemination of information about individuals that is maintained in systems of records by federal agencies.\n', '- [Privacy Protection Act of 1980](https://epic.org/privacy/ppa/) - The Privacy Protection Act of 1980 protects journalists from being required to turn over to law enforcement any work product and documentary materials, including sources, before it is disseminated to the public.\n', '- [AI Bill of Rights](https://www.whitehouse.gov/ostp/ai-bill-of-rights/) - The Blueprint for an AI Bill of Rights is a guide for a society that protects all people from IA threats based on five principles: Safe and Effective Systems, Algorithmic Discrimination Protections, Data Privacy, Notice and Explanation, and  Human Alternatives, Consideration, and Fallback.\n']"