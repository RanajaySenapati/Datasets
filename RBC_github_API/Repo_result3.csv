keyword,git_name,owner_name,git_url,star_count,forks_count,contributor_count,contributors_list,code_language,last_updated_date,readme_url,readme_text
Model Explainability,interpretml/interpret,interpretml,https://api.github.com/repos/interpretml/interpret,5457,664,30,"['https://api.github.com/users/interpret-ml', 'https://api.github.com/users/paulbkoch', 'https://api.github.com/users/msplants', 'https://api.github.com/users/luisffranca', 'https://api.github.com/users/wamartin-aml', 'https://api.github.com/users/Harsha-Nori', 'https://api.github.com/users/nopdive', 'https://api.github.com/users/dependabot%5Bbot%5D', 'https://api.github.com/users/imatiach-msft', 'https://api.github.com/users/ecederstrand', 'https://api.github.com/users/Ashton-Sidhu', 'https://api.github.com/users/blengerich', 'https://api.github.com/users/bamdevm', 'https://api.github.com/users/xiaohk', 'https://api.github.com/users/microsoftopensource', 'https://api.github.com/users/zhangxz1123', 'https://api.github.com/users/mtl-tony', 'https://api.github.com/users/ajyl', 'https://api.github.com/users/raethlein', 'https://api.github.com/users/daikikatsuragawa', 'https://api.github.com/users/eddy-geek', 'https://api.github.com/users/gliptak', 'https://api.github.com/users/itsoum', 'https://api.github.com/users/jruales', 'https://api.github.com/users/mczhu', 'https://api.github.com/users/msftgits', 'https://api.github.com/users/prateekiiest', 'https://api.github.com/users/vbernardes', 'https://api.github.com/users/chhetri22', 'https://api.github.com/users/ncherrier']",C++,2023-04-26T13:22:54Z,https://raw.githubusercontent.com/interpretml/interpret/develop/README.md,"['# InterpretML\n', '\n', '[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/interpretml/interpret/develop?labpath=examples%2Fpython%2FInterpretable_Classification_Methods.ipynb)\n', '![License](https://img.shields.io/github/license/interpretml/interpret.svg?style=flat-square)\n', '![Python Version](https://img.shields.io/pypi/pyversions/interpret.svg?style=flat-square)\n', '![Package Version](https://img.shields.io/pypi/v/interpret.svg?style=flat-square)\n', '![Conda](https://img.shields.io/conda/v/conda-forge/interpret)\n', '![Build Status](https://img.shields.io/azure-devops/build/ms/interpret/293/develop.svg?style=flat-square)\n', '![Coverage](https://img.shields.io/azure-devops/coverage/ms/interpret/293/develop.svg?style=flat-square)\n', '![Maintenance](https://img.shields.io/maintenance/yes/2023?style=flat-square)\n', '<br/>\n', '> ### In the beginning machines learned in darkness, and data scientists struggled in the void to explain them. \n', '> ### Let there be light.\n', '\n', ""InterpretML is an open-source package that incorporates state-of-the-art machine learning interpretability techniques under one roof. With this package, you can train interpretable glassbox models and explain blackbox systems. InterpretML helps you understand your model's global behavior, or understand the reasons behind individual predictions.\n"", '\n', 'Interpretability is essential for:\n', '- Model debugging - Why did my model make this mistake?\n', '- Feature Engineering - How can I improve my model?\n', '- Detecting fairness issues - Does my model discriminate?\n', ""- Human-AI cooperation - How can I understand and trust the model's decisions?\n"", '- Regulatory compliance - Does my model satisfy legal requirements?\n', '- High-risk applications - Healthcare, finance, judicial, ...\n', '\n', '![](https://github.com/interpretml/interpretml.github.io/blob/master/interpret-highlight.gif)\n', '\n', '# Installation\n', '\n', 'Python 3.7+ | Linux, Mac, Windows\n', '```sh\n', 'pip install interpret\n', '# OR\n', 'conda install -c conda-forge interpret\n', '```\n', '\n', '# Introducing the Explainable Boosting Machine (EBM)\n', '\n', 'EBM is an interpretable model developed at Microsoft Research<sup>[*](#citations)</sup>. It uses modern machine learning techniques like bagging, gradient boosting, and automatic interaction detection to breathe new life into traditional GAMs (Generalized Additive Models). This makes EBMs as accurate as state-of-the-art techniques like random forests and gradient boosted trees. However, unlike these blackbox models, EBMs produce exact explanations and are editable by domain experts.\n', '\n', '| Dataset/AUROC | Domain  | Logistic Regression | Random Forest | XGBoost         | Explainable Boosting Machine |\n', '|---------------|---------|:-------------------:|:-------------:|:---------------:|:----------------------------:|\n', '| Adult Income  | Finance | .907±.003           | .903±.002     | .927±.001       | **_.928±.002_**              |\n', '| Heart Disease | Medical | .895±.030           | .890±.008     | .851±.018       | **_.898±.013_**              |\n', '| Breast Cancer | Medical | **_.995±.005_**     | .992±.009     | .992±.010       | **_.995±.006_**              |\n', '| Telecom Churn | Business| .849±.005           | .824±.004     | .828±.010       | **_.852±.006_**              |\n', '| Credit Fraud  | Security| .979±.002           | .950±.007     | **_.981±.003_** | **_.981±.003_**              |\n', '\n', '[*Notebook for reproducing table*](https://nbviewer.jupyter.org/github/interpretml/interpret/blob/master/benchmarks/EBM%20Classification%20Comparison.ipynb)\n', '\n', '# Supported Techniques\n', '\n', '| Interpretability Technique  | Type               |\n', '|-----------------------------|--------------------|\n', '| [Explainable Boosting](https://interpret.ml/docs/ebm.html)        | glassbox model     |\n', '| [Decision Tree](https://interpret.ml/docs/dt.html)                | glassbox model     |\n', '| [Decision Rule List](https://interpret.ml/docs/dr.html)           | glassbox model     |\n', '| [Linear/Logistic Regression](https://interpret.ml/docs/lr.html)   | glassbox model     |\n', '| [SHAP Kernel Explainer](https://interpret.ml/docs/shap.html)      | blackbox explainer |\n', '| [LIME](https://interpret.ml/docs/lime.html)                       | blackbox explainer |\n', '| [Morris Sensitivity Analysis](https://interpret.ml/docs/msa.html) | blackbox explainer |\n', '| [Partial Dependence](https://interpret.ml/docs/pdp.html)          | blackbox explainer |\n', '\n', '# Train a glassbox model\n', '\n', ""Let's fit an Explainable Boosting Machine\n"", '\n', '```python\n', 'from interpret.glassbox import ExplainableBoostingClassifier\n', '\n', 'ebm = ExplainableBoostingClassifier()\n', 'ebm.fit(X_train, y_train)\n', '\n', '# or substitute with LogisticRegression, DecisionTreeClassifier, RuleListClassifier, ...\n', '# EBM supports pandas dataframes, numpy arrays, and handles ""string"" data natively.\n', '```\n', '\n', 'Understand the model\n', '```python\n', 'from interpret import show\n', '\n', 'ebm_global = ebm.explain_global()\n', 'show(ebm_global)\n', '```\n', '![Global Explanation Image](./examples/python/assets/readme_ebm_global_specific.PNG?raw=true)\n', '\n', '<br/>\n', '\n', 'Understand individual predictions\n', '```python\n', 'ebm_local = ebm.explain_local(X_test, y_test)\n', 'show(ebm_local)\n', '```\n', '![Local Explanation Image](./examples/python/assets/readme_ebm_local_specific.PNG?raw=true)\n', '\n', '<br/>\n', '\n', 'And if you have multiple model explanations, compare them\n', '```python\n', 'show([logistic_regression_global, decision_tree_global])\n', '```\n', '![Dashboard Image](./examples/python/assets/readme_dashboard.PNG?raw=true)\n', '\n', '<br/>\n', '\n', 'If you need to keep your data private, use Differentially Private EBMs (see [DP-EBMs](https://proceedings.mlr.press/v139/nori21a/nori21a.pdf))\n', '\n', '```python\n', 'from interpret.privacy import DPExplainableBoostingClassifier, DPExplainableBoostingRegressor\n', '\n', 'dp_ebm = DPExplainableBoostingClassifier(epsilon=1, delta=1e-5) # Specify privacy parameters\n', 'dp_ebm.fit(X_train, y_train)\n', '\n', 'show(dp_ebm.explain_global()) # Identical function calls to standard EBMs\n', '```\n', '\n', '<br/>\n', '<br/>\n', '\n', 'For more information, see the [documentation](https://interpret.ml/docs/getting-started.html).\n', '<br/>\n', '<br/>\n', '\n', '# Acknowledgements\n', '\n', 'InterpretML was originally created by (equal contributions): Samuel Jenkins, Harsha Nori, Paul Koch, and Rich Caruana\n', '\n', 'EBMs are fast derivative of GA2M, invented by: Yin Lou, Rich Caruana, Johannes Gehrke, and Giles Hooker\n', '\n', 'Many people have supported us along the way. Check out [ACKNOWLEDGEMENTS.md](./ACKNOWLEDGEMENTS.md)!\n', '\n', 'We also build on top of many great packages. Please check them out!\n', '\n', '[plotly](https://github.com/plotly/plotly.py) |\n', '[dash](https://github.com/plotly/dash) |\n', '[scikit-learn](https://github.com/scikit-learn/scikit-learn) |\n', '[lime](https://github.com/marcotcr/lime) |\n', '[shap](https://github.com/slundberg/shap) |\n', '[salib](https://github.com/SALib/SALib) |\n', '[skope-rules](https://github.com/scikit-learn-contrib/skope-rules) |\n', '[treeinterpreter](https://github.com/andosa/treeinterpreter) |\n', '[gevent](https://github.com/gevent/gevent) |\n', '[joblib](https://github.com/joblib/joblib) |\n', '[pytest](https://github.com/pytest-dev/pytest) |\n', '[jupyter](https://github.com/jupyter/notebook)\n', '\n', '# <a name=""citations"">Citations</a>\n', '\n', '<details open>\n', '  <summary><strong>InterpretML</strong></summary>\n', '  <hr/>\n', '\n', '  <details open>\n', '    <summary>\n', '      <em>""InterpretML: A Unified Framework for Machine Learning Interpretability"" (H. Nori, S. Jenkins, P. Koch, and R. Caruana 2019)</em>\n', '    </summary>\n', '    <br/>\n', '    <pre>\n', '@article{nori2019interpretml,\n', '  title={InterpretML: A Unified Framework for Machine Learning Interpretability},\n', '  author={Nori, Harsha and Jenkins, Samuel and Koch, Paul and Caruana, Rich},\n', '  journal={arXiv preprint arXiv:1909.09223},\n', '  year={2019}\n', '}\n', '    </pre>\n', '    <a href=""https://arxiv.org/pdf/1909.09223.pdf"">Paper link</a>\n', '  </details>\n', '\n', '  <hr/>\n', '</details>\n', '\n', '<details>\n', '  <summary><strong>Explainable Boosting</strong></summary>\n', '  <hr/>\n', '\n', '  <details>\n', '    <summary>\n', '      <em>""Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission"" (R. Caruana, Y. Lou, J. Gehrke, P. Koch, M. Sturm, and N. Elhadad 2015)</em>\n', '    </summary>\n', '    <br/>\n', '    <pre>\n', '@inproceedings{caruana2015intelligible,\n', '  title={Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission},\n', '  author={Caruana, Rich and Lou, Yin and Gehrke, Johannes and Koch, Paul and Sturm, Marc and Elhadad, Noemie},\n', '  booktitle={Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},\n', '  pages={1721--1730},\n', '  year={2015},\n', '  organization={ACM}\n', '}\n', '    </pre>\n', '    <a href=""https://www.microsoft.com/en-us/research/wp-content/uploads/2017/06/KDD2015FinalDraftIntelligibleModels4HealthCare_igt143e-caruanaA.pdf"">Paper link</a>\n', '  </details>\n', '\n', '  <details>\n', '    <summary>\n', '      <em>""Accurate intelligible models with pairwise interactions"" (Y. Lou, R. Caruana, J. Gehrke, and G. Hooker 2013)</em>\n', '    </summary>\n', '    <br/>\n', '    <pre>\n', '@inproceedings{lou2013accurate,\n', '  title={Accurate intelligible models with pairwise interactions},\n', '  author={Lou, Yin and Caruana, Rich and Gehrke, Johannes and Hooker, Giles},\n', '  booktitle={Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining},\n', '  pages={623--631},\n', '  year={2013},\n', '  organization={ACM}\n', '}\n', '    </pre>\n', '    <a href=""https://www.cs.cornell.edu/~yinlou/papers/lou-kdd13.pdf"">Paper link</a>\n', '  </details>\n', '\n', '  <details>\n', '    <summary>\n', '      <em>""Intelligible models for classification and regression"" (Y. Lou, R. Caruana, and J. Gehrke 2012)</em>\n', '    </summary>\n', '    <br/>\n', '    <pre>\n', '@inproceedings{lou2012intelligible,\n', '  title={Intelligible models for classification and regression},\n', '  author={Lou, Yin and Caruana, Rich and Gehrke, Johannes},\n', '  booktitle={Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining},\n', '  pages={150--158},\n', '  year={2012},\n', '  organization={ACM}\n', '}\n', '    </pre>\n', '    <a href=""https://www.cs.cornell.edu/~yinlou/papers/lou-kdd12.pdf"">Paper link</a>\n', '  </details>\n', '\n', '  <details>\n', '    <summary>\n', '      <em>""Interpretability, Then What? Editing Machine Learning Models to Reflect Human Knowledge and Values"" (Zijie J. Wang, Alex Kale, Harsha Nori, Peter Stella, Mark E. Nunnally, Duen Horng Chau, Mihaela Vorvoreanu, Jennifer Wortman Vaughan, Rich Caruana 2022)</em>\n', '    </summary>\n', '    <br/>\n', '    <pre>\n', '@article{wang2022interpretability,\n', '  title={Interpretability, Then What? Editing Machine Learning Models to Reflect Human Knowledge and Values},\n', '  author={Wang, Zijie J and Kale, Alex and Nori, Harsha and Stella, Peter and Nunnally, Mark E and Chau, Duen Horng and Vorvoreanu, Mihaela and Vaughan, Jennifer Wortman and Caruana, Rich},\n', '  journal={arXiv preprint arXiv:2206.15465},\n', '  year={2022}\n', '}\n', '    </pre>\n', '    <a href=""https://arxiv.org/pdf/2206.15465.pdf"">Paper link</a>\n', '  </details>\n', '\n', '  <details>\n', '    <summary>\n', '      <em>""Axiomatic Interpretability for Multiclass Additive Models"" (X. Zhang, S. Tan, P. Koch, Y. Lou, U. Chajewska, and R. Caruana 2019)</em>\n', '    </summary>\n', '    <br/>\n', '    <pre>\n', '@inproceedings{zhang2019axiomatic,\n', '  title={Axiomatic Interpretability for Multiclass Additive Models},\n', '  author={Zhang, Xuezhou and Tan, Sarah and Koch, Paul and Lou, Yin and Chajewska, Urszula and Caruana, Rich},\n', '  booktitle={Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \\& Data Mining},\n', '  pages={226--234},\n', '  year={2019},\n', '  organization={ACM}\n', '}\n', '    </pre>\n', '    <a href=""https://arxiv.org/pdf/1810.09092.pdf"">Paper link</a>\n', '  </details>\n', '\n', '  <details>\n', '    <summary>\n', '      <em>""Distill-and-compare: auditing black-box models using transparent model distillation"" (S. Tan, R. Caruana, G. Hooker, and Y. Lou 2018)</em>\n', '    </summary>\n', '    <br/>\n', '    <pre>\n', '@inproceedings{tan2018distill,\n', '  title={Distill-and-compare: auditing black-box models using transparent model distillation},\n', '  author={Tan, Sarah and Caruana, Rich and Hooker, Giles and Lou, Yin},\n', '  booktitle={Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},\n', '  pages={303--310},\n', '  year={2018},\n', '  organization={ACM}\n', '}\n', '    </pre>\n', '    <a href=""https://arxiv.org/pdf/1710.06169"">Paper link</a>\n', '  </details>\n', '\n', '  <details>\n', '    <summary>\n', '      <em>""Purifying Interaction Effects with the Functional ANOVA: An Efficient Algorithm for Recovering Identifiable Additive Models"" (B. Lengerich, S. Tan, C. Chang, G. Hooker, R. Caruana 2019)</em>\n', '    </summary>\n', '    <br/>\n', '    <pre>\n', '@article{lengerich2019purifying,\n', '  title={Purifying Interaction Effects with the Functional ANOVA: An Efficient Algorithm for Recovering Identifiable Additive Models},\n', '  author={Lengerich, Benjamin and Tan, Sarah and Chang, Chun-Hao and Hooker, Giles and Caruana, Rich},\n', '  journal={arXiv preprint arXiv:1911.04974},\n', '  year={2019}\n', '}\n', '    </pre>\n', '    <a href=""https://arxiv.org/pdf/1911.04974.pdf"">Paper link</a>\n', '  </details>\n', '\n', '  <details>\n', '    <summary>\n', '      <em>""Interpreting Interpretability: Understanding Data Scientists\' Use of Interpretability Tools for Machine Learning"" (H. Kaur, H. Nori, S. Jenkins, R. Caruana, H. Wallach, J. Wortman Vaughan 2020)</em>\n', '    </summary>\n', '    <br/>\n', '    <pre>\n', '@inproceedings{kaur2020interpreting,\n', ""  title={Interpreting Interpretability: Understanding Data Scientists' Use of Interpretability Tools for Machine Learning},\n"", '  author={Kaur, Harmanpreet and Nori, Harsha and Jenkins, Samuel and Caruana, Rich and Wallach, Hanna and Wortman Vaughan, Jennifer},\n', '  booktitle={Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},\n', '  pages={1--14},\n', '  year={2020}\n', '}\n', '    </pre>\n', '    <a href=""https://www.microsoft.com/en-us/research/publication/interpreting-interpretability-understanding-data-scientists-use-of-interpretability-tools-for-machine-learning/"">Paper link</a>\n', '  </details>\n', '\n', '  <details>\n', '    <summary>\n', '      <em>""How Interpretable and Trustworthy are GAMs?"" (C. Chang, S. Tan, B. Lengerich, A. Goldenberg, R. Caruana 2020)</em>\n', '    </summary>\n', '    <br/>\n', '    <pre>\n', '@article{chang2020interpretable,\n', '  title={How Interpretable and Trustworthy are GAMs?},\n', '  author={Chang, Chun-Hao and Tan, Sarah and Lengerich, Ben and Goldenberg, Anna and Caruana, Rich},\n', '  journal={arXiv preprint arXiv:2006.06466},\n', '  year={2020}\n', '}\n', '    </pre>\n', '    <a href=""https://arxiv.org/pdf/2006.06466.pdf"">Paper link</a>\n', '  </details>\n', '\n', '  <hr/>\n', '</details>\n', '\n', '<details>\n', '  <summary><strong>Differential Privacy</strong></summary>\n', '  <hr/>\n', '\n', '  <details>\n', '    <summary>\n', '      <em>""Accuracy, Interpretability, and Differential Privacy via Explainable Boosting"" (H. Nori, R. Caruana, Z. Bu, J. Shen, J. Kulkarni 2021)</em>\n', '    </summary>\n', '    <br/>\n', '    <pre>\n', '@inproceedings{pmlr-v139-nori21a,\n', '  title = \t {Accuracy, Interpretability, and Differential Privacy via Explainable Boosting},\n', '  author =       {Nori, Harsha and Caruana, Rich and Bu, Zhiqi and Shen, Judy Hanwen and Kulkarni, Janardhan},\n', '  booktitle = \t {Proceedings of the 38th International Conference on Machine Learning},\n', '  pages = \t {8227--8237},\n', '  year = \t {2021},\n', '  volume = \t {139},\n', '  series = \t {Proceedings of Machine Learning Research},\n', '  publisher =    {PMLR}\n', '}\n', '    </pre>\n', '    <a href=""https://proceedings.mlr.press/v139/nori21a/nori21a.pdf"">Paper link</a>\n', '  </details>\n', '\n', '  <hr/>\n', '</details>\n', '\n', '<details>\n', '  <summary><strong>LIME</strong></summary>\n', '  <hr/>\n', '\n', '  <details>\n', '    <summary>\n', '      <em>""Why should i trust you?: Explaining the predictions of any classifier"" (M. T. Ribeiro, S. Singh, and C. Guestrin 2016)</em>\n', '    </summary>\n', '    <br/>\n', '    <pre>\n', '@inproceedings{ribeiro2016should,\n', '  title={Why should i trust you?: Explaining the predictions of any classifier},\n', '  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},\n', '  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},\n', '  pages={1135--1144},\n', '  year={2016},\n', '  organization={ACM}\n', '}\n', '    </pre>\n', '    <a href=""https://arxiv.org/pdf/1602.04938.pdf"">Paper link</a>\n', '  </details>\n', '\n', '  <hr/>\n', '</details>\n', '\n', '<details>\n', '  <summary><strong>SHAP</strong></summary>\n', '  <hr/>\n', '\n', '  <details>\n', '    <summary>\n', '      <em>""A Unified Approach to Interpreting Model Predictions"" (S. M. Lundberg and S.-I. Lee 2017)</em>\n', '    </summary>\n', '    <br/>\n', '    <pre>\n', '@incollection{NIPS2017_7062,\n', ' title = {A Unified Approach to Interpreting Model Predictions},\n', ' author = {Lundberg, Scott M and Lee, Su-In},\n', ' booktitle = {Advances in Neural Information Processing Systems 30},\n', ' editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n', ' pages = {4765--4774},\n', ' year = {2017},\n', ' publisher = {Curran Associates, Inc.},\n', ' url = {https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf}\n', '}\n', '    </pre>\n', '    <a href=""https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf"">Paper link</a>\n', '  </details>\n', '\n', '  <details>\n', '    <summary>\n', '      <em>""Consistent individualized feature attribution for tree ensembles"" (Lundberg, Scott M and Erion, Gabriel G and Lee, Su-In 2018)</em>\n', '    </summary>\n', '    <br/>\n', '    <pre>\n', '@article{lundberg2018consistent,\n', '  title={Consistent individualized feature attribution for tree ensembles},\n', '  author={Lundberg, Scott M and Erion, Gabriel G and Lee, Su-In},\n', '  journal={arXiv preprint arXiv:1802.03888},\n', '  year={2018}\n', '}\n', '    </pre>\n', '    <a href=""https://arxiv.org/pdf/1802.03888"">Paper link</a>\n', '  </details>\n', '\n', '  <details>\n', '    <summary>\n', '      <em>""Explainable machine-learning predictions for the prevention of hypoxaemia during surgery"" (S. M. Lundberg et al. 2018)</em>\n', '    </summary>\n', '    <br/>\n', '    <pre>\n', '@article{lundberg2018explainable,\n', '  title={Explainable machine-learning predictions for the prevention of hypoxaemia during surgery},\n', '  author={Lundberg, Scott M and Nair, Bala and Vavilala, Monica S and Horibe, Mayumi and Eisses, Michael J and Adams, Trevor and Liston, David E and Low, Daniel King-Wai and Newman, Shu-Fang and Kim, Jerry and others},\n', '  journal={Nature Biomedical Engineering},\n', '  volume={2},\n', '  number={10},\n', '  pages={749},\n', '  year={2018},\n', '  publisher={Nature Publishing Group}\n', '}\n', '    </pre>\n', '    <a href=""https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6467492/pdf/nihms-1505578.pdf"">Paper link</a>\n', '  </details>\n', '\n', '  <hr/>\n', '</details>\n', '\n', '<details>\n', '  <summary><strong>Sensitivity Analysis</strong></summary>\n', '  <hr/>\n', '\n', '  <details>\n', '    <summary>\n', '      <em>""SALib: An open-source Python library for Sensitivity Analysis"" (J. D. Herman and W. Usher 2017)</em>\n', '    </summary>\n', '    <br/>\n', '    <pre>\n', '@article{herman2017salib,\n', '  title={SALib: An open-source Python library for Sensitivity Analysis.},\n', '  author={Herman, Jonathan D and Usher, Will},\n', '  journal={J. Open Source Software},\n', '  volume={2},\n', '  number={9},\n', '  pages={97},\n', '  year={2017}\n', '}\n', '    </pre>\n', '    <a href=""https://www.researchgate.net/profile/Will_Usher/publication/312204236_SALib_An_open-source_Python_library_for_Sensitivity_Analysis/links/5ac732d64585151e80a39547/SALib-An-open-source-Python-library-for-Sensitivity-Analysis.pdf?origin=publication_detail"">Paper link</a>\n', '  </details>\n', '\n', '  <details>\n', '    <summary>\n', '      <em>""Factorial sampling plans for preliminary computational experiments"" (M. D. Morris 1991)</em>\n', '    </summary>\n', '    <br/>\n', '    <pre>\n', '@article{morris1991factorial,\n', '  title={},\n', '  author={Morris, Max D},\n', '  journal={Technometrics},\n', '  volume={33},\n', '  number={2},\n', '  pages={161--174},\n', '  year={1991},\n', '  publisher={Taylor \\& Francis Group}\n', '}\n', '    </pre>\n', '    <a href=""https://abe.ufl.edu/Faculty/jjones/ABE_5646/2010/Morris.1991%20SA%20paper.pdf"">Paper link</a>\n', '  </details>\n', '\n', '  <hr/>\n', '</details>\n', '\n', '<details>\n', '  <summary><strong>Partial Dependence</strong></summary>\n', '  <hr/>\n', '\n', '  <details>\n', '    <summary>\n', '      <em>""Greedy function approximation: a gradient boosting machine"" (J. H. Friedman 2001)</em>\n', '    </summary>\n', '    <br/>\n', '    <pre>\n', '@article{friedman2001greedy,\n', '  title={Greedy function approximation: a gradient boosting machine},\n', '  author={Friedman, Jerome H},\n', '  journal={Annals of statistics},\n', '  pages={1189--1232},\n', '  year={2001},\n', '  publisher={JSTOR}\n', '}\n', '    </pre>\n', '    <a href=""https://projecteuclid.org/download/pdf_1/euclid.aos/1013203451"">Paper link</a>\n', '  </details>\n', '\n', '  <hr/>\n', '</details>\n', '\n', '<details>\n', '  <summary><strong>Open Source Software</strong></summary>\n', '  <hr/>\n', '\n', '  <details>\n', '    <summary>\n', '      <em>""Scikit-learn: Machine learning in Python"" (F. Pedregosa et al. 2011)</em>\n', '    </summary>\n', '    <br/>\n', '    <pre>\n', '@article{pedregosa2011scikit,\n', '  title={Scikit-learn: Machine learning in Python},\n', '  author={Pedregosa, Fabian and Varoquaux, Ga{\\""e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},\n', '  journal={Journal of machine learning research},\n', '  volume={12},\n', '  number={Oct},\n', '  pages={2825--2830},\n', '  year={2011}\n', '}\n', '    </pre>\n', '    <a href=""https://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf"">Paper link</a>\n', '  </details>\n', '\n', '  <details>\n', '    <summary>\n', '      <em>""Collaborative data science"" (Plotly Technologies Inc. 2015)</em>\n', '    </summary>\n', '    <br/>\n', '    <pre>\n', '@online{plotly, \n', '  author = {Plotly Technologies Inc.}, \n', '  title = {Collaborative data science}, \n', '  publisher = {Plotly Technologies Inc.}, \n', '  address = {Montreal, QC}, \n', '  year = {2015}, \n', '  url = {https://plot.ly}\n', '}\n', '    </pre>\n', '    <a href=""https://plot.ly"">Link</a>\n', '  </details>\n', '  \n', '  <details>\n', '    <summary>\n', '      <em>""Joblib: running python function as pipeline jobs"" (G. Varoquaux and O. Grisel 2009)</em>\n', '    </summary>\n', '    <br/>\n', '    <pre>\n', '@article{varoquaux2009joblib,\n', '  title={Joblib: running python function as pipeline jobs},\n', '  author={Varoquaux, Ga{\\""e}l and Grisel, O},\n', '  journal={packages. python. org/joblib},\n', '  year={2009}\n', '}\n', '    </pre>\n', '    <a href=""https://joblib.readthedocs.io/en/latest/"">Link</a>\n', '  </details>\n', '  \n', '  <hr/>\n', '</details>\n', '\n', '# Videos\n', '\n', '- [The Science Behind InterpretML: Explainable Boosting Machine](https://www.youtube.com/watch?v=MREiHgHgl0k)\n', '- [How to Explain Models with InterpretML Deep Dive](https://www.youtube.com/watch?v=WwBeKMQ0-I8)\n', '- [Black-Box and Glass-Box Explanation in Machine Learning](https://youtu.be/7uzNKY8pEhQ)\n', '- [Explainable AI explained!  By-design interpretable models with Microsofts InterpretML](https://www.youtube.com/watch?v=qPn9m30ojfc)\n', '- [Interpreting Machine Learning Models with InterpretML](https://www.youtube.com/watch?v=ERNuFfsknhk)\n', '\n', '# External links\n', '\n', '- [Interpretable or Accurate? Why Not Both?](https://towardsdatascience.com/interpretable-or-accurate-why-not-both-4d9c73512192)\n', '- [The Explainable Boosting Machine. As accurate as gradient boosting, as interpretable as linear regression.](https://towardsdatascience.com/the-explainable-boosting-machine-f24152509ebb)\n', '- [Performance And Explainability With EBM](https://blog.oakbits.com/ebm-algorithm.html)\n', '- [InterpretML: Another Way to Explain Your Model](https://towardsdatascience.com/interpretml-another-way-to-explain-your-model-b7faf0a384f8)\n', '- [A gentle introduction to GA2Ms, a white box model](https://www.fiddler.ai/blog/a-gentle-introduction-to-ga2ms-a-white-box-model)\n', '- [Model Interpretation with Microsoft’s Interpret ML](https://medium.com/@sand.mayur/model-interpretation-with-microsofts-interpret-ml-85aa0ad697ae)\n', '- [Explaining Model Pipelines With InterpretML](https://medium.com/@mariusvadeika/explaining-model-pipelines-with-interpretml-a9214f75400b)\n', '- [Explain Your Model with Microsoft’s InterpretML](https://medium.com/@Dataman.ai/explain-your-model-with-microsofts-interpretml-5daab1d693b4)\n', '- [On Model Explainability: From LIME, SHAP, to Explainable Boosting](https://everdark.github.io/k9/notebooks/ml/model_explain/model_explain.nb.html)\n', '- [Dealing with Imbalanced Data (Mortgage loans defaults)](https://mikewlange.github.io/ImbalancedData-/index.html)\n', '- [The right way to compute your Shapley Values](https://towardsdatascience.com/the-right-way-to-compute-your-shapley-values-cfea30509254)\n', '- [The Art of Sprezzatura for Machine Learning](https://towardsdatascience.com/the-art-of-sprezzatura-for-machine-learning-e2494c0db727)\n', '- [Mixing Art into the Science of Model Explainability](https://towardsdatascience.com/mixing-art-into-the-science-of-model-explainability-312b8216fa95)\n', '\n', '# Papers that use or compare EBMs\n', '\n', '\n', '- [Model Interpretability in Credit Insurance](http://hdl.handle.net/10400.5/27507)\n', '- [Federated Boosted Decision Trees with Differential Privacy](https://arxiv.org/pdf/2210.02910.pdf)\n', '- [GAM(E) CHANGER OR NOT? AN EVALUATION OF INTERPRETABLE MACHINE LEARNING MODELS](https://arxiv.org/pdf/2204.09123.pdf)\n', '- [GAM Coach: Towards Interactive and User-centered Algorithmic Recourse](https://arxiv.org/pdf/2302.14165.pdf)\n', '- [Revealing the Galaxy-Halo Connection Through Machine Learning](https://arxiv.org/pdf/2204.10332.pdf)\n', '- [Explainable Artificial Intelligence for COVID-19 Diagnosis Through Blood Test Variables](https://link.springer.com/content/pdf/10.1007/s40313-021-00858-y.pdf)\n', '- [Using Explainable Boosting Machines (EBMs) to Detect Common Flaws in Data](https://link.springer.com/chapter/10.1007/978-3-030-93736-2_40)\n', '- [Differentially Private Gradient Boosting on Linear Learners for Tabular Data Analysis](https://assets.amazon.science/fa/3a/a62ba73f4bbda1d880b678c39193/differentially-private-gradient-boosting-on-linear-learners-for-tabular-data-analysis.pdf)\n', '- [Concrete compressive strength prediction using an explainable boosting machine model](https://www.sciencedirect.com/science/article/pii/S2214509523000244/pdfft?md5=171c275b6bcae8897cef03d931e908e2&pid=1-s2.0-S2214509523000244-main.pdf)\n', '- [Estimate Deformation Capacity of Non-Ductile RC Shear Walls Using Explainable Boosting Machine](https://arxiv.org/pdf/2301.04652.pdf)\n', '- [Introducing the Rank-Biased Overlap as Similarity Measure for Feature Importance in Explainable Machine Learning: A Case Study on Parkinson’s Disease](https://link.springer.com/chapter/10.1007/978-3-031-15037-1_11)\n', '- [Targeting resources efficiently and justifiably by combining causal machine learning and theory](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9768181/pdf/frai-05-1015604.pdf)\n', '- [Extractive Text Summarization Using Generalized Additive Models with Interactions for Sentence Selection](https://arxiv.org/pdf/2212.10707.pdf)\n', '- [Death by Round Numbers: Glass-Box Machine Learning Uncovers Biases in Medical Practice](https://www.medrxiv.org/content/medrxiv/early/2022/11/28/2022.04.30.22274520.full.pdf)\n', '- [Post-Hoc Interpretation of Transformer Hyperparameters with Explainable Boosting Machines](https://www.cs.jhu.edu/~xzhan138/papers/BLACK2022.pdf)\n', '- [Interpretable machine learning for predicting pathologic complete response in patients treated with chemoradiation therapy for rectal adenocarcinoma](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9771385/pdf/frai-05-1059033.pdf)\n', '- [Exploring the Balance between Interpretability and Performance with carefully designed Constrainable Neural Additive Models](https://deliverypdf.ssrn.com/delivery.php?ID=998105006000069122073098120102102121021040051018055094125029122011041003059093125102072122106122077081069015087124028097016003127095087091028087010007035098086102086081014043013113004081117108011028041097095064071100112069081100069120077067116088100069070097093080074087115080072064086111126&EXT=pdf&INDEX=TRUE)\n', '- [Estimating Discontinuous Time-Varying Risk Factors and Treatment Benefits for COVID-19 with Interpretable ML](https://arxiv.org/pdf/2211.08991.pdf)\n', '- [Pest Presence Prediction Using Interpretable Machine Learning](https://arxiv.org/pdf/2205.07723.pdf)\n', '- [epitope1D: Accurate Taxonomy-Aware B-Cell Linear Epitope Prediction](https://www.biorxiv.org/content/10.1101/2022.10.17.512613v1.full.pdf)\n', '- [Explainable Boosting Machines for Slope Failure Spatial Predictive Modeling](https://www.mdpi.com/2072-4292/13/24/4991/htm)\n', '- [Micromodels for Efficient, Explainable, and Reusable Systems: A Case Study on Mental Health](https://arxiv.org/pdf/2109.13770.pdf)\n', '- [Identifying main and interaction effects of risk factors to predict intensive care admission in patients hospitalized with COVID-19](https://www.medrxiv.org/content/10.1101/2020.06.30.20143651v1.full.pdf)\n', '- [Comparing the interpretability of machine learning classifiers for brain tumour survival prediction](https://deliverypdf.ssrn.com/delivery.php?ID=760122118067103094108090123091079011028032009009023085005014014002123105085114025022024005047078031019089073120012025117073002064031071072113006066035001068125027021087087083085026100009018045107092063001023068071002124070107120120007014102094103069089119026110104107005031095001092090&EXT=pdf&INDEX=TRUE)\n', '- [Using Interpretable Machine Learning to Predict Maternal and Fetal Outcomes](https://arxiv.org/pdf/2207.05322.pdf)\n', '- [Calibrate: Interactive Analysis of Probabilistic Model Output](https://arxiv.org/pdf/2207.13770.pdf)\n', '- [Neural Additive Models: Interpretable Machine Learning with Neural Nets](https://arxiv.org/pdf/2004.13912.pdf)\n', '- [NODE-GAM: Neural Generalized Additive Model for Interpretable Deep Learning](https://arxiv.org/pdf/2106.01613.pdf)\n', '- [Scalable Interpretability via Polynomials](https://arxiv.org/pdf/2205.14108v1.pdf)\n', '- [Neural Basis Models for Interpretability](https://arxiv.org/pdf/2205.14120.pdf)\n', '- [ILMART: Interpretable Ranking with Constrained LambdaMART](https://arxiv.org/pdf/2206.00473.pdf)\n', '- [Integrating Co-Clustering and Interpretable Machine Learning for the Prediction of Intravenous Immunoglobulin Resistance in Kawasaki Disease](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9097874)\n', '- [GAMI-Net: An Explainable Neural Network based on Generalized Additive Models with Structured Interactions](https://arxiv.org/pdf/2003.07132v1.pdf)\n', '- [A Concept and Argumentation based Interpretable Model in High Risk Domains](https://arxiv.org/pdf/2208.08149.pdf)\n', '- [Analyzing the Differences be"
Model Explainability,RexYing/gnn-model-explainer,RexYing,https://api.github.com/repos/RexYing/gnn-model-explainer,681,153,6,"['https://api.github.com/users/RexYing', 'https://api.github.com/users/dtsbourg', 'https://api.github.com/users/JiaxuanYou', 'https://api.github.com/users/hnaik', 'https://api.github.com/users/devloop0', 'https://api.github.com/users/Stannislav']",Python,2023-04-26T08:44:24Z,https://raw.githubusercontent.com/RexYing/gnn-model-explainer/master/README.md,"['# gnn-explainer\n', '\n', 'This repository contains the source code for the paper `GNNExplainer: Generating Explanations for Graph Neural Networks` by [Rex Ying](https://cs.stanford.edu/people/rexy/), [Dylan Bourgeois](https://dtsbourg.me/), [Jiaxuan You](https://cs.stanford.edu/~jiaxuan/), [Marinka Zitnik](http://helikoid.si/cms/) & [Jure Leskovec](https://cs.stanford.edu/people/jure/), presented at [NeurIPS 2019](nips.cc).\n', '\n', '[[Arxiv]](https://arxiv.org/abs/1903.03894) [[BibTex]](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1903-03894) [[Google Scholar]](https://scholar.google.com/scholar?q=GNNExplainer%3A%20Generating%20Explanations%20for%20Graph%20Neural%20Networks%20Rex%20arXiv%202019)\n', '\n', '```\n', '@misc{ying2019gnnexplainer,\n', '    title={GNNExplainer: Generating Explanations for Graph Neural Networks},\n', '    author={Rex Ying and Dylan Bourgeois and Jiaxuan You and Marinka Zitnik and Jure Leskovec},\n', '    year={2019},\n', '    eprint={1903.03894},\n', '    archivePrefix={arXiv},\n', '    primaryClass={cs.LG}\n', '}\n', '```\n', '\n', '## Using the explainer\n', '\n', '### Installation\n', '\n', 'See [INSTALLATION.md](#)\n', '\n', ""### Replicating the paper's results\n"", '\n', '#### Training a GCN model \n', '\n', 'This is the model that will be explained. We do provide [pre-trained models](#TODO) for all of the experiments\n', 'that are shown in the paper. To re-train these models, run the following:\n', '\n', '```\n', 'python train.py --dataset=EXPERIMENT_NAME\n', '```\n', '\n', 'where `EXPERIMENT_NAME` is the experiment you want to replicate. \n', '\n', 'For a complete list of options in training the GCN models:\n', '\n', '```\n', 'python train.py --help\n', '```\n', '\n', '> TODO: Explain outputs\n', '\n', '#### Explaining a GCN model\n', '\n', 'To run the explainer, run the following:\n', '\n', '```\n', 'python explainer_main.py --dataset=EXPERIMENT_NAME\n', '```\n', '\n', 'where `EXPERIMENT_NAME` is the experiment you want to replicate. \n', '\n', '\n', 'For a complete list of options provided by the explainer:\n', '\n', '```\n', 'python train.py --help\n', '```\n', '\n', '#### Visualizing the explanations\n', '\n', '##### Tensorboard\n', '\n', 'The result of the optimization can be visualized through Tensorboard.\n', '\n', '```\n', 'tensorboard --logdir log\n', '```\n', '\n', 'You should then have access to visualizations served from `localhost`.\n', '\n', '#### Jupyter Notebook\n', '\n', 'We provide an example visualization through Jupyter Notebooks in the `notebook` folder. To try it:\n', '\n', '```\n', 'jupyter notebook\n', '```\n', '\n', 'The default visualizations are provided in `notebook/GNN-Explainer-Viz.ipynb`.\n', '\n', '> Note: For an interactive version, you must enable ipywidgets\n', '>\n', '> ```\n', '> jupyter nbextension enable --py widgetsnbextension\n', '> ```\n', '\n', 'You can now play around with the mask threshold in the `GNN-Explainer-Viz-interactive.ipynb`.\n', '> TODO: Explain outputs + visualizations + baselines\n', '\n', '#### D3,js\n', '\n', 'We provide export functionality so the generated masks can be visualized in other data visualization \n', 'frameworks, for example [d3.js](http://observablehq.com). We provide [an example visualization in Observable](https://observablehq.com/d/00c5dc74f359e7a1).\n', '\n', '#### Included experiments\n', '\n', '| Name     | `EXPERIMENT_NAME` | Description  |\n', '|----------|:-------------------:|--------------|\n', '| Synthetic #1 | `syn1`  | Random BA graph with House attachments.  |\n', '| Synthetic #2 | `syn2`  | Random BA graph with community features. | \n', '| Synthetic #3 | `syn3`  | Random BA graph with grid attachments.  |\n', '| Synthetic #4 | `syn4`  | Random Tree with cycle attachments. |\n', '| Synthetic #5 | `syn5`  | Random Tree with grid attachments. | \n', '| Enron        | `enron` | Enron email dataset [source](https://www.cs.cmu.edu/~enron/). |\n', '| PPI          | `ppi_essential` | Protein-Protein interaction dataset. |\n', '| | | |\n', '| Reddit*      | `REDDIT-BINARY`  | Reddit-Binary Graphs ([source](https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets)). |\n', '| Mutagenicity*      | `Mutagenicity`  | Predicting the mutagenicity of molecules ([source](https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets)). |\n', ""| Tox 21*      | `Tox21_AHR`  | Predicting a compound's toxicity ([source](https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets)). |\n"", '\n', '> Datasets with a * are passed with the `--bmname` parameter rather than `--dataset` as they require being downloaded manually.\n', '\n', ""> TODO: Provide all data for experiments packaged so we don't have to split the two.\n"", '\n', '\n', '### Using the explainer on other models\n', 'A graph attention model is provided. This repo is still being actively developed to support other\n', 'GNN models in the future.\n', '\n', '## Changelog\n', '\n', 'See [CHANGELOG.md](#)\n']"
Model Explainability,Trusted-AI/AIX360,Trusted-AI,https://api.github.com/repos/Trusted-AI/AIX360,1323,286,23,"['https://api.github.com/users/vijay-arya', 'https://api.github.com/users/kmyusk', 'https://api.github.com/users/sadhamanus', 'https://api.github.com/users/dennislwei', 'https://api.github.com/users/floidgilbert', 'https://api.github.com/users/rluss', 'https://api.github.com/users/ImgBotApp', 'https://api.github.com/users/pronics2004', 'https://api.github.com/users/Tomcli', 'https://api.github.com/users/jamescodella', 'https://api.github.com/users/rahulnair23ibm', 'https://api.github.com/users/animeshsingh', 'https://api.github.com/users/karthikeyansh', 'https://api.github.com/users/michaelhind', 'https://api.github.com/users/monindersingh', 'https://api.github.com/users/fabianlim', 'https://api.github.com/users/cclauss', 'https://api.github.com/users/kant', 'https://api.github.com/users/gaborpelesz', 'https://api.github.com/users/gdequeiroz', 'https://api.github.com/users/ishapuri', 'https://api.github.com/users/Marleen1', 'https://api.github.com/users/asm582']",Python,2023-04-26T02:59:05Z,https://raw.githubusercontent.com/Trusted-AI/AIX360/master/README.md,"['# AI Explainability 360 (v0.2.1) \n', '\n', '[![Build](https://github.com/Trusted-AI/AIX360/actions/workflows/Build.yml/badge.svg)](https://github.com/Trusted-AI/AIX360/actions/workflows/Build.yml)\n', '[![Documentation Status](https://readthedocs.org/projects/aix360/badge/?version=latest)](https://aix360.readthedocs.io/en/latest/?badge=latest)\n', '[![PyPI version](https://badge.fury.io/py/aix360.svg)](https://badge.fury.io/py/aix360)\n', '\n', 'The AI Explainability 360 toolkit is an open-source library that supports interpretability and explainability of datasets and machine learning models. The AI Explainability 360 Python package includes a comprehensive set of algorithms that cover different dimensions of explanations along with proxy explainability metrics.           \n', '\n', 'The [AI Explainability 360 interactive experience](http://aix360.mybluemix.net/data) provides a gentle introduction to the concepts and capabilities by walking through an example use case for different consumer personas. The [tutorials and example notebooks](./examples) offer a deeper, data scientist-oriented introduction. The complete API is also available. \n', '\n', 'There is no single approach to explainability that works best. There are many ways to explain: data vs. model, directly interpretable vs. post hoc explanation, local vs. global, etc. It may therefore be confusing to figure out which algorithms are most appropriate for a given use case. To help, we have created some [guidance material](http://aix360.mybluemix.net/resources#guidance) and a [chart](./aix360/algorithms/README.md) that can be consulted. \n', '\n', 'We have developed the package with extensibility in mind. This library is still in development. We encourage you to contribute your explainability algorithms, metrics, and use cases. To get started as a contributor, please join the [AI Explainability 360 Community on Slack](https://aix360.slack.com) by requesting an invitation [here](https://join.slack.com/t/aix360/shared_invite/enQtNzEyOTAwOTk1NzY2LTM1ZTMwM2M4OWQzNjhmNGRiZjg3MmJiYTAzNDU1MTRiYTIyMjFhZTI4ZDUwM2M1MGYyODkwNzQ2OWQzMThlN2Q). Please review the instructions to contribute code and python notebooks [here](CONTRIBUTING.md).\n', '\n', '## Supported explainability algorithms\n', '\n', '### Data explanation\n', '\n', '- ProtoDash ([Gurumoorthy et al., 2019](https://arxiv.org/abs/1707.01212))\n', '- Disentangled Inferred Prior VAE ([Kumar et al., 2018](https://openreview.net/forum?id=H1kG7GZAW))\n', '\n', '### Local post-hoc explanation \n', '\n', '- ProtoDash ([Gurumoorthy et al., 2019](https://arxiv.org/abs/1707.01212))\n', '- Contrastive Explanations Method ([Dhurandhar et al., 2018](https://papers.nips.cc/paper/7340-explanations-based-on-the-missing-towards-contrastive-explanations-with-pertinent-negatives))\n', '- Contrastive Explanations Method with Monotonic Attribute Functions ([Luss et al., 2019](https://arxiv.org/abs/1905.12698))\n', '- LIME ([Ribeiro et al. 2016](https://arxiv.org/abs/1602.04938),  [Github](https://github.com/marcotcr/lime))\n', '- SHAP ([Lundberg, et al. 2017](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions),  [Github](https://github.com/slundberg/shap))\n', '\n', '### Local direct explanation\n', '\n', '- Teaching AI to Explain its Decisions ([Hind et al., 2019](https://doi.org/10.1145/3306618.3314273)) \n', '- Order Constraints in Optimal Transport ([Lim et al.,2022](https://arxiv.org/abs/2110.07275), [Github](https://github.com/IBM/otoc))\n', '\n', '### Global direct explanation\n', '\n', '- CoFrNets (Continued Fraction Nets) ([Puri et al., 2021](https://papers.nips.cc/paper/2021/file/b538f279cb2ca36268b23f557a831508-Paper.pdf))\n', '- Boolean Decision Rules via Column Generation (Light Edition) ([Dash et al., 2018](https://papers.nips.cc/paper/7716-boolean-decision-rules-via-column-generation))\n', '- Generalized Linear Rule Models ([Wei et al., 2019](http://proceedings.mlr.press/v97/wei19a.html))\n', '- Fast Effective Rule Induction (Ripper) ([William W Cohen, 1995](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.107.2612&rep=rep1&type=pdf))\n', '\n', '### Global post-hoc explanation\xa0\n', '\n', '- ProfWeight ([Dhurandhar et al., 2018](https://papers.nips.cc/paper/8231-improving-simple-models-with-confidence-profiles))\n', '\n', '\n', '## Supported explainability metrics\n', '- Faithfulness ([Alvarez-Melis and Jaakkola, 2018](https://papers.nips.cc/paper/8003-towards-robust-interpretability-with-self-explaining-neural-networks))\n', '- Monotonicity ([Luss et al., 2019](https://arxiv.org/abs/1905.12698))\n', '\n', '## Setup\n', '\n', 'Supported Configurations:\n', '\n', '| OS      | Python version |\n', '| ------- | -------------- |\n', '| macOS   | 3.6  |\n', '| Ubuntu  | 3.6  |\n', '| Windows | 3.6  |\n', '\n', '### (Optional) Create a virtual environment\n', '\n', 'AI Explainability 360 requires specific versions of many Python packages which may conflict\n', 'with other projects on your system. A virtual environment manager is strongly\n', 'recommended to ensure dependencies may be installed safely. If you have trouble installing the toolkit, try this first.\n', '\n', '#### Conda\n', '\n', 'Conda is recommended for all configurations though Virtualenv is generally\n', 'interchangeable for our purposes. Miniconda is sufficient (see [the difference between Anaconda and\n', 'Miniconda](https://conda.io/docs/user-guide/install/download.html#anaconda-or-miniconda)\n', 'if you are curious) and can be installed from\n', '[here](https://conda.io/miniconda.html) if you do not already have it.\n', '\n', 'Then, to create a new Python 3.6 environment, run:\n', '\n', '```bash\n', 'conda create --name aix360 python=3.6\n', 'conda activate aix360\n', '```\n', '\n', 'The shell should now look like `(aix360) $`. To deactivate the environment, run:\n', '\n', '```bash\n', '(aix360)$ conda deactivate\n', '```\n', '\n', 'The prompt will return back to `$ ` or `(base)$`.\n', '\n', 'Note: Older versions of conda may use `source activate aix360` and `source\n', 'deactivate` (`activate aix360` and `deactivate` on Windows).\n', '\n', '\n', '### Installation\n', '\n', 'Clone the latest version of this repository:\n', '\n', '```bash\n', '(aix360)$ git clone https://github.com/Trusted-AI/AIX360\n', '```\n', '\n', ""If you'd like to run the examples and tutorial notebooks, download the datasets now and place them in\n"", 'their respective folders as described in\n', '[aix360/data/README.md](aix360/data/README.md).\n', '\n', 'Then, navigate to the root directory of the project which contains `setup.py` file and run:\n', '\n', '```bash\n', '(aix360)$ pip install -e .\n', '```\n', '\n', 'If you face any issues, please try upgrading pip and setuptools and uninstall any previous versions of aix360 before attempting the above step again. \n', '\n', '```bash\n', '(aix360)$ pip install --upgrade pip setuptools\n', '(aix360)$ pip uninstall aix360\n', '```\n', '\n', '## Running in Docker\n', '\n', '* Under `AIX360` directory build the container image from Dockerfile using `docker build -t aix360_docker .`\n', '* Start the container image using command `docker run -it -p 8888:8888 aix360_docker:latest bash` assuming port 8888 is free on your machine.\n', '* Inside the container start jupuyter lab using command `jupyter lab --allow-root --ip 0.0.0.0 --port 8888 --no-browser`\n', '* Access the sample tutorials on your machine using URL `localhost:8888`\n', '\n', '## PIP Installation of AI Explainability 360\n', '\n', 'If you would like to quickly start using the AI explainability 360 toolkit without cloning this repository, then you can install the [aix360 pypi package](https://pypi.org/project/aix360/) as follows. \n', '\n', '```bash\n', '(your environment)$ pip install aix360\n', '```\n', '\n', 'If you follow this approach, you may need to download the notebooks in the [examples](./examples) folder separately. \n', '\n', '\n', '## Using AI Explainability 360\n', '\n', 'The `examples` directory contains a diverse collection of jupyter notebooks\n', 'that use AI Explainability 360 in various ways. Both examples and tutorial notebooks illustrate\n', 'working code using the toolkit. Tutorials provide additional discussion that walks\n', 'the user through the various steps of the notebook. See the details about\n', 'tutorials and examples [here](examples/README.md). \n', '\n', '## Citing AI Explainability 360\n', '\n', 'If you are using AI Explainability 360 for your work, we encourage you to\n', '\n', '* Cite the following [paper](https://arxiv.org/abs/1909.03012). The bibtex entry is as follows: \n', '\n', '```\n', '@misc{aix360-sept-2019,\n', 'title = ""One Explanation Does Not Fit All: A Toolkit and Taxonomy of AI Explainability Techniques"",\n', 'author = {Vijay Arya and Rachel K. E. Bellamy and Pin-Yu Chen and Amit Dhurandhar and Michael Hind\n', ""and Samuel C. Hoffman and Stephanie Houde and Q. Vera Liao and Ronny Luss and Aleksandra Mojsilovi\\'c\n"", 'and Sami Mourad and Pablo Pedemonte and Ramya Raghavendra and John Richards and Prasanna Sattigeri\n', 'and Karthikeyan Shanmugam and Moninder Singh and Kush R. Varshney and Dennis Wei and Yunfeng Zhang},\n', 'month = sept,\n', 'year = {2019},\n', 'url = {https://arxiv.org/abs/1909.03012}\n', '}\n', '```\n', '\n', '* Put a star on this repository.\n', '\n', '* Share your success stories with us and others in the [AI Explainability 360 Community](https://aix360.slack.com). \n', '\n', '## AIX360 Videos\n', '\n', '* Introductory [video](https://www.youtube.com/watch?v=Yn4yduyoQh4) to AI\n', '  Explainability 360 by Vijay Arya and Amit Dhurandhar, September 5, 2019 (35 mins)\n', '\n', '## Acknowledgements\n', '\n', 'AIX360 is built with the help of several open source packages. All of these are listed in setup.py and some of these include: \n', '* Tensorflow https://www.tensorflow.org/about/bib\n', '* Pytorch https://github.com/pytorch/pytorch\n', '* scikit-learn https://scikit-learn.org/stable/about.html\n', '\n', '## License Information\n', '\n', 'Please view both the [LICENSE](https://github.com/vijay-arya/AIX360/blob/master/LICENSE) file and the folder [supplementary license](https://github.com/vijay-arya/AIX360/tree/master/supplementary%20license) present in the root directory for license information. \n', '\n']"
Model Explainability,slundberg/shap,slundberg,https://api.github.com/repos/slundberg/shap,19070,2866,30,"['https://api.github.com/users/slundberg', 'https://api.github.com/users/ryserrao', 'https://api.github.com/users/vivekchettiar', 'https://api.github.com/users/imatiach-msft', 'https://api.github.com/users/gabrieltseng', 'https://api.github.com/users/RAMitchell', 'https://api.github.com/users/QuentinAmbard', 'https://api.github.com/users/anusham1990', 'https://api.github.com/users/dependabot%5Bbot%5D', 'https://api.github.com/users/floidgilbert', 'https://api.github.com/users/jsu27', 'https://api.github.com/users/lrjball', 'https://api.github.com/users/kodonnell', 'https://api.github.com/users/JasonTam', 'https://api.github.com/users/maggiewu19', 'https://api.github.com/users/alexisdrakopoulos', 'https://api.github.com/users/jorgecarleitao', 'https://api.github.com/users/tlabarta', 'https://api.github.com/users/KOLANICH', 'https://api.github.com/users/alexander-pv', 'https://api.github.com/users/xzzxxzzx', 'https://api.github.com/users/moritzaugustin', 'https://api.github.com/users/parsatorb', 'https://api.github.com/users/JiechengZhao', 'https://api.github.com/users/sbrugman', 'https://api.github.com/users/SachinVarghese', 'https://api.github.com/users/ihopethiswillfi', 'https://api.github.com/users/mizukasai', 'https://api.github.com/users/JacobMcc', 'https://api.github.com/users/jeremiahpslewis']",Python,2023-04-26T14:23:30Z,https://raw.githubusercontent.com/slundberg/shap/master/README.md,"['\n', '\n', '<p align=""center"">\n', '  <img src=""https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/shap_header.svg"" width=""800"" />\n', '</p>\n', '\n', '---\n', '![example workflow](https://github.com/slundberg/shap/actions/workflows/run_tests.yml/badge.svg)\n', '[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/slundberg/shap/master)\n', '[![Documentation Status](https://readthedocs.org/projects/shap/badge/?version=latest)](https://shap.readthedocs.io/en/latest/?badge=latest)\n', '\n', '**SHAP (SHapley Additive exPlanations)** is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions (see [papers](#citations) for details and citations).\n', '\n', '<!--**SHAP (SHapley Additive exPlanations)** is a unified approach to explain the output of any machine learning model. SHAP connects game theory with local explanations, uniting several previous methods [1-7] and representing the only possible consistent and locally accurate additive feature attribution method based on expectations (see our [papers](#citations) for details and citations).-->\n', '\n', '\n', '\n', '## Install\n', '\n', 'SHAP can be installed from either [PyPI](https://pypi.org/project/shap) or [conda-forge](https://anaconda.org/conda-forge/shap):\n', '\n', '<pre>\n', 'pip install shap\n', '<i>or</i>\n', 'conda install -c conda-forge shap\n', '</pre>\n', '\n', '## Tree ensemble example (XGBoost/LightGBM/CatBoost/scikit-learn/pyspark models)\n', '\n', 'While SHAP can explain the output of any machine learning model, we have developed a high-speed exact algorithm for tree ensemble methods (see our [Nature MI paper](https://rdcu.be/b0z70)). Fast C++ implementations are supported for *XGBoost*, *LightGBM*, *CatBoost*, *scikit-learn* and *pyspark* tree models:\n', '\n', '```python\n', 'import xgboost\n', 'import shap\n', '\n', '# train an XGBoost model\n', 'X, y = shap.datasets.boston()\n', 'model = xgboost.XGBRegressor().fit(X, y)\n', '\n', ""# explain the model's predictions using SHAP\n"", '# (same syntax works for LightGBM, CatBoost, scikit-learn, transformers, Spark, etc.)\n', 'explainer = shap.Explainer(model)\n', 'shap_values = explainer(X)\n', '\n', ""# visualize the first prediction's explanation\n"", 'shap.plots.waterfall(shap_values[0])\n', '```\n', '\n', '<p align=""center"">\n', '  <img width=""616"" src=""https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_waterfall.png"" />\n', '</p>\n', '\n', 'The above explanation shows features each contributing to push the model output from the base value (the average model output over the training dataset we passed) to the model output. Features pushing the prediction higher are shown in red, those pushing the prediction lower are in blue. Another way to visualize the same explanation is to use a force plot (these are introduced in our [Nature BME paper](https://rdcu.be/baVbR)):\n', '\n', '```python\n', ""# visualize the first prediction's explanation with a force plot\n"", 'shap.plots.force(shap_values[0])\n', '```\n', '\n', '<p align=""center"">\n', '  <img width=""811"" src=""https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_instance.png"" />\n', '</p>\n', '\n', 'If we take many force plot explanations such as the one shown above, rotate them 90 degrees, and then stack them horizontally, we can see explanations for an entire dataset (in the notebook this plot is interactive):\n', '\n', '```python\n', '# visualize all the training set predictions\n', 'shap.plots.force(shap_values)\n', '```\n', '\n', '<p align=""center"">\n', '  <img width=""811"" src=""https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_dataset.png"" />\n', '</p>\n', '\n', ""To understand how a single feature effects the output of the model we can plot the SHAP value of that feature vs. the value of the feature for all the examples in a dataset. Since SHAP values represent a feature's responsibility for a change in the model output, the plot below represents the change in predicted house price as RM (the average number of rooms per house in an area) changes. Vertical dispersion at a single value of RM represents interaction effects with other features. To help reveal these interactions we can color by another feature. If we pass the whole explanation tensor to the `color` argument the scatter plot will pick the best feature to color by. In this case it picks RAD (index of accessibility to radial highways) since that highlights that the average number of rooms per house has less impact on home price for areas with a high RAD value.\n"", '\n', '```python\n', '# create a dependence scatter plot to show the effect of a single feature across the whole dataset\n', 'shap.plots.scatter(shap_values[:,""RM""], color=shap_values)\n', '```\n', '\n', '<p align=""center"">\n', '  <img width=""544"" src=""https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_scatter.png"" />\n', '</p>\n', '\n', '\n', 'To get an overview of which features are most important for a model we can plot the SHAP values of every feature for every sample. The plot below sorts features by the sum of SHAP value magnitudes over all samples, and uses SHAP values to show the distribution of the impacts each feature has on the model output. The color represents the feature value (red high, blue low). This reveals for example that a high LSTAT (% lower status of the population) lowers the predicted home price.\n', '\n', '```python\n', '# summarize the effects of all the features\n', 'shap.plots.beeswarm(shap_values)\n', '```\n', '\n', '<p align=""center"">\n', '  <img width=""583"" src=""https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_beeswarm.png"" />\n', '</p>\n', '\n', 'We can also just take the mean absolute value of the SHAP values for each feature to get a standard bar plot (produces stacked bars for multi-class outputs):\n', '\n', '```python\n', 'shap.plots.bar(shap_values)\n', '```\n', '\n', '<p align=""center"">\n', '  <img width=""570"" src=""https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_global_bar.png"" />\n', '</p>\n', '\n', '## Natural language example (transformers)\n', '\n', 'SHAP has specific support for natural language models like those in the Hugging Face transformers library. By adding coalitional rules to traditional Shapley values we can form games that explain large modern NLP model using very few function evaluations. Using this functionality is as simple as passing a supported transformers pipeline to SHAP:\n', '\n', '```python\n', 'import transformers\n', 'import shap\n', '\n', '# load a transformers pipeline model\n', ""model = transformers.pipeline('sentiment-analysis', return_all_scores=True)\n"", '\n', '# explain the model on two sample inputs\n', 'explainer = shap.Explainer(model) \n', 'shap_values = explainer([""What a great movie! ...if you have no taste.""])\n', '\n', ""# visualize the first prediction's explanation for the POSITIVE output class\n"", 'shap.plots.text(shap_values[0, :, ""POSITIVE""])\n', '```\n', '\n', '<p align=""center"">\n', '  <img width=""811"" src=""https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/sentiment_analysis_plot.png"" />\n', '</p>\n', '\n', '## Deep learning example with DeepExplainer (TensorFlow/Keras models)\n', '\n', 'Deep SHAP is a high-speed approximation algorithm for SHAP values in deep learning models that builds on a connection with [DeepLIFT](https://arxiv.org/abs/1704.02685) described in the SHAP NIPS paper. The implementation here differs from the original DeepLIFT by using a distribution of background samples instead of a single reference value, and using Shapley equations to linearize components such as max, softmax, products, divisions, etc. Note that some of these enhancements have also been since integrated into DeepLIFT. TensorFlow models and Keras models using the TensorFlow backend are supported (there is also preliminary support for PyTorch):\n', '\n', '```python\n', '# ...include code from https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n', '\n', 'import shap\n', 'import numpy as np\n', '\n', '# select a set of background examples to take an expectation over\n', 'background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]\n', '\n', '# explain predictions of the model on four images\n', 'e = shap.DeepExplainer(model, background)\n', '# ...or pass tensors directly\n', '# e = shap.DeepExplainer((model.layers[0].input, model.layers[-1].output), background)\n', 'shap_values = e.shap_values(x_test[1:5])\n', '\n', '# plot the feature attributions\n', 'shap.image_plot(shap_values, -x_test[1:5])\n', '```\n', '\n', '<p align=""center"">\n', '  <img width=""820"" src=""https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/mnist_image_plot.png"" />\n', '</p>\n', '\n', ""The plot above explains ten outputs (digits 0-9) for four different images. Red pixels increase the model's output while blue pixels decrease the output. The input images are shown on the left, and as nearly transparent grayscale backings behind each of the explanations. The sum of the SHAP values equals the difference between the expected model output (averaged over the background dataset) and the current model output. Note that for the 'zero' image the blank middle is important, while for the 'four' image the lack of a connection on top makes it a four instead of a nine.\n"", '\n', '\n', '## Deep learning example with GradientExplainer (TensorFlow/Keras/PyTorch models)\n', '\n', 'Expected gradients combines ideas from [Integrated Gradients](https://arxiv.org/abs/1703.01365), SHAP, and [SmoothGrad](https://arxiv.org/abs/1706.03825) into a single expected value equation. This allows an entire dataset to be used as the background distribution (as opposed to a single reference value) and allows local smoothing. If we approximate the model with a linear function between each background data sample and the current input to be explained, and we assume the input features are independent then expected gradients will compute approximate SHAP values. In the example below we have explained how the 7th intermediate layer of the VGG16 ImageNet model impacts the output probabilities.\n', '\n', '```python\n', 'from keras.applications.vgg16 import VGG16\n', 'from keras.applications.vgg16 import preprocess_input\n', 'import keras.backend as K\n', 'import numpy as np\n', 'import json\n', 'import shap\n', '\n', '# load pre-trained model and choose two images to explain\n', ""model = VGG16(weights='imagenet', include_top=True)\n"", 'X,y = shap.datasets.imagenet50()\n', 'to_explain = X[[39,41]]\n', '\n', '# load the ImageNet class names\n', 'url = ""https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json""\n', 'fname = shap.datasets.cache(url)\n', 'with open(fname) as f:\n', '    class_names = json.load(f)\n', '\n', '# explain how the input to the 7th layer of the model explains the top two classes\n', 'def map2layer(x, layer):\n', '    feed_dict = dict(zip([model.layers[0].input], [preprocess_input(x.copy())]))\n', '    return K.get_session().run(model.layers[layer].input, feed_dict)\n', 'e = shap.GradientExplainer(\n', '    (model.layers[7].input, model.layers[-1].output),\n', '    map2layer(X, 7),\n', '    local_smoothing=0 # std dev of smoothing noise\n', ')\n', 'shap_values,indexes = e.shap_values(map2layer(to_explain, 7), ranked_outputs=2)\n', '\n', '# get the names for the classes\n', 'index_names = np.vectorize(lambda x: class_names[str(x)][1])(indexes)\n', '\n', '# plot the explanations\n', 'shap.image_plot(shap_values, to_explain, index_names)\n', '```\n', '\n', '<p align=""center"">\n', '  <img width=""500"" src=""https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/gradient_imagenet_plot.png"" />\n', '</p>\n', '\n', 'Predictions for two input images are explained in the plot above. Red pixels represent positive SHAP values that increase the probability of the class, while blue pixels represent negative SHAP values the reduce the probability of the class. By using `ranked_outputs=2` we explain only the two most likely classes for each input (this spares us from explaining all 1,000 classes).\n', '\n', '## Model agnostic example with KernelExplainer (explains any function)\n', '\n', 'Kernel SHAP uses a specially-weighted local linear regression to estimate SHAP values for any model. Below is a simple example for explaining a multi-class SVM on the classic iris dataset.\n', '\n', '```python\n', 'import sklearn\n', 'import shap\n', 'from sklearn.model_selection import train_test_split\n', '\n', '# print the JS visualization code to the notebook\n', 'shap.initjs()\n', '\n', '# train a SVM classifier\n', 'X_train,X_test,Y_train,Y_test = train_test_split(*shap.datasets.iris(), test_size=0.2, random_state=0)\n', ""svm = sklearn.svm.SVC(kernel='rbf', probability=True)\n"", 'svm.fit(X_train, Y_train)\n', '\n', '# use Kernel SHAP to explain test set predictions\n', 'explainer = shap.KernelExplainer(svm.predict_proba, X_train, link=""logit"")\n', 'shap_values = explainer.shap_values(X_test, nsamples=100)\n', '\n', '# plot the SHAP values for the Setosa output of the first instance\n', 'shap.force_plot(explainer.expected_value[0], shap_values[0][0,:], X_test.iloc[0,:], link=""logit"")\n', '```\n', '<p align=""center"">\n', '  <img width=""810"" src=""https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/iris_instance.png"" />\n', '</p>\n', '\n', 'The above explanation shows four features each contributing to push the model output from the base value (the average model output over the training dataset we passed) towards zero. If there were any features pushing the class label higher they would be shown in red.\n', '\n', 'If we take many explanations such as the one shown above, rotate them 90 degrees, and then stack them horizontally, we can see explanations for an entire dataset. This is exactly what we do below for all the examples in the iris test set:\n', '\n', '```python\n', '# plot the SHAP values for the Setosa output of all instances\n', 'shap.force_plot(explainer.expected_value[0], shap_values[0], X_test, link=""logit"")\n', '```\n', '<p align=""center"">\n', '  <img width=""813"" src=""https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/iris_dataset.png"" />\n', '</p>\n', '\n', '## SHAP Interaction Values\n', '\n', 'SHAP interaction values are a generalization of SHAP values to higher order interactions. Fast exact computation of pairwise interactions are implemented for tree models with `shap.TreeExplainer(model).shap_interaction_values(X)`. This returns a matrix for every prediction, where the main effects are on the diagonal and the interaction effects are off-diagonal. These values often reveal interesting hidden relationships, such as how the increased risk of death peaks for men at age 60 (see the NHANES notebook for details):\n', '\n', '<p align=""center"">\n', '  <img width=""483"" src=""https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/nhanes_age_sex_interaction.png"" />\n', '</p>\n', '\n', '## Sample notebooks\n', '\n', 'The notebooks below demonstrate different use cases for SHAP. Look inside the notebooks directory of the repository if you want to try playing with the original notebooks yourself.\n', '\n', '### TreeExplainer\n', '\n', 'An implementation of Tree SHAP, a fast and exact algorithm to compute SHAP values for trees and ensembles of trees.\n', '\n', '- [**NHANES survival model with XGBoost and SHAP interaction values**](https://slundberg.github.io/shap/notebooks/NHANES%20I%20Survival%20Model.html) - Using mortality data from 20 years of followup this notebook demonstrates how to use XGBoost and `shap` to uncover complex risk factor relationships.\n', '\n', '- [**Census income classification with LightGBM**](https://slundberg.github.io/shap/notebooks/tree_explainer/Census%20income%20classification%20with%20LightGBM.html) - Using the standard adult census income dataset, this notebook trains a gradient boosting tree model with LightGBM and then explains predictions using `shap`.\n', '\n', '- [**League of Legends Win Prediction with XGBoost**](https://slundberg.github.io/shap/notebooks/League%20of%20Legends%20Win%20Prediction%20with%20XGBoost.html) - Using a Kaggle dataset of 180,000 ranked matches from League of Legends we train and explain a gradient boosting tree model with XGBoost to predict if a player will win their match.\n', '\n', '### DeepExplainer\n', '\n', 'An implementation of Deep SHAP, a faster (but only approximate) algorithm to compute SHAP values for deep learning models that is based on connections between SHAP and the DeepLIFT algorithm.\n', '\n', '- [**MNIST Digit classification with Keras**](https://slundberg.github.io/shap/notebooks/deep_explainer/Front%20Page%20DeepExplainer%20MNIST%20Example.html) - Using the MNIST handwriting recognition dataset, this notebook trains a neural network with Keras and then explains predictions using `shap`.\n', '\n', '- [**Keras LSTM for IMDB Sentiment Classification**](https://slundberg.github.io/shap/notebooks/deep_explainer/Keras%20LSTM%20for%20IMDB%20Sentiment%20Classification.html) - This notebook trains an LSTM with Keras on the IMDB text sentiment analysis dataset and then explains predictions using `shap`. \n', '\n', '### GradientExplainer\n', '\n', 'An implementation of expected gradients to approximate SHAP values for deep learning models. It is based on connections between SHAP and the Integrated Gradients algorithm. GradientExplainer is slower than DeepExplainer and makes different approximation assumptions.\n', '\n', '- [**Explain an Intermediate Layer of VGG16 on ImageNet**](https://slundberg.github.io/shap/notebooks/gradient_explainer/Explain%20an%20Intermediate%20Layer%20of%20VGG16%20on%20ImageNet.html) - This notebook demonstrates how to explain the output of a pre-trained VGG16 ImageNet model using an internal convolutional layer.\n', '\n', '### LinearExplainer\n', '\n', 'For a linear model with independent features we can analytically compute the exact SHAP values. We can also account for feature correlation if we are willing to estimate the feature covariance matrix. LinearExplainer supports both of these options.\n', '\n', '- [**Sentiment Analysis with Logistic Regression**](https://slundberg.github.io/shap/notebooks/linear_explainer/Sentiment%20Analysis%20with%20Logistic%20Regression.html) - This notebook demonstrates how to explain a linear logistic regression sentiment analysis model.\n', '\n', '### KernelExplainer\n', '\n', 'An implementation of Kernel SHAP, a model agnostic method to estimate SHAP values for any model. Because it makes no assumptions about the model type, KernelExplainer is slower than the other model type specific algorithms.\n', '\n', '- [**Census income classification with scikit-learn**](https://slundberg.github.io/shap/notebooks/Census%20income%20classification%20with%20scikit-learn.html) - Using the standard adult census income dataset, this notebook trains a k-nearest neighbors classifier using scikit-learn and then explains predictions using `shap`.\n', '\n', ""- [**ImageNet VGG16 Model with Keras**](https://slundberg.github.io/shap/notebooks/ImageNet%20VGG16%20Model%20with%20Keras.html) - Explain the classic VGG16 convolutional nerual network's predictions for an image. This works by applying the model agnostic Kernel SHAP method to a super-pixel segmented image.\n"", '\n', '- [**Iris classification**](https://slundberg.github.io/shap/notebooks/Iris%20classification%20with%20scikit-learn.html) - A basic demonstration using the popular iris species dataset. It explains predictions from six different models in scikit-learn using `shap`.\n', '\n', '## Documentation notebooks\n', '\n', 'These notebooks comprehensively demonstrate how to use specific functions and objects. \n', '\n', '- [`shap.decision_plot` and `shap.multioutput_decision_plot`](https://slundberg.github.io/shap/notebooks/plots/decision_plot.html)\n', '\n', '- [`shap.dependence_plot`](https://slundberg.github.io/shap/notebooks/plots/dependence_plot.html)\n', '\n', '## Methods Unified by SHAP\n', '\n', '1. *LIME:* Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. ""Why should i trust you?: Explaining the predictions of any classifier."" Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2016.\n', '\n', '2. *Shapley sampling values:* Strumbelj, Erik, and Igor Kononenko. ""Explaining prediction models and individual predictions with feature contributions."" Knowledge and information systems 41.3 (2014): 647-665.\n', '\n', '3. *DeepLIFT:* Shrikumar, Avanti, Peyton Greenside, and Anshul Kundaje. ""Learning important features through propagating activation differences."" arXiv preprint arXiv:1704.02685 (2017).\n', '\n', '4. *QII:* Datta, Anupam, Shayak Sen, and Yair Zick. ""Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems."" Security and Privacy (SP), 2016 IEEE Symposium on. IEEE, 2016.\n', '\n', '5. *Layer-wise relevance propagation:* Bach, Sebastian, et al. ""On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation."" PloS one 10.7 (2015): e0130140.\n', '\n', '6. *Shapley regression values:* Lipovetsky, Stan, and Michael Conklin. ""Analysis of regression in game theory approach."" Applied Stochastic Models in Business and Industry 17.4 (2001): 319-330.\n', '\n', '7. *Tree interpreter:* Saabas, Ando. Interpreting random forests. http://blog.datadive.net/interpreting-random-forests/\n', '\n', '## Citations\n', '\n', ""The algorithms and visualizations used in this package came primarily out of research in [Su-In Lee's lab](https://suinlee.cs.washington.edu) at the University of Washington, and Microsoft Research. If you use SHAP in your research we would appreciate a citation to the appropriate paper(s):\n"", '\n', '- For general use of SHAP you can read/cite our [NeurIPS paper](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions) ([bibtex](https://raw.githubusercontent.com/slundberg/shap/master/docs/references/shap_nips.bib)). \n', '- For TreeExplainer you can read/cite our [Nature Machine Intelligence paper](https://www.nature.com/articles/s42256-019-0138-9) ([bibtex](https://raw.githubusercontent.com/slundberg/shap/master/docs/references/tree_explainer.bib); [free access](https://rdcu.be/b0z70)).\n', '- For GPUTreeExplainer you can read/cite [this article](https://arxiv.org/abs/2010.13972).\n', '- For `force_plot` visualizations and medical applications you can read/cite our [Nature Biomedical Engineering paper](https://www.nature.com/articles/s41551-018-0304-0) ([bibtex](https://raw.githubusercontent.com/slundberg/shap/master/docs/references/nature_bme.bib); [free access](https://rdcu.be/baVbR)).\n', '\n', '<img height=""1"" width=""1"" style=""display:none"" src=""https://www.facebook.com/tr?id=189147091855991&ev=PageView&noscript=1"" />\n']"
Model Explainability,SeldonIO/alibi,SeldonIO,https://api.github.com/repos/SeldonIO/alibi,2010,222,18,"['https://api.github.com/users/jklaise', 'https://api.github.com/users/RobertSamoilescu', 'https://api.github.com/users/ascillitoe', 'https://api.github.com/users/dependabot%5Bbot%5D', 'https://api.github.com/users/mauicv', 'https://api.github.com/users/alexcoca', 'https://api.github.com/users/arnaudvl', 'https://api.github.com/users/gipster', 'https://api.github.com/users/MarcoGorelli', 'https://api.github.com/users/adriangonz', 'https://api.github.com/users/ahousley', 'https://api.github.com/users/ChristopherGS', 'https://api.github.com/users/daavoo', 'https://api.github.com/users/jimbudarz', 'https://api.github.com/users/sanjass', 'https://api.github.com/users/VinceShieh', 'https://api.github.com/users/abs428', 'https://api.github.com/users/oscarfco']",Python,2023-04-26T13:11:54Z,https://raw.githubusercontent.com/SeldonIO/alibi/master/README.md,"['<p align=""center"">\n', '  <img src=""https://raw.githubusercontent.com/SeldonIO/alibi/master/doc/source/_static/Alibi_Explain_Logo_rgb.png"" alt=""Alibi Logo"" width=""50%"">\n', '</p>\n', '\n', '<!--- BADGES: START --->\n', '\n', '[![Build Status](https://github.com/SeldonIO/alibi-detect/workflows/CI/badge.svg?branch=master)][#build-status]\n', '[![Documentation Status](https://readthedocs.org/projects/alibi/badge/?version=latest)][#docs-package]\n', '[![codecov](https://codecov.io/gh/SeldonIO/alibi/branch/master/graph/badge.svg)](https://codecov.io/gh/SeldonIO/alibi)\n', '[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/alibi?logo=pypi&style=flat&color=blue)][#pypi-package]\n', '[![PyPI - Package Version](https://img.shields.io/pypi/v/alibi?logo=pypi&style=flat&color=orange)][#pypi-package]\n', '[![Conda (channel only)](https://img.shields.io/conda/vn/conda-forge/alibi?logo=anaconda&style=flat&color=orange)][#conda-forge-package]\n', '[![GitHub - License](https://img.shields.io/github/license/SeldonIO/alibi?logo=github&style=flat&color=green)][#github-license]\n', '[![Slack channel](https://img.shields.io/badge/chat-on%20slack-e51670.svg)][#slack-channel]\n', '\n', '<!--- Hide platform for now as platform agnostic --->\n', '<!--- [![Conda - Platform](https://img.shields.io/conda/pn/conda-forge/alibi?logo=anaconda&style=flat)][#conda-forge-package]--->\n', '\n', '[#github-license]: https://github.com/SeldonIO/alibi/blob/master/LICENSE\n', '[#pypi-package]: https://pypi.org/project/alibi/\n', '[#conda-forge-package]: https://anaconda.org/conda-forge/alibi\n', '[#docs-package]: https://docs.seldon.io/projects/alibi/en/stable/\n', '[#build-status]: https://github.com/SeldonIO/alibi/actions?query=workflow%3A%22CI%22\n', '[#slack-channel]: https://join.slack.com/t/seldondev/shared_invite/zt-vejg6ttd-ksZiQs3O_HOtPQsen_labg\n', '<!--- BADGES: END --->\n', '---\n', '\n', '[Alibi](https://docs.seldon.io/projects/alibi) is an open source Python library aimed at machine learning model inspection and interpretation.\n', 'The focus of the library is to provide high-quality implementations of black-box, white-box, local and global\n', 'explanation methods for classification and regression models.\n', '*  [Documentation](https://docs.seldon.io/projects/alibi/en/stable/)\n', '\n', ""If you're interested in outlier detection, concept drift or adversarial instance detection, check out our sister project [alibi-detect](https://github.com/SeldonIO/alibi-detect).\n"", '\n', '<table>\n', '  <tr valign=""top"">\n', '    <td width=""50%"" >\n', '        <a href=""https://docs.seldon.io/projects/alibi/en/stable/examples/anchor_image_imagenet.html"">\n', '            <br>\n', '            <b>Anchor explanations for images</b>\n', '            <br>\n', '            <br>\n', '            <img src=""https://github.com/SeldonIO/alibi/raw/master/doc/source/_static/anchor_image.png"">\n', '        </a>\n', '    </td>\n', '    <td width=""50%"">\n', '        <a href=""https://docs.seldon.io/projects/alibi/en/stable/examples/integrated_gradients_imdb.html"">\n', '            <br>\n', '            <b>Integrated Gradients for text</b>\n', '            <br>\n', '            <br>\n', '            <img src=""https://github.com/SeldonIO/alibi/raw/master/doc/source/_static/ig_text.png"">\n', '        </a>\n', '    </td>\n', '  </tr>\n', '  <tr valign=""top"">\n', '    <td width=""50%"">\n', '        <a href=""https://docs.seldon.io/projects/alibi/en/stable/methods/CFProto.html"">\n', '            <br>\n', '            <b>Counterfactual examples</b>\n', '            <br>\n', '            <br>\n', '            <img src=""https://github.com/SeldonIO/alibi/raw/master/doc/source/_static/cf.png"">\n', '        </a>\n', '    </td>\n', '    <td width=""50%"">\n', '        <a href=""https://docs.seldon.io/projects/alibi/en/stable/methods/ALE.html"">\n', '            <br>\n', '            <b>Accumulated Local Effects</b>\n', '            <br>\n', '            <br>\n', '            <img src=""https://github.com/SeldonIO/alibi/raw/master/doc/source/_static/ale.png"">\n', '        </a>\n', '    </td>\n', '  </tr>\n', '</table>\n', '\n', '## Table of Contents\n', '\n', '* [Installation and Usage](#installation-and-usage)\n', '* [Supported Methods](#supported-methods)\n', '  * [Model Explanations](#model-explanations)\n', '  * [Model Confidence](#model-confidence)\n', '  * [Prototypes](#prototypes)\n', '  * [References and Examples](#references-and-examples)\n', '* [Citations](#citations)\n', '\n', '## Installation and Usage\n', 'Alibi can be installed from:\n', '\n', '- PyPI or GitHub source (with `pip`)\n', '- Anaconda (with `conda`/`mamba`)\n', '\n', '### With pip\n', '\n', '- Alibi can be installed from [PyPI](https://pypi.org/project/alibi):\n', '\n', '  ```bash\n', '  pip install alibi\n', '  ```\n', '  \n', '- Alternatively, the development version can be installed:\n', '  ```bash\n', '  pip install git+https://github.com/SeldonIO/alibi.git \n', '  ```\n', '\n', '- To take advantage of distributed computation of explanations, install `alibi` with `ray`:\n', '  ```bash\n', '  pip install alibi[ray]\n', '  ```\n', '\n', '- For SHAP support, install `alibi` as follows:\n', '  ```bash\n', '  pip install alibi[shap]\n', '  ```\n', '\n', '### With conda \n', '\n', 'To install from [conda-forge](https://conda-forge.org/) it is recommended to use [mamba](https://mamba.readthedocs.io/en/stable/), \n', 'which can be installed to the *base* conda enviroment with:\n', '\n', '```bash\n', 'conda install mamba -n base -c conda-forge\n', '```\n', '\n', '- For the standard Alibi install:\n', '  ```bash\n', '  mamba install -c conda-forge alibi\n', '  ```\n', '\n', '- For distributed computing support:\n', '  ```bash\n', '  mamba install -c conda-forge alibi ray\n', '  ```\n', '\n', '- For SHAP support:\n', '  ```bash\n', '  mamba install -c conda-forge alibi shap\n', '  ```\n', '\n', '### Usage\n', 'The alibi explanation API takes inspiration from `scikit-learn`, consisting of distinct initialize,\n', 'fit and explain steps. We will use the [AnchorTabular](https://docs.seldon.io/projects/alibi/en/stable/methods/Anchors.html)\n', 'explainer to illustrate the API:\n', '\n', '```python\n', 'from alibi.explainers import AnchorTabular\n', '\n', '# initialize and fit explainer by passing a prediction function and any other required arguments\n', 'explainer = AnchorTabular(predict_fn, feature_names=feature_names, category_map=category_map)\n', 'explainer.fit(X_train)\n', '\n', '# explain an instance\n', 'explanation = explainer.explain(x)\n', '```\n', '\n', 'The explanation returned is an `Explanation` object with attributes `meta` and `data`. `meta` is a dictionary\n', 'containing the explainer metadata and any hyperparameters and `data` is a dictionary containing everything\n', 'related to the computed explanation. For example, for the Anchor algorithm the explanation can be accessed\n', ""via `explanation.data['anchor']` (or `explanation.anchor`). The exact details of available fields varies\n"", 'from method to method so we encourage the reader to become familiar with the\n', '[types of methods supported](https://docs.seldon.io/projects/alibi/en/stable/overview/algorithms.html).\n', ' \n', '\n', '## Supported Methods\n', 'The following tables summarize the possible use cases for each method.\n', '\n', '### Model Explanations\n', '| Method                                                                                                       |    Models    |     Explanations      | Classification | Regression | Tabular | Text | Images | Categorical features | Train set required | Distributed |\n', '|:-------------------------------------------------------------------------------------------------------------|:------------:|:---------------------:|:--------------:|:----------:|:-------:|:----:|:------:|:--------------------:|:------------------:|:-----------:|\n', '| [ALE](https://docs.seldon.io/projects/alibi/en/stable/methods/ALE.html)                                      |      BB      |        global         |       ✔        |     ✔      |    ✔    |      |        |                      |                    |             |\n', '| [Partial Dependence](https://docs.seldon.io/projects/alibi/en/stable/methods/PartialDependence.html)         |    BB WB     |        global         |       ✔        |     ✔      |    ✔    |      |        |          ✔           |                    |             |\n', '| [PD Variance](https://docs.seldon.io/projects/alibi/en/stable/methods/PartialDependenceVariance.html)        |    BB WB     |        global         |       ✔        |     ✔      |    ✔    |      |        |          ✔           |                    |             |\n', '| [Permutation Importance](https://docs.seldon.io/projects/alibi/en/stable/methods/PermutationImportance.html) |      BB      |        global         |       ✔        |     ✔      |    ✔    |      |        |          ✔           |                    |             |\n', '| [Anchors](https://docs.seldon.io/projects/alibi/en/stable/methods/Anchors.html)                              |      BB      |         local         |       ✔        |            |    ✔    |  ✔   |   ✔    |          ✔           |    For Tabular     |             |\n', '| [CEM](https://docs.seldon.io/projects/alibi/en/stable/methods/CEM.html)                                      | BB* TF/Keras |         local         |       ✔        |            |    ✔    |      |   ✔    |                      |      Optional      |             |\n', '| [Counterfactuals](https://docs.seldon.io/projects/alibi/en/stable/methods/CF.html)                           | BB* TF/Keras |         local         |       ✔        |            |    ✔    |      |   ✔    |                      |         No         |             |\n', '| [Prototype Counterfactuals](https://docs.seldon.io/projects/alibi/en/stable/methods/CFProto.html)            | BB* TF/Keras |         local         |       ✔        |            |    ✔    |      |   ✔    |          ✔           |      Optional      |             |\n', '| [Counterfactuals with RL](https://docs.seldon.io/projects/alibi/en/stable/methods/CFRL.html)                 |      BB      |         local         |       ✔        |            |    ✔    |      |   ✔    |          ✔           |         ✔          |             |\n', '| [Integrated Gradients](https://docs.seldon.io/projects/alibi/en/stable/methods/IntegratedGradients.html)     |   TF/Keras   |         local         |       ✔        |     ✔      |    ✔    |  ✔   |   ✔    |          ✔           |      Optional      |             |\n', '| [Kernel SHAP](https://docs.seldon.io/projects/alibi/en/stable/methods/KernelSHAP.html)                       |      BB      | local <br></br>global |       ✔        |     ✔      |    ✔    |      |        |          ✔           |         ✔          |      ✔      |\n', '| [Tree SHAP](https://docs.seldon.io/projects/alibi/en/stable/methods/TreeSHAP.html)                           |      WB      | local <br></br>global |       ✔        |     ✔      |    ✔    |      |        |          ✔           |      Optional      |             |\n', '| [Similarity explanations](https://docs.seldon.io/projects/alibi/en/stable/methods/Similarity.html)           |      WB      |         local         |       ✔        |     ✔      |    ✔    |  ✔   |   ✔    |          ✔           |         ✔          |             |\n', '\n', '### Model Confidence\n', 'These algorithms provide **instance-specific** scores measuring the model confidence for making a\n', 'particular prediction.\n', '\n', '|Method|Models|Classification|Regression|Tabular|Text|Images|Categorical Features|Train set required|\n', '|:---|:---|:---:|:---:|:---:|:---:|:---:|:---:|:---|\n', '|[Trust Scores](https://docs.seldon.io/projects/alibi/en/stable/methods/TrustScores.html)|BB|✔| |✔|✔(1)|✔(2)| |Yes|\n', '|[Linearity Measure](https://docs.seldon.io/projects/alibi/en/stable/methods/LinearityMeasure.html)|BB|✔|✔|✔| |✔| |Optional|\n', '\n', 'Key:\n', ' - **BB** - black-box (only require a prediction function)\n', ' - **BB\\*** - black-box but assume model is differentiable\n', ' - **WB** - requires white-box model access. There may be limitations on models supported\n', ' - **TF/Keras** - TensorFlow models via the Keras API\n', ' - **Local** - instance specific explanation, why was this prediction made?\n', ' - **Global** - explains the model with respect to a set of instances\n', ' - **(1)** -  depending on model\n', ' - **(2)** -  may require dimensionality reduction\n', '\n', '### Prototypes\n', 'These algorithms provide a **distilled** view of the dataset and help construct a 1-KNN **interpretable** classifier.\n', '\n', '|Method|Classification|Regression|Tabular|Text|Images|Categorical Features|Train set labels|\n', '|:-----|:-------------|:---------|:------|:---|:-----|:-------------------|:---------------|\n', '|[ProtoSelect](https://docs.seldon.io/projects/alibi/en/latest/methods/ProtoSelect.html)|✔| |✔|✔|✔|✔| Optional       |\n', '\n', '\n', '## References and Examples\n', '- Accumulated Local Effects (ALE, [Apley and Zhu, 2016](https://arxiv.org/abs/1612.08468))\n', '  - [Documentation](https://docs.seldon.io/projects/alibi/en/stable/methods/ALE.html)\n', '  - Examples:\n', '    [California housing dataset](https://docs.seldon.io/projects/alibi/en/stable/examples/ale_regression_california.html),\n', '    [Iris dataset](https://docs.seldon.io/projects/alibi/en/stable/examples/ale_classification.html)\n', '\n', '- Partial Dependence ([J.H. Friedman, 2001](https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boostingmachine/10.1214/aos/1013203451.full))\n', '  - [Documentation](https://docs.seldon.io/projects/alibi/en/stable/methods/PartialDependence.html)\n', '  - Examples:\n', '    [Bike rental](https://docs.seldon.io/projects/alibi/en/stable/examples/pdp_regression_bike.html)\n', '\n', '- Partial Dependence Variance([Greenwell et al., 2018](https://arxiv.org/abs/1805.04755))\n', '  - [Documentation](https://docs.seldon.io/projects/alibi/en/stable/methods/PartialDependenceVariance.html)\n', '  - Examples:\n', '    [Friedman’s regression problem](https://docs.seldon.io/projects/alibi/en/stable/examples/pd_variance_regression_friedman.html)\n', '\n', '- Permutation Importance([Breiman, 2001](https://link.springer.com/article/10.1023/A:1010933404324); [Fisher et al., 2018](https://arxiv.org/abs/1801.01489))\n', '  - [Documentation](https://docs.seldon.io/projects/alibi/en/stable/methods/PermutationImportance.html)\n', '  - Examples:\n', ""    [Who's Going to Leave Next?](https://docs.seldon.io/projects/alibi/en/stable/examples/permutation_importance_classification_leave.html)\n"", '\n', '- Anchor explanations ([Ribeiro et al., 2018](https://homes.cs.washington.edu/~marcotcr/aaai18.pdf))\n', '  - [Documentation](https://docs.seldon.io/projects/alibi/en/stable/methods/Anchors.html)\n', '  - Examples:\n', '    [income prediction](https://docs.seldon.io/projects/alibi/en/stable/examples/anchor_tabular_adult.html),\n', '    [Iris dataset](https://docs.seldon.io/projects/alibi/en/stable/examples/anchor_tabular_iris.html),\n', '    [movie sentiment classification](https://docs.seldon.io/projects/alibi/en/stable/examples/anchor_text_movie.html),\n', '    [ImageNet](https://docs.seldon.io/projects/alibi/en/stable/examples/anchor_image_imagenet.html),\n', '    [fashion MNIST](https://docs.seldon.io/projects/alibi/en/stable/examples/anchor_image_fashion_mnist.html)\n', '\n', '- Contrastive Explanation Method (CEM, [Dhurandhar et al., 2018](https://papers.nips.cc/paper/7340-explanations-based-on-the-missing-towards-contrastive-explanations-with-pertinent-negatives))\n', '  - [Documentation](https://docs.seldon.io/projects/alibi/en/stable/methods/CEM.html)\n', '  - Examples: [MNIST](https://docs.seldon.io/projects/alibi/en/stable/examples/cem_mnist.html),\n', '    [Iris dataset](https://docs.seldon.io/projects/alibi/en/stable/examples/cem_iris.html)\n', '\n', '- Counterfactual Explanations (extension of\n', '  [Wachter et al., 2017](https://arxiv.org/abs/1711.00399))\n', '  - [Documentation](https://docs.seldon.io/projects/alibi/en/stable/methods/CF.html)\n', '  - Examples: \n', '    [MNIST](https://docs.seldon.io/projects/alibi/en/stable/examples/cf_mnist.html)\n', '\n', '- Counterfactual Explanations Guided by Prototypes ([Van Looveren and Klaise, 2019](https://arxiv.org/abs/1907.02584))\n', '  - [Documentation](https://docs.seldon.io/projects/alibi/en/stable/methods/CFProto.html)\n', '  - Examples:\n', '    [MNIST](https://docs.seldon.io/projects/alibi/en/stable/examples/cfproto_mnist.html),\n', '    [California housing dataset](https://docs.seldon.io/projects/alibi/en/stable/examples/cfproto_housing.html),\n', '    [Adult income (one-hot)](https://docs.seldon.io/projects/alibi/en/stable/examples/cfproto_cat_adult_ohe.html),\n', '    [Adult income (ordinal)](https://docs.seldon.io/projects/alibi/en/stable/examples/cfproto_cat_adult_ord.html)\n', '\n', '- Model-agnostic Counterfactual Explanations via RL([Samoilescu et al., 2021](https://arxiv.org/abs/2106.02597))\n', '  - [Documentation](https://docs.seldon.io/projects/alibi/en/stable/methods/CFRL.html)\n', '  - Examples:\n', '    [MNIST](https://docs.seldon.io/projects/alibi/en/stable/examples/cfrl_mnist.html),\n', '    [Adult income](https://docs.seldon.io/projects/alibi/en/stable/examples/cfrl_adult.html)\n', '\n', '- Integrated Gradients ([Sundararajan et al., 2017](https://arxiv.org/abs/1703.01365))\n', '  - [Documentation](https://docs.seldon.io/projects/alibi/en/stable/methods/IntegratedGradients.html),\n', '  - Examples:\n', '    [MNIST example](https://docs.seldon.io/projects/alibi/en/stable/examples/integrated_gradients_mnist.html),\n', '    [Imagenet example](https://docs.seldon.io/projects/alibi/en/stable/examples/integrated_gradients_imagenet.html),\n', '    [IMDB example](https://docs.seldon.io/projects/alibi/en/stable/examples/integrated_gradients_imdb.html).\n', '\n', '- Kernel Shapley Additive Explanations ([Lundberg et al., 2017](https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions))\n', '  - [Documentation](https://docs.seldon.io/projects/alibi/en/stable/methods/KernelSHAP.html)\n', '  - Examples:\n', '    [SVM with continuous data](https://docs.seldon.io/projects/alibi/en/stable/examples/kernel_shap_wine_intro.html),\n', '    [multinomial logistic regression with continous data](https://docs.seldon.io/projects/alibi/en/stable/examples/kernel_shap_wine_lr.html),\n', '    [handling categorical variables](https://docs.seldon.io/projects/alibi/en/stable/examples/kernel_shap_adult_lr.html)\n', '    \n', '- Tree Shapley Additive Explanations ([Lundberg et al., 2020](https://www.nature.com/articles/s42256-019-0138-9))\n', '  - [Documentation](https://docs.seldon.io/projects/alibi/en/stable/methods/TreeSHAP.html)\n', '  - Examples:\n', '    [Interventional (adult income, xgboost)](https://docs.seldon.io/projects/alibi/en/stable/examples/interventional_tree_shap_adult_xgb.html),\n', '    [Path-dependent (adult income, xgboost)](https://docs.seldon.io/projects/alibi/en/stable/examples/path_dependent_tree_shap_adult_xgb.html)\n', '    \n', '- Trust Scores ([Jiang et al., 2018](https://arxiv.org/abs/1805.11783))\n', '  - [Documentation](https://docs.seldon.io/projects/alibi/en/stable/methods/TrustScores.html)\n', '  - Examples:\n', '    [MNIST](https://docs.seldon.io/projects/alibi/en/stable/examples/trustscore_mnist.html),\n', '    [Iris dataset](https://docs.seldon.io/projects/alibi/en/stable/examples/trustscore_mnist.html)\n', '\n', '- Linearity Measure\n', '  - [Documentation](https://docs.seldon.io/projects/alibi/en/stable/methods/LinearityMeasure.html)\n', '  - Examples:\n', '    [Iris dataset](https://docs.seldon.io/projects/alibi/en/stable/examples/linearity_measure_iris.html),\n', '    [fashion MNIST](https://docs.seldon.io/projects/alibi/en/stable/examples/linearity_measure_fashion_mnist.html)\n', '\n', '- ProtoSelect\n', '  - [Documentation](https://docs.seldon.io/projects/alibi/en/latest/methods/ProtoSelect.html)\n', '  - Examples:\n', '    [Adult Census & CIFAR10](https://docs.seldon.io/projects/alibi/en/latest/examples/protoselect_adult_cifar10.html)\n', '\n', '- Similarity explanations\n', '  - [Documentation](https://docs.seldon.io/projects/alibi/en/stable/methods/Similarity.html)\n', '  - Examples:\n', '    [20 news groups dataset](https://docs.seldon.io/projects/alibi/en/stable/examples/similarity_explanations_20ng.html),\n', '    [ImageNet dataset](https://docs.seldon.io/projects/alibi/en/stable/examples/similarity_explanations_imagenet.html),\n', '    [MNIST dataset](https://docs.seldon.io/projects/alibi/en/stable/examples/similarity_explanations_mnist.html)\n', '\n', '## Citations\n', 'If you use alibi in your research, please consider citing it.\n', '\n', 'BibTeX entry:\n', '\n', '```\n', '@article{JMLR:v22:21-0017,\n', '  author  = {Janis Klaise and Arnaud Van Looveren and Giovanni Vacanti and Alexandru Coca},\n', '  title   = {Alibi Explain: Algorithms for Explaining Machine Learning Models},\n', '  journal = {Journal of Machine Learning Research},\n', '  year    = {2021},\n', '  volume  = {22},\n', '  number  = {181},\n', '  pages   = {1-7},\n', '  url     = {http://jmlr.org/papers/v22/21-0017.html}\n', '}\n', '```\n']"
Model Explainability,cdpierse/transformers-interpret,cdpierse,https://api.github.com/repos/cdpierse/transformers-interpret,1006,85,7,"['https://api.github.com/users/cdpierse', 'https://api.github.com/users/pabvald', 'https://api.github.com/users/cwenner', 'https://api.github.com/users/lalitpagaria', 'https://api.github.com/users/Voyz', 'https://api.github.com/users/rinapch', 'https://api.github.com/users/dependabot%5Bbot%5D']",Python,2023-04-26T03:54:18Z,https://raw.githubusercontent.com/cdpierse/transformers-interpret/master/README.md,"['<p align=""center"">\n', '    <a id=""transformers-intepret"" href=""#transformers-intepret"">\n', '        <img src=""https://github.com/cdpierse/transformers-interpret/blob/master/images/tight%401920x_transparent.png"" alt=""Transformers Intepret Title"" title=""Transformers Intepret Title"" width=""600""/>\n', '    </a>\n', '</p>\n', '\n', '<p align=""center""> Explainability for any 🤗 Transformers models in 2 lines.</p>\n', '\n', '<h1 align=""center""></h1>\n', '\n', '<p align=""center"">\n', '    <a href=""https://opensource.org/licenses/Apache-2.0"">\n', '        <img src=""https://img.shields.io/badge/License-Apache%202.0-blue.svg""/>\n', '    <a href=""https://github.com/cdpierse/transformers-interpret/actions/workflows/unit_tests.yml"">\n', '        <img src=""https://github.com/cdpierse/transformers-interpret/actions/workflows/unit_tests.yml/badge.svg"">\n', '    </a>\n', '            <a href=""https://github.com/cdpierse/transformers-interpret/releases"">\n', '        <img src=""https://img.shields.io/pypi/v/transformers_interpret?label=version""/>\n', '    </a>\n', '        <a href=""https://pepy.tech/project/transformers-interpret"">\n', '        <img src=""https://static.pepy.tech/personalized-badge/transformers-interpret?period=total&units=abbreviation&left_color=black&right_color=brightgreen&left_text=Downloads"">\n', '    </a>\n', '</p>\n', '\n', 'Transformers Interpret is a model explainability tool designed to work exclusively with the 🤗 [transformers][transformers] package.\n', '\n', 'In line with the philosophy of the Transformers package Transformers Interpret allows any transformers model to be explained in just two lines. Explainers are available for both text and computer vision models. Visualizations are also available in notebooks and as savable png and html files.\n', '\n', 'Check out the streamlit [demo app here](https://share.streamlit.io/cdpierse/transformers-interpret-streamlit/main/app.py)\n', '\n', '## Install\n', '\n', '```posh\n', 'pip install transformers-interpret\n', '```\n', '\n', '## Quick Start\n', '\n', '### Text Explainers\n', '\n', '<details><summary>Sequence Classification Explainer and Pairwise Sequence Classification</summary>\n', '\n', '<p>\n', ""Let's start by initializing a transformers' model and tokenizer, and running it through the `SequenceClassificationExplainer`.\n"", '\n', 'For this example we are using `distilbert-base-uncased-finetuned-sst-2-english`, a distilbert model finetuned on a sentiment analysis task.\n', '\n', '```python\n', 'from transformers import AutoModelForSequenceClassification, AutoTokenizer\n', 'model_name = ""distilbert-base-uncased-finetuned-sst-2-english""\n', 'model = AutoModelForSequenceClassification.from_pretrained(model_name)\n', 'tokenizer = AutoTokenizer.from_pretrained(model_name)\n', '\n', '# With both the model and tokenizer initialized we are now able to get explanations on an example text.\n', '\n', 'from transformers_interpret import SequenceClassificationExplainer\n', 'cls_explainer = SequenceClassificationExplainer(\n', '    model,\n', '    tokenizer)\n', 'word_attributions = cls_explainer(""I love you, I like you"")\n', '```\n', '\n', 'Which will return the following list of tuples:\n', '\n', '```python\n', '>>> word_attributions\n', ""[('[CLS]', 0.0),\n"", "" ('i', 0.2778544699186709),\n"", "" ('love', 0.7792370723380415),\n"", "" ('you', 0.38560088858031094),\n"", "" (',', -0.01769750505546915),\n"", "" ('i', 0.12071898121557832),\n"", "" ('like', 0.19091105304734457),\n"", "" ('you', 0.33994871536713467),\n"", "" ('[SEP]', 0.0)]\n"", '```\n', '\n', 'Positive attribution numbers indicate a word contributes positively towards the predicted class, while negative numbers indicate a word contributes negatively towards the predicted class. Here we can see that **I love you** gets the most attention.\n', '\n', ""You can use `predicted_class_index` in case you'd want to know what the predicted class actually is:\n"", '\n', '```python\n', '>>> cls_explainer.predicted_class_index\n', 'array(1)\n', '```\n', '\n', 'And if the model has label names for each class, we can see these too using `predicted_class_name`:\n', '\n', '```python\n', '>>> cls_explainer.predicted_class_name\n', ""'POSITIVE'\n"", '```\n', '\n', '#### Visualize Classification attributions\n', '\n', ""Sometimes the numeric attributions can be difficult to read particularly in instances where there is a lot of text. To help with that we also provide the `visualize()` method that utilizes Captum's in built viz library to create a HTML file highlighting the attributions.\n"", '\n', 'If you are in a notebook, calls to the `visualize()` method will display the visualization in-line. Alternatively you can pass a filepath in as an argument and an HTML file will be created, allowing you to view the explanation HTML in your browser.\n', '\n', '```python\n', 'cls_explainer.visualize(""distilbert_viz.html"")\n', '```\n', '\n', '<a href=""https://github.com/cdpierse/transformers-interpret/blob/master/images/distilbert_example.png"">\n', '<img src=""https://github.com/cdpierse/transformers-interpret/blob/master/images/distilbert_example.png"" width=""80%"" height=""80%"" align=""center""/>\n', '</a>\n', '\n', '#### Explaining Attributions for Non Predicted Class\n', '\n', ""Attribution explanations are not limited to the predicted class. Let's test a more complex sentence that contains mixed sentiments.\n"", '\n', 'In the example below we pass `class_name=""NEGATIVE""` as an argument indicating we would like the attributions to be explained for the **NEGATIVE** class regardless of what the actual prediction is. Effectively because this is a binary classifier we are getting the inverse attributions.\n', '\n', '```python\n', 'cls_explainer = SequenceClassificationExplainer(model, tokenizer)\n', 'attributions = cls_explainer(""I love you, I like you, I also kinda dislike you"", class_name=""NEGATIVE"")\n', '```\n', '\n', 'In this case, `predicted_class_name` still returns a prediction of the **POSITIVE** class, because the model has generated the same prediction but nonetheless we are interested in looking at the attributions for the negative class regardless of the predicted result.\n', '\n', '```python\n', '>>> cls_explainer.predicted_class_name\n', ""'POSITIVE'\n"", '```\n', '\n', 'But when we visualize the attributions we can see that the words ""**...kinda dislike**"" are contributing to a prediction of the ""NEGATIVE""\n', 'class.\n', '\n', '```python\n', 'cls_explainer.visualize(""distilbert_negative_attr.html"")\n', '```\n', '\n', '<a href=""https://github.com/cdpierse/transformers-interpret/blob/master/images/distilbert_example_negative.png"">\n', '<img src=""https://github.com/cdpierse/transformers-interpret/blob/master/images/distilbert_example_negative.png"" width=""80%"" height=""80%"" align=""center"" />\n', '</a>\n', '\n', 'Getting attributions for different classes is particularly insightful for multiclass problems as it allows you to inspect model predictions for a number of different classes and sanity-check that the model is ""looking"" at the right things.\n', '\n', 'For a detailed explanation of this example please checkout this [multiclass classification notebook.](notebooks/multiclass_classification_example.ipynb)\n', '\n', '### Pairwise Sequence Classification\n', '\n', ""The `PairwiseSequenceClassificationExplainer` is a variant of the the `SequenceClassificationExplainer` that is designed to work with classification models that expect the input sequence to be two inputs separated by a models' separator token. Common examples of this are [NLI models](https://arxiv.org/abs/1705.02364) and [Cross-Encoders ](https://www.sbert.net/docs/pretrained_cross-encoders.html) which are commonly used to score two inputs similarity to one another.\n"", '\n', 'This explainer calculates pairwise attributions for two passed inputs `text1` and `text2` using the model\n', 'and tokenizer given in the constructor.\n', '\n', 'Also, since a common use case for pairwise sequence classification is to compare two inputs similarity - models of this nature typically only have a single output node rather than multiple for each class. The pairwise sequence classification has some useful utility functions to make interpreting single node outputs clearer.\n', '\n', 'By default for models that output a single node the attributions are with respect to the inputs pushing the scores closer to 1.0, however if you want to see the\n', 'attributions with respect to scores closer to 0.0 you can pass `flip_sign=True`. For similarity\n', 'based models this is useful, as the model might predict a score closer to 0.0 for the two inputs\n', 'and in that case we would flip the attributions sign to explain why the two inputs are dissimilar.\n', '\n', ""Let's start by initializing a cross-encoder model and tokenizer from the suite of [pre-trained cross-encoders ](https://www.sbert.net/docs/pretrained_cross-encoders.html)provided by [sentence-transformers](https://github.com/UKPLab/sentence-transformers).\n"", '\n', 'For this example we are using `""cross-encoder/ms-marco-MiniLM-L-6-v2""`, a high quality cross-encoder trained on the [MSMarco dataset](https://github.com/microsoft/MSMARCO-Passage-Ranking) a passage ranking dataset for question answering and machine reading comprehension.\n', '\n', '```python\n', 'from transformers import AutoModelForSequenceClassification, AutoTokenizer\n', '\n', 'from transformers_interpret import PairwiseSequenceClassificationExplainer\n', '\n', 'model = AutoModelForSequenceClassification.from_pretrained(""cross-encoder/ms-marco-MiniLM-L-6-v2"")\n', 'tokenizer = AutoTokenizer.from_pretrained(""cross-encoder/ms-marco-MiniLM-L-6-v2"")\n', '\n', 'pairwise_explainer = PairwiseSequenceClassificationExplainer(model, tokenizer)\n', '\n', '# the pairwise explainer requires two string inputs to be passed, in this case given the nature of the model\n', '# we pass a query string and a context string. The question we are asking of our model is ""does this context contain a valid answer to our question""\n', '# the higher the score the better the fit.\n', '\n', 'query = ""How many people live in Berlin?""\n', 'context = ""Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.""\n', 'pairwise_attr = pairwise_explainer(query, context)\n', '```\n', '\n', 'Which returns the following attributions:\n', '\n', '```python\n', '>>> pairwise_attr\n', ""[('[CLS]', 0.0),\n"", "" ('how', -0.037558652124213034),\n"", "" ('many', -0.40348581975409786),\n"", "" ('people', -0.29756140282349425),\n"", "" ('live', -0.48979015417391764),\n"", "" ('in', -0.17844527885888117),\n"", "" ('berlin', 0.3737346097442739),\n"", "" ('?', -0.2281428913480142),\n"", "" ('[SEP]', 0.0),\n"", "" ('berlin', 0.18282430604641564),\n"", "" ('has', 0.039114659489254834),\n"", "" ('a', 0.0820056652212297),\n"", "" ('population', 0.35712150914643026),\n"", "" ('of', 0.09680870840224687),\n"", "" ('3', 0.04791760029513795),\n"", "" (',', 0.040330986539774266),\n"", "" ('520', 0.16307677913176166),\n"", "" (',', -0.005919693904602767),\n"", "" ('03', 0.019431649515841844),\n"", "" ('##1', -0.0243808667024702),\n"", "" ('registered', 0.07748341753369632),\n"", "" ('inhabitants', 0.23904087299731255),\n"", "" ('in', 0.07553221327346359),\n"", "" ('an', 0.033112821611999875),\n"", "" ('area', -0.025378852244447532),\n"", "" ('of', 0.026526373859562906),\n"", "" ('89', 0.0030700151809002147),\n"", "" ('##1', -0.000410387092186983),\n"", "" ('.', -0.0193147139126114),\n"", "" ('82', 0.0073800833347678774),\n"", "" ('square', 0.028988305990861576),\n"", "" ('kilometers', 0.02071182933829008),\n"", "" ('.', -0.025901070914318036),\n"", "" ('[SEP]', 0.0)]\n"", '```\n', '\n', '#### Visualize Pairwise Classification attributions\n', '\n', 'Visualizing the pairwise attributions is no different to the sequence classification explaine. We can see that in both the `query` and `context` there is a lot of positive attribution for the word `berlin` as well the words `population` and `inhabitants` in the `context`, good signs that our model understands the textual context of the question asked.\n', '\n', '```python\n', 'pairwise_explainer.visualize(""cross_encoder_attr.html"")\n', '```\n', '\n', '<a href=""https://github.com/cdpierse/transformers-interpret/blob/master/images/pairwise_cross_encoder_example.png"">\n', '<img src=""https://github.com/cdpierse/transformers-interpret/blob/master/images/pairwise_cross_encoder_example.png"" width=""100%"" height=""100%"" align=""center"" />\n', '</a>\n', '\n', 'If we were more interested in highlighting the input attributions that pushed the model away from the positive class of this single node output we could pass:\n', '\n', '```python\n', 'pairwise_attr = explainer(query, context, flip_sign=True)\n', '```\n', '\n', 'This simply inverts the sign of the attributions ensuring that they are with respect to the model outputting 0 rather than 1.\n', '\n', '</details>\n', '\n', '<details><summary>MultiLabel Classification Explainer</summary>\n', '<p>\n', '\n', ""This explainer is an extension of the `SequenceClassificationExplainer` and is thus compatible with all sequence classification models from the Transformers package. The key change in this explainer is that it caclulates attributions for each label in the model's config and returns a dictionary of word attributions w.r.t to each label. The `visualize()` method also displays a table of attributions with attributions calculated per label.\n"", '\n', '```python\n', 'from transformers import AutoModelForSequenceClassification, AutoTokenizer\n', 'from transformers_interpret import MultiLabelClassificationExplainer\n', '\n', 'model_name = ""j-hartmann/emotion-english-distilroberta-base""\n', 'model = AutoModelForSequenceClassification.from_pretrained(model_name)\n', 'tokenizer = AutoTokenizer.from_pretrained(model_name)\n', '\n', '\n', 'cls_explainer = MultiLabelClassificationExplainer(model, tokenizer)\n', '\n', '\n', 'word_attributions = cls_explainer(""There were many aspects of the film I liked, but it was frightening and gross in parts. My parents hated it."")\n', '```\n', '\n', ""This produces a dictionary of word attributions mapping labels to a list of tuples for each word and it's attribution score.\n"", '\n', '<details><summary>Click to see word attribution dictionary</summary>\n', '\n', '```python\n', '>>> word_attributions\n', ""{'anger': [('<s>', 0.0),\n"", ""           ('There', 0.09002208622000409),\n"", ""           ('were', -0.025129709879675187),\n"", ""           ('many', -0.028852677974079328),\n"", ""           ('aspects', -0.06341968013631565),\n"", ""           ('of', -0.03587626320752477),\n"", ""           ('the', -0.014813095892961287),\n"", ""           ('film', -0.14087587475098232),\n"", ""           ('I', 0.007367876912617766),\n"", ""           ('liked', -0.09816592066307557),\n"", ""           (',', -0.014259517291745674),\n"", ""           ('but', -0.08087144668471376),\n"", ""           ('it', -0.10185214349220136),\n"", ""           ('was', -0.07132244710777856),\n"", ""           ('frightening', -0.4125361737439814),\n"", ""           ('and', -0.021761663818889918),\n"", ""           ('gross', -0.10423745223600908),\n"", ""           ('in', -0.02383646952201854),\n"", ""           ('parts', -0.027137622525091033),\n"", ""           ('.', -0.02960415694062459),\n"", ""           ('My', 0.05642774605113695),\n"", ""           ('parents', 0.11146648216326158),\n"", ""           ('hated', 0.8497975489280364),\n"", ""           ('it', 0.05358116678115284),\n"", ""           ('.', -0.013566277162080632),\n"", ""           ('', 0.09293256725788422),\n"", ""           ('</s>', 0.0)],\n"", "" 'disgust': [('<s>', 0.0),\n"", ""             ('There', -0.035296263203072),\n"", ""             ('were', -0.010224922196739717),\n"", ""             ('many', -0.03747571761725605),\n"", ""             ('aspects', 0.007696321643436715),\n"", ""             ('of', 0.0026740873113235107),\n"", ""             ('the', 0.0025752851265661335),\n"", ""             ('film', -0.040890035285783645),\n"", ""             ('I', -0.014710007408208579),\n"", ""             ('liked', 0.025696806663391577),\n"", ""             (',', -0.00739107098314569),\n"", ""             ('but', 0.007353791868893654),\n"", ""             ('it', -0.00821368234753605),\n"", ""             ('was', 0.005439709067819798),\n"", ""             ('frightening', -0.8135974168445725),\n"", ""             ('and', -0.002334953123414774),\n"", ""             ('gross', 0.2366024374426269),\n"", ""             ('in', 0.04314772995234148),\n"", ""             ('parts', 0.05590472194035334),\n"", ""             ('.', -0.04362554293972562),\n"", ""             ('My', -0.04252694977895808),\n"", ""             ('parents', 0.051580790911406944),\n"", ""             ('hated', 0.5067406070057585),\n"", ""             ('it', 0.0527491071885104),\n"", ""             ('.', -0.008280280618652273),\n"", ""             ('', 0.07412384603053103),\n"", ""             ('</s>', 0.0)],\n"", "" 'fear': [('<s>', 0.0),\n"", ""          ('There', -0.019615758046045408),\n"", ""          ('were', 0.008033402634196246),\n"", ""          ('many', 0.027772367717635423),\n"", ""          ('aspects', 0.01334130725685673),\n"", ""          ('of', 0.009186049991879768),\n"", ""          ('the', 0.005828877177384549),\n"", ""          ('film', 0.09882910753644959),\n"", ""          ('I', 0.01753565003544039),\n"", ""          ('liked', 0.02062597344466885),\n"", ""          (',', -0.004469530636560965),\n"", ""          ('but', -0.019660439408176984),\n"", ""          ('it', 0.0488084071292538),\n"", ""          ('was', 0.03830859527501167),\n"", ""          ('frightening', 0.9526443954511705),\n"", ""          ('and', 0.02535156284103706),\n"", ""          ('gross', -0.10635301961551227),\n"", ""          ('in', -0.019190425328209065),\n"", ""          ('parts', -0.01713006453323631),\n"", ""          ('.', 0.015043169035757302),\n"", ""          ('My', 0.017068079071414916),\n"", ""          ('parents', -0.0630781275517486),\n"", ""          ('hated', -0.23630028921273583),\n"", ""          ('it', -0.056057044429020306),\n"", ""          ('.', 0.0015102052077844612),\n"", ""          ('', -0.010045048665404609),\n"", ""          ('</s>', 0.0)],\n"", "" 'joy': [('<s>', 0.0),\n"", ""         ('There', 0.04881772670614576),\n"", ""         ('were', -0.0379316152427468),\n"", ""         ('many', -0.007955371089444285),\n"", ""         ('aspects', 0.04437296429416574),\n"", ""         ('of', -0.06407011137335743),\n"", ""         ('the', -0.07331568926973099),\n"", ""         ('film', 0.21588462483311055),\n"", ""         ('I', 0.04885724513463952),\n"", ""         ('liked', 0.5309510543276107),\n"", ""         (',', 0.1339765195225006),\n"", ""         ('but', 0.09394079060730279),\n"", ""         ('it', -0.1462792330432028),\n"", ""         ('was', -0.1358591558323458),\n"", ""         ('frightening', -0.22184169339341142),\n"", ""         ('and', -0.07504142930419291),\n"", ""         ('gross', -0.005472075984252812),\n"", ""         ('in', -0.0942152657437379),\n"", ""         ('parts', -0.19345218754215965),\n"", ""         ('.', 0.11096247277185402),\n"", ""         ('My', 0.06604512262645984),\n"", ""         ('parents', 0.026376541098236207),\n"", ""         ('hated', -0.4988319510231699),\n"", ""         ('it', -0.17532499366236615),\n"", ""         ('.', -0.022609976138939034),\n"", ""         ('', -0.43417114685294833),\n"", ""         ('</s>', 0.0)],\n"", "" 'neutral': [('<s>', 0.0),\n"", ""             ('There', 0.045984598036642205),\n"", ""             ('were', 0.017142566357474697),\n"", ""             ('many', 0.011419348619472542),\n"", ""             ('aspects', 0.02558593440287365),\n"", ""             ('of', 0.0186162232003498),\n"", ""             ('the', 0.015616416841815963),\n"", ""             ('film', -0.021190511300570092),\n"", ""             ('I', -0.03572427925026324),\n"", ""             ('liked', 0.027062554960050455),\n"", ""             (',', 0.02089914209290366),\n"", ""             ('but', 0.025872618597570115),\n"", ""             ('it', -0.002980407262316265),\n"", ""             ('was', -0.022218157611174086),\n"", ""             ('frightening', -0.2982516449116045),\n"", ""             ('and', -0.01604643529040792),\n"", ""             ('gross', -0.04573829263548096),\n"", ""             ('in', -0.006511536166676108),\n"", ""             ('parts', -0.011744224307968652),\n"", ""             ('.', -0.01817041167875332),\n"", ""             ('My', -0.07362312722231429),\n"", ""             ('parents', -0.06910711601816408),\n"", ""             ('hated', -0.9418903509267312),\n"", ""             ('it', 0.022201795222373488),\n"", ""             ('.', 0.025694319747309045),\n"", ""             ('', 0.04276690822325994),\n"", ""             ('</s>', 0.0)],\n"", "" 'sadness': [('<s>', 0.0),\n"", ""             ('There', 0.028237893283377526),\n"", ""             ('were', -0.04489910545229568),\n"", ""             ('many', 0.004996044977269471),\n"", ""             ('aspects', -0.1231292680125582),\n"", ""             ('of', -0.04552690725956671),\n"", ""             ('the', -0.022077819961347042),\n"", ""             ('film', -0.14155752357877663),\n"", ""             ('I', 0.04135347872193571),\n"", ""             ('liked', -0.3097732540526099),\n"", ""             (',', 0.045114660009053134),\n"", ""             ('but', 0.0963352125332619),\n"", ""             ('it', -0.08120617610094617),\n"", ""             ('was', -0.08516150809170213),\n"", ""             ('frightening', -0.10386889639962761),\n"", ""             ('and', -0.03931986389970189),\n"", ""             ('gross', -0.2145059013625132),\n"", ""             ('in', -0.03465423285571697),\n"", ""             ('parts', -0.08676627134611635),\n"", ""             ('.', 0.19025217371906333),\n"", ""             ('My', 0.2582092561303794),\n"", ""             ('parents', 0.15432351476960307),\n"", ""             ('hated', 0.7262186310977987),\n"", ""             ('it', -0.029160655114499095),\n"", ""             ('.', -0.002758524253450406),\n"", ""             ('', -0.33846410359182094),\n"", ""             ('</s>', 0.0)],\n"", "" 'surprise': [('<s>', 0.0),\n"", ""              ('There', 0.07196110795254315),\n"", ""              ('were', 0.1434314520711312),\n"", ""              ('many', 0.08812238369489701),\n"", ""              ('aspects', 0.013432396769890982),\n"", ""              ('of', -0.07127508805657243),\n"", ""              ('the', -0.14079766624810955),\n"", ""              ('film', -0.16881201614906485),\n"", ""              ('I', 0.040595668935112135),\n"", ""              ('liked', 0.03239855530171577),\n"", ""              (',', -0.17676382558158257),\n"", ""              ('but', -0.03797939330341559),\n"", ""              ('it', -0.029191325089641736),\n"", ""              ('was', 0.01758013584108571),\n"", ""              ('frightening', -0.221738963726823),\n"", ""              ('and', -0.05126920277135527),\n"", ""              ('gross', -0.33986913466614044),\n"", ""              ('in', -0.018180366628697),\n"", ""              ('parts', 0.02939418603252064),\n"", ""              ('.', 0.018080129971003226),\n"", ""              ('My', -0.08060162218059498),\n"", ""              ('parents', 0.04351719139081836),\n"", ""              ('hated', -0.6919028585285265),\n"", ""              ('it', 0.0009574844165327357),\n"", ""              ('.', -0.059473118237873344),\n"", ""              ('', -0.465690452620123),\n"", ""              ('</s>', 0.0)]}\n"", '```\n', '\n', '</details>\n', '\n', '#### Visualize MultiLabel Classification attributions\n', '\n', ""Sometimes the numeric attributions can be difficult to read particularly in instances where there is a lot of text. To help with that we also provide the `visualize()` method that utilizes Captum's in built viz library to create a HTML file highlighting the attributions. For this explainer attributions will be show w.r.t to each label.\n"", '\n', 'If you are in a notebook, calls to the `visualize()` method will display the visualization in-line. Alternatively you can pass a filepath in as an argument and an HTML file will be created, allowing you to view the explanation HTML in your browser.\n', '\n', '```python\n', 'cls_explainer.visualize(""multilabel_viz.html"")\n', '```\n', '\n', '<a href=""https://github.com/cdpierse/transformers-interpret/blob/master/images/multilabel_example.png"">\n', '<img src=""https://github.com/cdpierse/transformers-interpret/blob/master/images/multilabel_example.png"" width=""80%"" height=""80%"" align=""center""/>\n', '</a>\n', '\n', '</details>\n', '\n', '<details><summary>Zero Shot Classification Explainer</summary>\n', '\n', '_Models using this explainer must be previously trained on NLI classification downstream tasks and have a label in the model\'s config called either ""entailment"" or ""ENTAILMENT""._\n', '\n', 'This explainer allows for attributions to be calculated for zero shot classification like models. In order to achieve this we use the same methodology employed by Hugging face. For those not familiar method employed by Hugging Face to achieve zero shot classification the way this works is by exploiting the ""entailment"" label of NLI models. Here is a [link](https://arxiv.org/abs/1909.00161) to a paper explaining more about it. A list of NLI models guaranteed to be compatible with this explainer can be found on the [model hub](https://huggingface.co/models?filter=pytorch&pipeline_tag=zero-shot-classification).\n', '\n', ""Let's start by initializing a transformers' sequence classification model and tokenizer trained specifically on a NLI task, and passing it to the ZeroShotClassificationExplainer.\n"", '\n', 'For this example we are using `facebook/bart-large-mnli` which is a checkpoint for a bart-large model trained on the\n', '[MNLI dataset](https://huggingface.co/datasets/multi_nli). This model typically predicts whether a sentence pair are an entailment, neutral, or a contradiction, however for zero-shot we only look the entailment label.\n', '\n', 'Notice that we pass our own custom labels `[""finance"", ""technology"", ""sports""]` to the class instance. Any number of labels can be passed including as little as one. Whichever label scores highest for entailment can be accessed via `predicted_label`, however the attributions themselves are calculated for every label. If you want to see the attributions for a particular label it is recommended just to pass in that one label and then the attributions will be guaranteed to be calculated w.r.t. that label.\n', '\n', '```python\n', 'from transformers import AutoModelForSequenceClassification, AutoTokenizer\n', 'from transformers_interpret import ZeroShotClassificationExplainer\n', '\n', 'tokenizer = AutoTokenizer.from_pretrained(""facebook/bart-large-mnli"")\n', '\n', 'model = AutoModelForSequenceClassification.from_pretrained(""facebook/bart-large-mnli"")\n', '\n', '\n', 'zero_shot_explainer = ZeroShotClassificationExplainer(model, tokenizer)\n', '\n', '\n', 'word_attributions = zero_shot_explainer(\n', '    ""Today apple released the new Macbook showing off a range of new features found in the proprietary silicon chip computer. "",\n', '    labels = [""finance"", ""technology"", ""sports""],\n', ')\n', '\n', '```\n', '\n', 'Which will return the following dict of attribution tuple lists for each label:\n', '\n', '```python\n', '>>> word_attributions\n', ""{'finance': [('<s>', 0.0),\n"", ""  ('Today', 0.0),\n"", ""  ('apple', -0.016100065046282107),\n"", ""  ('released', 0.3348383988281792),\n"", ""  ('the', -0.8932952916127369),\n"", ""  ('new', 0.14207183688642497),\n"", ""  ('Mac', 0.016309545780430777),\n"", ""  ('book', -0.06956802041125129),\n"", ""  ('showing', -0.12661404114316252),\n"", ""  ('off', -0.11470154900720078),\n"", ""  ('a', -0.03299250484912159),\n"", ""  ('range', -0.002532332125100561),\n"", ""  ('of', -0.022451943898971004),\n"", ""  ('new', -0.01859870581213379),\n"", ""  ('features', -0.020774327263810944),\n"", ""  ('found', -0.007734346326330102),\n"", ""  ('in', 0.005100588658589585),\n"", ""  ('the', 0.04711084622588314),\n"", ""  ('proprietary', 0.046352064964644286),\n"", ""  ('silicon', -0.0033502000158946127),\n"", ""  ('chip', -0.010419324929115785),\n"", ""  ('computer', -0.11507972995022273),\n"", ""  ('.', 0.12237840300907425)],\n"", "" 'technology': [('<s>', 0.0),\n"", ""  ('Today', 0.0),\n"", ""  ('apple', 0.22505152647747717),\n"", ""  ('released', -0.16164146624851905),\n"", ""  ('the', 0.5026975657258089),\n"", ""  ('new', 0.052589263167955536),\n"", ""  ('Mac', 0.2528325960993759),\n"", ""  ('book', -0.06445090203729663),\n"", ""  ('showing', -0.21204922293777534),\n"", ""  ('off', 0.06319714817612732),\n"", ""  ('a', 0.032048012090796815),\n"", ""  ('range', 0.08553079346908955),\n"", ""  ('of', 0.1409201107994034),\n"", ""  ('new', 0.0515261917112576),\n"", ""  ('features', -0.09656406466213506),\n"", ""  ('found', 0.02336613296843605),\n"", ""  ('in', -0.0011649894272190678),\n"", ""  ('the', 0.14229640664777807),\n"", ""  ('proprietary', -0.23169065661847646),\n"", ""  ('silicon', 0.5963924257008087),\n"", ""  ('chip', -0.19908474233975806),\n"", ""  ('computer', 0.030620295844734646),\n"", ""  ('.', 0.1995076958535378)],\n"", "" 'sports': [('<s>', 0.0),\n"", ""  ('Today', 0.0),\n"", ""  ('apple', 0.1776618164760026),\n"", ""  ('released', 0.10067773539491479),\n"", ""  ('the', 0.4813466937627506),\n"", ""  ('new', -0.018555244191949295),\n"", ""  ('Mac', 0.016338241133536224),\n"", ""  ('book', 0.39311969562943677),\n"", ""  ('showing', 0.03579210145504227),\n"", ""  ('off', 0.0016710813632476176),\n"", ""  ('a', 0.04367940034297261),\n"", ""  ('range', 0.06076859006993011),\n"", ""  ('of', 0.11039711284328052),\n"", ""  ('new', 0.003932416031994724),\n"", ""  ('features', -0.009660883377622588),\n"", ""  ('found', -0.06507586539836184),\n"", ""  ('in', 0.2957812911667922),\n"", ""  ('the', 0.1584106228974514),\n"", ""  ('proprietary', 0.0005789280604917397),\n"", ""  ('silicon', -0.04693795680472678),\n"", ""  ('chip', -0.1699508539245465),\n"", ""  ('computer', -0.4290823663975582),\n"", ""  ('.', 0.469314992542427)]}\n"", '```\n', '\n', 'We can find out which label was predicted with:\n', '\n', '```python\n', '>>> zero_shot_explainer.predicted_label\n', ""'technology'\n"", '```\n', '\n', '#### Visualize Zero Shot Classification attributions\n', '\n', 'For the `ZeroShotClassificationExplainer` the visualize() method returns a table similar to the `SequenceClassificationExplainer` but with attributions for every label.\n', '\n', '```python\n', 'zero_shot_explainer.visualize(""zero_shot.html"")\n', '```\n', '\n', '<a href=""https://github.com/cdpierse/transformers-interpret/blob/master/images/zero_shot_example.png"">\n', '<img src=""https://github.com/cdpierse/transformers-interpret/blob/master/images/zero_shot_example.png"" width=""100%"" height=""100%"" align=""center"" />\n', '</a>\n', '\n', '</details>\n', '\n', '<details><summary>Question Answering Explainer</summary>\n', '\n', ""Let's start by initializing a transformers' Question Answering model and tokenizer, and running it through the `QuestionAnsweringExplainer`.\n"", '\n', 'For this example we are using `bert-large-uncased-whole-word-masking-finetuned-squad`, a bert model finetuned on a SQuAD.\n', '\n', '```python\n', 'from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n', 'from transformers_interpret import QuestionAnsweringExplainer\n', '\n', 'tokenizer = AutoTokenizer.from_pretrained(""bert-large-uncased-whole-word-masking-finetuned-squad"")\n', 'model = AutoModelForQuestionAnswering.from_pretrained(""bert-large-uncased-whole-word-masking-finetuned-squad"")\n', '\n', 'qa_explainer = QuestionAnsweringExplainer(\n', '    model,\n', '    tokenizer,\n', ')\n', '\n', 'context = """"""\n', 'In Artificial Intelligence and machine learning, Natural Language Processing relates to the usage of machines to process and understand human language.\n', 'Many researchers currently work in this space.\n', '""""""\n', '\n', 'word_attributions = qa_explainer(\n', '    ""What is natural language processing ?"",\n', '    context,\n', ')\n', '```\n', '\n', 'Which will return the following dict containing word attributions for both the predicted start and end positions for the answer.\n', '\n', '```python\n', '>>> word_attributions\n', ""{'start': [('[CLS]', 0.0),\n"", ""  ('what', 0.9177170660377296),\n"", ""  ('is', 0.13382234898765258),\n"", ""  ('natural', 0.08061747350142005),\n"", ""  ('language', 0.013138062762511409),\n"", ""  ('processing', 0.11135923869816286),\n"", ""  ('?', 0.00858057388924361),\n"", ""  ('[SEP]', -0.09646373141894966),\n"", ""  ('in', 0.01545633993975799),\n"", ""  ('artificial', 0.0472082598707737),\n"", ""  ('intelligence', 0.026687249355110867),\n"", ""  ('and', 0.01675371260058537),\n"", ""  ('machine', -0.08429502436554961),\n"", ""  ('learning', 0.0044827685126163355),\n"", ""  (',', -0.02401013152520878),\n"", ""  ('natural', -0.0016756080249823537),\n"", ""  ('language', 0.0026815068421401885),\n"", ""  ('processing', 0.06773157580722854),\n"", ""  ('relates', 0.03884601576992908),\n"", ""  ('to', 0.009783797821526368),\n"", ""  ('the', -0.026650922910540952),\n"", ""  ('usage', -0.01067"
Model Explainability,jphall663/interpretable_machine_learning_with_python,jphall663,https://api.github.com/repos/jphall663/interpretable_machine_learning_with_python,633,200,5,"['https://api.github.com/users/jphall663', 'https://api.github.com/users/esztiorm', 'https://api.github.com/users/ntache', 'https://api.github.com/users/aosama', 'https://api.github.com/users/APorterOReilly']",Python,2023-03-10T14:09:23Z,https://raw.githubusercontent.com/jphall663/interpretable_machine_learning_with_python/master/README.md,"['# Responsible Machine Learning with Python\n', 'Examples of techniques for training interpretable machine learning (ML) models, explaining ML models, and debugging ML models for accuracy, discrimination, and security.\n', '\n', '\n', '### Overview\n', '\n', ""Usage of artificial intelligence (AI) and ML models is likely to become more commonplace as larger swaths of the economy embrace automation and data-driven decision-making. While these predictive systems can be quite accurate, they have often been inscrutable and unappealable black boxes that produce only numeric predictions with no accompanying explanations. Unfortunately, recent studies and recent events have drawn attention to mathematical and sociological flaws in prominent weak AI and ML systems, but practitioners don’t often have the right tools to pry open ML models and debug them. This series of notebooks introduces several approaches that increase transparency, accountability, and trustworthiness in ML models. If you are a data scientist or analyst and you want to train accurate, interpretable ML models, explain ML models to your customers or managers, test those models for security vulnerabilities or social discrimination, or if you have concerns about documentation, validation, or regulatory requirements, then this series of Jupyter notebooks is for you! (But *please* don't take these notebooks or associated materials as legal compliance advice.)\n"", '\n', 'The notebooks highlight techniques such as:\n', '* [Monotonic XGBoost models, partial dependence, individual conditional expectation plots, and Shapley explanations](https://github.com/jphall663/interpretable_machine_learning_with_python#enhancing-transparency-in-machine-learning-models-with-python-and-xgboost---notebook)\n', '* [Decision tree surrogates, reason codes, and ensembles of explanations](https://github.com/jphall663/interpretable_machine_learning_with_python#increase-transparency-and-accountability-in-your-machine-learning-project-with-python---notebook)\n', '* [Disparate impact analysis](https://github.com/jphall663/interpretable_machine_learning_with_python#increase-fairness-in-your-machine-learning-project-with-disparate-impact-analysis-using-python-and-h2o---notebook)\n', '* [LIME](https://github.com/jphall663/interpretable_machine_learning_with_python#explain-your-predictive-models-to-business-stakeholders-with-lime-using-python-and-h2o---notebook)\n', '* [Sensitivity and residual analysis](https://github.com/jphall663/interpretable_machine_learning_with_python#testing-machine-learning-models-for-accuracy-trustworthiness-and-stability-with-python-and-h2o---notebook)\n', '  * [Advanced sensitivity analysis for model debugging](https://github.com/jphall663/interpretable_machine_learning_with_python#part-1-sensitivity-analysis---notebook)\n', '  * [Advanced residual analysis for model debugging](https://github.com/jphall663/interpretable_machine_learning_with_python#part-2-residual-analysis---notebook)\n', '* [Detailed model comparison and model selection by cross-validated ranking](https://github.com/jphall663/interpretable_machine_learning_with_python#from-glm-to-gbm-building-the-case-for-complexity---notebook)\n', '\n', 'The notebooks can be accessed through:\n', '* [H2O Aquarium (Recommended)](https://github.com/jphall663/interpretable_machine_learning_with_python#h2o-aquarium-recommended)\n', '* [Virtualenv (Advanced)](https://github.com/jphall663/interpretable_machine_learning_with_python#virtualenv-installation)\n', '* [Docker container (Advanced)](https://github.com/jphall663/interpretable_machine_learning_with_python#docker-installation)\n', '* [Manual installation (Advanced)](https://github.com/jphall663/interpretable_machine_learning_with_python#manual-installation)\n', '\n', '#### Further reading:\n', '* [*Machine Learning: Considerations for fairly and transparently expanding access to credit*](http://info.h2o.ai/rs/644-PKX-778/images/Machine%20Learning%20-%20Considerations%20for%20Fairly%20and%20Transparently%20Expanding%20Access%20to%20Credit.pdf)\n', '* [*A Responsible Machine Learning Workflow with Focus on Interpretable Models, Post-hoc Explanation, and Discrimination Testing*](https://www.mdpi.com/2078-2489/11/3/137)\n', '* [*An Introduction to Machine Learning Interpretability, 2nd Edition*](https://www.h2o.ai/wp-content/uploads/2019/08/An-Introduction-to-Machine-Learning-Interpretability-Second-Edition.pdf)\n', '* [*On the Art and Science of Explainable Machine Learning*](https://arxiv.org/pdf/1810.02909.pdf)\n', '* [*Proposals for model vulnerability and security*](https://www.oreilly.com/ideas/proposals-for-model-vulnerability-and-security)\n', '* [*Proposed Guidelines for the Responsible Use of Explainable Machine Learning*](https://arxiv.org/pdf/1906.03533.pdf)\n', '* [*Real-World Strategies for Model Debugging*](https://medium.com/@jphall_22520/strategies-for-model-debugging-aa822f1097ce)\n', '* [*Warning Signs: Security and Privacy in an Age of Machine Learning*](https://fpf.org/wp-content/uploads/2019/09/FPF_WarningSigns_Report.pdf)\n', '* [*Why you should care about debugging machine learning models*](https://www.oreilly.com/radar/why-you-should-care-about-debugging-machine-learning-models/)\n', '\n', '***\n', '\n', '### Enhancing Transparency in Machine Learning Models with Python and XGBoost - [Notebook](https://nbviewer.jupyter.org/github/jphall663/interpretable_machine_learning_with_python/blob/master/xgboost_pdp_ice.ipynb)\n', '\n', '![](./readme_pics/pdp_ice.png)\n', '\n', 'Monotonicity constraints can turn opaque, complex models into transparent, and potentially regulator-approved models, by ensuring predictions only increase or only decrease for any change in a given input variable. In this notebook, I will demonstrate how to use monotonicity constraints in the popular open source gradient boosting package XGBoost to train an interpretable and accurate nonlinear classifier on the UCI credit card default data.\n', '\n', 'Once we have trained a monotonic XGBoost model, we will use partial dependence plots and individual conditional expectation (ICE) plots to investigate the internal mechanisms of the model and to verify its monotonic behavior. Partial dependence plots show us the way machine-learned response functions change based on the values of one or two input variables of interest while averaging out the effects of all other input variables. ICE plots can be used to create more localized descriptions of model predictions, and ICE plots pair nicely with partial dependence plots. An example of generating regulator mandated reason codes from high fidelity Shapley explanations for any model prediction is also presented. The combination of monotonic XGBoost, partial dependence, ICE, and Shapley explanations is likely one of the most direct ways to create an interpretable machine learning model today.\n', '\n', '\n', '### Increase Transparency and Accountability in Your Machine Learning Project with Python - [Notebook](https://nbviewer.jupyter.org/github/jphall663/interpretable_machine_learning_with_python/blob/master/dt_surrogate_loco.ipynb)\n', '\n', '![](./readme_pics/dt_surrogate.png)\n', '\n', 'Gradient boosting machines (GBMs) and other complex machine learning models are popular and accurate prediction tools, but they can be difficult to interpret. Surrogate models, feature importance, and reason codes can be used to explain and increase transparency in machine learning models. In this notebook, we will train a GBM on the UCI credit card default data. Then we’ll train a decision tree surrogate model on the original inputs and predictions of the complex GBM model and see how the variable importance and interactions displayed in the surrogate model yield an overall, approximate flowchart of the complex model’s predictions. We will also analyze the global variable importance of the GBM and compare this information to the surrogate model, our domain expertise, and our reasonable expectations.\n', '\n', 'To get a better picture of the complex model’s local behavior and to enhance the accountability of the model’s predictions, we will use a variant of the leave-one-covariate-out (LOCO) technique. LOCO enables us to calculate the local contribution each input variable makes toward each model prediction. We will then rank the local contributions to generate reason codes that describe, in plain English, the model’s decision process for every prediction.\n', '\n', '### Increase Fairness in Your Machine Learning Project with Disparate Impact Analysis using Python and H2O - [Notebook](https://nbviewer.jupyter.org/github/jphall663/interpretable_machine_learning_with_python/blob/master/dia.ipynb)\n', '\n', '<img src=""./readme_pics/dia.png"" height=""400"">\n', '\n', 'Fairness is an incredibly important, but highly complex entity. So much so that leading scholars have yet to agree on a strict definition. However, there is a practical way to discuss and handle observational fairness, or how your model predictions affect different groups of people. This procedure is often known as disparate impact analysis (DIA). DIA is far from perfect, as it relies heavily on user-defined thresholds and reference levels to measure disparity and does not attempt to remediate disparity or provide information on sources of disparity, but it is a fairly straightforward method to quantify your model’s behavior across sensitive demographic segments or other potentially interesting groups of observations. Some types of DIA are also an accepted, regulation-compliant tool for fair-lending purposes in the U.S. financial services industry. If it’s good enough for multibillion-dollar credit portfolios, it’s probably good enough for your project.\n', '\n', 'This example DIA notebook starts by training a monotonic gradient boosting machine (GBM) classifier on the UCI credit card default data using the popular open source library, h2o. A probability cutoff for making credit decisions is selected by maximizing the F1 statistic and confusion matrices are generated to summarize the GBM’s decisions across men and women. A basic DIA procedure is then conducted using the information stored in the confusion matrices and some traditional fair lending measures.\n', '\n', '### Explain Your Predictive Models to Business Stakeholders with LIME using Python and H2O - [Notebook](https://nbviewer.jupyter.org/github/jphall663/interpretable_machine_learning_with_python/blob/master/lime.ipynb)\n', '\n', '![](./readme_pics/lime.png)\n', '\n', 'Machine learning can create very accurate predictive models, but these models can be almost impossible to explain to your boss, your customers, or even your regulators. This notebook will use (Local Interpretable Model-agnostic Explanations) LIME to increase transparency and accountability in a complex GBM model trained on the UCI credit card default data. LIME is a method for building linear surrogate models for local regions in a data set, often single rows of data. LIME sheds light on how model predictions are made and describes local model mechanisms for specific rows of data. Because the LIME sampling process may feel abstract to some practitioners, this notebook will also introduce a more straightforward method of creating local samples for LIME.\n', '\n', 'Once local samples have been generated, we will fit LIME models to understand local trends in the complex model’s predictions. LIME can also tell us the local contribution of each input variable toward each model prediction, and these contributions can be sorted to create reason codes -- plain English explanations of every model prediction. We will also validate the fit of the LIME model to enhance trust in our explanations using the local model’s R2 statistic and a ranked prediction plot.\n', '\n', '### Testing Machine Learning Models for Accuracy, Trustworthiness, and Stability with Python and H2O - [Notebook](https://nbviewer.jupyter.org/github/jphall663/interpretable_machine_learning_with_python/blob/master/resid_sens_analysis.ipynb)\n', '\n', '![](./readme_pics/resid.png)\n', '\n', 'Because ML model predictions can vary drastically for small changes in input variable values, especially outside of training input domains, sensitivity analysis is perhaps the most important validation technique for increasing trust in ML model predictions. Sensitivity analysis investigates whether model behavior and outputs remain stable when input data is intentionally perturbed, or other changes are simulated in input data. In this notebook, we will enhance trust in a complex credit default model by testing and debugging its predictions with sensitivity analysis.\n', '\n', 'We’ll further enhance trust in our model using residual analysis. Residuals refer to the difference between the recorded value of a target variable and the predicted value of a target variable for each row in a data set. Generally, the residuals of a well-fit model should be randomly distributed, because good models will account for most phenomena in a data set, except for random error. In this notebook, we will create residual plots for a complex model to debug any accuracy problems arising from overfitting or outliers.\n', '\n', '### Machine Learning Model Debugging with Python: All Models are Wrong ... but Why is _My_ Model Wrong? (And Can I Fix It?)\n', '\n', '##### Part 1: Sensitivity Analysis - [Notebook](https://nbviewer.jupyter.org/github/jphall663/interpretable_machine_learning_with_python/blob/master/debugging_sens_analysis_redux.ipynb)\n', '\n', '![](/readme_pics/sa.png)\n', '\n', 'Sensitivity analysis is the perturbation of data under a trained model. It can take many forms and arguably Shapley feature importance, partial dependence, individual conditional expectation, and adversarial examples are all types of sensitivity analysis. This notebook focuses on using these different types of sensitivity analysis to discover error mechanisms and security vulnerabilities and to assess stability and fairness in a trained XGBoost model. It begins by loading the UCI credit card default data and then training an interpretable, monotonically constrained XGBoost model. After the model is trained, global and local Shapley feature importance is calculated. These Shapley values help inform the application of partial dependence and ICE, and together these results guide a search for adversarial examples. The notebook closes by exposing the trained model to a random attack and analyzing the attack results.\n', '\n', 'These model debugging exercises uncover accuracy, drift, and security problems such as over-emphasis of important features and impactful yet non-robust interactions. Several remediation mechanisms are proposed including editing of final model artifacts to remove or fix errors, missing value injection or regularization during training to lessen the impact of certain features or interactions, and assertion-based missing value injection during scoring to mitigate the effect of non-robust interactions.\n', '\n', '##### Part 2: Residual Analysis - [Notebook](https://nbviewer.jupyter.org/github/jphall663/interpretable_machine_learning_with_python/blob/master/debugging_resid_analysis_redux.ipynb)\n', '\n', '![](readme_pics/resid2.png)\n', '\n', 'In general, residual analysis can be characterized as a careful study of when and how models make mistakes. A better understanding of mistakes will hopefully lead to fewer of them. This notebook uses variants of residual analysis to find error mechanisms and security vulnerabilities and to assess stability and fairness in a trained XGBoost model. It begins by loading the UCI credit card default data and then training an interpretable, monotonically constrained XGBoost gradient boosting machine (GBM) model. (Pearson correlation with the prediction target is used to determine the direction of the monotonicity constraints for each input variable.) After the model is trained, its logloss residuals are analyzed and explained thoroughly and the constrained GBM is compared to a benchmark linear model. These model debugging exercises uncover accuracy, drift, and security problems such as over-emphasis of important variables and strong signal in model residuals. Several remediation mechanisms are proposed including missing value injection during training, additional data collection, and use of assertions to correct known problems during scoring.\n', '\n', '### From GLM to GBM: Building the Case For Complexity - [Notebook](https://nbviewer.jupyter.org/github/jphall663/interpretable_machine_learning_with_python/blob/master/glm_mgbm_gbm.ipynb)\n', '\n', '![](readme_pics/hist_pd_ice_lo.png)\n', '\n', 'This notebook uses the same credit card default scenario to show how monotonicity constraints, Shapley values and other post-hoc explanations, and discrimination testing can enable practitioners to create direct comparisons between GLM and GBM models. Several candidate probability of default models are selected for comparison using feature selection methods, like LASSO, and by cross-validated ranking. Comparisons then enable building from GLM to more complex GBM models in a step-by-step manner, while retaining model transparency and the ability to test for discrimination. This notebook shows that GBMs can yield better accuracy, more revenue, and that GBMs are also likely to fulfill many model documentation, adverse action notice, and discrimination testing requirements.\n', '\n', '## Using the Examples\n', '\n', '### H2O Aquarium (recommended)\n', '\n', 'H2O Aquarium is a free educational environment that hosts versions of these notebooks among many other H2o-related resources. To use these notebooks in Aquarium:\n', '\n', '1. Navigate to the Aquarium URL: https://aquarium.h2o.ai.\n', '\n', '2. Create a new Aquarium account.\n', '\n', '3. Check the registered email inbox and use the temporary password to login to Aquarium.\n', '\n', '4. Click `Browse Labs`.\n', '\n', '5. Click `View Detail` under *Open Source MLI Workshop*.\n', '\n', '6. Click `Start Lab` (this can take several minutes).\n', '\n', '7. Click on the *Jupyter URL* when it becomes available.\n', '\n', '8. Enter the token `h2o`.\n', '\n', '9. Click the `patrick_hall_mli` folder.\n', '\n', '10. Browse/run the Jupyter notebooks.\n', '\n', '11. Click `End Lab` when you are finished.\n', '\n', '### Virtualenv Installation\n', '\n', 'For avid Python users, creating a Python virtual environment is a convenient way to run these notebooks.\n', '\n', '1. Install [Git](https://git-scm.com/downloads).\n', '\n', '2. Clone this repository with the examples.</br>\n', '`$ git clone https://github.com/jphall663/interpretable_machine_learning_with_python.git`\n', '\n', '3. Install Anaconda Python 5.1.0 from the [Anaconda archives](https://repo.continuum.io/archive/) and add it to your system path.\n', '\n', '4. Change directories into the cloned repository.</br>\n', '`$ cd interpretable_machine_learning_with_python`\n', '\n', '5. Create a Python 3.6 virtual environment.</br>\n', '`$ virtualenv -p /path/to/anaconda3/bin/python3.6 env_iml`\n', '\n', '6. Activate the virtual environment.</br>\n', '`$ source env_iml/bin/activate`\n', '\n', '7. Install the correct packages for the example notebooks.</br>\n', '`$ pip install -r requirements.txt`\n', '\n', '8. Start Jupyter.</br>\n', '`$ jupyter notebook`\n', '\n', '### Docker Installation\n', '\n', 'A Dockerfile is provided to build a docker container with all necessary packages and dependencies. This is a way to use these examples if you are on Mac OS X, \\*nix, or Windows 10. To do so:\n', '\n', '1. Install and start [docker](https://www.docker.com/).\n', '\n', 'From a terminal:\n', '\n', '2. Create a directory for the Dockerfile.</br>\n', '`$ mkdir anaconda_py36_h2o_xgboost_graphviz_shap`\n', '\n', '3. Fetch the Dockerfile.</br>\n', '`$ curl https://raw.githubusercontent.com/jphall663/interpretable_machine_learning_with_python/master/anaconda_py36_h2o_xgboost_graphviz_shap/Dockerfile > anaconda_py36_h2o_xgboost_graphviz_shap/Dockerfile`\n', '\n', '4. Build a docker image from the Dockefile.</br>\n', '`docker build -t iml anaconda_py36_h2o_xgboost_graphviz_shap`\n', '\n', '5. Start the docker image and the Jupyter notebook server.</br>\n', ' `docker run -i -t -p 8888:8888 iml:latest /bin/bash -c ""/opt/conda/bin/jupyter notebook --notebook-dir=/interpretable_machine_learning_with_python --allow-root --ip=\'*\' --port=8888 --no-browser""`\n', '\n', '6. Navigate to port 8888 on your machine, probably `http://localhost:8888/`.\n', '\n', '\n', '### Manual Installation\n', '\n', '1. Anaconda Python 5.1.0 from the [Anaconda archives](https://repo.continuum.io/archive/).\n', '2. [Java](https://java.com/download).\n', '3. The latest stable [h2o](https://www.h2o.ai/download/) Python package.\n', '4. [Git](https://git-scm.com/downloads).\n', '5. [XGBoost](https://github.com/dmlc/xgboost) with Python bindings.\n', '6. [GraphViz](http://www.graphviz.org/).\n', '7. [Seaborn](https://pypi.org/project/seaborn/) package.\n', '8. [Shap](https://pypi.org/project/shap/) package.  \n', '\n', 'Anaconda Python, Java, Git, and GraphViz must be added to your system path.\n', '\n', 'From a terminal:\n', '\n', '9. Clone the repository with examples.</br>\n', '`$ git clone https://github.com/jphall663/interpretable_machine_learning_with_python.git`\n', '\n', '10. `$ cd interpretable_machine_learning_with_python`\n', '\n', '11. Start the Jupyter notebook server.</br>\n', '`$ jupyter notebook`\n', '\n', '12. Navigate to the port Jupyter directs you to on your machine, probably `http://localhost:8888/`.\n']"
Model Explainability,MAIF/shapash,MAIF,https://api.github.com/repos/MAIF/shapash,2164,268,23,"['https://api.github.com/users/ThomasBouche', 'https://api.github.com/users/ThibaudReal', 'https://api.github.com/users/yg79', 'https://api.github.com/users/Francesco-Marini', 'https://api.github.com/users/MaxGdr', 'https://api.github.com/users/MLecardonnel', 'https://api.github.com/users/guillaume-vignal', 'https://api.github.com/users/FlorineGreciet', 'https://api.github.com/users/GAP01', 'https://api.github.com/users/SebastienBidault', 'https://api.github.com/users/mathisbarthere', 'https://api.github.com/users/blanoe', 'https://api.github.com/users/githubyako', 'https://api.github.com/users/YL79', 'https://api.github.com/users/dependabot%5Bbot%5D', 'https://api.github.com/users/johannmartin95', 'https://api.github.com/users/peterdhansen', 'https://api.github.com/users/amnaabbassi', 'https://api.github.com/users/amorea04', 'https://api.github.com/users/ptitFicus', 'https://api.github.com/users/DragonWarrior15', 'https://api.github.com/users/susmitpy', 'https://api.github.com/users/yvanzubro']",Python,2023-04-26T09:53:26Z,https://raw.githubusercontent.com/MAIF/shapash/master/README.md,"['<p align=""center"">\n', '<img src=""https://raw.githubusercontent.com/MAIF/shapash/master/docs/_static/shapash-resize.png"" width=""300"" title=""shapash-logo"">\n', '</p>\n', '\n', '\n', '<p align=""center"">\n', '  <!-- Tests -->\n', '  <a href=""https://github.com/MAIF/shapash/workflows/Build%20%26%20Test/badge.svg"">\n', '    <img src=""https://github.com/MAIF/shapash/workflows/Build%20%26%20Test/badge.svg"" alt=""tests"">\n', '  </a>\n', '  <!-- PyPi -->\n', '  <a href=""https://img.shields.io/pypi/v/shapash"">\n', '    <img src=""https://img.shields.io/pypi/v/shapash"" alt=""pypi"">\n', '  </a>\n', '  <!-- Downloads -->\n', '  <a href=""https://static.pepy.tech/personalized-badge/shapash?period=total&units=international_system&left_color=grey&right_color=orange&left_text=Downloads"">\n', '    <img src=""https://static.pepy.tech/personalized-badge/shapash?period=total&units=international_system&left_color=grey&right_color=orange&left_text=Downloads"" alt=""downloads"">\n', '  </a>\n', '  <!-- Python Version -->\n', '  <a href=""https://img.shields.io/pypi/pyversions/shapash"">\n', '    <img src=""https://img.shields.io/pypi/pyversions/shapash"" alt=""pyversion"">\n', '  </a>\n', '  <!-- License -->\n', '  <a href=""https://img.shields.io/pypi/l/shapash"">\n', '    <img src=""https://img.shields.io/pypi/l/shapash"" alt=""license"">\n', '  </a>\n', '  <!-- Doc -->\n', '  <a href=""https://shapash.readthedocs.io/en/latest/"">\n', '    <img src=""https://readthedocs.org/projects/shapash/badge/?version=latest"" alt=""doc"">\n', '  </a>\n', '</p>\n', '\n', ""## 🎉 What's new ?\n"", '\n', '\n', '| Version       | New Feature                                                                           | Description                                                                                                                            | Tutorial |\n', '|:-------------:|:-------------------------------------------------------------------------------------:|:--------------------------------------------------------------------------------------------------------------------------------------:|:--------:|\n', '| 2.3.x         |  Additional dataset columns <br> [New demo](https://shapash-demo.ossbymaif.fr/) <br> [Article](https://pub.towardsai.net/shapash-2-3-0-comprehensive-model-interpretation-40b50157c2fb)                                                                | In Webapp: Target and error columns added to dataset and possibility to add features outside the model for more filtering options            |  [<img src=""https://raw.githubusercontent.com/MAIF/shapash/master/docs/_static/add_column_icon.png"" width=""50"" title=""add_column"">](https://github.com/MAIF/shapash/blob/master/tutorial/webapp/tuto-webapp01-additional-data.ipynb)\n', '| 2.3.x         |  Identity card <br> [New demo](https://shapash-demo.ossbymaif.fr/) <br> [Article](https://pub.towardsai.net/shapash-2-3-0-comprehensive-model-interpretation-40b50157c2fb)                                                                  | In Webapp: New identity card to summarize the information of the selected sample                  |  [<img src=""https://raw.githubusercontent.com/MAIF/shapash/master/docs/_static/identity_card.png"" width=""50"" title=""identity"">](https://github.com/MAIF/shapash/blob/master/tutorial/webapp/tuto-webapp01-additional-data.ipynb)\n', '| 2.2.x         |  Picking samples <br> [Article](https://www.kdnuggets.com/2022/11/picking-examples-understand-machine-learning-model.html)                                                                | New tab in the webapp for picking samples. The graph represents the ""True Values Vs Predicted Values""            |  [<img src=""https://raw.githubusercontent.com/MAIF/shapash/master/docs/_static/picking.png"" width=""50"" title=""picking"">](https://github.com/MAIF/shapash/blob/master/tutorial/plot/tuto-plot06-prediction_plot.ipynb)\n', '| 2.2.x         |  Dataset Filter <br>                                                              | New tab in the webapp to filter data. And several improvements in the webapp: subtitles, labels, screen adjustments                   |  [<img src=""https://raw.githubusercontent.com/MAIF/shapash/master/docs/_static/webapp.png"" width=""50"" title=""webapp"">](https://github.com/MAIF/shapash/blob/master/tutorial/tutorial01-Shapash-Overview-Launch-WebApp.ipynb)\n', '| 2.0.x         |  Refactoring Shapash <br>                                                                   | Refactoring attributes of compile methods and init. Refactoring implementation for new backends                   |  [<img src=""https://raw.githubusercontent.com/MAIF/shapash/master/docs/_static/modular.png"" width=""50"" title=""modular"">](https://github.com/MAIF/shapash/blob/master/tutorial/backend/tuto-backend-01.ipynb)\n', '| 1.7.x         |  Variabilize Colors <br>                                                                   | Giving possibility to have your own colour palette for outputs adapted to your design                   |  [<img src=""https://raw.githubusercontent.com/MAIF/shapash/master/docs/_static/variabilize-colors.png"" width=""50"" title=""variabilize-colors"">](https://github.com/MAIF/shapash/blob/master/tutorial/common/tuto-common02-colors.ipynb)\n', '| 1.6.x         |  Explainability Quality Metrics <br> [Article](https://towardsdatascience.com/building-confidence-on-explainability-methods-66b9ee575514)                                                                   | To help increase confidence in explainability methods, you can evaluate the relevance of your explainability using 3 metrics: **Stability**, **Consistency** and **Compacity**                   |  [<img src=""https://raw.githubusercontent.com/MAIF/shapash/master/docs/_static/quality-metrics.png"" width=""50"" title=""quality-metrics"">](https://github.com/MAIF/shapash/blob/master/tutorial/explainability_quality/tuto-quality01-Builing-confidence-explainability.ipynb) \n', '| 1.5.x         |  ACV Backend <br>                                                                     | A new way of estimating Shapley values using ACV. [More info about ACV here](https://towardsdatascience.com/the-right-way-to-compute-your-shapley-values-cfea30509254).                   |  [<img src=""https://raw.githubusercontent.com/MAIF/shapash/master/docs/_static/wheel.png"" width=""50"" title=""wheel-acv-backend"">](tutorial/explainer/tuto-expl03-Shapash-acv-backend.ipynb)    |\n', '| 1.4.x         |  Groups of features <br> [Demo](https://shapash-demo2.ossbymaif.fr/)                  | You can now regroup features that share common properties together. <br>This option can be useful if your model has a lot of features. |  [<img src=""https://raw.githubusercontent.com/MAIF/shapash/master/docs/_static/groups_features.gif"" width=""120"" title=""groups-features"">](https://github.com/MAIF/shapash/blob/master/tutorial/common/tuto-common01-groups_of_features.ipynb)    | \n', '| 1.3.x         |  Shapash Report <br> [Demo](https://shapash.readthedocs.io/en/latest/report.html)     | A standalone HTML report that constitutes a basis of an audit document.                                                                |  [<img src=""https://raw.githubusercontent.com/MAIF/shapash/master/docs/_static/report-icon.png"" width=""50"" title=""shapash-report"">](https://github.com/MAIF/shapash/blob/master/tutorial/report/tuto-shapash-report01.ipynb)    | \n', '\n', '\n', '## 🔍 Overview\n', '\n', '**Shapash** is a Python library which aims to make machine learning interpretable and understandable by everyone.\n', 'It provides several types of visualization that display explicit labels that everyone can understand. \n', '\n', 'Data Scientists can understand their models easily and share their results. End users can understand the decision proposed by a model using a summary of the most influential criteria.\n', '\n', 'Shapash also contributes to data science auditing by displaying usefull information about any model and data in a unique report. \n', '\n', '- Readthedocs: [![documentation badge](https://readthedocs.org/projects/shapash/badge/?version=latest)](https://shapash.readthedocs.io/en/latest/)\n', '- [Presentation video for french speakers](https://www.youtube.com/watch?v=r1R_A9B9apk)\n', '- Medium:\n', '  - [Understand your model with Shapash - Towards AI](https://pub.towardsai.net/shapash-making-ml-models-understandable-by-everyone-8f96ad469eb3) \n', '  - [Model auditability - Towards DS](https://towardsdatascience.com/shapash-1-3-2-announcing-new-features-for-more-auditable-ai-64a6db71c919)\n', '  - [Group of features - Towards AI](https://pub.towardsai.net/machine-learning-6011d5d9a444)\n', '  - [Building confidence on explainability - Towards DS](https://towardsdatascience.com/building-confidence-on-explainability-methods-66b9ee575514)\n', '  - [Picking Examples to Understand Machine Learning Model](https://www.kdnuggets.com/2022/11/picking-examples-understand-machine-learning-model.html)\n', '  - [Enhancing Webapp Built-In Features for Comprehensive Machine Learning Model Interpretation](https://pub.towardsai.net/shapash-2-3-0-comprehensive-model-interpretation-40b50157c2fb)\n', '\n', '\n', '<p align=""center"">\n', '  <img src=""https://raw.githubusercontent.com/MAIF/shapash/master/docs/_static/shapash_global.gif"" width=""800"">\n', '</p>\n', '\n', '## 🤝 Contributors\n', '\n', '<div align=""left"">\n', '  <div style=""display: flex; align-items: flex-start;"">\n', '    <img align=middle src=""https://github.com/MAIF/shapash/blob/master/docs/_static/logo_maif.png"" width=""18%""/>\n', '    <img align=middle src=""https://github.com/MAIF/shapash/blob/master/docs/_static/logo_quantmetry.png"" width=""18%"" />\n', '    <img align=middle src=""https://github.com/MAIF/shapash/blob/master/docs/_static/logo_societe_generale.png"" width=""18%"" /> \n', '    <img align=middle src=""https://github.com/MAIF/shapash/blob/master/docs/_static/logo_groupe_vyv.png"" width=""18%"" /> \n', '    <img align=middle src=""https://github.com/MAIF/shapash/blob/master/docs/_static/logo_SixfoisSept.png"" width=""18%"" /> \n', '  </div>\n', '</div>\n', '\n', '\n', '## 🏆 Awards\n', '\n', '<a href=""https://raw.githubusercontent.com/MAIF/shapash/master/docs/_static/awards-argus-or.png"">\n', '  <img align=""left"" src=""https://raw.githubusercontent.com/MAIF/shapash/master/docs/_static/awards-argus-or.png"" width=""180"" />\n', '</a>\n', '\n', '<a href=""https://www.kdnuggets.com/2021/04/shapash-machine-learning-models-understandable.html"">\n', '  <img src=""https://www.kdnuggets.com/images/tkb-2104-g.png?raw=true"" width=""65"" />\n', '</a>  \n', '\n', '\n', '## 🔥 Features\n', '\n', '- Display clear and understandable results: plots and outputs use **explicit labels** for each feature and its values\n', '\n', '<p align=""center"">\n', '  <img align=""left"" src=""https://github.com/MAIF/shapash/blob/master/docs/_static/shapash-grid-images-02.png?raw=true"" width=""28%""/>\n', '  <img src=""https://github.com/MAIF/shapash/blob/master/docs/_static/shapash-grid-images-06.png?raw=true"" width=""28%"" />\n', '  <img align=""right"" src=""https://github.com/MAIF/shapash/blob/master/docs/_static/shapash-grid-images-04.png?raw=true"" width=""28%"" /> \n', '</p>\n', '\n', '<p align=""center"">\n', '  <img align=""left"" src=""https://github.com/MAIF/shapash/blob/master/docs/_static/shapash-grid-images-01.png?raw=true"" width=""28%"" />\n', '  <img src=""https://github.com/MAIF/shapash/blob/master/docs/_static/shapash-resize.png?raw=true"" width=""18%"" />\n', '  <img align=""right"" src=""https://github.com/MAIF/shapash/blob/master/docs/_static/shapash-grid-images-13.png?raw=true"" width=""28%"" /> \n', '</p>\n', '\n', '<p align=""center"">\n', '  <img align=""left"" src=""https://github.com/MAIF/shapash/blob/master/docs/_static/shapash-grid-images-12.png?raw=true"" width=""33%"" />\n', '  <img src=""https://github.com/MAIF/shapash/blob/master/docs/_static/shapash-grid-images-03.png?raw=true"" width=""28%"" />\n', '  <img align=""right"" src=""https://github.com/MAIF/shapash/blob/master/docs/_static/shapash-grid-images-10.png?raw=true"" width=""25%"" /> \n', '</p>\n', '\n', '\n', '- Allow Data Scientists to quickly understand their models by using a **webapp** to easily navigate between global and local explainability, and understand how the different features contribute: [Live Demo Shapash-Monitor](https://shapash-demo.ossbymaif.fr/)\n', '\n', '- **Summarize and export** the local explanation\n', '> **Shapash** proposes a short and clear local explanation. It allows each user, whatever their Data background, to understand a local prediction of a supervised model thanks to a summarized and explicit explanation\n', '\n', '\n', '- **Evaluate** the quality of your explainability using different metrics\n', '\n', '- Easily share and discuss results with non-Data users\n', '\n', '- Select subsets for further analysis of explainability by filtering on explanatory and additional features, correct or wrong predictions. [Picking Examples to Understand Machine Learning Model](https://www.kdnuggets.com/2022/11/picking-examples-understand-machine-learning-model.html)\n', '\n', '- Deploy interpretability part of your project: From model training to deployment (API or Batch Mode)\n', '\n', '- Contribute to the **auditability of your model** by generating a **standalone HTML report** of your projects. [Report Example](https://shapash.readthedocs.io/en/latest/report.html) \n', '>We hope that this report will bring a valuable support to auditing models and data related to a better AI governance. \n', 'Data Scientists can now deliver to anyone who is interested in their project **a document that freezes different aspects of their work as a basis of an audit report**. \n', 'This document can be easily shared across teams (internal audit, DPO, risk, compliance...).\n', '\n', '<p align=""center"">\n', '  <img src=""https://raw.githubusercontent.com/MAIF/shapash/master/docs/_static/shapash-report-demo.gif"" width=""800"">\n', '</p>\n', '\n', '## ⚙️ How Shapash works \n', '**Shapash** is an overlay package for libraries dedicated to the interpretability of models. It uses Shap or Lime backend\n', 'to compute contributions.\n', '**Shapash** builds on the different steps necessary to build a machine learning model to make the results understandable\n', '\n', '<p align=""center"">\n', '  <img src=""https://raw.githubusercontent.com/MAIF/shapash/master/docs/_static/shapash-diagram.png"" width=""700"" title=""diagram"">\n', '</p>\n', '\n', '**Shapash** works for Regression, Binary Classification or Multiclass problem. <br />\n', 'It is compatible with many models: *Catboost*, *Xgboost*, *LightGBM*, *Sklearn Ensemble*, *Linear models*, *SVM*. <br />\n', 'Shapash can use category-encoders object, sklearn ColumnTransformer or simply features dictionary. <br />\n', '- Category_encoder: *OneHotEncoder*, *OrdinalEncoder*, *BaseNEncoder*, *BinaryEncoder*, *TargetEncoder*\n', '- Sklearn ColumnTransformer: *OneHotEncoder*, *OrdinalEncoder*, *StandardScaler*, *QuantileTransformer*, *PowerTransformer*\n', '\n', '## 🛠 Installation\n', '\n', 'Shapash is intended to work with Python versions 3.8 to 3.10. Installation can be done with pip:\n', '\n', '```\n', 'pip install shapash\n', '```\n', '\n', 'In order to generate the Shapash Report some extra requirements are needed.\n', 'You can install these using the following command :  \n', '```\n', 'pip install shapash[report]\n', '```\n', '\n', 'If you encounter **compatibility issues** you may check the corresponding section in the Shapash documentation [here](https://shapash.readthedocs.io/en/latest/installation-instructions/index.html).\n', '\n', '## 🕐 Quickstart\n', '\n', 'The 4 steps to display results:\n', '\n', '- Step 1: Declare SmartExplainer Object\n', '  > There 1 mandatory parameter in compile method: Model\n', '  > You can declare features dict here to specify the labels to display\n', '\n', '```\n', 'from shapash import SmartExplainer\n', 'xpl = SmartExplainer(\n', '  model=regressor,\n', '  features_dict=house_dict,  # Optional parameter\n', '  preprocessing=encoder, # Optional: compile step can use inverse_transform method\n', '  postprocessing=postprocess, # Optional: see tutorial postprocessing  \n', ')\n', '```\n', '\n', '- Step 2: Compile  Dataset, ...\n', '  > There 1 mandatory parameter in compile method: Dataset\n', ' \n', '```\n', 'xpl.compile(\n', '    x=Xtest,    \n', '    y_pred=y_pred, # Optional: for your own prediction (by default: model.predict)\n', '    y_target=yTest, # Optional: allows to display True Values vs Predicted Values\n', '    additional_data=X_additional, # Optional: additional dataset of features for Webapp\n', '    additional_features_dict=features_dict_additional, # Optional: dict additional data    \n', ')\n', '```  \n', '\n', '- Step 3: Display output\n', '  > There are several outputs and plots available. for example, you can launch the web app:\n', '\n', '```\n', 'app = xpl.run_app()\n', '``` \n', '\n', '[Live Demo Shapash-Monitor](https://shapash-demo.ossbymaif.fr/)\n', '\n', '- Step 4: Generate the Shapash Report\n', '  > This step allows to generate a standalone html report of your project using the different splits\n', '  of your dataset and also the metrics you used:\n', '\n', '```\n', 'xpl.generate_report(\n', ""    output_file='path/to/output/report.html',\n"", ""    project_info_file='path/to/project_info.yml',\n"", '    x_train=Xtrain,\n', '    y_train=ytrain,\n', '    y_test=ytest,\n', '    title_story=""House prices report"",\n', '    title_description=""""""This document is a data science report of the kaggle house prices tutorial project.\n', '        It was generated using the Shapash library."""""",\n', '    metrics=[{‘name’: ‘MSE’, ‘path’: ‘sklearn.metrics.mean_squared_error’}]\n', ')\n', '```\n', '\n', '[Report Example](https://shapash.readthedocs.io/en/latest/report.html)\n', '\n', '- Step 5: From training to deployment : SmartPredictor Object\n', '  > Shapash provides a SmartPredictor object to deploy the summary of local explanation for the operational needs.\n', '  It is an object dedicated to deployment, lighter than SmartExplainer with additional consistency checks.\n', '  SmartPredictor can be used with an API or in batch mode. It provides predictions, detailed or summarized local \n', '  explainability using appropriate wording.\n', '  \n', '```\n', 'predictor = xpl.to_smartpredictor()\n', '```\n', 'See the tutorial part to know how to use the SmartPredictor object\n', '\n', '## 📖  Tutorials\n', 'This github repository offers many tutorials to allow you to easily get started with Shapash.\n', '\n', '\n', '<details><summary><b>Overview</b> </summary>\n', '\n', '- [Launch the webapp with a concrete use case](tutorial/tutorial01-Shapash-Overview-Launch-WebApp.ipynb)\n', '- [Jupyter Overviews - The main outputs and methods available with the SmartExplainer object](tutorial/tutorial02-Shapash-overview-in-Jupyter.ipynb)\n', '- [Shapash in production: From model training to deployment (API or Batch Mode)](tutorial/tutorial03-Shapash-overview-model-in-production.ipynb)\n', '- [Use groups of features](tutorial/common/tuto-common01-groups_of_features.ipynb)\n', '- [Deploy local explainability in production with SmartPredictor](tutorial/predictor/tuto-smartpredictor-introduction-to-SmartPredictor.ipynb)\n', '\n', '</details>\n', '\n', '<details><summary><b>Charts and plots</b> </summary>\n', '\n', '- [**Shapash** Features Importance](tutorial/plot/tuto-plot03-features-importance.ipynb)\n', '- [Contribution plot to understand how one feature affects a prediction](tutorial/plot/tuto-plot02-contribution_plot.ipynb)\n', '- [Summarize, display and export local contribution using filter and local_plot method](tutorial/plot/tuto-plot01-local_plot-and-to_pandas.ipynb)\n', '- [Contributions Comparing plot to understand why predictions on several individuals are different](tutorial/plot/tuto-plot04-compare_plot.ipynb)\n', '- [Visualize interactions between couple of variables](tutorial/plot/tuto-plot05-interactions-plot.ipynb)\n', '- [Customize colors in Webapp, plots and report](tutorial/common/tuto-common02-colors.ipynb)\n', '\n', '</details>\n', '\n', '<details><summary><b>Different ways to use Encoders and Dictionaries</b> </summary>\n', '\n', '- [Use Category_Encoder & inverse transformation](tutorial/encoder/tuto-encoder01-using-category_encoder.ipynb)\n', '- [Use ColumnTransformers](tutorial/encoder/tuto-encoder02-using-columntransformer.ipynb)\n', '- [Use Simple Python Dictionnaries](tutorial/encoder/tuto-encoder03-using-dict.ipynb)\n', '\n', '</details>\n', '\n', '<details><summary><b>Displaying data with postprocessing</b> </summary>\n', '\n', '[Using postprocessing parameter in compile method](tutorial/postprocess/tuto-postprocess01.ipynb)\n', '\n', '</details>\n', '\n', '<details><summary><b>Using different backends</b> </summary>\n', '\n', '- [Compute Shapley Contributions using **Shap**](tutorial/explainer/tuto-expl01-Shapash-Viz-using-Shap-contributions.ipynb)\n', '- [Use **Lime** to compute local explanation, Summarize-it with **Shapash**](tutorial/explainer/tuto-expl02-Shapash-Viz-using-Lime-contributions.ipynb)\n', '- [Use **ACV backend** to compute Active Shapley Values and SDP global importance](tutorial/explainer/tuto-expl03-Shapash-acv-backend.ipynb)\n', '- [Compile faster Lime and consistency of contributions](tutorial/explainer/tuto-expl04-Shapash-compute-Lime-faster.ipynb)\n', '\n', '</details>\n', '\n', '<details><summary><b>Evaluating the quality of your explainability</b> </summary>\n', '\n', '- [Building confidence on explainability methods using **Stability**, **Consistency** and **Compacity** metrics](tutorial/explainability_quality/tuto-quality01-Builing-confidence-explainability.ipynb)\n', '\n', '</details>\n', '\n', '<details><summary><b>Generate a report of your project</b> </summary>\n', '\n', '- [Generate a standalone HTML report of your project with generate_report](tutorial/report/tuto-shapash-report01.ipynb)\n', '\n', '</details>\n', '\n', '<details><summary><b>Analysing your model via Shapash WebApp</b> </summary>\n', '\n', '- [Add features outside of the model for more exploration options](tutorial/webapp/tuto-webapp01-additional-data.ipynb)\n', '\n', '</details>\n']"
Model Explainability,dylan-slack/Modeling-Uncertainty-Local-Explainability,dylan-slack,https://api.github.com/repos/dylan-slack/Modeling-Uncertainty-Local-Explainability,27,10,1,['https://api.github.com/users/dylan-slack'],Python,2023-03-14T23:25:14Z,https://raw.githubusercontent.com/dylan-slack/Modeling-Uncertainty-Local-Explainability/main/README.md,"['# Reliable Post hoc Explanations: Modeling Uncertainty in Explainability\n', '\n', 'Welcome to the code for our paper, Reliable Post hoc Explanations: Modeling Uncertainty in Explainability, published at NeurIPS 2021. We encourage you to read the [full paper](https://arxiv.org/abs/2008.05030).\n', '\n', 'Visualizing the posteriors of BayesLIME explanations on an image of a dog and COMPAS:\n', '\n', '<p float=""middle"">\n', '<img src=""visualization/diego.gif"" width=""300"" heigh=""300"">\n', '<img src=""data/posteriors_fig_1.png"" width=""500"" heigh=""300"">\n', '</p>\n', '\n', '## Citation\n', '\n', 'If you found this work useful, please cite us:\n', '\n', '```\n', '@inproceedings{reliableposthoc:neurips21,\n', '  author = {Dylan Slack and Sophie Hilgard and Sameer Singh and Himabindu Lakkaraju},\n', '  title = { {Reliable Post hoc Explanations Modeling Uncertainty in Explainability} },\n', '  booktitle = {Neural Information Processing Systems (NeurIPS)},\n', '  year = {2021}\n', '}\n', '```\n', '\n', '## Examples\n', '\n', ""An example usage of the explainer is provided in `./visualization/image_posterior_example.py`, where we visualize the posterior of a BayesLIME explanation on an image of the first author's dog.\n"", '\n', '## Experiments\n', '\n', '### Data\n', '\n', '#### Tabular Data\n', '\n', 'The German Credit + COMPAS datasets are included in the `./data` folder. Within experiments, the german credit data set is called as `--dataset german` and compas is called as `--dataset compas`.\n', '\n', '#### MNIST\n', '\n', 'The MNIST data is set to download automatically on the first run.\n', '\n', 'In places where the MNIST data is accepted, by specifying the `--dataset` flag, it is possible to select the digit on which to run the experiment by specifying, for example, `--dataset mnist_1` for the 1 digit or `--dataset mnist_3` for the 3 digit, and so on.\n', '\n', '#### ImageNet\n', '\n', 'To download the ImageNet data, use [this script](https://github.com/mf1024/ImageNet-Datasets-Downloader), selecting the appropriate class indices (e.g., n02108915 is the French Bulldog class used in the paper). For example, to download the French Bulldog data, run:\n', '\n', '```python\n', 'python ./downloader.py \n', '    -data_root ./data/imagenet/frenchbulldog \\\n', '    -use_class_list True \\\n', '    -class_list n02108915 \\\n', '    -images_per_class 100 \n', '```\n', '\n', 'Once the imagenet dataset is installed, it can be called with `--dataset imagenet_classname` where `classname` is the name of the folder where the data is stored (for instance `frenchbulldog` running the script above).\n', '\n', '### Models\n', '\n', 'The tabular models are trained when they are called in experiments. The pre-trained MNIST model is provided in the `./data/mnist` subfolder. The VGG16 IMAGENET model will be downloaded when it is called.\n', '\n', '### Experiments\n', '\n', 'Code to run experiments from the paper is included in the `./experiments` directory within the project.\n', '\n', '### Hardware Requirements\n', '\n', 'For image experiments, GPU/TPU acceleration is recommended. I ran most of the experiments for this paper with a single NVIDIA 2080TI and a few with a NVIDIA Titan RTX.\n', '\n', ""For the tabular experiments, it's possible to run them on CPU. I tested this using a 1.4 GHz Intel Core i5 from a 2019 MacBook Pro, and it seemed to work fine. In places in the experiments where multithreading is used (`--n_threads`) in the experiments, be careful to use a value less than the avaliable cores on your CPU. I noticed that if I set `--n_threads` value too high on the MacBook, it caused it to freeze. \n"", '\n', '### Questions\n', '\n', 'You can reach out to [dslack@uci.edu](mailto:dslack@uci.edu) with any questions.\n', '\n']"
Model Explainability,pbiecek/ema,pbiecek,https://api.github.com/repos/pbiecek/ema,159,35,8,"['https://api.github.com/users/pbiecek', 'https://api.github.com/users/tomaszbur', 'https://api.github.com/users/xiaochi-liu', 'https://api.github.com/users/hbaniecki', 'https://api.github.com/users/jtr13', 'https://api.github.com/users/MartinHoldrege', 'https://api.github.com/users/mvwestendorp', 'https://api.github.com/users/FrieseWoudloper']",Python,2023-04-20T12:17:34Z,https://raw.githubusercontent.com/pbiecek/ema/master/README.md,"['<img width=""300"" align=""right"" src=""figure/front4.png"">\n', '\n', '# Explanatory Model Analysis\n', '\n', '## Explore, Explain, and Examine Predictive Models\n', '\n', 'See the html website here: https://pbiecek.github.io/ema/\n', '\n', 'A note to readers: this text is a work in progress. \n', '\n', ""We've released this initial version to get more feedback. Feedback can be given at the GitHub repo https://github.com/pbiecek/ema/issues. We are primarily interested in the organization and consistency of the content, but any comments will be welcommed.\n"", '\n', '### Bibtex entry\n', '\n', '```\n', '@Book{,\n', '  author = {Przemyslaw Biecek and Tomasz Burzykowski},\n', '  title = {{Explanatory Model Analysis}},\n', '  publisher = {Chapman and Hall/CRC, New York},\n', '  year = {2021},\n', '  isbn = {9780367135591},\n', '  url = {https://ema.drwhy.ai/},\n', '}\n', '```\n', '\n', '### Model Studio\n', '\n', 'In this book we have used several predictive models. One can explore them with the Model Studio under following links\n', '\n', '* `titanic_rf_v4` model https://titanic-rf-v4.netlify.com\n', '* `titanic_rf_v6` model https://titanic-rf-v6.netlify.com\n', '* `titanic_gbm_v6` model https://titanic-gbm-v6.netlify.com\n', '* `titanic_lmr_v6` model https://titanic-lmr-v6.netlify.com\n', '\n', '![figure/titanic_rf_v6.png](figure/titanic_rf_v6.png)\n', '\n']"
Model Explainability,erikjhordan-rey/Android-Spotify-MVP,erikjhordan-rey,https://api.github.com/repos/erikjhordan-rey/Android-Spotify-MVP,183,64,1,['https://api.github.com/users/erikjhordan-rey'],Java,2023-03-05T09:31:40Z,https://raw.githubusercontent.com/erikjhordan-rey/Android-Spotify-MVP/master/README.md,"['# Android - Spotify + Model View Presenter (MVP) [![Build Status](https://travis-ci.org/erikjhordan-rey/Android-Spotify-MVP.svg?branch=master)](https://travis-ci.org/erikjhordan-rey/Android-Spotify-MVP)\n', 'Android Model View Presenter used to explain how to use this pattern in our android applications.\n', '\n', 'This example was created to support an article explanation [Model View Presenter en Android][1] (spanish).\n', '\n', 'Libraries used on the sample project\n', '------------------------------------\n', '* [AppCompat, CardView, RecyclerView, Material][2]\n', '* [Retrofit 2][4]\n', '* [RxJava & RxAndroid][5]\n', '* [Gradle Retrolambda Plugin][6]\n', '\n', '\n', '# Demo\n', '\n', '![](./art/spotify-mvp.png)\n', '\n', '\n', '# Access Token \n', '\n', 'The Spotify Api has been changed an Access Token is required. The app sample probably will response `401 unauthorized code`.\n', '\n', '1- * Get Your Access Token from [Spotify Api Doc](https://developer.spotify.com/web-api/console/get-search-item/)\n', '\n', '![](./art/token_spotify.png)\n', '\n', '2- The class `Constans` has a constant variable called `ACCESS_TOKEN` replace with your access token  \n', '\n', '\n', '3- Run the app, it should work!!\n', '\n', '\n', '# how does it work?\n', '\n', '![](./art/Telecine_2015-11-25-17-19-04.gif)\n', '\n', '[8]: http://mockito.org/\n', '[7]: http://robolectric.org/\n', '[6]: https://github.com/evant/gradle-retrolambda\n', '[5]: https://github.com/ReactiveX/RxAndroid\n', '[4]: http://square.github.io/retrofit/\n', '[2]: http://developer.android.com/intl/es/tools/support-library/index.html\n', '[1]: https://erikjhordan-rey.github.io/blog/2015/11/02/ANDROID-mvp.html\n', '\n', '\n', 'Do you want to contribute?\n', '--------------------------\n', '\n', 'Feel free to report or add any useful feature, I will be glad to improve it with your help.\n', '\n', '\n', 'Developed By\n', '------------\n', '\n', '* Erik Jhordan Rey  - <erikjhordan.rey@gmail.com>\n', '\n', 'License\n', '-------\n', '\n', '    Copyright 2016 Erik Jhordan Rey\n', '\n', '    Licensed under the Apache License, Version 2.0 (the ""License"");\n', '    you may not use this file except in compliance with the License.\n', '    You may obtain a copy of the License at\n', '\n', '       http://www.apache.org/licenses/LICENSE-2.0\n', '\n', '    Unless required by applicable law or agreed to in writing, software\n', '    distributed under the License is distributed on an ""AS IS"" BASIS,\n', '    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n', '    See the License for the specific language governing permissions and\n', '    limitations under the License.\n', '\n', '\n']"
Model Explainability,eludadev/css-docs,eludadev,https://api.github.com/repos/eludadev/css-docs,318,44,1,['https://api.github.com/users/eludadev'],,2023-04-26T16:49:46Z,https://raw.githubusercontent.com/eludadev/css-docs/main/README.md,"['## :sparkles: CSS Selectors\n', '\n', '| Preview                           | Selector                                                     | Description                                                  |\n', '| --------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |\n', '| [![](./assets/css_selectors_1.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/Child_combinator) | <p align=""center""><strong>a > b</strong><br /><a href=""https://developer.mozilla.org/en-US/docs/Web/CSS/Child_combinator"">Child Combinator</a></p> | Select all b elements that are directly inside of a elements. |\n', '| [![](./assets/css_selectors_2.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/Descendant_combinator) | <p align=""center""><strong>a &nbsp; b</strong><br /><a href=""https://developer.mozilla.org/en-US/docs/Web/CSS/Descendant_combinator"">Descendant Combinator</a></p> | Select all b elements that are anywhere inside of a elements. |\n', '| [![](./assets/css_selectors_3.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/Adjacent_sibling_combinator) | <p align=""center""><strong>a + b</strong><br /><a href=""https://developer.mozilla.org/en-US/docs/Web/CSS/Adjacent_sibling_combinator"">Adjacent sibling combinator</a></p> | Select all b elements that are immediately next to a elements. |\n', '| [![](./assets/css_selectors_4.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/General_sibling_combinator) | <p align=""center""><strong>a ~ b</strong><br /><a href=""https://developer.mozilla.org/en-US/docs/Web/CSS/General_sibling_combinator"">General sibling combinator</a></p> | Select all b elements that are anywhere after a elements. |\n', '| [![](./assets/css_selectors_5.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/Class_selectors) | <p align=""center""><strong>.cl</strong><br /><a href=""https://developer.mozilla.org/en-US/docs/Web/CSS/Class_selectors"">Class selector</a></p> | Select all elements that have the cl class name. |\n', '| [![](./assets/css_selectors_6.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/Type_selectors) | <p align=""center""><strong>a.cl</strong><br /><a href=""https://developer.mozilla.org/en-US/docs/Web/CSS/Type_selectors"">Tag + Class selector</a></p> | Select all a elements that have the cl class name. |\n', '| [![](./assets/css_selectors_7.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/Class_selectors) | <p align=""center""><strong>.cl1.cl2</strong><br /><a href=""https://developer.mozilla.org/en-US/docs/Web/CSS/Class_selectors"">Multiclass selector</a></p> | Select all elements that have both the cl1 and cl2 class names. |\n', '| [![](./assets/css_selectors_8.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/Attribute_selectors) | <p align=""center""><strong>a\\[x=y\\]</strong><br /><a href=""https://developer.mozilla.org/en-US/docs/Web/CSS/Attribute_selectors"">Attribute selector</a></p> | Select all a elements that have the x attribute set to y. |\n', '| [![](./assets/css_selectors_9.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/ID_selectors) | <p align=""center""><strong>#id1</strong><br /><a href=""https://developer.mozilla.org/en-US/docs/Web/CSS/ID_selectors"">ID selector</a></p> | Select the element with the id1 ID name. |\n', '| [![](./assets/css_selectors_10.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/Universal_selectors) | <p align=""center""><strong>*</strong><br /><a href=""https://developer.mozilla.org/en-US/docs/Web/CSS/Universal_selectors"">Universal selector</a></p> | Select all elements. |\n', '\n', '| High Resolution | Grayscale Print |\n', '| --------------- | --------------- |\n', '| [![](./assets/lowres-css_selectors.png)](./assets/css_selectors.png) | [![](./assets/lowres-css_selectors_print.png)](./assets/css_selectors_print.png) |\n', '\n', '## :sparkles: CSS Box Model\n', '\n', '| Preview                           | Property                                                     | Description                                                  |\n', '| --------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |\n', '| [![](./assets/css_box_model_1.png)](https://developer.mozilla.org/en-US/docs/Learn/CSS/Building_blocks/The_box_model) | <p align=""center"" markdown=""true"">[`box-sizing: border-box`](https://developer.mozilla.org/en-US/docs/Learn/CSS/Building_blocks/The_box_model)</p> | The `width` and `height` have the size of `content`+`padding`+`border` |\n', '| [![](./assets/css_box_model_2.png)](https://developer.mozilla.org/en-US/docs/Learn/CSS/Building_blocks/The_box_model) | <p align=""center"" markdown=""true"">[`box-sizing: content-box`](https://developer.mozilla.org/en-US/docs/Learn/CSS/Building_blocks/The_box_model)</p> | The `width` and `height` have the size of just `content` |\n', '\n', '| High Resolution |\n', '| --------------- |\n', '| [![](./assets/lowres-css_box_model.png)](./assets/css_box_model.png) |\n', '\n', '## :sparkles: CSS Grid Layout\n', '\n', '| Align Content                             |\n', '| --------------------------------- |\n', '| <p align=""center"" markdown=""true"">Distribute content along the horizontal axis.</p> |\n', '| <table><tr><td markdown=""true"">[![](./assets/css_grid_align_content_start.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td><td markdown=""true"">[`align-content: start`](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td><td markdown=""true"">[![](./assets/css_grid_align_content_space_around.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td><td markdown=""true"">[`align-content: space-around`](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td></tr><tr><td markdown=""true"">[![](./assets/css_grid_align_content_center.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td><td markdown=""true"">[`align-content: center`](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td><td markdown=""true"">[![](./assets/css_grid_align_content_space_between.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td><td markdown=""true"">[`align-content: space-between`](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td></tr><tr><td markdown=""true"">[![](./assets/css_grid_align_content_end.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td><td markdown=""true"">[`align-content: end`](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td><td markdown=""true"">[![](./assets/css_grid_align_content_stretch.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td><td markdown=""true"">[`align-content: stretch`](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td></tr></table> |\n', '\n', '| Justify Content                             |\n', '| --------------------------------- |\n', '| <p align=""center"" markdown=""true"">Distribute content along the vertical axis.</p> |\n', '| <table><tr><td markdown=""true"">[![](./assets/css_grid_justify_content_start.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td><td markdown=""true"">[`justify-content: start`](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td><td markdown=""true"">[![](./assets/css_grid_justify_content_space_around.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td><td markdown=""true"">[`justify-content: space-around`](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td></tr><tr><td markdown=""true"">[![](./assets/css_grid_justify_content_center.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td><td markdown=""true"">[`justify-content: center`](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td><td markdown=""true"">[![](./assets/css_grid_justify_content_space_between.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td><td markdown=""true"">[`justify-content: space-between`](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td></tr><tr><td markdown=""true"">[![](./assets/css_grid_justify_content_end.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td><td markdown=""true"">[`justify-content: end`](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td><td markdown=""true"">[![](./assets/css_grid_justify_content_stretch.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td><td markdown=""true"">[`justify-content: stretch`](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td></tr></table> |\n', '\n', '| Align Items                             |\n', '| --------------------------------- |\n', '| <p align=""center"" markdown=""true"">Distribute content along the horizontal axis within their grid area.</p> |\n', '| <table><tr><td markdown=""true"">[![](./assets/css_grid_align_items_start.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/align-items)</td><td markdown=""true"">[`align-items: start`](https://developer.mozilla.org/en-US/docs/Web/CSS/align-items)</td><td markdown=""true"">[![](./assets/css_grid_align_items_center.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/align-items)</td><td markdown=""true"">[`align-items: center`](https://developer.mozilla.org/en-US/docs/Web/CSS/align-items)</td></tr><tr><td markdown=""true"">[![](./assets/css_grid_align_items_end.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/align-items)</td><td markdown=""true"">[`align-items: end`](https://developer.mozilla.org/en-US/docs/Web/CSS/align-items)</td><td markdown=""true"">[![](./assets/css_grid_align_items_stretch.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/align-items)</td><td markdown=""true"">[`align-items: stretch`](https://developer.mozilla.org/en-US/docs/Web/CSS/align-items)</td></tr></table> |\n', '\n', '| Justify Items                            |\n', '| --------------------------------- |\n', '| <p align=""center"" markdown=""true"">Distribute content along the vertical axis within their grid area.</p> |\n', '| <table><tr><td markdown=""true"">[![](./assets/css_grid_justify_items_start.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-items)</td><td markdown=""true"">[`justify-items: start`](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-items)</td><td markdown=""true"">[![](./assets/css_grid_justify_items_center.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-items)</td><td markdown=""true"">[`justify-items: center`](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-items)</td></tr><tr><td markdown=""true"">[![](./assets/css_grid_justify_items_end.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-items)</td><td markdown=""true"">[`justify-items: end`](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-items)</td><td markdown=""true"">[![](./assets/css_grid_justify_items_stretch.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-items)</td><td markdown=""true"">[`justify-items: stretch`](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-items)</td></tr></table> |\n', '\n', '| High Resolution | Grayscale Print |\n', '| --------------- | --------------- |\n', '| [![](./assets/lowres-css_grid.png)](./assets/css_grid.png) | [![](./assets/lowres-css_grid_print.png)](./assets/css_grid_print.png) |\n', '\n', '## :sparkles: CSS Flexbox Layout\n', '\n', '| Flex Direction                             |\n', '| --------------------------------- |\n', '| <p align=""center"" markdown=""true"">The flex-direction CSS property sets how flex items are placed in the flex container defining the main axis and the direction (normal or reversed).</p> |\n', '| <table><tr><td markdown=""true"">[![](./assets/css_flexbox_flex_direction_row.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/flex-direction)</td><td markdown=""true"">[`flex-direction: row`](https://developer.mozilla.org/en-US/docs/Web/CSS/flex-direction)</td><td markdown=""true"">[![](./assets/css_flexbox_flex_direction_column.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/flex-direction)</td><td markdown=""true"">[`flex-direction: column`](https://developer.mozilla.org/en-US/docs/Web/CSS/flex-direction)</td></tr><tr><td markdown=""true"">[![](./assets/css_flexbox_flex_direction_row_reverse.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/flex-direction)</td><td markdown=""true"">[`flex-direction: row-reverse`](https://developer.mozilla.org/en-US/docs/Web/CSS/flex-direction)</td><td markdown=""true"">[![](./assets/css_flexbox_flex_direction_column_reverse.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/flex-direction)</td><td markdown=""true"">[`flex-direction: column-reverse`](https://developer.mozilla.org/en-US/docs/Web/CSS/flex-direction)</td></tr></table> |\n', '\n', '| Align Content                             |\n', '| --------------------------------- |\n', '| <p align=""center"" markdown=""true"">The CSS align-content property sets the distribution of space between and around content items along a flexbox\'s cross-axis.</p> |\n', '| <table><tr><td markdown=""true"">[![](./assets/css_flexbox_align_content_flex_start.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td><td markdown=""true"">[`align-content: flex-start`](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td><td markdown=""true"">[![](./assets/css_flexbox_align_content_space_around.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td><td markdown=""true"">[`align-content: space-around`](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td></tr><tr><td markdown=""true"">[![](./assets/css_flexbox_align_content_center.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td><td markdown=""true"">[`align-content: center`](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td><td markdown=""true"">[![](./assets/css_flexbox_align_content_space_between.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td><td markdown=""true"">[`align-content: space-between`](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td></tr><tr><td markdown=""true"">[![](./assets/css_flexbox_align_content_flex_end.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td><td markdown=""true"">[`align-content: flex-end`](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td><td markdown=""true"">[![](./assets/css_flexbox_align_content_stretch.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td><td markdown=""true"">[`align-content: stretch`](https://developer.mozilla.org/en-US/docs/Web/CSS/align-content)</td></tr></table> |\n', '\n', '| Justify Content                             |\n', '| --------------------------------- |\n', '| <p align=""center"" markdown=""true"">The CSS justify-content property defines how the browser distributes space between and around content items along the main-axis of a flex container.</p> |\n', '| <table><tr><td markdown=""true"">[![](./assets/css_flexbox_justify_content_flex_start.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td><td markdown=""true"">[`justify-content: flex-start`](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td><td markdown=""true"">[![](./assets/css_flexbox_justify_content_space_around.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td><td markdown=""true"">[`justify-content: space-around`](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td></tr><tr><td markdown=""true"">[![](./assets/css_flexbox_justify_content_center.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td><td markdown=""true"">[`justify-content: center`](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td><td markdown=""true"">[![](./assets/css_flexbox_justify_content_space_between.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td><td markdown=""true"">[`justify-content: space-between`](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td></tr><tr><td markdown=""true"">[![](./assets/css_flexbox_justify_content_flex_end.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td><td markdown=""true"">[`justify-content: flex-end`](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td><td markdown=""true"">[![](./assets/css_flexbox_justify_content_stretch.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td><td markdown=""true"">[`justify-content: stretch`](https://developer.mozilla.org/en-US/docs/Web/CSS/justify-content)</td></tr></table> |\n', '\n', '| Align Items                             |\n', '| --------------------------------- |\n', '| <p align=""center"" markdown=""true"">The CSS align-items property sets the align-self value on all direct children as a group. In Flexbox, it controls the alignment of items on the Cross Axis.</p> |\n', '| <table><tr><td markdown=""true"">[![](./assets/css_flexbox_align_items_flex_start.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/align-items)</td><td markdown=""true"">[`align-items: flex-start`](https://developer.mozilla.org/en-US/docs/Web/CSS/align-items)</td><td markdown=""true"">[![](./assets/css_flexbox_align_items_center.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/align-items)</td><td markdown=""true"">[`align-items: center`](https://developer.mozilla.org/en-US/docs/Web/CSS/align-items)</td></tr><tr><td markdown=""true"">[![](./assets/css_flexbox_align_items_flex_end.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/align-items)</td><td markdown=""true"">[`align-items: flex-end`](https://developer.mozilla.org/en-US/docs/Web/CSS/align-items)</td><td markdown=""true"">[![](./assets/css_flexbox_align_items_stretch.png)](https://developer.mozilla.org/en-US/docs/Web/CSS/align-items)</td><td markdown=""true"">[`align-items: stretch`](https://developer.mozilla.org/en-US/docs/Web/CSS/align-items)</td></tr></table> |\n', '\n', '| High Resolution | Grayscale Print |\n', '| --------------- | --------------- |\n', '| [![](./assets/lowres-css_flexbox.png)](./assets/css_flexbox.png) | [![](./assets/lowres-css_flexbox_print.png)](./assets/css_flexbox_print.png) |']"
Model Explainability,ur-whitelab/exmol,ur-whitelab,https://api.github.com/repos/ur-whitelab/exmol,221,36,7,"['https://api.github.com/users/whitead', 'https://api.github.com/users/geemi725', 'https://api.github.com/users/hgandhi2411', 'https://api.github.com/users/maclandrol', 'https://api.github.com/users/eltociear', 'https://api.github.com/users/navneeth3005', 'https://api.github.com/users/aditis44']",Python,2023-04-06T15:49:33Z,https://raw.githubusercontent.com/ur-whitelab/exmol/main/README.md,"['# Explaining why that molecule\n', '\n', '[![GitHub](https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white)](https://github.com/ur-whitelab/exmol)\n', '[![tests](https://github.com/ur-whitelab/exmol/actions/workflows/tests.yml/badge.svg)](https://github.com/ur-whitelab/exmol) [![paper](https://github.com/ur-whitelab/exmol/actions/workflows/paper.yml/badge.svg)](https://github.com/ur-whitelab/exmol) [![docs](https://github.com/ur-whitelab/exmol/actions/workflows/docs.yml/badge.svg)](https://ur-whitelab.github.io/exmol/)\n', '[![PyPI version](https://badge.fury.io/py/exmol.svg)](https://badge.fury.io/py/exmol)\n', '[![MIT license](https://img.shields.io/badge/License-MIT-blue.svg)](https://lbesson.mit-license.org/)\n', '\n', '- [Explaining why that molecule](#explaining-why-that-molecule)\n', '  - [Install](#install)\n', '  - [Quickstart](#quickstart)\n', '  - [Counterfactual Generation](#counterfactual-generation)\n', '  - [Descriptor Attribution](#descriptor-attribution)\n', '  - [Usage](#usage)\n', '  - [Further Examples](#further-examples)\n', '  - [Chemical Space](#chemical-space)\n', '  - [SVG](#svg)\n', '  - [Disable Progress Bars](#disable-progress-bars)\n', '  - [API and Docs](#api-and-docs)\n', '  - [Developing](#developing)\n', '  - [Citation](#citation)\n', '\n', '\n', '`exmol` is a package to explain black-box predictions of molecules. The package uses model agnostic explanations to help users understand why a molecule is predicted to have a property.\n', '\n', '## Install\n', '\n', '```sh\n', 'pip install exmol\n', '```\n', '\n', '## Quickstart\n', '\n', 'See [the tutorial](https://ur-whitelab.github.io/exmol/paper2_LIME/Tutorial.html) to give an overview of the basic usage of exmol.\n', '\n', '## Counterfactual Generation\n', '\n', 'Our package implements the Model Agnostic Counterfactual Compounds with STONED to generate counterfactuals.\n', 'A counterfactual can explain a prediction by showing what would have to change in the molecule to change its predicted class. Here is an example of a counterfactual:\n', '\n', '> This package is not popular. If the package had a logo, it would be popular.\n', '\n', 'In addition to having a changed prediction, a molecular counterfactual must be similar to its base molecule as much as possible. Here is an example of a molecular counterfactual:\n', '\n', '<img alt=""counterfactual demo"" src=""https://raw.githubusercontent.com/ur-whitelab/exmol/main/paper1_CFs/svg_figs/counterfactual.png"" width=""400"">\n', '\n', 'The counterfactual shows that if the carboxylic acid were an ester, the molecule would be active. It is up to the user to translate this set of structures into a meaningful sentence.\n', '\n', '## Descriptor Attribution\n', 'This package also implements Model Agnostic Descriptor Attribution for molecules using LIME.\n', 'Descriptor attributions can explain a prediction by computing QSARs for molecular structure properties independent of features used for model predictions. Here is an example of descriptor attribution:\n', '\n', '<img alt=""descriptor demo"" src=""https://raw.githubusercontent.com/ur-whitelab/exmol/main/paper2_LIME/descriptor.png"" width=""800"">\n', '\n', 'The descriptor t-statistics show which chemical properties or substructures influence properety prediction for the pictured molecule. LIME is a perturbation based method and the descriptor attributions depend on the perturbed chemical space created around the molecule of interest.\n', '\n', '## Usage\n', '\n', ""Let's assume you have a deep learning model `my_model(s)` that takes in one SMILES string and outputs a predicted binary class. We first expand chemical space around the prediction of interest\n"", '\n', '```py\n', 'import exmol\n', '\n', '# mol of interest\n', ""base = 'Cc1onc(-c2ccccc2Cl)c1C(=O)NC1C(=O)N2C1SC(C)(C)C2C(=O)O'\n"", '\n', 'samples = exmol.sample_space(base, my_model, batched=False)\n', '```\n', '\n', 'We uses `batched=False` to indicate `my_model` cannot handle a batch of SMILES, just one at a time.  If your model takes SELFIES, just pass `use_selfies=True` to `sample_space`. Now we select counterfactuals from that space and plot them.\n', '\n', '```py\n', 'cfs = exmol.cf_explain(samples)\n', 'exmol.plot_cf(cfs)\n', '```\n', '\n', '<img alt=""set of counterfactuals"" src=""https://raw.githubusercontent.com/ur-whitelab/exmol/main/paper1_CFs/svg_figs/rf-simple.png"" width=""500"">\n', '\n', 'We can also plot the space around the counterfactual. This is computed via PCA of the affinity matrix -- the similarity (Tanimoto of ECFP4) with the base molecule.\n', 'Due to how similarity is calculated, the base is going to be the farthest from all other molecules. Thus your base should fall on the left (or right) extreme of your plot.\n', '\n', '```py\n', 'cfs = exmol.cf_explain(samples)\n', 'exmol.plot_space(samples, cfs)\n', '```\n', '<img alt=""chemical space"" src=""https://raw.githubusercontent.com/ur-whitelab/exmol/main/paper1_CFs/svg_figs/rf-space.png"" width=""600"">\n', '\n', 'Each counterfactual is a Python `dataclass` with information allowing it to be used in your own analysis:\n', '\n', '```py\n', 'print(cfs[1])\n', '```\n', '```\n', '{\n', ""'smiles': 'Cc1onc(-c2ccccc2Cl)c1C(=O)NC1C(=O)N2C1SC(C)(C)C2C',\n"", ""'selfies': '[C][C][O][N][=C][Branch1_1][Branch2_3][C][=C][C][=C][C][=C][Ring1][Branch1_2][Cl][C]\n"", '            [Expl=Ring1][N][C][Branch1_2][C][=O][N][C][C][Branch1_2][C][=O][N][C][Ring1][Branch1_1][S][C]\n', ""            [Branch1_1][C][C][Branch1_1][C][C][C][Ring1][Branch1_3][C]',\n"", ""'similarity': 0.8,\n"", ""'yhat': 1,\n"", ""'index': 1813,\n"", ""'position': array([-7.8032394 ,  0.51781263]),\n"", ""'is_origin': False,\n"", ""'cluster': -1,\n"", ""'label': 'Counterfactual 1'\n"", '}\n', '```\n', '\n', ""We can use the same chemical space to get descriptor attributions for the molecule. Along with `samples`, we also need to specify the `descriptor_type` to get attributions. You can select from `Classic` Rdkit descriptors, `MACCS` fingerprint descriptors, `ECFP` substructure descriptors. The default `descriptor_type` is `MACCS`. If you'd like to use regression coefficients for analysis, specify `return_beta=True`. The descriptor t-statistics are stored in `descriptors.tstats` attribute for the base molecule and can be accessed using `space_tstats = space[0].descriptors.tstats`. `plot_descriptors` saves a plot as shown below in the `output_file`.\n"", '\n', '```py\n', ""beta = exmol.lime_explain(samples, descriptor_type='ECFP', return_beta=True)\n"", ""exmol.plot_descriptors(samples, output_file='ecfp.svg')\n"", '```\n', '<img alt=""ecfp descriptors"" src=""https://raw.githubusercontent.com/ur-whitelab/exmol/main/paper2_LIME/ECFP.svg"" width=""400"">\n', '\n', 'You can use a more typical atom attribution plot as well, although note that some information is lost in this representation.\n', '\n', '```py\n', 'exmol.plot_utils.similarity_map_using_tstats(samples[0])\n', '```\n', '<img alt=""molecule attribution by coloring each atom"" src=""https://raw.githubusercontent.com/ur-whitelab/exmol/main/paper2_LIME/mol-attr.png"">\n', '\n', '\n', 'You can also plot the chemical space colored by fit to see how well the regression fits the original model. To plot by fit, regression coefficients `beta` need to be passed in as an argument.\n', '\n', '```py\n', 'exmol.plot_utils.plot_space_by_fit(\n', '    samples,\n', '    [samples[0]],\n', '    beta=beta,\n', '    mol_size=(300, 250),\n', ""    figure_kwargs={'figsize': (7,5)},\n"", ')\n', '```\n', '<img alt=""chemical space by fit"" src=""https://raw.githubusercontent.com/ur-whitelab/exmol/main/paper2_LIME/space_by_fit.png"" width=""500"">\n', '\n', 'It is also possible to get global attributions for multiple base molecules. For this, the user should create a space around each instance of interest and concatenate these spaces. Then use this joint space to do lime explanations:\n', '\n', '```py\n', ""beta = exmol.lime_explain(joint_space, descriptor_type='ECFP', return_beta=True, multiple_bases=True)\n"", '```\n', '\n', ""`lime_explain()` uses a linear surrogate model for descriptor explanations. You can also use a custom surrogate model instead of a linear model. To do so, just add desired descriptors to the chemical space using the `add_descriptors()` function and then use a custom model on samples to get explanations. For example, add ECFP descriptors using `exmol.add_descriptors(samples, descriptor_type='ECFP')`.\n"", '\n', '## Further Examples\n', '\n', 'You can find more examples by looking at the exact code used to generate all figures from our paper [in the docs webpage](https://ur-whitelab.github.io/exmol/toc.html).\n', '\n', '## Chemical Space\n', '\n', 'When calling `exmol.sample_space` you can pass `preset=<preset>`, which can be\n', 'one of the following:\n', '\n', ""* `'narrow'`: Only one change to molecular structure, reduced set of possible bonds/elements\n"", ""* `'medium'`: Default. One or two changes to molecular structure, reduced set of possible bonds/elements\n"", ""* `'wide'`: One through five changes to molecular structure, large set of possible bonds/elements\n"", ""* `'chemed'`: A restricted set where only pubchem molecules are considered.\n"", '* `\'custom\'`: A restricted set where only molecules provided by the ""data"" key are considered.\n', ""* `'synspace'`: Chemical space is generated by running retro and forward synthesis reactions, so all generaterd molecules are synthetically feasible. Uses [synspace](https://github.com/whitead/synspace) package.\n"", '\n', 'You can also pass `num_samples` as a ""request"" for number of samples. You will typically end up with less due to\n', 'degenerate molecules. See API for complete description.\n', '\n', '## SVG\n', '\n', 'Molecules are by default drawn as PNGs. If you would like to have them drawn as SVGs, call `insert_svg` after calling\n', '`plot_space` or `plot_cf`\n', '\n', '```py\n', 'import skunk\n', 'exmol.plot_cf(exps)\n', 'svg = exmol.insert_svg(exps, mol_fontsize=16)\n', '\n', '# for Jupyter Notebook\n', 'skunk.display(svg)\n', '\n', '# To save to file\n', ""with open('myplot.svg', 'w') as f:\n"", '    f.write(svg)\n', '```\n', '\n', 'This is done with the [skunk🦨 library](https://github.com/whitead/skunk).\n', '\n', '## Disable Progress Bars\n', '\n', 'If `exmol` is being too loud, add `quiet = True` to `sample_space` arguments.\n', '\n', '## API and Docs\n', '\n', '[Read API here](https://ur-whitelab.github.io/exmol/api.html). You should also read the paper (see below) for a more exact\n', 'description of the methods and implementation.\n', '\n', '## Developing\n', '\n', 'This repo uses pre-commit, so after cloning run `pip install -r requirements.txt` and `pre-commit install` prior to committing.\n', '\n', '## Citation\n', '\n', 'For the counterfactual work, please cite [Wellawatte et al.](https://pubs.rsc.org/en/content/articlelanding/2022/sc/d1sc05259d#!divAbstract)\n', '\n', '```bibtex\n', '@Article{wellawatte_seshadri_white_2021,\n', 'author =""Wellawatte, Geemi P. and Seshadri, Aditi and White, Andrew D."",\n', 'title  =""Model agnostic generation of counterfactual explanations for molecules"",\n', 'journal  =""Chem. Sci."",\n', 'year  =""2022"",\n', 'pages  =""-"",\n', 'publisher  =""The Royal Society of Chemistry"",\n', 'doi  =""10.1039/D1SC05259D"",\n', 'url  =""http://dx.doi.org/10.1039/D1SC05259D"",\n', '}\n', '```\n', '\n', 'For the descriptor explanations, please cite [Gandhi et. al.](https://doi.org/10.26434/chemrxiv-2022-v5p6m-v2)\n', '\n', '```bibtex\n', '@Article{gandhi_white_2022,\n', 'place = ""Cambridge"",\n', 'title = ""Explaining structure-activity relationships using locally faithful surrogate models"",\n', 'DOI = ""10.26434/chemrxiv-2022-v5p6m"",\n', 'url = ""https://doi.org/10.26434/chemrxiv-2022-v5p6m""\n', 'journal = ""ChemRxiv"",\n', 'publisher = ""Cambridge Open Engage"",\n', 'author = ""Gandhi, Heta A. and White, Andrew D."",\n', 'year = ""2022""\n', '}\n', '```\n']"
Model Explainability,oegedijk/explainerdashboard,oegedijk,https://api.github.com/repos/oegedijk/explainerdashboard,1668,218,15,"['https://api.github.com/users/oegedijk', 'https://api.github.com/users/oegesam', 'https://api.github.com/users/raybellwaves', 'https://api.github.com/users/mekomlusa', 'https://api.github.com/users/Simon-Free', 'https://api.github.com/users/brandonserna', 'https://api.github.com/users/rajgupt', 'https://api.github.com/users/yanhong-zhao-ef', 'https://api.github.com/users/achimgaedke', 'https://api.github.com/users/absynthe', 'https://api.github.com/users/jenoOvchi', 'https://api.github.com/users/hugocool', 'https://api.github.com/users/haizadtarik', 'https://api.github.com/users/tunayokumus', 'https://api.github.com/users/woochan-jang']",Python,2023-04-24T20:01:48Z,https://raw.githubusercontent.com/oegedijk/explainerdashboard/master/README.md,"['![GitHub Workflow Status (branch)](https://img.shields.io/github/actions/workflow/status/oegedijk/explainerdashboard/explainerdashboard.yml?branch=main)\r\n', '![https://pypi.python.org/pypi/explainerdashboard/](https://img.shields.io/pypi/v/explainerdashboard.svg)\r\n', '![https://anaconda.org/conda-forge/explainerdashboard/](https://anaconda.org/conda-forge/explainerdashboard/badges/version.svg)\r\n', '[![codecov](https://codecov.io/gh/oegedijk/explainerdashboard/branch/master/graph/badge.svg?token=0XU6HNEGBK)](undefined)\r\n', '[![Downloads](https://static.pepy.tech/badge/explainerdashboard)](https://pepy.tech/project/explainerdashboard)\r\n', '\r\n', '# explainerdashboard\r\n', 'by: Oege Dijk\r\n', '\r\n', 'This package makes it convenient to quickly deploy a dashboard web app\r\n', 'that explains the workings of a (scikit-learn compatible) machine \r\n', 'learning model. The dashboard provides interactive plots on model performance, \r\n', 'feature importances, feature contributions to individual predictions, \r\n', '""what if"" analysis,\r\n', 'partial dependence plots, SHAP (interaction) values, visualisation of individual\r\n', 'decision trees, etc. \r\n', '\r\n', 'You can also interactively explore components of the dashboard in a \r\n', 'notebook/colab environment (or just launch a dashboard straight from there). \r\n', 'Or design a dashboard with your own [custom layout](https://explainerdashboard.readthedocs.io/en/latest/buildcustom.html) \r\n', 'and explanations (thanks to the modular design of the library). And you can combine multiple dashboards into\r\n', 'a single [ExplainerHub](https://explainerdashboard.readthedocs.io/en/latest/hub.html).\r\n', '\r\n', 'Dashboards can be exported to static html directly from a running dashboard, or \r\n', 'programmatically as an artifact as part of an automated CI/CD deployment process.\r\n', '\r\n', ' Examples deployed at: [titanicexplainer.herokuapp.com](http://titanicexplainer.herokuapp.com), \r\n', ' detailed documentation at [explainerdashboard.readthedocs.io](http://explainerdashboard.readthedocs.io), \r\n', ' example notebook on how to launch dashboard for different models [here](notebooks/dashboard_examples.ipynb), and an example notebook on how to interact with the explainer object [here](notebooks/explainer_examples.ipynb).\r\n', '\r\n', ' Works with `scikit-learn`, `xgboost`, `catboost`, `lightgbm`, and `skorch` \r\n', ' (sklearn wrapper for tabular PyTorch models) and others.\r\n', '\r\n', ' ## Installation\r\n', '\r\n', 'You can install the package through pip:\r\n', '\r\n', '`pip install explainerdashboard`\r\n', '\r\n', 'or conda-forge:\r\n', '\r\n', '`conda install -c conda-forge explainerdashboard`\r\n', '\r\n', '## Demonstration:\r\n', '\r\n', '![explainerdashboard.gif](explainerdashboard.gif)\r\n', '\r\n', '<!-- [![Dashboard Screenshot](https://i.postimg.cc/Gm8RnKVb/Screenshot-2020-07-01-at-13-25-19.png)](https://postimg.cc/PCj9mWd7) -->\r\n', '(for live demonstration see [titanicexplainer.herokuapp.com](http://titanicexplainer.herokuapp.com))\r\n', '## Background\r\n', '\r\n', 'In a lot of organizations, especially governmental, but with the GDPR also increasingly in private sector, it is becoming more and more important to be able to explain the inner workings of your machine learning algorithms. Customers have to some extent a right to an explanation why they received a certain prediction, and more and more internal and external regulators require it. With recent innovations in explainable AI (e.g. SHAP values) the old black box trope is no longer valid, but it can still take quite a bit of data wrangling and plot manipulation to get the explanations out of a model. This library aims to make this easy.\r\n', '\r\n', 'The goal is manyfold:\r\n', '- Make it easy for data scientists to quickly inspect the workings and performance of their model in a few lines of code\r\n', '- Make it possible for non data scientist stakeholders such as managers, directors, internal and external watchdogs to interactively inspect the inner workings of the model without having to depend on a data scientist to generate every plot and table\r\n', '- Make it easy to build an application that explains individual predictions of your model for customers that ask for an explanation\r\n', ""- Explain the inner workings of the model to the people working (human-in-the-loop) with it so that they gain understanding what the model does and doesn't do. This is important so that they can gain an intuition for when the model is likely missing information and may have to be overruled. \r\n"", '\r\n', '\r\n', 'The library includes:\r\n', '- *Shap values* (i.e. what is the contributions of each feature to each individual prediction?)\r\n', '- *Permutation importances* (how much does the model metric deteriorate when you shuffle a feature?)\r\n', '- *Partial dependence plots* (how does the model prediction change when you vary a single feature?\r\n', '- *Shap interaction values* (decompose the shap value into a direct effect an interaction effects)\r\n', '- For Random Forests and xgboost models: visualisation of individual decision trees\r\n', '- Plus for classifiers: precision plots, confusion matrix, ROC AUC plot, PR AUC plot, etc\r\n', '- For regression models: goodness-of-fit plots, residual plots, etc. \r\n', '\r\n', 'The library is designed to be modular so that it should be easy to design your own interactive dashboards with plotly dash, with most of the work of calculating and formatting data, and rendering plots and tables handled by `explainerdashboard`, so that you can focus on the layout\r\n', 'and project specific textual explanations. (i.e. design it so that it will be interpretable for business users in your organization, not just data scientists)\r\n', '\r\n', 'Alternatively, there is a built-in standard dashboard with pre-built tabs (that you can switch off individually)\r\n', '\r\n', '## Examples of use\r\n', '\r\n', 'Fitting a model, building the explainer object, building the dashboard, and then running it can be as simple as:\r\n', '\r\n', '```python\r\n', 'ExplainerDashboard(ClassifierExplainer(RandomForestClassifier().fit(X_train, y_train), X_test, y_test)).run()\r\n', '```\r\n', '\r\n', 'Below a multi-line example, adding a few extra parameters. \r\n', 'You can group onehot encoded categorical variables together using the `cats` \r\n', 'parameter. You can either pass a dict specifying a list of onehot cols per\r\n', 'categorical feature, or if you encode using e.g. \r\n', ""`pd.get_dummies(df.Name, prefix=['Name'])` (resulting in column names `'Name_Adam', 'Name_Bob'`) \r\n"", ""you can simply pass the prefix `'Name'`:\r\n"", '\r\n', '```python\r\n', 'from sklearn.ensemble import RandomForestClassifier\r\n', 'from explainerdashboard import ClassifierExplainer, ExplainerDashboard\r\n', 'from explainerdashboard.datasets import titanic_survive, titanic_names\r\n', '\r\n', 'feature_descriptions = {\r\n', '    ""Sex"": ""Gender of passenger"",\r\n', '    ""Gender"": ""Gender of passenger"",\r\n', '    ""Deck"": ""The deck the passenger had their cabin on"",\r\n', '    ""PassengerClass"": ""The class of the ticket: 1st, 2nd or 3rd class"",\r\n', '    ""Fare"": ""The amount of money people paid"", \r\n', '    ""Embarked"": ""the port where the passenger boarded the Titanic. Either Southampton, Cherbourg or Queenstown"",\r\n', '    ""Age"": ""Age of the passenger"",\r\n', '    ""No_of_siblings_plus_spouses_on_board"": ""The sum of the number of siblings plus the number of spouses on board"",\r\n', '    ""No_of_parents_plus_children_on_board"" : ""The sum of the number of parents plus the number of children on board"",\r\n', '}\r\n', '\r\n', 'X_train, y_train, X_test, y_test = titanic_survive()\r\n', 'train_names, test_names = titanic_names()\r\n', 'model = RandomForestClassifier(n_estimators=50, max_depth=5)\r\n', 'model.fit(X_train, y_train)\r\n', '\r\n', 'explainer = ClassifierExplainer(model, X_test, y_test, \r\n', ""                                cats=['Deck', 'Embarked',\r\n"", ""                                    {'Gender': ['Sex_male', 'Sex_female', 'Sex_nan']}],\r\n"", ""                                cats_notencoded={'Embarked': 'Stowaway'}, # defaults to 'NOT_ENCODED'\r\n"", '                                descriptions=feature_descriptions, # adds a table and hover labels to dashboard\r\n', ""                                labels=['Not survived', 'Survived'], # defaults to ['0', '1', etc]\r\n"", '                                idxs = test_names, # defaults to X.index\r\n', '                                index_name = ""Passenger"", # defaults to X.index.name\r\n', '                                target = ""Survival"", # defaults to y.name\r\n', '                                )\r\n', '\r\n', 'db = ExplainerDashboard(explainer, \r\n', '                        title=""Titanic Explainer"", # defaults to ""Model Explainer""\r\n', '                        shap_interaction=False, # you can switch off tabs with bools\r\n', '                        )\r\n', 'db.run(port=8050)\r\n', '```\r\n', '\r\n', 'For a regression model you can also pass the units of the target variable (e.g. \r\n', 'dollars):\r\n', '\r\n', '```python\r\n', 'X_train, y_train, X_test, y_test = titanic_fare()\r\n', 'model = RandomForestRegressor().fit(X_train, y_train)\r\n', '\r\n', 'explainer = RegressionExplainer(model, X_test, y_test, \r\n', ""                                cats=['Deck', 'Embarked', 'Sex'],\r\n"", '                                descriptions=feature_descriptions, \r\n', '                                units = ""$"", # defaults to """"\r\n', '                                )\r\n', '\r\n', 'ExplainerDashboard(explainer).run()\r\n', '```\r\n', '\r\n', '`y_test` is actually optional, although some parts of the dashboard like performance\r\n', 'metrics will obviously not be available: `ExplainerDashboard(ClassifierExplainer(model, X_test)).run()`.\r\n', '\r\n', ""You can export a dashboard to static html with `db.save_html('dashboard.html')`.\r\n"", '\r\n', 'For a simplified single page dashboard try `ExplainerDashboard(explainer, simple=True)`.\r\n', '\r\n', '<details><summary>Show simplified dashboard screenshot</summary>\r\n', '<p>\r\n', '\r\n', '\r\n', '![docs/source/screenshots/simple_classifier_dashboard.png](docs/source/screenshots/simple_classifier_dashboard.png)\r\n', '\r\n', '</p>\r\n', '</details>\r\n', '<p></p>\r\n', '\r\n', '### ExplainerHub\r\n', '\r\n', 'You can combine multiple dashboards and host them in a single place using \r\n', '[ExplainerHub](https://explainerdashboard.readthedocs.io/en/latest/hub.html):\r\n', '\r\n', '```python\r\n', 'db1 = ExplainerDashboard(explainer1, title=""Classifier Explainer"", \r\n', '         description=""Model predicting survival on H.M.S. Titanic"")\r\n', 'db2 = ExplainerDashboard(explainer2, title=""Regression Explainer"",\r\n', '         description=""Model predicting ticket price on H.M.S. Titanic"")\r\n', 'hub = ExplainerHub([db1, db2])\r\n', 'hub.run()\r\n', '```\r\n', '\r\n', 'You can adjust titles and descriptions, manage users and logins, store and load \r\n', 'from config, manage the hub through a CLI and more. See the \r\n', '[ExplainerHub documentation](https://explainerdashboard.readthedocs.io/en/latest/hub.html).\r\n', '\r\n', '<details><summary>Show ExplainerHub screenshot</summary>\r\n', '<p>\r\n', '\r\n', '\r\n', '![docs/source/screenshots/explainerhub.png](docs/source/screenshots/explainerhub.png)\r\n', '\r\n', '</p>\r\n', '</details>\r\n', '<p></p>\r\n', '\r\n', '\r\n', '### Dealing with slow calculations\r\n', '\r\n', 'Some of the calculations for the dashboard such as calculating SHAP (interaction) values\r\n', 'and permutation importances can be slow for large datasets and complicated models. \r\n', 'There are a few tricks to make this less painful:\r\n', '\r\n', '1. Switching off the interactions tab (`shap_interaction=False`) and disabling\r\n', '    permutation importances (`no_permutations=True`). Especially SHAP interaction\r\n', '    values can be very slow to calculate, and often are not needed for analysis.\r\n', '    For permutation importances you can set the `n_jobs` parameter to speed up\r\n', '    the calculation in parallel.\r\n', '2. Storing the explainer. The calculated properties are only calculated once\r\n', '    for each instance, however each time when you instantiate a new explainer\r\n', '    instance they will have to be recalculated. You can store them with\r\n', '    `explainer.dump(""explainer.joblib"")` and load with e.g. \r\n', '    `ClassifierExplainer.from_file(""explainer.joblib"")`. All calculated properties\r\n', '    are stored along with the explainer.\r\n', '3. Using a smaller (test) dataset, or using smaller decision trees. \r\n', '    TreeShap computational complexity is `O(TLD^2)`, where `T` is the \r\n', '    number of trees, `L` is the maximum number of leaves in any tree and \r\n', '    `D` the maximal depth of any tree. So reducing the number of leaves or average\r\n', '    depth in the decision tree can really speed up SHAP calculations.\r\n', '4. Pre-computing shap values. Perhaps you already have calculated the shap values\r\n', '    somewhere, or you can calculate them off on a giant cluster somewhere, or\r\n', '    your model supports [GPU generated shap values](https://github.com/rapidsai/gputreeshap). \r\n', '    You can simply add these pre-calculated shap values to the explainer \r\n', '    with `explainer.set_shap_values()` and `explainer.set_shap_interaction_values()` methods.\r\n', '5. Plotting only a random sample of points. When you have a lots of observations,\r\n', '    simply rendering the plots may get slow as well. You can pass the `plot_sample`\r\n', '    parameter to render a (different each time) random sample of observations\r\n', '    for the various scatter plots in the dashboard. E.g.: \r\n', '    `ExplainerDashboard(explainer, plot_sample=1000).run()`\r\n', '\r\n', '## Launching from within a notebook\r\n', '\r\n', 'When working inside Jupyter or Google Colab you can use \r\n', ""`ExplainerDashboard(mode='inline')`, `ExplainerDashboard(mode='external')` or\r\n"", ""`ExplainerDashboard(mode='jupyterlab')`, to run the dashboard inline in the notebook,\r\n"", ""or in a seperate tab but keep the notebook interactive. (`db.run(mode='inline')` \r\n"", 'now also works)\r\n', '\r\n', 'There is also a specific interface for quickly displaying interactive components\r\n', 'inline in your notebook: `InlineExplainer()`. For example you can use \r\n', '`InlineExplainer(explainer).shap.dependence()` to display the shap dependence\r\n', 'component interactively in your notebook output cell.\r\n', '\r\n', '## Command line tool\r\n', '\r\n', 'You can store explainers to disk with `explainer.dump(""explainer.joblib"")`\r\n', 'and then run them from the command-line:\r\n', '\r\n', '```bash\r\n', '$ explainerdashboard run explainer.joblib\r\n', '```\r\n', '\r\n', 'Or store the full configuration of a dashboard to `.yaml` with e.g.\r\n', '`dashboard.to_yaml(""dashboard.yaml"", explainerfile=""explainer.joblib"", dump_explainer=True)` and run it with:\r\n', '\r\n', '```bash\r\n', '$ explainerdashboard run dashboard.yaml\r\n', '```\r\n', '\r\n', 'You can also build explainers from the commandline with `explainerdashboard build`.\r\n', 'See [explainerdashboard CLI documentation](https://explainerdashboard.readthedocs.io/en/latest/cli.html)\r\n', 'for details. \r\n', '\r\n', '## Customizing your dashboard\r\n', '\r\n', 'The dashboard is highly modular and customizable so that you can adjust it your\r\n', 'own needs and project. \r\n', '\r\n', '### Changing bootstrap theme\r\n', '\r\n', 'You can change the bootstrap theme by passing a link to the appropriate css\r\n', 'file. You can use the convenient [themes](https://dash-bootstrap-components.opensource.faculty.ai/docs/themes/) module of \r\n', '[dash_bootstrap_components](https://dash-bootstrap-components.opensource.faculty.ai/docs/) to generate\r\n', 'the css url for you:\r\n', '\r\n', '```python\r\n', 'import dash_bootstrap_components as dbc\r\n', '\r\n', 'ExplainerDashboard(explainer, bootstrap=dbc.themes.FLATLY).run()\r\n', '```\r\n', '\r\n', 'See the [dbc themes documentation](https://dash-bootstrap-components.opensource.faculty.ai/docs/themes/)\r\n', 'and [bootwatch website](https://bootswatch.com/) for the different themes that are supported.\r\n', '\r\n', '### Switching off tabs\r\n', '\r\n', 'You can switch off individual tabs using boolean flags. This also makes sure\r\n', ""that expensive calculations for that tab don't get executed:\r\n"", '\r\n', '```python\r\n', 'ExplainerDashboard(explainer,\r\n', '                    importances=False,\r\n', '                    model_summary=True,\r\n', '                    contributions=True,\r\n', '                    whatif=True,\r\n', '                    shap_dependence=True,\r\n', '                    shap_interaction=False,\r\n', '                    decision_trees=True)\r\n', '```\r\n', '\r\n', '### Hiding components\r\n', '\r\n', 'You can also hide individual components on the various tabs:\r\n', '\r\n', '```python\r\n', '    ExplainerDashboard(explainer, \r\n', '        # importances tab:\r\n', '        hide_importances=True,\r\n', '        # classification stats tab:\r\n', '        hide_globalcutoff=True, hide_modelsummary=True, \r\n', '        hide_confusionmatrix=True, hide_precision=True, \r\n', '        hide_classification=True, hide_rocauc=True, \r\n', '        hide_prauc=True, hide_liftcurve=True, hide_cumprecision=True,\r\n', '        # regression stats tab:\r\n', '        # hide_modelsummary=True, \r\n', '        hide_predsvsactual=True, hide_residuals=True, \r\n', '        hide_regvscol=True,\r\n', '        # individual predictions tab:\r\n', '        hide_predindexselector=True, hide_predictionsummary=True,\r\n', '        hide_contributiongraph=True, hide_pdp=True, \r\n', '        hide_contributiontable=True,\r\n', '        # whatif tab:\r\n', '        hide_whatifindexselector=True, hide_whatifprediction=True,\r\n', '        hide_inputeditor=True, hide_whatifcontributiongraph=True, \r\n', '        hide_whatifcontributiontable=True, hide_whatifpdp=True,\r\n', '        # shap dependence tab:\r\n', '        hide_shapsummary=True, hide_shapdependence=True,\r\n', '        # shap interactions tab:\r\n', '        hide_interactionsummary=True, hide_interactiondependence=True,\r\n', '        # decisiontrees tab:\r\n', '        hide_treeindexselector=True, hide_treesgraph=True, \r\n', '        hide_treepathtable=True, hide_treepathgraph=True,\r\n', '        ).run()\r\n', '```\r\n', '\r\n', '### Hiding toggles and dropdowns inside components\r\n', '\r\n', 'You can also hide individual toggles and dropdowns using `**kwargs`. However they\r\n', 'are not individually targeted, so if you pass `hide_cats=True` then the group\r\n', 'cats toggle will be hidden on every component that has one:\r\n', '\r\n', '```python\r\n', 'ExplainerDashboard(explainer, \r\n', '                    no_permutations=True, # do not show or calculate permutation importances\r\n', '                    hide_poweredby=True, # hide the poweredby:explainerdashboard footer\r\n', ""                    hide_popout=True, # hide the 'popout' button from each graph\r\n"", '                    hide_depth=True, # hide the depth (no of features) dropdown\r\n', '                    hide_sort=True, # hide sort type dropdown in contributions graph/table\r\n', '                    hide_orientation=True, # hide orientation dropdown in contributions graph/table\r\n', '                    hide_type=True, # hide shap/permutation toggle on ImportancesComponent \r\n', '                    hide_dropna=True, # hide dropna toggle on pdp component\r\n', '                    hide_sample=True, # hide sample size input on pdp component\r\n', '                    hide_gridlines=True, # hide gridlines on pdp component\r\n', '                    hide_gridpoints=True, # hide gridpoints input on pdp component\r\n', '                    hide_cats_sort=True, # hide the sorting option for categorical features\r\n', '                    hide_cutoff=True, # hide cutoff selector on classification components\r\n', '                    hide_percentage=True, # hide percentage toggle on classificaiton components\r\n', '                    hide_log_x=True, # hide x-axis logs toggle on regression plots\r\n', '                    hide_log_y=True, # hide y-axis logs toggle on regression plots\r\n', '                    hide_ratio=True, # hide the residuals type dropdown\r\n', '                    hide_points=True, # hide the show violin scatter markers toggle\r\n', '                    hide_winsor=True, # hide the winsorize input\r\n', '                    hide_wizard=True, # hide the wizard toggle in lift curve component\r\n', '                    hide_range=True, # hide the range subscript on feature input\r\n', ""                    hide_star_explanation=True, # hide the '* indicates observed label` text\r\n"", ')\r\n', '```\r\n', '\r\n', '### Setting default values\r\n', '\r\n', 'You can also set default values for the various dropdowns and toggles. \r\n', 'All the components with their parameters can be found [in the documentation](https://explainerdashboard.readthedocs.io/en/latest/components.html).\r\n', 'Some examples of useful parameters to pass:\r\n', '\r\n', '```python\r\n', 'ExplainerDashboard(explainer, \r\n', '                    higher_is_better=False, # flip green and red in contributions graph\r\n', '                    n_input_cols=3, # divide feature inputs into 3 columns on what if tab\r\n', ""                    col='Fare', # initial feature in shap graphs\r\n"", ""                    color_col='Age', # color feature in shap dependence graph\r\n"", ""                    interact_col='Age', # interaction feature in shap interaction\r\n"", '                    depth=5, # only show top 5 features\r\n', ""                    sort = 'low-to-high', # sort features from lowest shap to highest in contributions graph/table\r\n"", '                    cats_topx=3, # show only the top 3 categories for categorical features\r\n', ""                    cats_sort='alphabet', # short categorical features alphabetically\r\n"", ""                    orientation='horizontal', # horizontal bars in contributions graph\r\n"", ""                    index='Rugg, Miss. Emily', # initial index to display\r\n"", ""                    pdp_col='Fare', # initial pdp feature\r\n"", '                    cutoff=0.8, # cutoff for classification plots\r\n', '                    round=2 # rounding to apply to floats\r\n', ""                    show_metrics=['accuracy', 'f1', custom_metric] # only show certain metrics \r\n"", '                    plot_sample=1000, # only display a 1000 random markers in scatter plots\r\n', '                    )\r\n', '```\r\n', '\r\n', '\r\n', '### Designing your own layout\r\n', '\r\n', 'All the components in the dashboard are modular and re-usable, which means that \r\n', 'you can build your own custom [dash](https://dash.plotly.com/) dashboards \r\n', 'around them.\r\n', '\r\n', 'By using the built-in `ExplainerComponent` class it is easy to build your\r\n', 'own layouts, with just a bare minimum of knowledge of HTML and [bootstrap](https://dash-bootstrap-components.opensource.faculty.ai/docs/quickstart/). For\r\n', 'example if you only wanted to display the `ConfusionMatrixComponent` and \r\n', '`ShapContributionsGraphComponent`, but hide\r\n', 'a few toggles:\r\n', '\r\n', '```python\r\n', 'from explainerdashboard.custom import *\r\n', '\r\n', 'class CustomDashboard(ExplainerComponent):\r\n', '    def __init__(self, explainer, name=None):\r\n', '        super().__init__(explainer, title=""Custom Dashboard"")\r\n', '        self.confusion = ConfusionMatrixComponent(explainer, name=self.name+""cm"",\r\n', '                            hide_selector=True, hide_percentage=True,\r\n', '                            cutoff=0.75)\r\n', '        self.contrib = ShapContributionsGraphComponent(explainer, name=self.name+""contrib"",\r\n', '                            hide_selector=True, hide_cats=True, \r\n', '                            hide_depth=True, hide_sort=True,\r\n', ""                            index='Rugg, Miss. Emily')\r\n"", '        \r\n', '    def layout(self):\r\n', '        return dbc.Container([\r\n', '            dbc.Row([\r\n', '                dbc.Col([\r\n', '                    html.H1(""Custom Demonstration:""),\r\n', '                    html.H3(""How to build your own layout using ExplainerComponents."")\r\n', '                ])\r\n', '            ]),\r\n', '            dbc.Row([\r\n', '                dbc.Col([\r\n', '                    self.confusion.layout(),\r\n', '                ]),\r\n', '                dbc.Col([\r\n', '                    self.contrib.layout(),\r\n', '                ])\r\n', '            ])\r\n', '        ])\r\n', '\r\n', 'db = ExplainerDashboard(explainer, CustomDashboard, hide_header=True).run()\r\n', '```\r\n', '\r\n', '<details><summary>Show example custom dashboard screenshot</summary>\r\n', '<p>\r\n', '\r\n', '\r\n', '![docs/source/screenshots/custom_dashboard.png](docs/source/screenshots/custom_dashboard.png)\r\n', '\r\n', '</p>\r\n', '\r\n', '</details>\r\n', '<p></p>\r\n', '\r\n', '\r\n', 'You can use this to define your own layouts, specifically tailored to your\r\n', 'own model, project and needs. You can use the [ExplainerComposites](https://github.com/oegedijk/explainerdashboard/blob/master/explainerdashboard/dashboard_components/composites.py) that\r\n', 'are used for the tabs of the default dashboard as a starting point, and edit\r\n', 'them to reorganize components, add text, etc. \r\n', 'See [custom dashboard documentation](https://explainerdashboard.readthedocs.io/en/latest/custom.html)\r\n', 'for more details. A deployed custom dashboard can be found [here](http://titanicexplainer.herokuapp.com/custom/)([source code](https://github.com/oegedijk/explainingtitanic/blob/master/buildcustom.py)).\r\n', '\r\n', '## Deployment\r\n', '\r\n', 'If you wish to use e.g. `gunicorn` or `waitress` to deploy the dashboard you should add \r\n', '`app = db.flask_server()` to your code to expose the Flask server. You can then \r\n', 'start the server with e.g. `gunicorn dashboard:app` \r\n', '(assuming the file you defined the dashboard in was called `dashboard.py`). \r\n', 'See also the [ExplainerDashboard section](https://explainerdashboard.readthedocs.io/en/latest/dashboards.html) \r\n', 'and the [deployment section of the documentation](https://explainerdashboard.readthedocs.io/en/latest/deployment.html).\r\n', '\r\n', 'It can be helpful to store your `explainer` and dashboard layout to disk, and \r\n', 'then reload, e.g.:\r\n', '\r\n', '**generate_dashboard.py**:\r\n', '```python\r\n', 'from explainerdashboard import ClassifierExplainer, ExplainerDashboard\r\n', 'from explainerdashboard.custom import *\r\n', '\r\n', 'explainer = ClassifierExplainer(model, X_test, y_test)\r\n', '\r\n', '# building an ExplainerDashboard ensures that all necessary properties \r\n', '# get calculated:\r\n', 'db = ExplainerDashboard(explainer, [ShapDependenceComposite, WhatIfComposite],\r\n', ""                        title='Awesome Dashboard', hide_whatifpdp=True)\r\n"", '\r\n', '# store both the explainer and the dashboard configuration:\r\n', 'db.to_yaml(""dashboard.yaml"", explainerfile=""explainer.joblib"", dump_explainer=True)\r\n', '```\r\n', '\r\n', 'You can then reload it in **dashboard.py**:\r\n', '```python\r\n', 'from explainerdashboard import ClassifierExplainer, ExplainerDashboard\r\n', '\r\n', '# you can override params during load from_config:\r\n', 'db = ExplainerDashboard.from_config(""dashboard.yaml"", title=""Awesomer Title"")\r\n', '\r\n', 'app = db.flask_server()\r\n', '```\r\n', '\r\n', 'And then run it with:\r\n', '\r\n', '```sh\r\n', '    $ gunicorn dashboard:app\r\n', '```\r\n', '\r\n', 'or with waitress (also works on Windows):\r\n', '\r\n', '```sh\r\n', '    $ waitress-serve dashboard:app\r\n', '```\r\n', '\r\n', '### Minimizing memory usage\r\n', '\r\n', 'When you deploy a dashboard with a dataset with a large number of rows (`n`) and columns (`m`),\r\n', 'the memory usage of the dashboard can be substantial. You can check the (approximate)\r\n', 'memory usage with `explainer.memory_usage()`. (as a side note: if you have lots\r\n', 'of rows, you probably want to set the `plot_sample` parameter as well)\r\n', '\r\n', 'In order to reduce the memory footprint there are a number of things you can do:\r\n', '\r\n', '1. Not including shap interaction tab: shap interaction values are shape (`n*m*m`),\r\n', '    so can take a subtantial amount of memory.\r\n', ""2. Setting a lower precision. By default shap values are stored as `'float64'`,\r\n"", ""    but you can store them as `'float32'` instead and save half the space:\r\n"", ""    ```ClassifierExplainer(model, X_test, y_test, precision='float32')```. You \r\n"", '    can also set a lower precision on your `X_test` dataset yourself ofcourse.\r\n', '3. For multi class classifier, by default `ClassifierExplainer` calculates\r\n', ""    shap values for all classes. If you're only interested in a single class\r\n"", '    you can drop the other shap values: `explainer.keep_shap_pos_label_only(pos_label)`\r\n', '4. Storing data externally. You can for example only store a subset of 10.000 rows in\r\n', '    the explainer itself (enough to generate importance and dependence plots),\r\n', '    and store the rest of your millions of rows of input data in an external file \r\n', '    or database:\r\n', '    - with `explainer.set_X_row_func()` you can set a function that takes \r\n', '        an `index` as argument and returns a single row dataframe with model\r\n', '        compatible input data for that index. This function can include a query\r\n', '        to a database or fileread. \r\n', '    - with `explainer.set_y_func()` you can set a function that takes \r\n', '        and `index` as argument and returns the observed outcome `y` for\r\n', '        that index.\r\n', '    - with `explainer.set_index_list_func()` you can set a function \r\n', '        that returns a list of available indexes that can be queried. Only gets\r\n', '        called upon start of the dashboard.\r\n', '\r\n', '    If you have a very large number of indexes and the user is able to look\r\n', '    them up elsewhere, you can also replace the index dropdowns with a simple free\r\n', '    text field with `index_dropdown=False`. Only valid indexes (i.e. in the \r\n', '    `get_index_list()` list) get propagated\r\n', '    to other components by default, but this can be overriden with `index_check=False`. \r\n', '    Instead of an ``index_list_func`` you can also set an \r\n', '    ``explainer.set_index_check_func(func)`` which should return a bool whether\r\n', '    the ``index`` exists or not. \r\n', '\r\n', '    Important: these function can be called multiple times by multiple independent\r\n', '    components, so probably best to implement some kind of caching functionality.\r\n', '    The functions you pass can be also methods, so you have access to all of the\r\n', '    internals of the explainer.\r\n', '    \r\n', '\r\n', '## Documentation\r\n', '\r\n', 'Documentation can be found at [explainerdashboard.readthedocs.io](https://explainerdashboard.readthedocs.io/en/latest/).\r\n', '\r\n', 'Example notebook on how to launch dashboards for different model types here: [dashboard_examples.ipynb](notebooks/dashboard_examples.ipynb).\r\n', '\r\n', 'Example notebook on how to interact with the explainer object here: [explainer_examples.ipynb](notebooks/explainer_examples.ipynb).\r\n', '\r\n', 'Example notebook on how to design a custom dashboard: [custom_examples.ipynb](notebooks/custom_examples.ipynb).\r\n', '\r\n', '\r\n', '\r\n', '## Deployed example:\r\n', '\r\n', 'You can find an example dashboard at [titanicexplainer.herokuapp.com](http://titanicexplainer.herokuapp.com) \r\n', '\r\n', '(source code at [https://github.com/oegedijk/explainingtitanic](https://github.com/oegedijk/explainingtitanic))\r\n', '\r\n', '## Citation:\r\n', '\r\n', 'A doi can be found at [zenodo](https://zenodo.org/record/7633294)\r\n']"
Model Explainability,llSourcell/world_models,llSourcell,https://api.github.com/repos/llSourcell/world_models,79,27,1,['https://api.github.com/users/llSourcell'],Python,2023-02-11T01:56:44Z,https://raw.githubusercontent.com/llSourcell/world_models/master/README.md,"['# Overview\n', '\n', 'This is the code for [this](https://youtu.be/rV1SIOJzj0c) video on Youtube by Siraj Raval.  An implementation of the ideas from this paper https://arxiv.org/pdf/1803.10122.pdf Code base adapted from https://github.com/hardmaru/estool . For full installation and run instructions see this blog post: https://applied-data.science/blog/hallucinogenic-deep-reinforcement-learning-using-python-and-keras\n', '\n', '## Credits\n', '\n', 'Credits for this code go to [AppliedDataSciencePartners](https://github.com/AppliedDataSciencePartners)\n']"
Model Explainability,xiangwang1223/tree_enhanced_embedding_model,xiangwang1223,https://api.github.com/repos/xiangwang1223/tree_enhanced_embedding_model,73,14,1,['https://api.github.com/users/xiangwang1223'],,2023-01-15T06:59:25Z,https://raw.githubusercontent.com/xiangwang1223/tree_enhanced_embedding_model/master/README.md,"['# Tree-enhanced Embedding Model\n', 'This is our project for the paper:\n', "">Xiang Wang, Xiangnan He, Fuli Feng, Liqiang Nie and Tat-Seng Chua (2018). [TEM: Tree-enhanced Embedding Model for Explainable Recommendation](https://dl.acm.org/citation.cfm?id=3178876.3186066). In WWW'18, Lyon, France, April 23–27, 2018.\n"", '\n', 'Author: Dr. Xiang Wang (xiangwang at u.nus.edu)\n', '\n', '## Introduction\n', 'Tree-enhanced Embedding Mode (TEM) is a new recommendation framework, which combines the strong representation ability of embeddingbased and interpretability of tree-based models. At its core is an easy-to-interpret decision-tree and attention network, making the recommendation process fully transparent and explainable.\n', '\n', '## Citation \n', 'If you want to use our codes and datasets in your research, please cite:\n', '```\n', '@inproceedings{TEM2018,\n', '  author    = {Xiang Wang and\n', '               Xiangnan He and\n', '               Fuli Feng and\n', '               Liqiang Nie and\n', '               Tat{-}Seng Chua},\n', '  title     = {{TEM:} Tree-enhanced Embedding Model for Explainable Recommendation},\n', '  booktitle = {{WWW}},\n', '  pages     = {1543--1552},\n', '  year      = {2018},\n', '}\n', '```\n', '## Codes\n', 'We are finding license suitable to release this software. Currently codes are under request and will be released later.\n', '\n', '## Dataset\n', 'We provide two rich-attribute datasets: London-Attractions (LON-A) and New-York-City-Restaurant (NYC-R) datasets, which have user profiles and item attributes, and are collected from [TripAdvisor](https://www.tripadvisor.com.sg/).\n', '* `London_Attractions_Complete_Review.csv`\n', '  * All positive instances.\n', ""  * Each line is a review, where the fields of user profiles and item attributes start with 'u' and 'i', respectively.\n"", '\n', '* `New_York_City_Restaurant_Complete_Review.csv`\n', '  * All positive instances.\n', ""  * Each line is a review, where the fields of user profiles and item attributes start with 'u' and 'i', respectively.\n""]"
Model Explainability,smazzanti/tds_black_box_models_more_explainable,smazzanti,https://api.github.com/repos/smazzanti/tds_black_box_models_more_explainable,67,41,1,['https://api.github.com/users/smazzanti'],Python,2023-04-12T09:22:56Z,https://raw.githubusercontent.com/smazzanti/tds_black_box_models_more_explainable/master/README.md,"['# Black-Box models are actually more explainable than a Logistic Regression\n', '\n', 'Jupyter Notebook used for writing the article ""Black-Box models are actually more explainable than a Logistic Regression"" published in Towards Data Science: https://towardsdatascience.com/black-box-models-are-actually-more-explainable-than-a-logistic-regression-f263c22795d\n']"
Model Explainability,mark-watson/cancer-deep-learning-model,mark-watson,https://api.github.com/repos/mark-watson/cancer-deep-learning-model,68,47,1,['https://api.github.com/users/mark-watson'],Python,2023-04-12T11:44:09Z,https://raw.githubusercontent.com/mark-watson/cancer-deep-learning-model/master/README.md,"['# Keras Deep Neural Network using Breast Cancer Data with Explanation of Predictions\n', '\n', 'This model is trained on 497 training examples and is tested for accuracy on 151 different testing examples. The accuracy is about 97%.\n', '\n', 'The Python example code provides a simple example of using CSV data files with TensorFlow and training a model with three hidden layers.\n', '\n', 'I assume that you have Keras and TensorFlow installed.\n', '\n', '## Donate on Patreon to support all of my projects\n', '\n', 'Please visit [https://www.patreon.com/markwatson](https://www.patreon.com/markwatson) and sign up to donate $1/month\n', '\n', '## Uses the IntegratedVarients library to explain predictions made by a trained model\n', '\n', 'Please [read this excellent paper](https://arxiv.org/pdf/1703.01365.pdf)\n', 'by Mukund Sundararajan, Ankur Taly, and Qiqi Yan\n', '\n', 'When making a prediction, you can get a scaling of which input features most contributed to a classifiaction made by the model.\n', '\n', 'For example:\n', '\n', '````````\n', '** Contributions to classification for sample type  benign sample  **\n', '\t Clump Thickness :\t -15\n', '\t Uniformity of Cell Size :\t 19\n', '\t Uniformity of Cell Shape :\t -5\n', '\t Marginal Adhesion :\t -15\n', '\t Single Epithelial Cell Size :\t -100\n', '\t Bare Nuclei :\t -5\n', '\t Bland Chromatin :\t -70\n', '\t Normal Nucleoli :\t -5\n', '\t Mitoses :\t 9\n', '** Contributions to classification for sample type  malignant sample  **\n', '\t Clump Thickness :\t 27\n', '\t Uniformity of Cell Size :\t 8\n', '\t Uniformity of Cell Shape :\t 15\n', '\t Marginal Adhesion :\t -21\n', '\t Single Epithelial Cell Size :\t -8\n', '\t Bare Nuclei :\t 100\n', '\t Bland Chromatin :\t 20\n', '\t Normal Nucleoli :\t 5\n', '\t Mitoses :\t 3\n', '````````\n', '## A version of this code was used in a book I wrote\n', '\n', 'The [github repository for my book ""Introduction to Cognitive Computing""](https://github.com/mark-watson/cognitive-computing-book)\n', 'contains an older version of this example.\n', '\n', '# Universary of Wisconcin Cancer Data\n', '\n', '````````\n', '- 0 Clump Thickness               1 - 10\n', '- 1 Uniformity of Cell Size       1 - 10\n', '- 2 Uniformity of Cell Shape      1 - 10\n', '- 3 Marginal Adhesion             1 - 10\n', '- 4 Single Epithelial Cell Size   1 - 10\n', '- 5 Bare Nuclei                   1 - 10\n', '- 6 Bland Chromatin               1 - 10\n', '- 7 Normal Nucleoli               1 - 10\n', '- 8 Mitoses                       1 - 10\n', '- 9 Class (0 for benign, 1 for malignant)\n', '````````\n', '\n', 'I modified the original data slightly by removing the randomized patient ID and changing the target class values from (2,4) to (0,1) for (no cancer, cancer).\n', '\n', 'The CSV file loader in the TensorFlow contrib learn library expects header lines. The following is the first few lines of train.csv:\n', '\n', '````````\n', '10,10,10,8,6,1,8,9,1,1\n', '6,2,1,1,1,1,7,1,1,0\n', '2,5,3,3,6,7,7,5,1,1\n', '````````\n', '\n', 'The last value on each input line is 0 or 1 indicating the target classification.\n', '\n', 'This example just has 2 target classifications, but you can have any number. Label target class values 0, 1, 2, etc.\n']"
Model Explainability,koriavinash1/BioExp,koriavinash1,https://api.github.com/repos/koriavinash1/BioExp,26,6,3,"['https://api.github.com/users/parthnatekar', 'https://api.github.com/users/zsfVishnu', 'https://api.github.com/users/koriavinash1']",Python,2023-02-19T07:04:41Z,https://raw.githubusercontent.com/koriavinash1/BioExp/master/README.md,"['# BioExp\n', '[![Build Status](https://travis-ci.org/koriavinash1/BioExp.svg?branch=master)](https://travis-ci.org/koriavinash1/BioExp)\n', '[![Documentation Status](https://readthedocs.org/projects/bioexp/badge/?version=latest)](https://bioexp.readthedocs.io/en/latest/?badge=latest)\n', '[![PyPI version](https://badge.fury.io/py/BioExp.svg)](https://badge.fury.io/py/BioExp)\n', '[![Downloads](https://pepy.tech/badge/bioexp)](https://pepy.tech/project/bioexp)\n', '[![arXiv](https://img.shields.io/badge/arXiv-2008.06457-<COLOR>.svg)](https://arxiv.org/abs/2008.06457)\n', '[![arXiv](https://img.shields.io/badge/arXiv-1909.01498-<COLOR>.svg)](https://arxiv.org/abs/1909.01498)\n', '[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n', '\n', 'Explaining Deep Learning Models which perform various image processing tasks in the medical images and natural images.\n', '\n', '# Features\n', '\n', '- [x] Dissection Analysis\n', '- [x] Ablation Analysis\n', '- [x] Uncertainity Analysis\n', '   - [x] Epistemic Uncertainty using Bayesian Dropout\n', '   - [x] Aleatoric Uncertainty using Test Time Augmentation\n', '- [x] Activation Maximization\n', '- [x] CAM Analysis\n', '- [x] RCT on input and concept space \n', '- [x] Concept generation clustering analysis\n', '   - [x] wts based clustering\n', '   - [x] feature based clustering\n', '- [x] Concept Identification\n', '  - [x] Dissection based\n', '  - [x] Flow based\n', '- [x] Causal Graph \n', '- [x] Inference Methods\n', '- [ ] Counterfactuals on Visual Trails\n', '- [ ] Counterfactual Generation\n', '- [ ] Ante-hoc methods (Meta-Causation)\n', '\n', '# Citations\n', 'If you use BioExp, please cite the following papers:\n', '\n', '```\n', '@article{kori2020abstracting,\n', '  title={Abstracting Deep Neural Networks into Concept Graphs for Concept Level Interpretability},\n', '  author={Kori, Avinash and Natekar, Parth and Krishnamurthi, Ganapathy and Srinivasan, Balaji},\n', '  journal={arXiv preprint arXiv:2008.06457},\n', '  year={2020}\n', '}\n', '\n', '@article{natekar2020demystifying,\n', '  title={Demystifying Brain Tumor Segmentation Networks: Interpretability and Uncertainty Analysis},\n', '  author={Natekar, Parth and Kori, Avinash and Krishnamurthi, Ganapathy},\n', '  journal={Frontiers in Computational Neuroscience},\n', '  volume={14},\n', '  pages={6},\n', '  year={2020},\n', '  publisher={Frontiers}\n', '}\n', '```\n', '\n', '# Defined Pipeline\n', '![pipeline](./imgs/pipeline.png)\n', '\n', '# Installation\n', 'Running of the explainability pipeline requires a GPU and several deep learning modules. \n', '\n', '### Requirements\n', ""- 'pandas'\n"", ""- 'numpy'\n"", ""- 'scipy==1.6.0'\n"", ""- 'matplotlib'\n"", ""- 'pillow'\n"", ""- 'simpleITK'\n"", ""- 'opencv-python'\n"", ""- 'tensorflow-gpu==1.14'\n"", ""- 'keras'\n"", ""- 'keras-vis'\n"", ""- 'lucid'\n"", '\n', 'The following command will install only the dependencies listed above.\n', '\n', '```\n', 'pip install BioExp\n', '```\n', '\n', '# Ablation\n', '\n', '## Usage\n', '```\n', 'from BioExp.spatial import Ablation\n', '\n', 'A = spatial.Ablation(model = model, \n', '\t\t\t\tweights_pth = weights_path, \n', '\t\t\t\tmetric      = dice_label_coef, \n', '\t\t\t\tlayer_name  = layer_name, \n', '\t\t\t\ttest_image  = test_image, \n', '\t\t\t\tgt \t    = gt, \n', '\t\t\t\tclasses     = infoclasses, \n', '\t\t\t\tnclasses    = 4)\n', '\n', 'df = A.ablate_filter(step = 1)\n', '```\n', '\n', '# Dissection\n', '\n', '## Usage\n', '```\n', 'from BioExp.spatial import Dissector\n', '\n', ""layer_name = 'conv2d_3'\n"", 'infoclasses = {}\n', ""for i in range(1): infoclasses['class_'+str(i)] = (i,)\n"", ""infoclasses['whole'] = (1,2,3)\n"", '\n', 'dissector = Dissector(model=model,\n', '                        layer_name = layer_name)\n', '\n', 'threshold_maps = dissector.get_threshold_maps(dataset_path = data_root_path,\n', '                                                save_path  = savepath,\n', '                                                percentile = 85)\n', 'dissector.apply_threshold(image, threshold_maps, \n', '                        nfeatures =9, \n', '                        save_path = savepath, \n', '                        ROI       = ROI)\n', '\n', 'dissector.quantify_gt_features(image, gt, \n', '                        threshold_maps, \n', '                        nclasses   = infoclass, \n', '                        nfeatures  = 9, \n', '                        save_path  = savepath,\n', '                        save_fmaps = False, \n', '                        ROI        = ROI)\n', '```\n', '## Results\n', '\n', '![dissection](./imgs/dissection.png)\n', '\n', '\n', '# GradCAM\n', '\n', '## Usage\n', '```\n', 'from BioExp.spatial import cam\n', '\n', 'dice = flow.cam(model, img, gt, \n', '\t\t\t\tnclasses = nclasses, \n', '\t\t\t\tsave_path = save_path, \n', '\t\t\t\tlayer_idx = -1, \n', '\t\t\t\tthreshol = 0.5,\n', ""\t\t\t\tmodifier = 'guided')\n"", '\n', '```\n', '## Results\n', '![gradcam](./imgs/gradcam.png)\n', '\n', '\n', '# Activation Maximization\n', '\n', '## Usage\n', '```\n', 'from BioExp.concept.feature import Feature_Visualizer\n', '\n', 'class Load_Model(Model):\n', '\n', ""  model_path = '../../saved_models/model_flair_scaled/model.pb'\n"", '  image_shape = [None, 1, 240, 240]\n', '  image_value_range = (0, 10)\n', ""  input_name = 'input_1'\n"", '\n', ""E = Feature_Visualizer(Load_Model, savepath = '../results/', regularizer_params={'L1':1e-3, 'rotate':8})\n"", ""a = E.run(layer = 'conv2d_17', class_ = 'None', channel = 95, transforms=True)\n"", '\n', '```\n', '\n', '##Activation Results\n', '![lucid](./imgs/lucid.png)\n', '\n', '\n', '# Uncertainty\n', '\n', '## Usage\n', '```\n', 'from BioExp.uncertainty import uncertainty\n', '\n', 'D = uncertainty(test_image)\n', '            \n', '# for aleatoric\n', 'mean, var = D.aleatoric(model, iterations = 50)\n', '\n', '# for epistemic\n', 'mean, var = D.epistemic(model, iterations = 50)\n', ' \n', '# for combined\n', 'mean, var = D.combined(model, iterations = 50)\n', '\n', '```\n', '## Results\n', '![un](./imgs/uncertainty.png)\n', '\n', '\n', '# Radiomics\n', '## Usage\n', '```\n', 'from BioExp.helpers import radfeatures\n', 'feat_extractor = radfeatures.ExtractRadiomicFeatures(image, mask, save_path = pth)\n', 'df = feat_extractor.all_features()\n', '```\n', '\n', '# Causal Inference Pipeline\n', '![un](./imgs/causal_pipeline.png)\n', '\n', '# Contact\n', '- Avinash Kori (koriavinash1@gmail.com)\n', '- Parth Natekar (parth@smail.iitm.ac.in)\n']"
Model Explainability,HelmholtzAI-Consultants-Munich/fg-clustering,HelmholtzAI-Consultants-Munich,https://api.github.com/repos/HelmholtzAI-Consultants-Munich/fg-clustering,23,6,4,"['https://api.github.com/users/lisa-sousa', 'https://api.github.com/users/DoTha', 'https://api.github.com/users/hpelin', 'https://api.github.com/users/georgii-helmholtz']",Python,2023-03-31T06:19:20Z,https://raw.githubusercontent.com/HelmholtzAI-Consultants-Munich/fg-clustering/main/README.md,"['<div align=""center"">\n', '\n', '<img src=""https://raw.githubusercontent.com/HelmholtzAI-Consultants-Munich/fg-clustering/main/docs/source/_figures/FGC_Logo.png"" width=""200"">\n', '\t\n', '\n', '# *Forest-Guided Clustering* - Shedding light into the Random Forest Black Box \n', '\n', '[![test](https://github.com/HelmholtzAI-Consultants-Munich/fg-clustering/actions/workflows/test.yml/badge.svg)](https://github.com/HelmholtzAI-Consultants-Munich/fg-clustering/actions/workflows/test.yml)\n', '[![PyPI](https://img.shields.io/pypi/v/fgclustering.svg)](https://pypi.org/project/fgclustering)\n', '[![stars](https://img.shields.io/github/stars/HelmholtzAI-Consultants-Munich/forest_guided_clustering?logo=GitHub&color=yellow)](https://github.com/HelmholtzAI-Consultants-Munich/forest_guided_clustering/stargazers)\n', '[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n', '[![cite](https://zenodo.org/badge/397931780.svg)](https://zenodo.org/badge/latestdoi/397931780)\n', '\t\n', '[Docs] | [Tutorials]\n', '\n', '[Docs]: https://forest-guided-clustering.readthedocs.io/en/latest/\n', '[Tutorials]: https://github.com/HelmholtzAI-Consultants-Munich/fg-clustering/tree/main/tutorials\n', '\n', '</div>\n', '\n', 'Forest-Guided Clustering (FGC) is an explainability method for Random Forest models. Standard explainability methods (e.g. feature importance) assume independence of model features and hence, are not suited in the presence of correlated features. The Forest-Guided Clustering algorithm does not assume independence of model features, because it computes the feature importance based on subgroups of instances that follow similar decision rules within the Random Forest model. Hence, this method is well suited for cases with high correlation among model features. \n', '\n', 'For a detailed comparison of FGC and Permutation Feature Importance, please have a look at the Notebook [Introduction to FGC: Comparison of Forest-Guided Clustering and Feature Importance](https://github.com/HelmholtzAI-Consultants-Munich/fg-clustering/blob/main/tutorials/introduction_to_FGC_comparing_FGC_to_FI.ipynb).\n', '\n', '## Documentation\n', '\n', 'Please see [here](https://forest-guided-clustering.readthedocs.io/) for full documentation on:\n', '\n', '- Getting Started (installation, basic usage)\n', '- Theoretical Background (introduction, general algorith, feature importance)\n', '- Tutorials (simple use cases, special cases)\n', '- API documentation\n', '\n', 'For a short introduction to Forest-Guided Clustering, click below:\n', '\n', '<div align=""center"">\n', '\n', '[![Video](http://i.vimeocdn.com/video/1501376117-3e402fde211d1a52080fb16b317efc3786a34d0be852a81cfe3a03aa89adc475-d_295x166)](https://vimeo.com/746443233/07ddf2290b)\n', '\n', '</div>\n', '\n', '## Installation\n', '\n', '### Requirements\n', '\n', 'This packages was tested for ```Python 3.7 - 3.11``` on ubuntu, macos and windows. It depends on the ```kmedoids``` python package. If you are using windows or macos, you may need to first install Rust/Cargo with:\n', '\n', '```\n', 'conda install -c conda-forge rust\n', '```\n', '\n', 'If this does not work, please try to install Cargo from source:\n', '\n', '```\n', 'git clone https://github.com/rust-lang/cargo\n', 'cd cargo\n', 'cargo build --release\n', '```\n', '\n', 'For further information on the kmedoids package, please visit [this page](https://pypi.org/project/kmedoids/).\n', '\n', 'All other required packages are automatically installed if installation is done via ```pip```.\n', '\n', '\n', '### Install Options\n', '\n', 'The installation of the package is done via pip. Note: if you are using conda, first install pip with: ```conda install pip```.\n', '\n', 'PyPI install:\n', '\n', '```\n', 'pip install fgclustering\n', '```\n', '\n', '\n', 'Installation from source:\n', '\n', '```\n', 'git clone https://github.com/HelmholtzAI-Consultants-Munich/fg-clustering.git\n', '```\n', '\n', '- Installation as python package (run inside directory):\n', '\n', '\t\tpip install .   \n', '\n', '\n', '- Development Installation as python package (run inside directory):\n', '\n', '\t\tpip install -e . [dev]\n', '\n', '\n', '## Basic Usage\n', '\n', 'To get explainability of your Random Forest model via Forest-Guided Clustering, you simply need to run the following commands:\n', '\n', '```python\n', 'from fgclustering import FgClustering\n', '   \n', '# initialize and run fgclustering object\n', ""fgc = FgClustering(model=rf, data=data, target_column='target')\n"", 'fgc.run()\n', '   \n', '# visualize results\n', 'fgc.plot_global_feature_importance()\n', 'fgc.plot_local_feature_importance()\n', 'fgc.plot_decision_paths()\n', '   \n', '# obtain optimal number of clusters and vector that contains the cluster label of each data point\n', 'optimal_number_of_clusters = fgc.k\n', 'cluster_labels = fgc.cluster_labels\n', '```\n', '\n', 'where \n', '\n', '- ```model=rf``` is a Random Forest Classifier or Regressor object,\n', '- ```data=data``` is a dataset containing the same features as required by the Random Forest model, and\n', ""- ```target_column='target'``` is the name of the target column (i.e. *target*) in the provided dataset. \n"", '\n', 'For detailed instructions, please have a look at the Notebook [Introduction to FGC: Simple Use Cases](https://github.com/HelmholtzAI-Consultants-Munich/fg-clustering/blob/main/tutorials/introduction_to_FGC_use_cases.ipynb).\n', '\n', '**Usage on big datasets**\n', '\n', 'If you are working with the dataset containing large number of samples, you can use one of the following strategies:\n', '\n', '- Use the cores you have at your disposal to parallelize the optimization of the cluster number. You can do so by setting the parameter ```n_jobs``` to a value > 1 in the ```run()``` function.\n', '- Use the faster implementation of the pam method that K-Medoids algorithm uses to find the clusters by setting the parameter  ```method_clustering``` to *fasterpam* in the ```run()``` function.\n', '- Use subsampling technique\n', '\n', 'For detailed instructions, please have a look at the Notebook [Special Case: FGC for Big Datasets](https://github.com/HelmholtzAI-Consultants-Munich/fg-clustering/blob/main/tutorials/special_case_big_data_with_FGC.ipynb).\n', '\n', '## Contributing\n', ' \n', ""Contributions are more than welcome! Everything from code to notebooks to examples and documentation are all equally valuable so please don't feel you can't contribute. To contribute please fork the project make your changes and submit a pull request. We will do our best to work through any issues with you and get your code merged into the main branch.\n"", '\n', '## How to cite\n', '\n', 'If Forest-Guided Clustering is useful for your research, consider citing the package:\n', '\n', '```\n', '@software{lisa_sousa_2022_7823042,\n', '    author       = {Lisa Barros de Andrade e Sousa,\n', '                     Helena Pelin,\n', '                     Dominik Thalmeier,\n', '                     Marie Piraud},\n', '    title        = {{Forest-Guided Clustering - Explainability for Random Forest Models}},\n', '    month        = april,\n', '    year         = 2022,\n', '    publisher    = {Zenodo},\n', '    version      = {v1.0.3},\n', '    doi          = {10.5281/zenodo.7823042},\n', '    url          = {https://doi.org/10.5281/zenodo.7823042}\n', '}\n', '```\n', '\n', '## License\n', '\n', '```fgclustering``` is released under the MIT license. See [LICENSE](https://github.com/HelmholtzAI-Consultants-Munich/fg-clustering/blob/main/LICENSE) for additional details about it.\n']"
Model Explainability,JoaoLages/diffusers-interpret,JoaoLages,https://api.github.com/repos/JoaoLages/diffusers-interpret,215,10,3,"['https://api.github.com/users/JoaoLages', 'https://api.github.com/users/TomPham97', 'https://api.github.com/users/andrewizbatista']",Python,2023-04-26T11:51:11Z,https://raw.githubusercontent.com/JoaoLages/diffusers-interpret/main/README.md,"['<div align=""center"">\r\n', '\r\n', '# Diffusers-Interpret 🤗🧨🕵️\u200d♀️\r\n', '\r\n', '![PyPI Latest Package Version](https://img.shields.io/pypi/v/diffusers-interpret?logo=pypi&style=flat&color=orange) ![GitHub License](https://img.shields.io/github/license/JoaoLages/diffusers-interpret?logo=github&style=flat&color=green) \r\n', '\r\n', '`diffusers-interpret` is a model explainability tool built on top of [🤗 Diffusers](https://github.com/huggingface/diffusers)\r\n', '</div>\r\n', '\r\n', '## Installation\r\n', '\r\n', 'Install directly from PyPI:\r\n', '\r\n', '    pip install --upgrade diffusers-interpret\r\n', '\r\n', '## Usage\r\n', '\r\n', ""Let's see how we can interpret the **[new 🎨🎨🎨 Stable Diffusion](https://github.com/huggingface/diffusers#new--stable-diffusion-is-now-fully-compatible-with-diffusers)!**\r\n"", '\r\n', '1. [Explanations for StableDiffusionPipeline](#explanations-for-stablediffusionpipeline)\r\n', '2. [Explanations for StableDiffusionImg2ImgPipeline](#explanations-for-stablediffusionimg2imgpipeline)\r\n', '3. [Explanations for StableDiffusionInpaintPipeline](#explanations-for-stablediffusioninpaintpipeline)\r\n', '\r\n', '### Explanations for StableDiffusionPipeline\r\n', '[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JoaoLages/diffusers-interpret/blob/main/notebooks/stable_diffusion_example_colab.ipynb)\r\n', '\r\n', '```python\r\n', 'import torch\r\n', 'from diffusers import StableDiffusionPipeline\r\n', 'from diffusers_interpret import StableDiffusionPipelineExplainer\r\n', '\r\n', 'pipe = StableDiffusionPipeline.from_pretrained(\r\n', '    ""CompVis/stable-diffusion-v1-4"", \r\n', '    use_auth_token=True,\r\n', ""    revision='fp16',\r\n"", '    torch_dtype=torch.float16\r\n', "").to('cuda')\r\n"", '\r\n', '# optional: reduce memory requirement with a speed trade off \r\n', 'pipe.enable_attention_slicing()\r\n', '\r\n', '# pass pipeline to the explainer class\r\n', 'explainer = StableDiffusionPipelineExplainer(pipe)\r\n', '\r\n', '# generate an image with `explainer`\r\n', 'prompt = ""A cute corgi with the Eiffel Tower in the background""\r\n', ""with torch.autocast('cuda'):\r\n"", '    output = explainer(\r\n', '        prompt, \r\n', '        num_inference_steps=15\r\n', '    )\r\n', '```\r\n', '\r\n', 'If you are having GPU memory problems, try reducing `n_last_diffusion_steps_to_consider_for_attributions`, `height`, `width` and/or `num_inference_steps`.\r\n', '```\r\n', 'output = explainer(\r\n', '    prompt, \r\n', '    num_inference_steps=15,\r\n', '    height=448,\r\n', '    width=448,\r\n', '    n_last_diffusion_steps_to_consider_for_attributions=5\r\n', ')\r\n', '```\r\n', '\r\n', 'You can completely deactivate token/pixel attributions computation by passing `n_last_diffusion_steps_to_consider_for_attributions=0`.  \r\n', '\r\n', 'Gradient checkpointing also reduces GPU usage, but makes computations a bit slower:\r\n', '```\r\n', 'explainer = StableDiffusionPipelineExplainer(pipe, gradient_checkpointing=True)\r\n', '```\r\n', '\r\n', 'To see the final generated image:\r\n', '```python\r\n', 'output.image\r\n', '```\r\n', '\r\n', '![](assets/corgi_eiffel_tower.png)\r\n', '\r\n', 'You can also check all the images that the diffusion process generated at the end of each step:\r\n', '```python\r\n', 'output.all_images_during_generation.show()\r\n', '```\r\n', '![](assets/image_slider_cropped.gif)\r\n', '\r\n', 'To analyse how a token in the input `prompt` influenced the generation, you can study the token attribution scores:\r\n', '```python\r\n', '>>> output.token_attributions # (token, attribution)\r\n', ""[('a', 1063.0526),\r\n"", "" ('cute', 415.62888),\r\n"", "" ('corgi', 6430.694),\r\n"", "" ('with', 1874.0208),\r\n"", "" ('the', 1223.2847),\r\n"", "" ('eiffel', 4756.4556),\r\n"", "" ('tower', 4490.699),\r\n"", "" ('in', 2463.1294),\r\n"", "" ('the', 655.4624),\r\n"", "" ('background', 3997.9395)]\r\n"", '```\r\n', '\r\n', 'Or their computed normalized version, in percentage:\r\n', '```python\r\n', '>>> output.token_attributions.normalized # (token, attribution_percentage)\r\n', ""[('a', 3.884),\r\n"", "" ('cute', 1.519),\r\n"", "" ('corgi', 23.495),\r\n"", "" ('with', 6.847),\r\n"", "" ('the', 4.469),\r\n"", "" ('eiffel', 17.378),\r\n"", "" ('tower', 16.407),\r\n"", "" ('in', 8.999),\r\n"", "" ('the', 2.395),\r\n"", "" ('background', 14.607)]\r\n"", '```\r\n', '\r\n', 'Or plot them!\r\n', '```python\r\n', 'output.token_attributions.plot(normalize=True)\r\n', '```\r\n', '![](assets/token_attributions_1.png)\r\n', '\r\n', '\r\n', '`diffusers-interpret` also computes these token/pixel attributions for generating a particular part of the image. \r\n', '\r\n', 'To do that, call `explainer` with a particular 2D bounding box defined in `explanation_2d_bounding_box`:\r\n', '\r\n', '```python\r\n', ""with torch.autocast('cuda'):\r\n"", '    output = explainer(\r\n', '        prompt, \r\n', '        num_inference_steps=15, \r\n', '        explanation_2d_bounding_box=((70, 180), (400, 435)), # (upper left corner, bottom right corner)\r\n', '    )\r\n', 'output.image\r\n', '```\r\n', '![](assets/corgi_eiffel_tower_box_1.png)\r\n', '\r\n', 'The generated image now has a <span style=""color:red""> **red bounding box** </span> to indicate the region of the image that is being explained.\r\n', '\r\n', 'The attributions are now computed only for the area specified in the image.\r\n', '\r\n', '```python\r\n', '>>> output.token_attributions.normalized # (token, attribution_percentage)\r\n', ""[('a', 1.891),\r\n"", "" ('cute', 1.344),\r\n"", "" ('corgi', 23.115),\r\n"", "" ('with', 11.995),\r\n"", "" ('the', 7.981),\r\n"", "" ('eiffel', 5.162),\r\n"", "" ('tower', 11.603),\r\n"", "" ('in', 11.99),\r\n"", "" ('the', 1.87),\r\n"", "" ('background', 23.05)]\r\n"", '```\r\n', '\r\n', '### Explanations for StableDiffusionImg2ImgPipeline\r\n', '[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JoaoLages/diffusers-interpret/blob/main/notebooks/stable_diffusion_img2img_example.ipynb)\r\n', '\r\n', '```python\r\n', 'import torch\r\n', 'import requests\r\n', 'from PIL import Image\r\n', 'from io import BytesIO\r\n', 'from diffusers import StableDiffusionImg2ImgPipeline\r\n', 'from diffusers_interpret import StableDiffusionImg2ImgPipelineExplainer\r\n', '\r\n', '\r\n', 'pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\r\n', '    ""CompVis/stable-diffusion-v1-4"", \r\n', '    use_auth_token=True,\r\n', "").to('cuda')\r\n"", '\r\n', 'explainer = StableDiffusionImg2ImgPipelineExplainer(pipe)\r\n', '\r\n', 'prompt = ""A fantasy landscape, trending on artstation""\r\n', '\r\n', ""# let's download an initial image\r\n"", 'url = ""https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg""\r\n', '\r\n', 'response = requests.get(url)\r\n', 'init_image = Image.open(BytesIO(response.content)).convert(""RGB"")\r\n', 'init_image = init_image.resize((448, 448))\r\n', '\r\n', ""with torch.autocast('cuda'):\r\n"", '    output = explainer(\r\n', '        prompt=prompt, init_image=init_image, strength=0.75\r\n', '    )\r\n', '```\r\n', '\r\n', '`output` will have all the properties that were presented for [StableDiffusionPipeline](#explanations-for-stablediffusionpipeline).\r\n', 'For example, to see the gif version of all the images during generation:\r\n', '```python\r\n', 'output.all_images_during_generation.gif()\r\n', '```\r\n', '![](assets/img2img_1.gif)\r\n', '\r\n', 'Additionally, it is also possible to visualize pixel attributions of the input image as a saliency map:\r\n', '```python\r\n', 'output.input_saliency_map.show()\r\n', '```\r\n', '![](assets/pixel_attributions_1.png)\r\n', '\r\n', 'or access their values directly:\r\n', '```python\r\n', '>>> output.pixel_attributions\r\n', 'array([[ 1.2714844 ,  4.15625   ,  7.8203125 , ...,  2.7753906 ,\r\n', '         2.1308594 ,  0.66552734],\r\n', '       [ 5.5078125 , 11.1953125 ,  4.8125    , ...,  5.6367188 ,\r\n', '         6.8828125 ,  3.0136719 ],\r\n', '       ...,\r\n', '       [ 0.21386719,  1.8867188 ,  2.2109375 , ...,  3.0859375 ,\r\n', '         2.7421875 ,  0.7871094 ],\r\n', '       [ 0.85791016,  0.6694336 ,  1.71875   , ...,  3.8496094 ,\r\n', '         1.4589844 ,  0.5727539 ]], dtype=float32)\r\n', '```\r\n', 'or the normalized version:\r\n', '```python\r\n', '>>> output.pixel_attributions.normalized \r\n', 'array([[7.16054201e-05, 2.34065039e-04, 4.40411852e-04, ...,\r\n', '        1.56300011e-04, 1.20002325e-04, 3.74801020e-05],\r\n', '       [3.10180156e-04, 6.30479713e-04, 2.71022669e-04, ...,\r\n', '        3.17439699e-04, 3.87615233e-04, 1.69719147e-04],\r\n', '       ...,\r\n', '       [1.20442292e-05, 1.06253210e-04, 1.24512037e-04, ...,\r\n', '        1.73788882e-04, 1.54430119e-04, 4.43271674e-05],\r\n', '       [4.83144104e-05, 3.77000870e-05, 9.67938031e-05, ...,\r\n', '        2.16796136e-04, 8.21647482e-05, 3.22554370e-05]], dtype=float32)\r\n', '```\r\n', '\r\n', '**Note:** Passing `explanation_2d_bounding_box` to the `explainer` will also change these values to explain a specific part of the **output** image. \r\n', ""<ins>The attributions are always calculated for the model's input (image and text) with respect to the output image.</ins>\r\n"", '\r\n', '### Explanations for StableDiffusionInpaintPipeline\r\n', '[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JoaoLages/diffusers-interpret/blob/main/notebooks/stable_diffusion_inpaint_example.ipynb)\r\n', '\r\n', 'Same as [StableDiffusionImg2ImgPipeline](#explanations-for-stablediffusionimg2imgpipeline), but now we also pass a `mask_image` argument to `explainer`.\r\n', '\r\n', '```python\r\n', 'import torch\r\n', 'import requests\r\n', 'from PIL import Image\r\n', 'from io import BytesIO\r\n', 'from diffusers import StableDiffusionInpaintPipeline\r\n', 'from diffusers_interpret import StableDiffusionInpaintPipelineExplainer\r\n', '\r\n', '\r\n', 'def download_image(url):\r\n', '    response = requests.get(url)\r\n', '    return Image.open(BytesIO(response.content)).convert(""RGB"")\r\n', '\r\n', '\r\n', 'pipe = StableDiffusionInpaintPipeline.from_pretrained(\r\n', '    ""CompVis/stable-diffusion-v1-4"", \r\n', '    use_auth_token=True,\r\n', "").to('cuda')\r\n"", '\r\n', 'explainer = StableDiffusionInpaintPipelineExplainer(pipe)\r\n', '\r\n', 'prompt = ""a cat sitting on a bench""\r\n', '\r\n', 'img_url = ""https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png""\r\n', 'mask_url = ""https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png""\r\n', '\r\n', 'init_image = download_image(img_url).resize((448, 448))\r\n', 'mask_image = download_image(mask_url).resize((448, 448))\r\n', '\r\n', ""with torch.autocast('cuda'):\r\n"", '    output = explainer(\r\n', '        prompt=prompt, init_image=init_image, mask_image=mask_image, strength=0.75\r\n', '    )\r\n', '```\r\n', '\r\n', '`output` will have all the properties that were presented for [StableDiffusionImg2ImgPipeline](#explanations-for-stablediffusionimg2imgpipeline) and [StableDiffusionPipeline](#explanations-for-stablediffusionpipeline).  \r\n', 'For example, to see the gif version of all the images during generation:\r\n', '```python\r\n', 'output.all_images_during_generation.gif()\r\n', '```\r\n', '![](assets/inpaint_1.gif)\r\n', '\r\n', 'The only difference in `output` now, is that we can now see the masked part of the image:\r\n', '```python\r\n', 'output.input_saliency_map.show()\r\n', '```\r\n', '![](assets/pixel_attributions_inpaint_1.png)\r\n', '\r\n', 'Check other functionalities and more implementation examples in [here](https://github.com/JoaoLages/diffusers-interpret/blob/main/notebooks/).\r\n', '\r\n', '## Future Development\r\n', '- [x] ~~Add interactive display of all the images that were generated in the diffusion process~~\r\n', '- [x] ~~Add explainer for StableDiffusionImg2ImgPipeline~~\r\n', '- [x] ~~Add explainer for StableDiffusionInpaintPipeline~~\r\n', '- [ ] Add attentions visualization \r\n', '- [ ] Add unit tests\r\n', '- [ ] Website for documentation\r\n', '- [ ] Do not require another generation every time the `explanation_2d_bounding_box` argument is changed\r\n', '- [ ] Add interactive bounding-box and token attributions visualization\r\n', '- [ ] Add more explainability methods\r\n', '\r\n', '## Contributing\r\n', ""Feel free to open an [Issue](https://github.com/JoaoLages/diffusers-interpret/issues) or create a [Pull Request](https://github.com/JoaoLages/diffusers-interpret/pulls) and let's get started 🚀\r\n"", '\r\n', '## Credits\r\n', '\r\n', 'A special thanks to:\r\n', '- [@andrewizbatista](https://github.com/andrewizbatista) for creating a great [image slider](https://github.com/JoaoLages/diffusers-interpret/pull/1) to show all the generated images during diffusion! 💪 \r\n', '- [@TomPham97](https://github.com/TomPham97) for README improvements, the [GIF visualization](https://github.com/JoaoLages/diffusers-interpret/pull/9) and the [token attributions plot](https://github.com/JoaoLages/diffusers-interpret/pull/13) 😁\r\n']"
Model Explainability,gearsuccess/DER,gearsuccess,https://api.github.com/repos/gearsuccess/DER,39,21,1,['https://api.github.com/users/gearsuccess'],Python,2022-11-25T08:36:55Z,https://raw.githubusercontent.com/gearsuccess/DER/master/README.md,"['## Requirements\n', '\n', '- Python 2.7\n', '- TensorFlow 1.8\n', '- numpy 1.14\n', '\n', '\n', '## Project Structure\n', '\n', '    .\n', '    ├── conf                    # Config files\n', '    ├── data\n', '        ├── der_data            # Experiment data   \n', '    ├── results                 # Results saving\n', '    ├── model          \n', '        ├── DER.py              # The graph of DER\n', '    ├── data_loader.py          \n', '    ├── evaluate.py             # Evaluation code\n', '    ├── main.py                 # The entrance of the project\n', '    └── solver.py               # The solver of the project     \n', '\n', '\n', '\n', '\n', '## Config\n', '\n', 'example: \n', '\n', '[path]\n', '\n', 'root_path = your-local-path/DER\n', '\n', 'input_data_type = data/der_data/category\n', '\n', 'output_path = results\n', '\n', 'log_conf_path = conf/logging.conf\n', '\n', '\n', '[parameters]\n', '\n', 'global_dimension = 8\n', '\n', 'word_dimension = 64\n', '\n', 'batch_size = 50\n', '\n', 'epoch = 50\n', '\n', 'learning_rate = 0.001\n', '\n', 'reg = 1\n', '\n', 'mode = validation\n', '\n', 'merge = FM\n', '\n', 'concat = 1\n', '\n', 'item_review_combine = add\n', '\n', 'item_review_combine_c = 0.5\n', '\n', 'lmd = 1\n', '\n', 'drop_out_rate = 0.7\n', '\n', '\n', '## Usage\n', '\n', '1. Install all the required packages\n', '\n', '2. process the raw data into the formats according to data/der_data/data_format\n', '\n', '3. Run python main.py\n', '\n', '\n', '## Author# DER\n']"
Model Explainability,AI4LIFE-GROUP/OpenXAI,AI4LIFE-GROUP,https://api.github.com/repos/AI4LIFE-GROUP/OpenXAI,149,20,8,"['https://api.github.com/users/chirag126', 'https://api.github.com/users/jiaqima', 'https://api.github.com/users/y12uc231', 'https://api.github.com/users/s1682978', 'https://api.github.com/users/eshika', 'https://api.github.com/users/hnaik', 'https://api.github.com/users/Y0mingZhang', 'https://api.github.com/users/ehsankiakojouri']",JavaScript,2023-04-24T17:43:52Z,https://raw.githubusercontent.com/AI4LIFE-GROUP/OpenXAI/main/README.md,"['# OpenXAI : Towards a Transparent Evaluation of Model Explanations\n', '\n', '----\n', '\n', '[**Website**](https://open-xai.github.io/) | [**arXiv Paper**](https://arxiv.org/abs/2206.11104)\n', '\n', '**OpenXAI** is the first general-purpose lightweight library that provides a comprehensive list of functions to systematically evaluate the quality of explanations generated by attribute-based explanation methods. OpenXAI supports the development of new datasets (both synthetic and real-world) and explanation methods, with a strong bent towards promoting systematic, reproducible, and transparent evaluation of explanation methods.\n', '\n', 'OpenXAI is an open-source initiative that comprises of a collection of curated high-stakes datasets, models, and evaluation metrics, and provides a simple and easy-to-use API that enables researchers and practitioners to benchmark explanation methods using just a few lines of code.\n', '\n', '\n', '## Updates\n', '- `0.0.0`: OpenXAI is live! Now, you can submit the result for benchmarking an post-hoc explanation method on an evaluation metric. Checkout [here](https://open-xai.github.io/quick-start)!\n', '- OpenXAI white paper is on [arXiv](https://arxiv.org/abs/2206.11104)!\n', '\n', '\n', '## Unique Features of OpenXAI\n', '- *Diverse areas of XAI research*: OpenXAI includes ready-to-use API interfaces for seven state-of-the-art feature attribution methods and 22 metrics to quantify their performance. Further, it provides a flexible synthetic data generator to synthesize datasets of varying sizes, complexity, and dimensionality that facilitate the construction of ground truth explanations and a comprehensive collection of real-world datasets.\n', '- *Data functions*: OpenXAI provides extensive data functions, including data evaluators, meaningful data splits, explanation methods, and evaluation metrics.\n', '- *Leaderboards*: OpenXAI provides the first ever public XAI leaderboards to promote transparency, and to allow users to easily compare the performance of multiple explanation methods.\n', '- *Open-source initiative*: OpenXAI is an open-source initiative and easily extensible.\n', '\n', '## Installation\n', '\n', '### Using `pip`\n', '\n', 'To install the core environment dependencies of OpenXAI, use `pip` by cloning the OpenXAI repo into your local environment:\n', '\n', '```bash\n', 'pip install -e . \n', '```\n', '\n', '## Design of OpenXAI\n', '\n', 'OpenXAI is an open-source ecosystem comprising XAI-ready datasets, implementations of state-of-the-art explanation methods, evaluation metrics, leaderboards and documentation to promote transparency and collaboration around evaluations of post hoc explanations. OpenXAI can readily be used to *benchmark* new explanation methods as well as incorporate them into our framework and leaderboards. By enabling *systematic and efficient evaluation* and benchmarking of existing and new explanation methods, OpenXAI can inform and accelerate new research in the emerging field of XAI.\n', '\n', '### OpenXAI DataLoaders\n', '\n', 'OpenXAI provides a Dataloader class that can be used to load the aforementioned collection of synthetic and real-world datasets as well as any other custom datasets, and ensures that they are XAI-ready. More specifically, this class takes as input the name of an existing OpenXAI dataset or a new dataset (name of the .csv file), and outputs a train set which can then be used to train a predictive model, a test set which can be used to generate local explanations of the trained model, as well as any ground-truth explanations (if and when available). If the dataset already comes with pre-determined train and test splits, this class loads train and test sets from those pre-determined splits. Otherwise, it divides the entire dataset randomly into train (70%) and test (30%) sets. Users can also customize the percentages of train-test splits.\n', '\n', 'For a concrete example, the code snippet below shows how to import the Dataloader class and load an existing OpenXAI dataset:\n', '\n', '```python\n', 'from openxai.dataloader import return_loaders\n', 'loader_train, loader_test = return_loaders(data_name=‘german’, download=True)\n', '# get an input instance from the test dataset\n', 'inputs, labels = iter(loader_test).next()\n', '```\n', '\n', '### OpenXAI Pre-trained models\n', '\n', 'We also pre-trained two classes of predictive models (e.g., deep neural networks of varying degrees of complexity, logistic regression models etc.) and incorporated them into the OpenXAI framework so that they can be readily used for benchmarking explanation methods. The code snippet below shows how to load OpenXAI’s pre-trained models using our LoadModel class.\n', '\n', '```python\n', 'from openxai import LoadModel\n', ""model = LoadModel(data_name= 'german', ml_model='ann', pretrained=True)\n"", '```\n', '\n', 'Adding additional pre-trained models into the OpenXAI framework is as simple as uploading a file with details about model architecture and parameters in a specific template. Users can also submit requests to incorporate custom pre-trained models into the OpenXAI framework by filling a simple form and providing details about model architecture and parameters.\n', '\n', '### OpenXAI Explainers\n', '\n', 'All the explanation methods included in OpenXAI are readily accessible through the *Explainer* class, and users just have to specify the method name in order to invoke the appropriate method and generate explanations as shown in the above code snippet. Users can easily incorporate their own custom explanation methods into the OpenXAI framework by extending the *Explainer* class and including the code for their methods in the *get_explanations* function of this class.\n', '\n', '```python\n', 'from openxai import Explainer\n', ""exp_method = Explainer(method= 'lime',model=model, dataset_tensor=inputs)\n"", 'explanations= exp_method.get_explanation(inputs, labels)\n', '```\n', '\n', 'Users can then submit a request to incorporate their custom methods into OpenXAI library by filling a form and providing the GitHub link to their code as well as a summary of their explanation method.\n', '\n', '### OpenXAI Evaluation\n', '\n', 'Benchmarking an explanation method using evaluation metrics is quite simple and the code snippet below describes how to invoke the RIS metric. Users can easily incorporate their own custom evaluation metrics into OpenXAI by filling a form and providing the GitHub link to their code as well as a summary of their metric. Note that the code should be in the form of a function which takes as input data instances, corresponding model predictions and their explanations, as well as OpenXAI’s model object and returns a numerical score. Finally, the input_dict is described [here](https://github.com/AI4LIFE-GROUP/OpenXAI/blob/main/OpenXAI%20quickstart.ipynb).\n', '\n', '```python\n', 'from openxai import Evaluator\n', 'metric_evaluator = Evaluator(input_dict, inputs, labels, model, exp_method)\n', ""score = metric_evaluator.evaluate(metric='RIS')\n"", '```\n', '\n', '### OpenXAI Metrics\n', '\n', '#### Ground-truth Faithfulness\n', 'OpenXAI includes the following metrics to calculate the agreement between ground-truth explanations (i.e., coefficients of logistic regression models) and explanations generated by state-of-the-art methods.\n', '\n', '1. `Feature Agreement (FA)` metric computes the fraction of top-K features that are common between a given post hoc explanation and the corresponding ground truth explanation.\n', '2. `Rank Agreement (RA)` metric measures the fraction of top-K features that are not only common between a given post hoc explanation and the corresponding ground truth explanation, but also have the same position in the respective rank orders.\n', '3. `Sign Agreement (SA)` metric computes the fraction of top-K features that are not only common between a given post hoc explanation and the corresponding ground truth explanation, but also share the same sign (direction of contribution) in both the explanations.\n', '4. `Signed Rank Agreement (SRA)` metric computes the fraction of top-K features that are not only common between a given post hoc explanation and the corresponding ground truth explanation, but also share the same feature attribution sign (direction of contribution) and position (rank) in both the explanations.\n', '5. `Rank Correlation (RC)` metric computes the Spearman’s rank correlation coefficient to measure the agreement between feature rankings provided by a given post hoc explanation and the corresponding ground truth explanation.\n', '6. `Pairwise Rank Agreement (PRA)` metric captures if the relative ordering of every pair of features is the same for a given post hoc explanation as well as the corresponding ground truth explanation i.e., if feature A is more important than B according to one explanation, then the same should be true for the other explanation. More specifically, this metric computes the fraction of feature pairs for which the relative ordering is the same between the two explanations.\n', '\n', '#### Predicted Faithfulness\n', 'OpenXAI includes two complementary predictive faithfulness metrics: i) `Prediction Gap on Important feature perturbation (PGI)` which measures the difference in prediction probability that results from perturbing the features deemed as influential by a given post hoc explanation, and ii) `Prediction Gap on Unimportant feature perturbation (PGU)` which measures the difference in prediction probability that results from perturbing the features deemed as unimportant by a given post hoc explanation.\n', '\n', '#### Stability\n', 'OpenXAI incorporates three stability metrics: i) `Relative Input Stability (RIS)` which measure the maximum change in explanation relative to changes in the inputs, ii) `Relative Representation Stability (RRS)` which measure the maximum change in explanation relative to changes in the internal representation learned by the model, and iii) `Relative Output Stability (ROS)` which measure the maximum change in explanation relative to changes in output prediction probabilities.\n', '\n', '#### Fairness\n', 'We report the average of all faithfulness and stability metric values across instances in the majority and minority subgroups, and then take the absolute difference between them to check if there are significant disparities.\n', '\n', '### OpenXAI Leaderboards\n', '\n', 'Every explanation method in OpenXAI is a benchmark, and we provide dataloaders, pre-trained models, together with explanation methods and performance evaluation metrics. To participate in the leaderboard for a specific benchmark, follow these steps:\n', '\n', '* Use the OpenXAI benchmark dataloader to retrieve a given dataset.\n', '\n', '* Use the OpenXAI LoadModel to load a pre-trained model.\n', '\n', '* Use the OpenXAI Explainer to load a post hoc explanation method.\n', '\n', '* Submit the performance of the explanation method for a given metric.\n', '\n', '## Cite Us\n', '\n', 'If you find OpenXAI benchmark useful, cite our paper:\n', '\n', '```\n', '@inproceedings{\n', 'agarwal2022openxai,\n', 'title={Open{XAI}: Towards a Transparent Evaluation of Model Explanations},\n', 'author={Chirag Agarwal and Satyapriya Krishna and Eshika Saxena and Martin Pawelczyk and Nari Johnson and Isha Puri and Marinka Zitnik and Himabindu Lakkaraju},\n', 'booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},\n', 'year={2022},\n', 'url={https://openreview.net/forum?id=MU2495w47rz}\n', '}\n', '```\n', '\n', '## Contact\n', '\n', 'Reach us at [openxaibench@gmail.com](mailto:openxaibench@gmail.com) or open a GitHub issue.\n', '\n', '## License\n', 'OpenXAI codebase is under MIT license. For individual dataset usage, please refer to the dataset license found on the website.\n']"
Model Explainability,iancovert/removal-explanations,iancovert,https://api.github.com/repos/iancovert/removal-explanations,54,11,1,['https://api.github.com/users/iancovert'],Python,2023-03-14T03:00:04Z,https://raw.githubusercontent.com/iancovert/removal-explanations/main/README.md,"['# Removal-based explanations\n', '\n', 'This repository implements a large number of *removal-based explanations*, a class of model explanation approaches that unifies many existing methods (e.g., SHAP, LIME, Meaningful Perturbations, L2X, permutation tests). Our [paper](https://arxiv.org/abs/2011.14878) presents a framework that allows us to implement many of these methods in a lightweight, modular codebase.\n', '\n', ""Our implementation does not take advantage of certain approximation approaches that make these methods fast in practice, so you may prefer to continue using the original implementations (e.g., [SHAP](https://github.com/slundberg/shap), [LIME](https://github.com/marcotcr/lime), [SAGE](https://github.com/iancovert/sage/)). We also haven't implemented every method, e.g., we do not support image blurring or feature selection approaches.\n"", '\n', '## Usage\n', '\n', 'To begin, you need to clone the repository and install the library into your Python environment:\n', '\n', '```bash\n', 'pip install .\n', '```\n', '\n', 'Our code is designed around the framework described in the paper. Each model explanation method is specified by three choices:\n', '\n', '1. **Feature removal:** how the model is evaluated when features are held out\n', ""2. **Model behavior:** a target quantity that's analyzed as features are removed (e.g., an individual prediction, or the model loss)\n"", ""3. **Summary technique:** how each feature's influence is summarized (e.g., using Shapley values)\n"", '\n', 'The general use pattern looks like this:\n', '\n', '```python\n', 'from rexplain import removal, behavior, summary\n', '\n', '# Get model and data\n', 'x, y = ...\n', 'model = ...\n', '\n', '# 1) Feature removal\n', 'extension = removal.MarginalExtension(x[:512], model)\n', '\n', '# 2) Model behavior\n', 'game = behavior.PredictionGame(x[0], extension)\n', '\n', '# 3) Summary technique\n', 'attr = summary.ShapleyValue(game)\n', 'plt.bar(np.arange(len(attr)), attr)\n', '```\n', '\n', 'For usage examples, see the following notebooks:\n', '\n', '- [Census](https://github.com/iancovert/removal-explanations/blob/main/notebooks/census.ipynb) shows how to explain individual predictions\n', ""- [MNIST](https://github.com/iancovert/removal-explanations/blob/main/notebooks/mnist.ipynb) shows how to explain the model's loss for individual predictions\n"", '- [Breast cancer (BRCA)](https://github.com/iancovert/removal-explanations/blob/main/notebooks/brca.ipynb) shows how to explain the dataset loss\n', '\n', '# Authors\n', '\n', '- Ian Covert (<icovert@cs.washington.edu>)\n', '- Scott Lundberg\n', '- Su-In Lee\n', '\n', '# References\n', '\n', 'Ian Covert, Scott Lundberg, Su-In Lee. ""Explaining by Removing: A Unified Framework For Model Explanation."" *arXiv preprint:2011.14878*\n']"
Model Explainability,Nelsonchris1/ML-explainability-app,Nelsonchris1,https://api.github.com/repos/Nelsonchris1/ML-explainability-app,17,7,2,"['https://api.github.com/users/Nelsonchris1', 'https://api.github.com/users/AnsahMohammad']",Python,2023-01-02T08:26:36Z,https://raw.githubusercontent.com/Nelsonchris1/ML-explainability-app/main/README.md,"['# ML-explainability-app\n', '\n', '[![Streamlit App](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://share.streamlit.io/nelsonchris1/ml-explainability-app/main/app.py)\n', '\n', '## ✅ Description\n', 'Designing black box machine learning algorithms are sometimes challenging and confusing to explain. But in reality, there are diffrenet ways to explain these models and also understand how each featue contributes to the accuracy of the model.\n', 'explainMyModel is an open source  webapp built to easily explain Supervised machine learning models merely by dragging and dropping without writing any code. \n', '\n', '## ✅ Technologies and libraries used\n', '| Name  | Brief |\n', '| ------------- | ------------- |\n', '| Streamlit  |  *An open source app framework for building beautiful data and ML apps* |\n', '| eli5  | *python library to debug ML classifier and expain thier predictions*  |\n', '| pdpbox | *python library used to visualize the impact of features on models* |\n', '| shap | *python library using game theoristic approach to explain outputs of ML models*|\n', '| Lime | |\n', '| Pandas | *python library for data manipulaion and analysis*|\n', '| Numpy | *Python library for numerical computation*|\n', '\n', '\n', '\n', '## ✅ Features to implement\n', 'Currently, explainMyModel uses library **eli5** for Permutation Importance, **pdpbox** for Partial Dependency plot and **shap** for SHAP(SHapley Additive exPlanations) values. Features to implement are;\n', '* LIME \n', '* Comparing features with pdpbox\n', '* Better UI\n', '* Add tree and deep earning explainer for shap values to improve result speed\n', '\n', '## ✅ Navigate explainMyModel\n', '### Home \n', '---> Gives a brief overview about the app and also explanation about what each explain feature means and how they work\n', '### explan\n', '---> This is the heart of the app, here the user uploads features on the app and selects any of the explain feature to get explanation \n', '### Tutorial\n', '---> This part contains some code template to help you save some of the files required to eplain the model.  \n', '\n', '## ✅ How to contribute\n', '* Fork the repo\n', '* Clone the repo\n', '* Navigate to your local repository\n', '* Pull latest changes from upstream to local repository\n', '* create new branch\n', '* Contribute\n', '* Commit changes\n', '* Push changes to your fork\n', '* Begin pull request(PR)\n']"
Model Explainability,iandragulet/xG_Model_Workflow,iandragulet,https://api.github.com/repos/iandragulet/xG_Model_Workflow,30,9,1,['https://api.github.com/users/iandragulet'],Python,2023-03-18T22:23:31Z,https://raw.githubusercontent.com/iandragulet/xG_Model_Workflow/main/README.md,"['# xG_model_workflow\n', 'Comprehensive Guide to explaining, creating and using an xG model \n', '\n', 'Part 1 covers an introduction, importing data, cleaning data and some data exploration\n', '\n', 'Part 2 is all about creating and testing a logistic regression model.\n', '\n', 'Part 3 is on applications of xG \n', '\n', 'note:  3 will be avaliable in the not so distant future\n']"
Model Explainability,ModelOriented/treeshap,ModelOriented,https://api.github.com/repos/ModelOriented/treeshap,57,15,5,"['https://api.github.com/users/konrad-komisarczyk', 'https://api.github.com/users/maksymiuks', 'https://api.github.com/users/hbaniecki', 'https://api.github.com/users/pbiecek', 'https://api.github.com/users/kapsner']",R,2023-04-25T12:42:51Z,https://raw.githubusercontent.com/ModelOriented/treeshap/master/README.md,"['\n', '<!-- README.md is generated from README.Rmd. Please edit that file -->\n', '\n', '# treeshap\n', '\n', '<!-- badges: start -->\n', '\n', '<!-- badges: end -->\n', '\n', 'In the era of complicated classifiers conquering their market, sometimes\n', 'even the authors of algorithms do not know the exact manner of building\n', 'a tree ensemble model. The difficulties in models’ structures are one of\n', 'the reasons why most users use them simply like black-boxes. But, how\n', 'can they know whether the prediction made by the model is reasonable?\n', '`treeshap` is an efficient answer for this question. Due to implementing\n', 'an optimised alghoritm for tree ensemble models, it calculates the SHAP\n', 'values in polynomial (instead of exponential) time. This metric is the\n', 'only possible way to measure the influence of every feature regardless\n', 'of the permutation of features. Moreover, `treeshap` package shares a\n', 'bunch of functions to unify the structure of a model. Currently it\n', 'supports models produced with `XGBoost`, `LightGBM`, `GBM`, `Catboost`,\n', '`ranger` and `randomForest`.\n', '\n', '## Installation\n', '\n', 'You can install the released version of treeshap using package\n', '`devtools` with:\n', '\n', '``` r\n', ""devtools::install_github('ModelOriented/treeshap')\n"", '```\n', '\n', '## Example\n', '\n', 'First of all, let’s focus on an example how to represent a `xgboost`\n', 'model as a unified model object:\n', '\n', '``` r\n', 'library(treeshap)\n', 'library(xgboost)\n', ""data <- fifa20$data[colnames(fifa20$data) != 'work_rate']\n"", 'target <- fifa20$target\n', 'param <- list(objective = ""reg:squarederror"", max_depth = 6)\n', 'xgb_model <- xgboost::xgboost(as.matrix(data), params = param, label = target, nrounds = 200, verbose = 0)\n', 'unified <- xgboost.unify(xgb_model, data)\n', 'head(unified$model)\n', '#>   Tree Node   Feature Decision.type Split Yes No Missing Prediction Cover\n', '#> 1    0    0   overall            <=  81.5   2  3       2         NA 18278\n', '#> 2    0    1   overall            <=  73.5   4  5       4         NA 17949\n', '#> 3    0    2   overall            <=  84.5   6  7       6         NA   329\n', '#> 4    0    3   overall            <=  69.5   8  9       8         NA 15628\n', '#> 5    0    4 potential            <=  79.5  10 11      10         NA  2321\n', '#> 6    0    5 potential            <=  83.5  12 13      12         NA   221\n', '```\n', '\n', 'Having the object of unified structure, it is a piece of cake to produce\n', 'shap values of for a specific observations. The `treeshap()` function\n', 'requires passing two data arguments: one representing an ensemble model\n', 'unified representation and one with the observations about which we want\n', 'to get the explanations. Obviously, the latter one should contain the\n', 'same columns as data used during building the model.\n', '\n', '``` r\n', 'treeshap1 <- treeshap(unified,  data[700:800, ], verbose = 0)\n', 'treeshap1$shaps[1:3, 1:6]\n', '#>            age height_cm weight_kg overall potential international_reputation\n', '#> 700   297154.4  5769.186 12136.316 8739757  212428.8               -50855.738\n', '#> 701 -2550066.6 16011.136  3134.526 6525123  244814.2                22784.430\n', '#> 702   300830.3 -9023.299 15374.550 8585145  479118.8                 2374.351\n', '```\n', '\n', 'We can also compute SHAP values for interactions. As an example we will\n', 'calculate them for a model built with simpler (only 5 columns) data.\n', '\n', '``` r\n', 'data2 <- fifa20$data[, 1:5]\n', 'xgb_model2 <- xgboost::xgboost(as.matrix(data2), params = param, label = target, nrounds = 200, verbose = 0)\n', 'unified2 <- xgboost.unify(xgb_model2, data2)\n', '\n', 'treeshap_interactions <- treeshap(unified2,  data2[1:300, ], interactions = TRUE, verbose = 0)\n', 'treeshap_interactions$interactions[, , 1:2]\n', '#> , , 1\n', '#> \n', '#>                   age  height_cm  weight_kg     overall  potential\n', '#> age       -1886241.70   -3984.09  -96765.97   -47245.92  1034657.6\n', '#> height_cm    -3984.09 -628797.41  -35476.11  1871689.75   685472.2\n', '#> weight_kg   -96765.97  -35476.11 -983162.25  2546930.16  1559453.5\n', '#> overall     -47245.92 1871689.75 2546930.16 55289985.16 12683135.3\n', '#> potential  1034657.61  685472.23 1559453.46 12683135.27   868268.7\n', '#> \n', '#> , , 2\n', '#> \n', '#>                  age  height_cm  weight_kg    overall  potential\n', '#> age       -2349987.9  306165.41  120483.91 -9871270.0  960198.02\n', '#> height_cm   306165.4  -78810.31  -48271.61  -991020.7  -44632.74\n', '#> weight_kg   120483.9  -48271.61  -21657.14  -615688.2 -380810.70\n', '#> overall   -9871270.0 -991020.68 -615688.21 57384425.2 9603937.05\n', '#> potential   960198.0  -44632.74 -380810.70  9603937.1 2994190.74\n', '```\n', '\n', '## Plotting results\n', '\n', 'The package currently provides 4 plotting functions that can be used:\n', '\n', '### Feature Contribution (Break-Down)\n', '\n', 'On this plot we can see how features contribute into the prediction for\n', 'a single observation. It is similar to the Break Down plot from\n', '[iBreakDown](https://github.com/ModelOriented/iBreakDown) package, which\n', 'uses different method to approximate SHAP\n', 'values.\n', '\n', '``` r\n', 'plot_contribution(treeshap1, obs = 1, min_max = c(0, 16000000))\n', '```\n', '\n', '<img src=""man/figures/README-plot_contribution_example-1.png"" width=""100%"" />\n', '\n', '### Feature Importance\n', '\n', 'This plot shows us average absolute impact of features on the prediction\n', 'of the\n', 'model.\n', '\n', '``` r\n', 'plot_feature_importance(treeshap1, max_vars = 6)\n', '```\n', '\n', '<img src=""man/figures/README-plot_importance_example-1.png"" width=""100%"" />\n', '\n', '### Feature Dependence\n', '\n', 'Using this plot we can see, how a single feature contributes into the\n', 'prediction depending on its\n', 'value.\n', '\n', '``` r\n', 'plot_feature_dependence(treeshap1, ""height_cm"")\n', '```\n', '\n', '<img src=""man/figures/README-plot_dependence_example-1.png"" width=""100%"" />\n', '\n', '### Interaction Plot\n', '\n', 'Simple plot to visualise an SHAP Interaction value of two features\n', 'depending on their values.\n', '\n', '``` r\n', 'plot_interaction(treeshap_interactions, ""height_cm"", ""overall"")\n', '```\n', '\n', '<img src=""man/figures/README-plot_interaction-1.png"" width=""100%"" />\n', '\n', '## How to use the unifying functions?\n', '\n', 'Even though the objects produced by the functions from `.unify()` family\n', '(`xgboost.unify()`, `lightgbm.unify()`, `gbm.unify()`,\n', '`catboost.unify()`, `randomForest.unify()`, `ranger.unify()`) are\n', 'identical when it comes to the structure, due to different possibilities\n', 'of saving and representing the trees among the packages, the usage of\n', 'functions is slightly different. As an argument, they all take an object\n', 'of appropriate model and dataset used to train the model. One of them,\n', '`catboost.unify()` requires also a transformed dataset used for training\n', 'the model - an object of class `catboost.Pool`.\n', '\n', '#### 1\\. GBM\n', '\n', 'An argument of `gbm.unify()` should be of `gbm` class - a gradient\n', 'boosting model.\n', '\n', '``` r\n', 'library(gbm)\n', 'library(treeshap)\n', ""x <- fifa20$data[colnames(fifa20$data) != 'work_rate']\n"", ""x['value_eur'] <- fifa20$target\n"", 'gbm_model <- gbm::gbm(\n', '  formula = value_eur ~ .,\n', '  data = x,\n', '  distribution = ""laplace"",\n', '  n.trees = 200,\n', '  cv.folds = 2,\n', '  interaction.depth = 2\n', ')\n', 'unified_gbm <- gbm.unify(gbm_model, x)\n', '```\n', '\n', '#### 2\\. Catboost\n', '\n', 'For representing correct names of features that are regarding during\n', 'splitting observations into sets, `catboost.unify()` requires passing\n', 'two arguments:\n', '\n', '``` r\n', 'library(treeshap)\n', 'library(catboost)\n', ""data <- fifa20$data[colnames(fifa20$data) != 'work_rate']\n"", 'label <- fifa20$target\n', 'dt.pool <- catboost::catboost.load_pool(data = as.data.frame(lapply(data, as.numeric)), label = label)\n', 'cat_model <- catboost::catboost.train(\n', '            dt.pool,\n', ""            params = list(loss_function = 'RMSE', iterations = 100,\n"", ""                          logging_level = 'Silent', allow_writing_files = FALSE))\n"", 'unified_catboost <- catboost.unify(cat_model, dt.pool, data)\n', '```\n', '\n', '## Setting reference dataset\n', '\n', 'Dataset used as a reference for calculating SHAP values is stored in\n', 'unified model representation object. It can be set any ime using\n', '`set_reference_dataset`\n', 'function.\n', '\n', '``` r\n', 'unified_catboost2 <- set_reference_dataset(unified_catboost, data[c(1000:2000), ])\n', '```\n', '\n', '## Other functionalities\n', '\n', 'Package also implements `predict` function for calculating model’s\n', 'predictions using unified representation.\n', '\n', '## How fast does it work?\n', '\n', 'Complexity of TreeSHAP is `O(TLD^2)`, where `T` is number of trees, `L`\n', 'is number of leaves in a tree and `D` is depth of a tree.\n', '\n', 'Our implementation works in speed comparable to original Lundberg’s\n', 'Python package `shap` implementation using C and Python.\n', '\n', 'In the following example our TreeSHAP implementation explains 300\n', 'observations on a model consisting of 200 trees of max depth = 6 in 1\n', 'second (on my almost 10 years old laptop with Intel i5-2520M).\n', '\n', '``` r\n', 'microbenchmark::microbenchmark(\n', '  treeshap = treeshap(unified,  data[1:300, ]), # using model and dataset from the example\n', '  times = 5\n', ')\n', '#> Unit: seconds\n', '#>      expr      min       lq     mean   median       uq      max neval\n', '#>  treeshap 1.027707 1.032991 1.032529 1.033427 1.034062 1.034459     5\n', '```\n', '\n', 'Complexity of SHAP interaction values computation is `O(MTLD^2)`, where\n', '`M` is number of variables in explained dataset, `T` is number of trees,\n', '`L` is number of leaves in a tree and `D` is depth of a tree.\n', '\n', 'SHAP Interaction values for 5 variables, model consisting of 200 trees\n', 'of max depth = 6 and 300 observations can be computed in less than 7\n', 'seconds.\n', '\n', '``` r\n', 'microbenchmark::microbenchmark(\n', '  treeshap = treeshap(unified2, data2[1:300, ], interactions = TRUE), # using model and dataset from the example\n', '  times = 5\n', ')\n', '#> Unit: seconds\n', '#>      expr      min      lq     mean  median       uq      max neval\n', '#>  treeshap 6.700848 6.70164 6.712134 6.70711 6.719313 6.731761     5\n', '```\n', '\n', '## References\n', '\n', '  - Scott M. Lundberg, Gabriel G. Erion, Su-In Lee, “Consistent\n', '    Individualized Feature Attribution for Tree Ensembles”, University\n', '    of Washington\n']"
Model Explainability,d909b/cxplain,d909b,https://api.github.com/repos/d909b/cxplain,109,30,1,['https://api.github.com/users/d909b'],Python,2023-04-07T11:41:35Z,https://raw.githubusercontent.com/d909b/cxplain/master/README.md,"['![CXPlain](http://schwabpatrick.com/img/cxplain_logo.png)\n', '\n', '![Code Coverage](https://img.shields.io/badge/Python-2.7,%203.7-blue)![Code Coverage](https://img.shields.io/badge/Coverage-88%25-green)\n', '\n', 'Causal Explanations (CXPlain) is a method for explaining the decisions of any machine-learning model. CXPlain uses explanation models trained with a causal objective to learn to explain machine-learning models, and to quantify the uncertainty of its explanations. This repository contains a reference implementation for neural explanation models, and several practical examples for different data modalities. Please see the manuscript at https://arxiv.org/abs/1910.12336 (NeurIPS 2019) for a description and experimental evaluation of CXPlain.\n', '\n', '## Install\n', '\n', 'To install the latest release:\n', '\n', '```\n', '$ pip install cxplain\n', '```\n', '\n', '## Use\n', '\n', 'A CXPlain model consists of four main components:\n', '- The model to be explained which can be any type of machine-learning model, including black-box models, such as neural networks and ensemble models.\n', '- The model builder that defines the structure of the explanation model to be used to explain the explained model.\n', '- The masking operation that defines how CXPlain will internally simulate the removal of input features from the set of available features.\n', '- The loss function that defines how the change in prediction accuracy incurred by removing an input feature will be measured by CXPlain.\n', '\n', 'After configuring these four components, you can fit a CXPlain instance to the same training data that was used to train your original model. The CXPlain instance can then explain any prediction of your explained model - even when no labels are available for that sample.\n', '\n', '```python\n', 'from tensorflow.python.keras.losses import categorical_crossentropy\n', 'from cxplain import MLPModelBuilder, ZeroMasking, CXPlain\n', '\n', 'x_train, y_train, x_test = ....  # Your dataset\n', 'explained_model = ...    # The model you wish to explain.\n', '\n', '# Define the model you want to use to explain your __explained_model__.\n', '# Here, we use a neural explanation model with a\n', '# multilayer perceptron (MLP) architecture.\n', 'model_builder = MLPModelBuilder(num_layers=2, num_units=64, batch_size=256, learning_rate=0.001)\n', '\n', '# Define your masking operation - the method of simulating the\n', '# removal of input features used internally by CXPlain - ZeroMasking is typically a sensible default choice for tabular and image data.\n', 'masking_operation = ZeroMasking()\n', '\n', ""# Define the loss with which each input features' associated reduction in prediction error is calculated.\n"", 'loss = categorical_crossentropy\n', '\n', '# Build and fit a CXPlain instance.\n', 'explainer = CXPlain(explained_model, model_builder, masking_operation, loss)\n', 'explainer.fit(x_train, y_train)\n', '\n', '# Use the __explainer__ to obtain explanations for the predictions of your __explained_model__.\n', 'attributions = explainer.explain(x_test)\n', '```\n', '\n', '## Examples\n', '\n', 'More practical examples for various input data modalities, including images, textual data and tabular data, and both regression and classification tasks are provided in form of Jupyter notebooks in the [examples/](examples) directory:\n', '- [Regression task on tabular data (Boston Housing)](examples/boston_housing.ipynb)\n', '- [Classification task on image data (CIFAR10)](examples/cifar10.ipynb)\n', '- [Classification task on image data (MNIST)](examples/mnist.ipynb)\n', '- [Classification task on textual data (IMDB)](examples/nlp.ipynb)\n', '- [Saving and loading CXPlain instances](examples/save_and_load.ipynb)\n', '\n', '![MNIST](http://schwabpatrick.com/img/mnist_samples.png)\n', '![ImageNet](http://schwabpatrick.com/img/imagenet_samples.png)\n', '<img src=""http://schwabpatrick.com/img/twitter_samples.png"" width=""310"">\n', '## Cite\n', '\n', 'Please consider citing, if you reference or use our methodology, code or results in your work:\n', '\n', '    @inproceedings{schwab2019cxplain,\n', '      title={{CXPlain: Causal Explanations for Model Interpretation under Uncertainty}},\n', '      author={Schwab, Patrick and Karlen, Walter},\n', '      booktitle={{Advances in Neural Information Processing Systems (NeurIPS)}},\n', '      year={2019}\n', '    }\n', '\n', '## License\n', '\n', '[MIT License](LICENSE.txt)\n', '\n', '## Acknowledgements\n', '\n', 'This work was partially funded by the Swiss National Science Foundation (SNSF) project No. 167302 within the National Research Program (NRP) 75 ""Big Data"". We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPUs used for this research. Patrick Schwab is an affiliated PhD fellow at the Max Planck ETH Center for Learning Systems.\n']"