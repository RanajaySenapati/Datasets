keyword,git_name,owner_name,git_url,star_count,forks_count,contributor_count,contributors_list,code_language,last_updated_date,readme_url,readme_text
Model Fairness,Trusted-AI/AIF360,Trusted-AI,https://api.github.com/repos/Trusted-AI/AIF360,2016,676,30,"['https://api.github.com/users/hoffmansc', 'https://api.github.com/users/nrkarthikeyan', 'https://api.github.com/users/michaelhind', 'https://api.github.com/users/animeshsingh', 'https://api.github.com/users/pronics2004', 'https://api.github.com/users/SSaishruthi', 'https://api.github.com/users/milevavantuyl', 'https://api.github.com/users/josue-rodriguez', 'https://api.github.com/users/sohiniu', 'https://api.github.com/users/gdequeiroz', 'https://api.github.com/users/krvarshney', 'https://api.github.com/users/autoih', 'https://api.github.com/users/barvek', 'https://api.github.com/users/zywind', 'https://api.github.com/users/monindersingh', 'https://api.github.com/users/Adebayo-Oshingbesan', 'https://api.github.com/users/ckadner', 'https://api.github.com/users/ivesulca', 'https://api.github.com/users/jimbudarz', 'https://api.github.com/users/romeokienzler', 'https://api.github.com/users/sreeja-g', 'https://api.github.com/users/dependabot%5Bbot%5D', 'https://api.github.com/users/mfeffer', 'https://api.github.com/users/adrinjalali', 'https://api.github.com/users/imgbot%5Bbot%5D', 'https://api.github.com/users/aitorres', 'https://api.github.com/users/bhavyaghai', 'https://api.github.com/users/hakimamarullah', 'https://api.github.com/users/chkoar', 'https://api.github.com/users/DanielRyszkaIBM']",Python,2023-04-25T14:07:50Z,https://raw.githubusercontent.com/Trusted-AI/AIF360/master/README.md,"['# AI Fairness 360 (AIF360)\n', '\n', '[![Continuous Integration](https://github.com/Trusted-AI/AIF360/actions/workflows/ci.yml/badge.svg)](https://github.com/Trusted-AI/AIF360/actions/workflows/ci.yml)\n', '[![Documentation](https://readthedocs.org/projects/aif360/badge/?version=latest)](http://aif360.readthedocs.io/en/latest/?badge=latest)\n', '[![PyPI version](https://badge.fury.io/py/aif360.svg)](https://badge.fury.io/py/aif360)\n', '[![CRAN\\_Status\\_Badge](http://www.r-pkg.org/badges/version/aif360)](https://cran.r-project.org/package=aif360)\n', '\n', 'The AI Fairness 360 toolkit is an extensible open-source library containing techniques developed by the\n', 'research community to help detect and mitigate bias in machine learning models throughout the AI application lifecycle. AI Fairness 360 package is available in both Python and R.\n', '\n', 'The AI Fairness 360 package includes\n', '1) a comprehensive set of metrics for datasets and models to test for biases,\n', '2) explanations for these metrics, and\n', '3) algorithms to mitigate bias in datasets and models.\n', 'It is designed to translate algorithmic research from the lab into the actual practice of domains as wide-ranging\n', 'as finance, human capital management, healthcare, and education. We invite you to use it and improve it.\n', '\n', 'The [AI Fairness 360 interactive experience](http://aif360.mybluemix.net/data)\n', 'provides a gentle introduction to the concepts and capabilities. The [tutorials\n', 'and other notebooks](./examples) offer a deeper, data scientist-oriented\n', 'introduction. The complete API is also available.\n', '\n', 'Being a comprehensive set of capabilities, it may be confusing to figure out\n', 'which metrics and algorithms are most appropriate for a given use case. To\n', 'help, we have created some [guidance\n', 'material](http://aif360.mybluemix.net/resources#guidance) that can be\n', 'consulted.\n', '\n', 'We have developed the package with extensibility in mind. This library is still\n', 'in development. We encourage the contribution of your metrics, explainers, and\n', 'debiasing algorithms.\n', '\n', 'Get in touch with us on [Slack](https://aif360.slack.com) (invitation\n', '[here](https://join.slack.com/t/aif360/shared_invite/zt-5hfvuafo-X0~g6tgJQ~7tIAT~S294TQ))!\n', '\n', '\n', '## Supported bias mitigation algorithms\n', '\n', '* Optimized Preprocessing ([Calmon et al., 2017](http://papers.nips.cc/paper/6988-optimized-pre-processing-for-discrimination-prevention))\n', '* Disparate Impact Remover ([Feldman et al., 2015](https://doi.org/10.1145/2783258.2783311))\n', '* Equalized Odds Postprocessing ([Hardt et al., 2016](https://papers.nips.cc/paper/6374-equality-of-opportunity-in-supervised-learning))\n', '* Reweighing ([Kamiran and Calders, 2012](http://doi.org/10.1007/s10115-011-0463-8))\n', '* Reject Option Classification ([Kamiran et al., 2012](https://doi.org/10.1109/ICDM.2012.45))\n', '* Prejudice Remover Regularizer ([Kamishima et al., 2012](https://rd.springer.com/chapter/10.1007/978-3-642-33486-3_3))\n', '* Calibrated Equalized Odds Postprocessing ([Pleiss et al., 2017](https://papers.nips.cc/paper/7151-on-fairness-and-calibration))\n', '* Learning Fair Representations ([Zemel et al., 2013](http://proceedings.mlr.press/v28/zemel13.html))\n', '* Adversarial Debiasing ([Zhang et al., 2018](https://arxiv.org/abs/1801.07593))\n', '* Meta-Algorithm for Fair Classification ([Celis et al., 2018](https://arxiv.org/abs/1806.06055))\n', '* Rich Subgroup Fairness ([Kearns, Neel, Roth, Wu, 2018](https://arxiv.org/abs/1711.05144))\n', '* Exponentiated Gradient Reduction ([Agarwal et al., 2018](https://arxiv.org/abs/1803.02453))\n', '* Grid Search Reduction ([Agarwal et al., 2018](https://arxiv.org/abs/1803.02453), [Agarwal et al., 2019](https://arxiv.org/abs/1905.12843))\n', '* Fair Data Adaptation ([Plečko and Meinshausen, 2020](https://www.jmlr.org/papers/v21/19-966.html), [Plečko et al., 2021](https://arxiv.org/abs/2110.10200))\n', '* Sensitive Set Invariance/Sensitive Subspace Robustness ([Yurochkin and Sun, 2020](https://arxiv.org/abs/2006.14168), [Yurochkin et al., 2019](https://arxiv.org/abs/1907.00020))\n', '\n', '## Supported fairness metrics\n', '\n', '* Comprehensive set of group fairness metrics derived from selection rates and error rates including rich subgroup fairness\n', '* Comprehensive set of sample distortion metrics\n', '* Generalized Entropy Index ([Speicher et al., 2018](https://doi.org/10.1145/3219819.3220046))\n', '* Differential Fairness and Bias Amplification ([Foulds et al., 2018](https://arxiv.org/pdf/1807.08362))\n', '* Bias Scan with Multi-Dimensional Subset Scan ([Zhang, Neill, 2017](https://arxiv.org/abs/1611.08292))\n', '\n', '## Setup\n', '\n', '### R\n', '\n', '``` r\n', 'install.packages(""aif360"")\n', '```\n', '\n', 'For more details regarding the R setup, please refer to instructions [here](aif360/aif360-r/README.md).\n', '\n', '### Python\n', '\n', 'Supported Python Configurations:\n', '\n', '| OS      | Python version |\n', '| ------- | -------------- |\n', '| macOS   | 3.7 – 3.10     |\n', '| Ubuntu  | 3.7 – 3.10     |\n', '| Windows | 3.7 – 3.10     |\n', '\n', '### (Optional) Create a virtual environment\n', '\n', 'AIF360 requires specific versions of many Python packages which may conflict\n', 'with other projects on your system. A virtual environment manager is strongly\n', 'recommended to ensure dependencies may be installed safely. If you have trouble\n', 'installing AIF360, try this first.\n', '\n', '#### Conda\n', '\n', 'Conda is recommended for all configurations though Virtualenv is generally\n', 'interchangeable for our purposes. [Miniconda](https://conda.io/miniconda.html)\n', 'is sufficient (see [the difference between Anaconda and\n', 'Miniconda](https://conda.io/docs/user-guide/install/download.html#anaconda-or-miniconda)\n', 'if you are curious) if you do not already have conda installed.\n', '\n', 'Then, to create a new Python 3.7 environment, run:\n', '\n', '```bash\n', 'conda create --name aif360 python=3.7\n', 'conda activate aif360\n', '```\n', '\n', 'The shell should now look like `(aif360) $`. To deactivate the environment, run:\n', '\n', '```bash\n', '(aif360)$ conda deactivate\n', '```\n', '\n', 'The prompt will return to `$ `.\n', '\n', 'Note: Older versions of conda may use `source activate aif360` and `source\n', 'deactivate` (`activate aif360` and `deactivate` on Windows).\n', '\n', '### Install with `pip`\n', '\n', 'To install the latest stable version from PyPI, run:\n', '\n', '```bash\n', 'pip install aif360\n', '```\n', '\n', 'Note: Some algorithms require additional dependencies (although the metrics will\n', 'all work out-of-the-box). To install with certain algorithm dependencies\n', 'included, run, e.g.:\n', '\n', '```bash\n', ""pip install 'aif360[LFR,OptimPreproc]'\n"", '```\n', '\n', 'or, for complete functionality, run:\n', '\n', '```bash\n', ""pip install 'aif360[all]'\n"", '```\n', '\n', 'The options for available extras are: `OptimPreproc, LFR, AdversarialDebiasing,\n', 'DisparateImpactRemover, LIME, ART, Reductions, notebooks, tests, docs, all`\n', '\n', 'If you encounter any errors, try the [Troubleshooting](#troubleshooting) steps.\n', '\n', '### Manual installation\n', '\n', 'Clone the latest version of this repository:\n', '\n', '```bash\n', 'git clone https://github.com/Trusted-AI/AIF360\n', '```\n', '\n', ""If you'd like to run the examples, download the datasets now and place them in\n"", 'their respective folders as described in\n', '[aif360/data/README.md](aif360/data/README.md).\n', '\n', 'Then, navigate to the root directory of the project and run:\n', '\n', '```bash\n', ""pip install --editable '.[all]'\n"", '```\n', '\n', '#### Run the Examples\n', '\n', 'To run the example notebooks, complete the manual installation steps above.\n', 'Then, if you did not use the `[all]` option, install the additional requirements\n', 'as follows:\n', '\n', '```bash\n', ""pip install -e '.[notebooks]'\n"", '```\n', '\n', 'Finally, if you did not already, download the datasets as described in\n', '[aif360/data/README.md](aif360/data/README.md).\n', '\n', '### Troubleshooting\n', '\n', 'If you encounter any errors during the installation process, look for your\n', 'issue here and try the solutions.\n', '\n', '#### TensorFlow\n', '\n', 'See the [Install TensorFlow with pip](https://www.tensorflow.org/install/pip)\n', 'page for detailed instructions.\n', '\n', ""Note: we require `'tensorflow >= 1.13.1'`.\n"", '\n', 'Once tensorflow is installed, try re-running:\n', '\n', '```bash\n', ""pip install 'aif360[AdversarialDebiasing]'\n"", '```\n', '\n', 'TensorFlow is only required for use with the\n', '`aif360.algorithms.inprocessing.AdversarialDebiasing` class.\n', '\n', '#### CVXPY\n', '\n', 'On MacOS, you may first have to install the Xcode Command Line Tools if you\n', 'never have previously:\n', '\n', '```sh\n', 'xcode-select --install\n', '```\n', '\n', 'On Windows, you may need to download the [Microsoft C++ Build Tools for Visual\n', 'Studio 2019](https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=BuildTools&rel=16).\n', 'See the [CVXPY Install](https://www.cvxpy.org/install/index.html#mac-os-x-windows-and-linux)\n', 'page for up-to-date instructions.\n', '\n', 'Then, try reinstalling via:\n', '\n', '```bash\n', ""pip install 'aif360[OptimPreproc]'\n"", '```\n', '\n', 'CVXPY is only required for use with the\n', '`aif360.algorithms.preprocessing.OptimPreproc` class.\n', '\n', '## Using AIF360\n', '\n', 'The `examples` directory contains a diverse collection of jupyter notebooks\n', 'that use AI Fairness 360 in various ways. Both tutorials and demos illustrate\n', 'working code using AIF360. Tutorials provide additional discussion that walks\n', 'the user through the various steps of the notebook. See the details about\n', '[tutorials and demos here](examples/README.md)\n', '\n', '## Citing AIF360\n', '\n', 'A technical description of AI Fairness 360 is available in this\n', '[paper](https://arxiv.org/abs/1810.01943). Below is the bibtex entry for this\n', 'paper.\n', '\n', '```\n', '@misc{aif360-oct-2018,\n', '    title = ""{AI Fairness} 360:  An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias"",\n', '    author = {Rachel K. E. Bellamy and Kuntal Dey and Michael Hind and\n', '\tSamuel C. Hoffman and Stephanie Houde and Kalapriya Kannan and\n', '\tPranay Lohia and Jacquelyn Martino and Sameep Mehta and\n', '\tAleksandra Mojsilovic and Seema Nagar and Karthikeyan Natesan Ramamurthy and\n', '\tJohn Richards and Diptikalyan Saha and Prasanna Sattigeri and\n', '\tMoninder Singh and Kush R. Varshney and Yunfeng Zhang},\n', '    month = oct,\n', '    year = {2018},\n', '    url = {https://arxiv.org/abs/1810.01943}\n', '}\n', '```\n', '\n', '## AIF360 Videos\n', '\n', '* Introductory [video](https://www.youtube.com/watch?v=X1NsrcaRQTE) to AI\n', '  Fairness 360 by Kush Varshney, September 20, 2018 (32 mins)\n', '\n', '## Contributing\n', 'The development fork for Rich Subgroup Fairness (`inprocessing/gerryfair_classifier.py`) is [here](https://github.com/sethneel/aif360). Contributions are welcome and a list of potential contributions from the authors can be found [here](https://trello.com/b/0OwPcbVr/gerryfair-development).\n']"
Model Fairness,facebookresearch/SlowFast,facebookresearch,https://api.github.com/repos/facebookresearch/SlowFast,5612,1132,26,"['https://api.github.com/users/haooooooqi', 'https://api.github.com/users/lyttonhao', 'https://api.github.com/users/feichtenhofer', 'https://api.github.com/users/bxiong1202', 'https://api.github.com/users/chayryali', 'https://api.github.com/users/ShoufaChen', 'https://api.github.com/users/AlexanderMelde', 'https://api.github.com/users/archen2019', 'https://api.github.com/users/r-barnes', 'https://api.github.com/users/StanislavGlebik', 'https://api.github.com/users/anhminh3105', 'https://api.github.com/users/chandra-siri', 'https://api.github.com/users/chaoyuaw', 'https://api.github.com/users/fmassa', 'https://api.github.com/users/karttikeya', 'https://api.github.com/users/leszfb', 'https://api.github.com/users/menglioculus', 'https://api.github.com/users/min-xu-ai', 'https://api.github.com/users/patricklabatut', 'https://api.github.com/users/renganxu', 'https://api.github.com/users/akindofyoga', 'https://api.github.com/users/Shumpei-Kikuta', 'https://api.github.com/users/tullie', 'https://api.github.com/users/ythu2', 'https://api.github.com/users/facebook-github-bot', 'https://api.github.com/users/kalyanvasudev']",Python,2023-04-26T09:23:28Z,https://raw.githubusercontent.com/facebookresearch/SlowFast/main/README.md,"['# PySlowFast\n', '\n', 'PySlowFast is an open source video understanding codebase from FAIR that provides state-of-the-art video classification models with efficient training. This repository includes implementations of the following methods:\n', '\n', '- [SlowFast Networks for Video Recognition](https://arxiv.org/abs/1812.03982)\n', '- [Non-local Neural Networks](https://arxiv.org/abs/1711.07971)\n', '- [A Multigrid Method for Efficiently Training Video Models](https://arxiv.org/abs/1912.00998)\n', '- [X3D: Progressive Network Expansion for Efficient Video Recognition](https://arxiv.org/abs/2004.04730)\n', '- [Multiscale Vision Transformers](https://arxiv.org/abs/2104.11227)\n', '- [A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning](https://arxiv.org/abs/2104.14558)\n', '- [MViTv2: Improved Multiscale Vision Transformers for Classification and Detection](https://arxiv.org/abs/2112.01526)\n', '- [Masked Feature Prediction for Self-Supervised Visual Pre-Training](https://arxiv.org/abs/2112.09133)\n', '- [Masked Autoencoders As Spatiotemporal Learners](https://arxiv.org/abs/2205.09113)\n', '- [Reversible Vision Transformers](https://openaccess.thecvf.com/content/CVPR2022/papers/Mangalam_Reversible_Vision_Transformers_CVPR_2022_paper.pdf)\n', '\n', '<div align=""center"">\n', '  <img src=""demo/ava_demo.gif"" width=""600px""/>\n', '</div>\n', '\n', '## Introduction\n', '\n', 'The goal of PySlowFast is to provide a high-performance, light-weight pytorch codebase provides state-of-the-art video backbones for video understanding research on different tasks (classification, detection, and etc). It is designed in order to support rapid implementation and evaluation of novel video research ideas. PySlowFast includes implementations of the following backbone network architectures:\n', '\n', '- SlowFast\n', '- Slow\n', '- C2D\n', '- I3D\n', '- Non-local Network\n', '- X3D\n', '- MViTv1 and MViTv2\n', '- Rev-ViT and Rev-MViT\n', '\n', '## Updates\n', ' - We now [Reversible Vision Transformers](https://openaccess.thecvf.com/content/CVPR2022/papers/Mangalam_Reversible_Vision_Transformers_CVPR_2022_paper.pdf). Both Reversible ViT and MViT models released. See [`projects/rev`](./projects/rev/README.md).\n', ' - We now support [MAE for Video](https://arxiv.org/abs/2104.11227.pdf). See [`projects/mae`](./projects/mae/README.md) for more information.\n', ' - We now support [MaskFeat](https://arxiv.org/abs/2112.09133). See [`projects/maskfeat`](./projects/maskfeat/README.md) for more information.\n', ' - We now support [MViTv2](https://arxiv.org/abs/2104.11227.pdf) in PySlowFast. See [`projects/mvitv2`](./projects/mvitv2/README.md) for more information.\n', ' - We now support [A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning](https://arxiv.org/abs/2104.14558). See [`projects/contrastive_ssl`](./projects/contrastive_ssl/README.md) for more information.\n', ' - We now support [Multiscale Vision Transformers](https://arxiv.org/abs/2104.11227.pdf) on Kinetics and ImageNet. See [`projects/mvit`](./projects/mvit/README.md) for more information.\n', ' - We now support [PyTorchVideo](https://github.com/facebookresearch/pytorchvideo) models and datasets. See [`projects/pytorchvideo`](./projects/pytorchvideo/README.md) for more information.\n', ' - We now support [X3D Models](https://arxiv.org/abs/2004.04730). See [`projects/x3d`](./projects/x3d/README.md) for more information.\n', ' - We now support [Multigrid Training](https://arxiv.org/abs/1912.00998) for efficiently training video models. See [`projects/multigrid`](./projects/multigrid/README.md) for more information.\n', ' - PySlowFast is released in conjunction with our [ICCV 2019 Tutorial](https://alexander-kirillov.github.io/tutorials/visual-recognition-iccv19/).\n', '\n', '## License\n', '\n', 'PySlowFast is released under the [Apache 2.0 license](LICENSE).\n', '\n', '## Model Zoo and Baselines\n', '\n', 'We provide a large set of baseline results and trained models available for download in the PySlowFast [Model Zoo](MODEL_ZOO.md).\n', '\n', '## Installation\n', '\n', 'Please find installation instructions for PyTorch and PySlowFast in [INSTALL.md](INSTALL.md). You may follow the instructions in [DATASET.md](slowfast/datasets/DATASET.md) to prepare the datasets.\n', '\n', '## Quick Start\n', '\n', 'Follow the example in [GETTING_STARTED.md](GETTING_STARTED.md) to start playing video models with PySlowFast.\n', '\n', '## Visualization Tools\n', '\n', 'We offer a range of visualization tools for the train/eval/test processes, model analysis, and for running inference with trained model.\n', 'More information at [Visualization Tools](VISUALIZATION_TOOLS.md).\n', '\n', '## Contributors\n', 'PySlowFast is written and maintained by [Haoqi Fan](https://haoqifan.github.io/), [Yanghao Li](https://lyttonhao.github.io/), [Bo Xiong](https://www.cs.utexas.edu/~bxiong/), [Wan-Yen Lo](https://www.linkedin.com/in/wanyenlo/), [Christoph Feichtenhofer](https://feichtenhofer.github.io/).\n', '\n', '## Citing PySlowFast\n', 'If you find PySlowFast useful in your research, please use the following BibTeX entry for citation.\n', '```BibTeX\n', '@misc{fan2020pyslowfast,\n', '  author =       {Haoqi Fan and Yanghao Li and Bo Xiong and Wan-Yen Lo and\n', '                  Christoph Feichtenhofer},\n', '  title =        {PySlowFast},\n', '  howpublished = {\\url{https://github.com/facebookresearch/slowfast}},\n', '  year =         {2020}\n', '}\n', '```\n']"
Model Fairness,aws/amazon-sagemaker-clarify,aws,https://api.github.com/repos/aws/amazon-sagemaker-clarify,56,33,13,"['https://api.github.com/users/larroy', 'https://api.github.com/users/Satish615', 'https://api.github.com/users/xgchena', 'https://api.github.com/users/keerthanvasist', 'https://api.github.com/users/milah', 'https://api.github.com/users/pinaraws', 'https://api.github.com/users/prkrishnan1', 'https://api.github.com/users/xinyu7030', 'https://api.github.com/users/eytsai', 'https://api.github.com/users/amazon-auto', 'https://api.github.com/users/jmikko', 'https://api.github.com/users/xiaoyi-cheng', 'https://api.github.com/users/dosatos']",Python,2023-04-24T16:47:23Z,https://raw.githubusercontent.com/aws/amazon-sagemaker-clarify/master/README.md,"['![Python package](https://github.com/aws/amazon-sagemaker-clarify/workflows/Python%20package/badge.svg)\n', '![Pypi](https://img.shields.io/pypi/v/smclarify.svg?maxAge=60)\n', '![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg?style=flat)\n', '\n', '# smclarify\n', '\n', 'Amazon Sagemaker Clarify\n', '\n', 'Bias detection and mitigation for datasets and models.\n', '\n', '\n', '# Installation\n', '\n', 'To install the package from PIP you can simply do:\n', '\n', '```\n', 'pip install smclarify\n', '```\n', '\n', 'You can see examples on running the Bias metrics on the notebooks in the [examples folder](https://github.com/aws/amazon-sagemaker-clarify/tree/master/examples).\n', '\n', '\n', '# Terminology\n', '\n', '### Facet\n', 'A facet is column or feature that will be used to measure bias against. A facet can have value(s) that designates that sample as ""***sensitive***"".\n', '\n', '### Label\n', 'The label is a column or feature which is the target for training a machine learning model. The label can have value(s) that designates that sample as having a ""***positive***"" outcome.\n', '\n', '### Bias measure\n', 'A bias measure is a function that returns a bias metric.\n', '\n', '### Bias metric\n', 'A bias metric is a numerical value indicating the level of bias detected as determined by a particular bias measure.\n', '\n', '### Bias report\n', 'A collection of bias metrics for a given dataset or a combination of a dataset and model.\n', '\n', '# Development\n', '\n', ""It's recommended that you setup a virtualenv.\n"", '\n', '```\n', 'virtualenv -p(which python3) venv\n', 'source venv/bin/activate.fish\n', 'pip install -e .[test]\n', 'cd src/\n', '../devtool all\n', '```\n', '\n', 'For running unit tests, do `pytest --pspec`. If you are using PyCharm, and cannot see the green run button next to the tests, open `Preferences` -> `Tools` -> `Python Integrated tools`, and set default test runner to `pytest`.\n', '\n', 'For Internal contributors, run ```../devtool integ_tests``` after creating virtualenv with the above steps to run the integration tests.\n']"
Model Fairness,jphall663/interpretable_machine_learning_with_python,jphall663,https://api.github.com/repos/jphall663/interpretable_machine_learning_with_python,633,200,5,"['https://api.github.com/users/jphall663', 'https://api.github.com/users/esztiorm', 'https://api.github.com/users/ntache', 'https://api.github.com/users/aosama', 'https://api.github.com/users/APorterOReilly']",Python,2023-03-10T14:09:23Z,https://raw.githubusercontent.com/jphall663/interpretable_machine_learning_with_python/master/README.md,"['# Responsible Machine Learning with Python\n', 'Examples of techniques for training interpretable machine learning (ML) models, explaining ML models, and debugging ML models for accuracy, discrimination, and security.\n', '\n', '\n', '### Overview\n', '\n', ""Usage of artificial intelligence (AI) and ML models is likely to become more commonplace as larger swaths of the economy embrace automation and data-driven decision-making. While these predictive systems can be quite accurate, they have often been inscrutable and unappealable black boxes that produce only numeric predictions with no accompanying explanations. Unfortunately, recent studies and recent events have drawn attention to mathematical and sociological flaws in prominent weak AI and ML systems, but practitioners don’t often have the right tools to pry open ML models and debug them. This series of notebooks introduces several approaches that increase transparency, accountability, and trustworthiness in ML models. If you are a data scientist or analyst and you want to train accurate, interpretable ML models, explain ML models to your customers or managers, test those models for security vulnerabilities or social discrimination, or if you have concerns about documentation, validation, or regulatory requirements, then this series of Jupyter notebooks is for you! (But *please* don't take these notebooks or associated materials as legal compliance advice.)\n"", '\n', 'The notebooks highlight techniques such as:\n', '* [Monotonic XGBoost models, partial dependence, individual conditional expectation plots, and Shapley explanations](https://github.com/jphall663/interpretable_machine_learning_with_python#enhancing-transparency-in-machine-learning-models-with-python-and-xgboost---notebook)\n', '* [Decision tree surrogates, reason codes, and ensembles of explanations](https://github.com/jphall663/interpretable_machine_learning_with_python#increase-transparency-and-accountability-in-your-machine-learning-project-with-python---notebook)\n', '* [Disparate impact analysis](https://github.com/jphall663/interpretable_machine_learning_with_python#increase-fairness-in-your-machine-learning-project-with-disparate-impact-analysis-using-python-and-h2o---notebook)\n', '* [LIME](https://github.com/jphall663/interpretable_machine_learning_with_python#explain-your-predictive-models-to-business-stakeholders-with-lime-using-python-and-h2o---notebook)\n', '* [Sensitivity and residual analysis](https://github.com/jphall663/interpretable_machine_learning_with_python#testing-machine-learning-models-for-accuracy-trustworthiness-and-stability-with-python-and-h2o---notebook)\n', '  * [Advanced sensitivity analysis for model debugging](https://github.com/jphall663/interpretable_machine_learning_with_python#part-1-sensitivity-analysis---notebook)\n', '  * [Advanced residual analysis for model debugging](https://github.com/jphall663/interpretable_machine_learning_with_python#part-2-residual-analysis---notebook)\n', '* [Detailed model comparison and model selection by cross-validated ranking](https://github.com/jphall663/interpretable_machine_learning_with_python#from-glm-to-gbm-building-the-case-for-complexity---notebook)\n', '\n', 'The notebooks can be accessed through:\n', '* [H2O Aquarium (Recommended)](https://github.com/jphall663/interpretable_machine_learning_with_python#h2o-aquarium-recommended)\n', '* [Virtualenv (Advanced)](https://github.com/jphall663/interpretable_machine_learning_with_python#virtualenv-installation)\n', '* [Docker container (Advanced)](https://github.com/jphall663/interpretable_machine_learning_with_python#docker-installation)\n', '* [Manual installation (Advanced)](https://github.com/jphall663/interpretable_machine_learning_with_python#manual-installation)\n', '\n', '#### Further reading:\n', '* [*Machine Learning: Considerations for fairly and transparently expanding access to credit*](http://info.h2o.ai/rs/644-PKX-778/images/Machine%20Learning%20-%20Considerations%20for%20Fairly%20and%20Transparently%20Expanding%20Access%20to%20Credit.pdf)\n', '* [*A Responsible Machine Learning Workflow with Focus on Interpretable Models, Post-hoc Explanation, and Discrimination Testing*](https://www.mdpi.com/2078-2489/11/3/137)\n', '* [*An Introduction to Machine Learning Interpretability, 2nd Edition*](https://www.h2o.ai/wp-content/uploads/2019/08/An-Introduction-to-Machine-Learning-Interpretability-Second-Edition.pdf)\n', '* [*On the Art and Science of Explainable Machine Learning*](https://arxiv.org/pdf/1810.02909.pdf)\n', '* [*Proposals for model vulnerability and security*](https://www.oreilly.com/ideas/proposals-for-model-vulnerability-and-security)\n', '* [*Proposed Guidelines for the Responsible Use of Explainable Machine Learning*](https://arxiv.org/pdf/1906.03533.pdf)\n', '* [*Real-World Strategies for Model Debugging*](https://medium.com/@jphall_22520/strategies-for-model-debugging-aa822f1097ce)\n', '* [*Warning Signs: Security and Privacy in an Age of Machine Learning*](https://fpf.org/wp-content/uploads/2019/09/FPF_WarningSigns_Report.pdf)\n', '* [*Why you should care about debugging machine learning models*](https://www.oreilly.com/radar/why-you-should-care-about-debugging-machine-learning-models/)\n', '\n', '***\n', '\n', '### Enhancing Transparency in Machine Learning Models with Python and XGBoost - [Notebook](https://nbviewer.jupyter.org/github/jphall663/interpretable_machine_learning_with_python/blob/master/xgboost_pdp_ice.ipynb)\n', '\n', '![](./readme_pics/pdp_ice.png)\n', '\n', 'Monotonicity constraints can turn opaque, complex models into transparent, and potentially regulator-approved models, by ensuring predictions only increase or only decrease for any change in a given input variable. In this notebook, I will demonstrate how to use monotonicity constraints in the popular open source gradient boosting package XGBoost to train an interpretable and accurate nonlinear classifier on the UCI credit card default data.\n', '\n', 'Once we have trained a monotonic XGBoost model, we will use partial dependence plots and individual conditional expectation (ICE) plots to investigate the internal mechanisms of the model and to verify its monotonic behavior. Partial dependence plots show us the way machine-learned response functions change based on the values of one or two input variables of interest while averaging out the effects of all other input variables. ICE plots can be used to create more localized descriptions of model predictions, and ICE plots pair nicely with partial dependence plots. An example of generating regulator mandated reason codes from high fidelity Shapley explanations for any model prediction is also presented. The combination of monotonic XGBoost, partial dependence, ICE, and Shapley explanations is likely one of the most direct ways to create an interpretable machine learning model today.\n', '\n', '\n', '### Increase Transparency and Accountability in Your Machine Learning Project with Python - [Notebook](https://nbviewer.jupyter.org/github/jphall663/interpretable_machine_learning_with_python/blob/master/dt_surrogate_loco.ipynb)\n', '\n', '![](./readme_pics/dt_surrogate.png)\n', '\n', 'Gradient boosting machines (GBMs) and other complex machine learning models are popular and accurate prediction tools, but they can be difficult to interpret. Surrogate models, feature importance, and reason codes can be used to explain and increase transparency in machine learning models. In this notebook, we will train a GBM on the UCI credit card default data. Then we’ll train a decision tree surrogate model on the original inputs and predictions of the complex GBM model and see how the variable importance and interactions displayed in the surrogate model yield an overall, approximate flowchart of the complex model’s predictions. We will also analyze the global variable importance of the GBM and compare this information to the surrogate model, our domain expertise, and our reasonable expectations.\n', '\n', 'To get a better picture of the complex model’s local behavior and to enhance the accountability of the model’s predictions, we will use a variant of the leave-one-covariate-out (LOCO) technique. LOCO enables us to calculate the local contribution each input variable makes toward each model prediction. We will then rank the local contributions to generate reason codes that describe, in plain English, the model’s decision process for every prediction.\n', '\n', '### Increase Fairness in Your Machine Learning Project with Disparate Impact Analysis using Python and H2O - [Notebook](https://nbviewer.jupyter.org/github/jphall663/interpretable_machine_learning_with_python/blob/master/dia.ipynb)\n', '\n', '<img src=""./readme_pics/dia.png"" height=""400"">\n', '\n', 'Fairness is an incredibly important, but highly complex entity. So much so that leading scholars have yet to agree on a strict definition. However, there is a practical way to discuss and handle observational fairness, or how your model predictions affect different groups of people. This procedure is often known as disparate impact analysis (DIA). DIA is far from perfect, as it relies heavily on user-defined thresholds and reference levels to measure disparity and does not attempt to remediate disparity or provide information on sources of disparity, but it is a fairly straightforward method to quantify your model’s behavior across sensitive demographic segments or other potentially interesting groups of observations. Some types of DIA are also an accepted, regulation-compliant tool for fair-lending purposes in the U.S. financial services industry. If it’s good enough for multibillion-dollar credit portfolios, it’s probably good enough for your project.\n', '\n', 'This example DIA notebook starts by training a monotonic gradient boosting machine (GBM) classifier on the UCI credit card default data using the popular open source library, h2o. A probability cutoff for making credit decisions is selected by maximizing the F1 statistic and confusion matrices are generated to summarize the GBM’s decisions across men and women. A basic DIA procedure is then conducted using the information stored in the confusion matrices and some traditional fair lending measures.\n', '\n', '### Explain Your Predictive Models to Business Stakeholders with LIME using Python and H2O - [Notebook](https://nbviewer.jupyter.org/github/jphall663/interpretable_machine_learning_with_python/blob/master/lime.ipynb)\n', '\n', '![](./readme_pics/lime.png)\n', '\n', 'Machine learning can create very accurate predictive models, but these models can be almost impossible to explain to your boss, your customers, or even your regulators. This notebook will use (Local Interpretable Model-agnostic Explanations) LIME to increase transparency and accountability in a complex GBM model trained on the UCI credit card default data. LIME is a method for building linear surrogate models for local regions in a data set, often single rows of data. LIME sheds light on how model predictions are made and describes local model mechanisms for specific rows of data. Because the LIME sampling process may feel abstract to some practitioners, this notebook will also introduce a more straightforward method of creating local samples for LIME.\n', '\n', 'Once local samples have been generated, we will fit LIME models to understand local trends in the complex model’s predictions. LIME can also tell us the local contribution of each input variable toward each model prediction, and these contributions can be sorted to create reason codes -- plain English explanations of every model prediction. We will also validate the fit of the LIME model to enhance trust in our explanations using the local model’s R2 statistic and a ranked prediction plot.\n', '\n', '### Testing Machine Learning Models for Accuracy, Trustworthiness, and Stability with Python and H2O - [Notebook](https://nbviewer.jupyter.org/github/jphall663/interpretable_machine_learning_with_python/blob/master/resid_sens_analysis.ipynb)\n', '\n', '![](./readme_pics/resid.png)\n', '\n', 'Because ML model predictions can vary drastically for small changes in input variable values, especially outside of training input domains, sensitivity analysis is perhaps the most important validation technique for increasing trust in ML model predictions. Sensitivity analysis investigates whether model behavior and outputs remain stable when input data is intentionally perturbed, or other changes are simulated in input data. In this notebook, we will enhance trust in a complex credit default model by testing and debugging its predictions with sensitivity analysis.\n', '\n', 'We’ll further enhance trust in our model using residual analysis. Residuals refer to the difference between the recorded value of a target variable and the predicted value of a target variable for each row in a data set. Generally, the residuals of a well-fit model should be randomly distributed, because good models will account for most phenomena in a data set, except for random error. In this notebook, we will create residual plots for a complex model to debug any accuracy problems arising from overfitting or outliers.\n', '\n', '### Machine Learning Model Debugging with Python: All Models are Wrong ... but Why is _My_ Model Wrong? (And Can I Fix It?)\n', '\n', '##### Part 1: Sensitivity Analysis - [Notebook](https://nbviewer.jupyter.org/github/jphall663/interpretable_machine_learning_with_python/blob/master/debugging_sens_analysis_redux.ipynb)\n', '\n', '![](/readme_pics/sa.png)\n', '\n', 'Sensitivity analysis is the perturbation of data under a trained model. It can take many forms and arguably Shapley feature importance, partial dependence, individual conditional expectation, and adversarial examples are all types of sensitivity analysis. This notebook focuses on using these different types of sensitivity analysis to discover error mechanisms and security vulnerabilities and to assess stability and fairness in a trained XGBoost model. It begins by loading the UCI credit card default data and then training an interpretable, monotonically constrained XGBoost model. After the model is trained, global and local Shapley feature importance is calculated. These Shapley values help inform the application of partial dependence and ICE, and together these results guide a search for adversarial examples. The notebook closes by exposing the trained model to a random attack and analyzing the attack results.\n', '\n', 'These model debugging exercises uncover accuracy, drift, and security problems such as over-emphasis of important features and impactful yet non-robust interactions. Several remediation mechanisms are proposed including editing of final model artifacts to remove or fix errors, missing value injection or regularization during training to lessen the impact of certain features or interactions, and assertion-based missing value injection during scoring to mitigate the effect of non-robust interactions.\n', '\n', '##### Part 2: Residual Analysis - [Notebook](https://nbviewer.jupyter.org/github/jphall663/interpretable_machine_learning_with_python/blob/master/debugging_resid_analysis_redux.ipynb)\n', '\n', '![](readme_pics/resid2.png)\n', '\n', 'In general, residual analysis can be characterized as a careful study of when and how models make mistakes. A better understanding of mistakes will hopefully lead to fewer of them. This notebook uses variants of residual analysis to find error mechanisms and security vulnerabilities and to assess stability and fairness in a trained XGBoost model. It begins by loading the UCI credit card default data and then training an interpretable, monotonically constrained XGBoost gradient boosting machine (GBM) model. (Pearson correlation with the prediction target is used to determine the direction of the monotonicity constraints for each input variable.) After the model is trained, its logloss residuals are analyzed and explained thoroughly and the constrained GBM is compared to a benchmark linear model. These model debugging exercises uncover accuracy, drift, and security problems such as over-emphasis of important variables and strong signal in model residuals. Several remediation mechanisms are proposed including missing value injection during training, additional data collection, and use of assertions to correct known problems during scoring.\n', '\n', '### From GLM to GBM: Building the Case For Complexity - [Notebook](https://nbviewer.jupyter.org/github/jphall663/interpretable_machine_learning_with_python/blob/master/glm_mgbm_gbm.ipynb)\n', '\n', '![](readme_pics/hist_pd_ice_lo.png)\n', '\n', 'This notebook uses the same credit card default scenario to show how monotonicity constraints, Shapley values and other post-hoc explanations, and discrimination testing can enable practitioners to create direct comparisons between GLM and GBM models. Several candidate probability of default models are selected for comparison using feature selection methods, like LASSO, and by cross-validated ranking. Comparisons then enable building from GLM to more complex GBM models in a step-by-step manner, while retaining model transparency and the ability to test for discrimination. This notebook shows that GBMs can yield better accuracy, more revenue, and that GBMs are also likely to fulfill many model documentation, adverse action notice, and discrimination testing requirements.\n', '\n', '## Using the Examples\n', '\n', '### H2O Aquarium (recommended)\n', '\n', 'H2O Aquarium is a free educational environment that hosts versions of these notebooks among many other H2o-related resources. To use these notebooks in Aquarium:\n', '\n', '1. Navigate to the Aquarium URL: https://aquarium.h2o.ai.\n', '\n', '2. Create a new Aquarium account.\n', '\n', '3. Check the registered email inbox and use the temporary password to login to Aquarium.\n', '\n', '4. Click `Browse Labs`.\n', '\n', '5. Click `View Detail` under *Open Source MLI Workshop*.\n', '\n', '6. Click `Start Lab` (this can take several minutes).\n', '\n', '7. Click on the *Jupyter URL* when it becomes available.\n', '\n', '8. Enter the token `h2o`.\n', '\n', '9. Click the `patrick_hall_mli` folder.\n', '\n', '10. Browse/run the Jupyter notebooks.\n', '\n', '11. Click `End Lab` when you are finished.\n', '\n', '### Virtualenv Installation\n', '\n', 'For avid Python users, creating a Python virtual environment is a convenient way to run these notebooks.\n', '\n', '1. Install [Git](https://git-scm.com/downloads).\n', '\n', '2. Clone this repository with the examples.</br>\n', '`$ git clone https://github.com/jphall663/interpretable_machine_learning_with_python.git`\n', '\n', '3. Install Anaconda Python 5.1.0 from the [Anaconda archives](https://repo.continuum.io/archive/) and add it to your system path.\n', '\n', '4. Change directories into the cloned repository.</br>\n', '`$ cd interpretable_machine_learning_with_python`\n', '\n', '5. Create a Python 3.6 virtual environment.</br>\n', '`$ virtualenv -p /path/to/anaconda3/bin/python3.6 env_iml`\n', '\n', '6. Activate the virtual environment.</br>\n', '`$ source env_iml/bin/activate`\n', '\n', '7. Install the correct packages for the example notebooks.</br>\n', '`$ pip install -r requirements.txt`\n', '\n', '8. Start Jupyter.</br>\n', '`$ jupyter notebook`\n', '\n', '### Docker Installation\n', '\n', 'A Dockerfile is provided to build a docker container with all necessary packages and dependencies. This is a way to use these examples if you are on Mac OS X, \\*nix, or Windows 10. To do so:\n', '\n', '1. Install and start [docker](https://www.docker.com/).\n', '\n', 'From a terminal:\n', '\n', '2. Create a directory for the Dockerfile.</br>\n', '`$ mkdir anaconda_py36_h2o_xgboost_graphviz_shap`\n', '\n', '3. Fetch the Dockerfile.</br>\n', '`$ curl https://raw.githubusercontent.com/jphall663/interpretable_machine_learning_with_python/master/anaconda_py36_h2o_xgboost_graphviz_shap/Dockerfile > anaconda_py36_h2o_xgboost_graphviz_shap/Dockerfile`\n', '\n', '4. Build a docker image from the Dockefile.</br>\n', '`docker build -t iml anaconda_py36_h2o_xgboost_graphviz_shap`\n', '\n', '5. Start the docker image and the Jupyter notebook server.</br>\n', ' `docker run -i -t -p 8888:8888 iml:latest /bin/bash -c ""/opt/conda/bin/jupyter notebook --notebook-dir=/interpretable_machine_learning_with_python --allow-root --ip=\'*\' --port=8888 --no-browser""`\n', '\n', '6. Navigate to port 8888 on your machine, probably `http://localhost:8888/`.\n', '\n', '\n', '### Manual Installation\n', '\n', '1. Anaconda Python 5.1.0 from the [Anaconda archives](https://repo.continuum.io/archive/).\n', '2. [Java](https://java.com/download).\n', '3. The latest stable [h2o](https://www.h2o.ai/download/) Python package.\n', '4. [Git](https://git-scm.com/downloads).\n', '5. [XGBoost](https://github.com/dmlc/xgboost) with Python bindings.\n', '6. [GraphViz](http://www.graphviz.org/).\n', '7. [Seaborn](https://pypi.org/project/seaborn/) package.\n', '8. [Shap](https://pypi.org/project/shap/) package.  \n', '\n', 'Anaconda Python, Java, Git, and GraphViz must be added to your system path.\n', '\n', 'From a terminal:\n', '\n', '9. Clone the repository with examples.</br>\n', '`$ git clone https://github.com/jphall663/interpretable_machine_learning_with_python.git`\n', '\n', '10. `$ cd interpretable_machine_learning_with_python`\n', '\n', '11. Start the Jupyter notebook server.</br>\n', '`$ jupyter notebook`\n', '\n', '12. Navigate to the port Jupyter directs you to on your machine, probably `http://localhost:8888/`.\n']"
Model Fairness,IBM/monitor-wml-model-with-watson-openscale,IBM,https://api.github.com/repos/IBM/monitor-wml-model-with-watson-openscale,12,18,8,"['https://api.github.com/users/scottdangelo', 'https://api.github.com/users/rhagarty', 'https://api.github.com/users/stevemart', 'https://api.github.com/users/sanjeevghimire', 'https://api.github.com/users/dolph', 'https://api.github.com/users/imgbot%5Bbot%5D', 'https://api.github.com/users/ljbennett62', 'https://api.github.com/users/sandhya-nayak']",Python,2021-08-17T17:13:42Z,https://raw.githubusercontent.com/IBM/monitor-wml-model-with-watson-openscale/master/README.md,"['# Monitor WML Model With Watson OpenScale\n', '\n', ""In this Code Pattern, we will use German Credit data to train, create, and deploy a machine learning model using [Watson Machine Learning](https://console.bluemix.net/catalog/services/machine-learning). We will create a data mart for this model with [Watson OpenScale](https://www.ibm.com/cloud/watson-openscale/) and configure OpenScale to monitor that deployment, and inject seven days' worth of historical records and measurements for viewing in the OpenScale Insights dashboard.\n"", '\n', 'When the reader has completed this Code Pattern, they will understand how to:\n', '\n', '* Create and deploy a machine learning model using the Watson Machine Learning service\n', '* Setup Watson OpenScale Data Mart\n', '* Bind Watson Machine Learning to the Watson OpenScale Data Mart\n', '* Add subscriptions to the Data Mart\n', '* Enable payload logging and performance monitor for subscribed assets\n', '* Enable Quality (Accuracy) monitor\n', '* Enable Fairness monitor\n', '* Enable Drift montitor\n', '* Score the German credit model using the Watson Machine Learning\n', '* Insert historic payloads, fairness metrics, and quality metrics into the Data Mart\n', '* Use Data Mart to access tables data via subscription\n', '\n', '![architecture](doc/source/images/architecture.png)\n', '\n', '## Flow\n', '\n', '1. The developer creates a Jupyter Notebook on Watson Studio.\n', '2. The Jupyter Notebook is connected to a PostgreSQL database, which is used to store Watson OpenScale data.\n', '3. The notebook is connected to Watson Machine Learning and a model is trained and deployed.\n', '4. Watson OpenScale is used by the notebook to log payload and monitor performance, quality, and fairness.\n', '\n', '## Prerequisites\n', '\n', '* An [IBM Cloud Account](https://cloud.ibm.com/)\n', '* [IBM Cloud CLI](https://cloud.ibm.com/docs/cli/reference/ibmcloud/download_cli.html#install_use)\n', '* [IBM Cloud Object Storage (COS)](https://www.ibm.com/cloud/object-storage)\n', '* An account on [IBM Watson Studio](https://dataplatform.cloud.ibm.com/).\n', '\n', '# Steps\n', '\n', '1. [Clone the repository](#1-clone-the-repository)\n', '1. [Use free internal DB or Create a Databases for PostgreSQL DB](#2-use-free-internal-db-or-create-a-databases-for-postgresql-db)\n', '1. [Create a Watson OpenScale service](#3-create-a-watson-openscale-service)\n', '1. [Create a Watson Machine Learning instance](#4-create-a-watson-machine-learning-instance)\n', '1. [Create a notebook in IBM Watson Studio on Cloud Pak for Data](#5-create-a-notebook-in-ibm-watson-studio-on-cloud-pak-for-data)\n', '1. [Run the notebook in IBM Watson Studio](#6-run-the-notebook-in-ibm-watson-studio)\n', '1. [Setup OpenScale to utilize the dashboard](#7-setup-openscale-to-utilize-the-dashboard)\n', '\n', '### 1. Clone the repository\n', '\n', '```bash\n', 'git clone https://github.com/IBM/monitor-wml-model-with-watson-openscale\n', 'cd monitor-wml-model-with-watson-openscale\n', '```\n', '\n', '### 2. Use free internal DB or Create a Databases for PostgreSQL DB\n', '\n', '#### If you wish, you can use the free internal Database with Watson OpenScale. To do this, make sure that the cell for `KEEP_MY_INTERNAL_POSTGRES = True` remains unchanged.\n', '\n', '#### If you have or wish to use a paid `Databases for Postgres` instance, follow these instructions:\n', '\n', '> Note: Services created must be in the same region, and space, as your Watson Studio service.\n', '\n', '* Using the [IBM Cloud Dashboard](https://cloud.ibm.com/catalog) catalog, search for PostgreSQL and choose the `Databases for Postgres` [service](https://console.bluemix.net/catalog/services/databases-for-postgresql).\n', '* Wait for the database to be provisioned.\n', '* Click on the `Service Credentials` tab on the left and then click `New credential +` to create the service credentials. Copy them or leave the tab open to use later in the notebook.\n', '* Make sure that the cell in the notebook that has:\n', '\n', '```python\n', 'KEEP_MY_INTERNAL_POSTGRES = True\n', '```\n', '\n', 'is changed to:\n', '\n', '```python\n', 'KEEP_MY_INTERNAL_POSTGRES = False\n', '```\n', '\n', '### 3. Create a Watson OpenScale service\n', '\n', 'Create Watson OpenScale, either on the IBM Cloud or using your On-Premise Cloud Pak for Data.\n', '\n', '<details><summary>On IBM Cloud</summary>\n', '\n', '* If you do not have an IBM Cloud account, [register for an account](https://cloud.ibm.com/registration)\n', '\n', '* Create a Watson OpenScale instance from the [IBM Cloud catalog](https://cloud.ibm.com/catalog/services/watson-openscale)\n', '\n', '* Select the *Lite* (Free) plan, enter a *Service name*, and click *Create*.\n', '\n', '* Click *Launch Application* to start Watson OpenScale.\n', '\n', '* Click *Auto setup* to automatically set up your Watson OpenScale instance with sample data.\n', '\n', '  ![Cloud auto setup](doc/source/images/cloud-auto-setup.png)\n', '\n', '* Click *Start tour*  to tour the Watson OpenScale dashboard.\n', '\n', '</details>\n', '\n', '<details><summary>On IBM Cloud Pak for Data platform</summary>\n', '\n', '> Note: This assumes that your Cloud Pak for Data Cluster Admin has already installed and provisioned OpenScale on the cluster.\n', '\n', '* In the Cloud Pak for Data instance, go the (☰) menu and under `Services` section, click on the `Instances` menu option.\n', '\n', '  ![Service](doc/source/images/services.png)\n', '\n', '* Find the `OpenScale-default` instance from the instances table and click the three vertical dots to open the action menu, then click on the `Open` option.\n', '\n', '  ![Openscale Tile](doc/source/images/services-wos-instance.png)\n', '\n', '* If you need to give other users access to the OpenScale instance, go the (☰) menu and under `Services` section, click on the `Instances` menu option.\n', '\n', '  ![Service](doc/source/images/services.png)\n', '\n', '* Find the `OpenScale-default` instance from the instances table and click the three vertical dots to open the action menu, then click on the `Manage access` option.\n', '\n', '  ![Openscale Tile](doc/source/images/services-wos-manageaccess.png)\n', '\n', '* To add users to the service instance, click the `Add users` button.\n', '\n', '  ![Openscale Tile](doc/source/images/services-wos-addusers.png)\n', '\n', '* For all of the user accounts, select the `Editor` role for each user and then click the `Add` button.\n', '\n', '  ![Openscale Tile](doc/source/images/services-wos-userrole.png)\n', '\n', '</details>\n', '\n', '### 4. Create a Watson Machine Learning instance\n', '\n', '* Under the `Settings` tab, scroll down to `Associated services`, click `+ Add service` and choose `Watson`:\n', '\n', '  ![Add service](https://github.com/IBM/pattern-images/blob/master/watson-studio/images/wml-add-menu.png)\n', '\n', '* Search for `Machine Learning`, Verify this service is being created in the same space as the app in Step 1, and click `Create`.\n', '\n', '  ![Create Machine Learning](https://raw.githubusercontent.com/IBM/pattern-images/master/machine-learning/create-machine-learning.png)\n', '\n', '* Alternately, you can choose an existing Machine Learning instance and click on `Select`.\n', '\n', '* The Watson Machine Learning service is now listed as one of your `Associated Services`.\n', '\n', '* In a different browser tab go to [https://cloud.ibm.com/](https://cloud.ibm.com/) and log in to the Dashboard.\n', '\n', '* Click on your Watson Machine Learning instance under `Services`, click on `Service credentials` and then on `View credentials` to see the credentials.\n', '\n', '  ![](https://raw.githubusercontent.com/IBM/pattern-images/master/machine-learning/ML-service-credentials.png)\n', '\n', '* Save the credentials in a file. You will use them inside the notebook.\n', '\n', '### 5. Create a notebook in IBM Watson Studio on Cloud Pak for Data\n', '\n', '* In [Watson Studio](https://dataplatform.cloud.ibm.com/) or your on-premise Cloud Pak for Data, click `New Project +` under Projects or, at the top of the page click `+ New` and choose the tile for `Data Science` and then `Create Project`.\n', '\n', ""* Using the project you've created, click on `+ Add to project` and then choose the  `Notebook` tile, OR in the `Assets` tab under `Notebooks` choose `+ New notebook` to create a notebook.\n"", '\n', '* Select the `From URL` tab. [1]\n', '\n', '* Enter a name for the notebook. [2]\n', '\n', '* Optionally, enter a description for the notebook. [3]\n', '\n', '* For `Runtime` select the `Default Spark Python 3.7 ` option. [4]\n', '\n', '* Under `Notebook URL` provide the following url: [https://raw.githubusercontent.com/IBM/monitor-wml-model-with-watson-openscale/master/notebooks/WatsonOpenScaleMachineLearning.ipynb](https://raw.githubusercontent.com/IBM/monitor-wml-model-with-watson-openscale/master/notebooks/WatsonOpenScaleMachineLearning.ipynb)\n', '\n', '> Note: The current default (as of 8/11/2021) is Python 3.8. This will cause an error when installing the `pyspark.sql SparkSession` library, so make sure that you are using Python 3.7\n', '\n', '* Click the `Create notebook` button. [6]\n', '\n', '![OpenScale Notebook Create](doc/source/images/OpenScaleNotebookCreate.png)\n', '\n', '### 6. Run the notebook in IBM Watson Studio\n', '\n', 'Follow the instructions for `Provision services and configure credentials`:\n', '\n', 'Your Cloud API key can be generated by going to the [**Users** section of the Cloud console](https://cloud.ibm.com/iam#/users).\n', '\n', '* From that page, click your name, scroll down to the **API Keys** section, and click **Create an IBM Cloud API key**.\n', '\n', '* Give your key a name and click **Create**, then copy the created key and paste it below.\n', '\n', 'Alternately, from the [IBM Cloud CLI](https://console.bluemix.net/docs/cli/reference/ibmcloud/download_cli.html#install_use) :\n', '\n', '```bash\n', 'ibmcloud login --sso\n', ""ibmcloud iam api-key-create 'my_key'\n"", '```\n', '\n', '* Enter the `CLOUD_API_KEY` in the cell `1.1 Cloud API key`.\n', '\n', '#### Create COS bucket and get credentials\n', '\n', '* In your [IBM Cloud Object Storage](https://www.ibm.com/cloud/object-storage)  instance, create a bucket with a globally unique name. The UI will let you know if there is a naming conflict. This will be used in cell *1.3.1* as *BUCKET_NAME*.\n', '\n', '* In your [IBM Cloud Object Storage](https://www.ibm.com/cloud/object-storage) instance, get the Service Credentials for use as `COS_API_KEY_ID`, `COS_RESOURCE_CRN`, and `COS_ENDPOINT`:\n', '\n', '  ![COS credentials](doc/source/images/cos-credentials.png)\n', '\n', '* Add the COS credentials in cell *1.2 Cloud object storage details*.\n', '\n', '* Insert your BUCKET_NAME in the cell *1.2.1 Bucket name*.\n', '\n', '* Either use the internal Database, which requires *No Changes* or Add your `DB_CREDENTIALS` after reading the instructions preceeding that cell and change the cell `KEEP_MY_INTERNAL_POSTGRES = True` to become `KEEP_MY_INTERNAL_POSTGRES = False`.\n', '\n', '* Move your cursor to each code cell and run the code in it. Read the comments for each cell to understand what the code is doing. **Important** when the code in a cell is still running, the label to the left changes to **In [\\*]**:.\n', '  Do **not** continue to the next cell until the code is finished running.\n', '\n', '## 7. Setup OpenScale to utilize the dashboard\n', '\n', 'Now that you have created a machine learning model, you can utilize the [OpenScale dashboard](https://aiopenscale.cloud.ibm.com) to gather insights.\n', '\n', '### Sample Output\n', '\n', 'You can find a sample notebook with output for [WatsonOpenScaleMachineLearning-with-outputs.ipynb](notebooks/with-outputs/WatsonOpenScaleMachineLearning-with-outputs.ipynb).\n', '\n', '#### Openscale Dashboard\n', '\n', '* Go to the instance of [Watson OpenScale](https://aiopenscale.cloud.ibm.com/aiopenscale/insights) for an IBM Cloud deployment, or to your deployed instance on Cloud Pak for Data on-premise version. Choose the `Insights` tab to get an overview of your monitored deployments, Accuracy alerts, and Fairness alerts.\n', '\n', '![WOS insights](doc/source/images/WOSinsights.png)\n', '\n', '* Click on the tile for the `Spark German Risk Deployment` and you can see tiles for the `Fairness`, `Accuracy`, and `Performance monitors`.\n', '\n', '![OpenScale monitors](doc/source/images/wos-credit-risk-dashboard.png)\n', '\n', '* Click on one of the tiles, such as *Drift* to view details. Click on a point on the graph for more information on that particular time slice.\n', '\n', '![Drift monitor](doc/source/images/wos-drift-monitor.png)\n', '\n', '* You will see which types of drift were detected. Click on the number to bring up a list of transactions that led to drift.\n', '\n', '![Drift transactions](doc/source/images/wos-drift-details.png)\n', '\n', ""* Click on the `Explain` icon on the left-hand menu and you'll see a list of transactions that have been run using an algorithm to provide explainability. Choose one and click `Explain`.\n"", '\n', '![Choose transaction to explain](doc/source/images/wos-explain-menu.png)\n', '\n', '* You will see a graph showing all the most influential features with the relative weights of contribution to the *Predicted outcome*.\n', '\n', '![View feature weights](doc/source/images/wos-explainability-feature-weights.png)\n', '\n', '* Click the `Inspect` tab and you can experiment with changing the values of various features to see how that would affect the outcome. Click the `Run analysis` button to see what changes would be required to change the outcome.\n', '\n', '![Inspect features and change](doc/source/images/wos-inspect-features.png)\n', '\n', '# License\n', '[Apache 2.0](LICENSE)\n']"
Model Fairness,ebagdasa/differential-privacy-vs-fairness,ebagdasa,https://api.github.com/repos/ebagdasa/differential-privacy-vs-fairness,30,11,2,"['https://api.github.com/users/ebagdasa', 'https://api.github.com/users/OmidPoursaeed']",Python,2023-03-11T01:35:18Z,https://raw.githubusercontent.com/ebagdasa/differential-privacy-vs-fairness/master/README.md,"['## Readme\n', '\n', 'The paper discusses how Differential Privacy (specifically DPSGD from [1]) \n', 'impacts model performance for underrepresented groups. \n', '\n', '### Usage\n', '\n', 'Configure environment by running: `pip install -r requirements.txt`\n', '\n', 'We use Python3.7 and GPU Nvidia TitanX.\n', '\n', 'File playing.py allows run the code. It uses `utils/params.yaml` \n', 'to set parameters from the paper and builds a graph on Tensorboard.\n', 'For Sentiment prediction we use `playing_nlp.py`.\n', '\n', '\n', 'Datasets:\n', '1. MNIST (part of PyTorch)\n', '2. Diversity in Faces (obtained from IBM [here](https://www.research.ibm.com/artificial-intelligence/trusted-ai/diversity-in-faces/#access))\n', '3. iNaturalist (download from [here](https://github.com/visipedia/inat_comp))\n', '4. UTKFace (from [here](http://aicip.eecs.utk.edu/wiki/UTKFace))\n', '5. AAE Twitter corpus (from [here](http://slanglab.cs.umass.edu/TwitterAAE/))\n', '\n', 'We use `compute_dp_sgd_privacy.py` copied from public [repo](https://github.com/tensorflow/privacy)\n', '\n', 'DP-FedAvg implementation is taken from public [repo](https://github.com/ebagdasa/backdoor_federated_learning)  \n', '\n', 'Implementation of DPSGD is based on TF Privacy [repo](https://github.com/tensorflow/privacy) and papers:\n', '\n', '[1] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang. Deep learning with differential privacy. In CCS, 2016.\n', '\n', '[2] H. B. McMahan and G. Andrew. A general approach to adding differential privacy to iterative training procedures. arXiv:1812.06210, 2018\n', '\n', '[3] H. B. McMahan, D. Ramage, K. Talwar, and L. Zhang. Learning differentially private recurrent language models. In ICLR, 2018\n']"
Model Fairness,ModelOriented/DALEX,ModelOriented,https://api.github.com/repos/ModelOriented/DALEX,1197,159,20,"['https://api.github.com/users/pbiecek', 'https://api.github.com/users/hbaniecki', 'https://api.github.com/users/maksymiuks', 'https://api.github.com/users/piotrpiatyszek', 'https://api.github.com/users/WojciechKretowicz', 'https://api.github.com/users/kevinykuo', 'https://api.github.com/users/jakwisn', 'https://api.github.com/users/12tafran', 'https://api.github.com/users/agosiewska', 'https://api.github.com/users/arturzolkowski', 'https://api.github.com/users/adrianstando', 'https://api.github.com/users/CahidArda', 'https://api.github.com/users/kmatusz', 'https://api.github.com/users/emiliawisnios', 'https://api.github.com/users/kasiapekala', 'https://api.github.com/users/MarcinKosinski', 'https://api.github.com/users/krzyzinskim', 'https://api.github.com/users/philip-khor', 'https://api.github.com/users/sai-krishna-msk', 'https://api.github.com/users/RoyalTS']",Python,2023-04-26T15:44:52Z,https://raw.githubusercontent.com/ModelOriented/DALEX/master/README.md,"['# moDel Agnostic Language for Exploration and eXplanation <img src=""man/figures/logo.png"" align=""right"" width=""150""/>\n', '\n', '[![R build status](https://github.com/ModelOriented/DALEX/workflows/R-CMD-check/badge.svg)](https://github.com/ModelOriented/DALEX/actions?query=workflow%3AR-CMD-check)\n', '[![Coverage\n', 'Status](https://img.shields.io/codecov/c/github/ModelOriented/DALEX/master.svg)](https://codecov.io/github/ModelOriented/DALEX?branch=master)\n', '[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/DALEX)](https://cran.r-project.org/package=DALEX)\n', '[![Total Downloads](http://cranlogs.r-pkg.org/badges/grand-total/DALEX?color=orange)](http://cranlogs.r-pkg.org/badges/grand-total/DALEX)\n', '[![DrWhy-eXtrAI](https://img.shields.io/badge/DrWhy-BackBone-373589)](http://drwhy.ai/#BackBone)\n', '\n', '[![Python-check](https://github.com/ModelOriented/DALEX/workflows/Python-check/badge.svg)](https://github.com/ModelOriented/DALEX/actions?query=workflow%3APython-check)\n', '[![Supported Python versions](https://img.shields.io/pypi/pyversions/dalex.svg)](https://pypi.org/project/dalex/)\n', '[![PyPI version](https://badge.fury.io/py/dalex.svg)](https://badge.fury.io/py/dalex)\n', '[![Downloads](https://pepy.tech/badge/dalex)](https://pepy.tech/project/dalex)\n', '\n', '\n', '## Overview\n', '\n', 'Unverified black box model is the path to the failure. Opaqueness leads to distrust. Distrust leads to ignoration. Ignoration leads to rejection.\n', '\n', 'The `DALEX` package xrays any model and helps to explore and explain its behaviour, helps to understand how complex models are working. The main function `explain()` creates a wrapper around a predictive model. Wrapped models may then be explored and compared with a collection of local and global explainers. Recent developents from the area of Interpretable Machine Learning/eXplainable Artificial Intelligence.\n', '\n', 'The philosophy behind `DALEX` explanations is described in the [Explanatory Model Analysis](https://pbiecek.github.io/ema/) e-book. The `DALEX` package is a part of [DrWhy.AI](http://DrWhy.AI) universe.\n', '\n', 'If you work with `scikit-learn`, `keras`, `H2O`, `tidymodels`, `xgboost`, `mlr` or `mlr3` in R, you may be interested in the [DALEXtra](https://github.com/ModelOriented/DALEXtra) package, which is an extension of `DALEX` with easy to use `explain_*()` functions for models created in these libraries.\n', '\n', '**[Additional overview of the dalex Python package is available.](https://github.com/ModelOriented/DALEX/tree/master/python/dalex)**\n', '\n', '<p align=""center"">\n', '<a href=""https://pbiecek.github.io/ema/introduction.html#bookstructure""><img src=""https://github.com/ModelOriented/DALEX/raw/master/misc/DALEXpiramide.png"" width=""800""/></a>\n', '</p>\n', '\n', '## Installation\n', '\n', 'The `DALEX` **R** package can be installed from [CRAN](https://cran.r-project.org/package=DALEX)\n', '\n', '```r\n', 'install.packages(""DALEX"")\n', '```\n', '\n', 'The `dalex` **Python** package is available on [PyPI](https://pypi.org/project/dalex/) and [conda-forge](https://anaconda.org/conda-forge/dalex)\n', '\n', '```console\n', 'pip install dalex -U\n', '\n', 'conda install -c conda-forge dalex\n', '```\n', '\n', '## Learn more\n', '\n', 'Machine Learning models are widely used and have various applications in classification or regression tasks. Due to increasing computational power, availability of new data sources and new methods, ML models are more and more complex. Models created with techniques like boosting, bagging of neural networks are true black boxes. It is hard to trace the link between input variables and model outcomes. They are use because of high performance, but lack of interpretability is one of their weakest sides.\n', '\n', 'In many applications we need to know, understand or prove how input variables are used in the model and what impact do they have on final model prediction. `DALEX` is a set of tools that help to understand how complex models are working.\n', '\n', '<p align=""center"">\n', '<a href=""https://github.com/ModelOriented/DALEX/raw/master/misc/cheatsheet_local_explainers.png""><img src=""https://github.com/ModelOriented/DALEX/raw/master/misc/cheatsheet_local_explainers.png"" width=""500""/></a>\n', '</p>\n', '\n', '## Resources\n', '\n', '* [Gentle introduction to DALEX with examples in R and Python](https://pbiecek.github.io/ema/)\n', '\n', '### R package\n', '\n', '* [Introduction to Responsible Machine Learning @ useR! 2021](https://github.com/MI2DataLab/ResponsibleML-UseR2021)\n', '* DALEX + mlr3 [@ BioColl 2021](https://github.com/pbiecek/BioColl2021) & [@ Open-Forest-Training 2021](https://github.com/pbiecek/Open-Forest-Training-2021/)\n', '* [Materials from Explanatory Model Analysis Workshop @ eRum 2020](https://github.com/pbiecek/XAIatERUM2020), [cheatsheet](https://github.com/pbiecek/XAIatERUM2020/blob/master/Cheatsheet.pdf)\n', '* How to use DALEX with: [keras](https://rawgit.com/pbiecek/DALEX_docs/master/vignettes/DALEX_and_keras.html), [parsnip](https://raw.githack.com/pbiecek/DALEX_docs/master/vignettes/DALEX_parsnip.html), [caret](https://raw.githack.com/pbiecek/DALEX_docs/master/vignettes/DALEX_caret.html), [mlr](https://raw.githack.com/pbiecek/DALEX_docs/master/vignettes/DALEX_mlr.html), [H2O](https://raw.githack.com/pbiecek/DALEX_docs/master/vignettes/DALEX_h2o.html), [xgboost](https://raw.githack.com/pbiecek/DALEX_docs/master/vignettes/DALEX_and_xgboost.html)\n', '* [Compare GBM models created in different languages](https://raw.githack.com/pbiecek/DALEX_docs/master/vignettes/Multilanguages_comparision.html): gbm and CatBoost in R / gbm in h2o / gbm in Python\n', '* [DALEX for fraud detection](https://rawgit.com/pbiecek/DALEX_docs/master/vignettes/DALEXverse%20and%20fraud%20detection.html)\n', '* [DALEX for teaching](https://raw.githack.com/pbiecek/DALEX_docs/master/vignettes/DALEX_teaching.html)\n', '* [XAI in the jungle of competing frameworks for machine learning](https://medium.com/@ModelOriented/xai-in-the-jungle-of-competing-frameworks-for-machine-learning-fa6e96a99644)\n', '\n', '### Python package\n', '\n', '* Introduction to the `dalex` package: [Titanic: tutorial and examples](https://dalex.drwhy.ai/python-dalex-titanic.html)\n', '* Key features explained: [FIFA20: explain default vs tuned model with dalex](https://dalex.drwhy.ai/python-dalex-fifa.html)\n', '* How to use dalex with: [xgboost](https://dalex.drwhy.ai/python-dalex-xgboost.html), [tensorflow](https://dalex.drwhy.ai/python-dalex-tensorflow.html)\n', '* More explanations: [residuals, shap, lime](https://dalex.drwhy.ai/python-dalex-new.html)\n', '* Introduction to the [Fairness module in dalex](https://dalex.drwhy.ai/python-dalex-fairness.html)\n', '* Introduction to the [Arena: interactive dashboard for model exploration](https://dalex.drwhy.ai/python-dalex-arena.html)\n', '* Code in the form of [jupyter notebook](https://github.com/ModelOriented/DALEX-docs/tree/master/jupyter-notebooks)\n', '* Changelog: [NEWS](https://github.com/ModelOriented/DALEX/blob/master/python/dalex/NEWS.md)\n', '\n', '### Talks about DALEX\n', '\n', '* [Talk with your model! at USeR 2020](https://www.youtube.com/watch?v=9WWn5ew8D8o)\n', '* [Talk about DALEX at Complexity Institute / NTU February 2018](https://github.com/pbiecek/Talks/blob/master/2018/DALEX_at_NTU_2018.pdf)\n', '* [Talk about DALEX at SER / WTU April 2018](https://github.com/pbiecek/Talks/blob/master/2018/SER_DALEX.pdf)\n', '* [Talk about DALEX at STWUR May 2018 (in Polish)](https://github.com/STWUR/eRementarz-29-05-2018)\n', '* [Talk about DALEX at BayArea 2018](https://github.com/pbiecek/Talks/blob/master/2018/DALEX_BayArea.pdf)\n', '* [Talk about DALEX at PyData Warsaw 2018](https://github.com/pbiecek/Talks/blob/master/2018/DALEX_PyDataWarsaw2018.pdf)\n', '\n', '## Citation\n', '\n', 'If you use `DALEX` in R or `dalex` in Python, please cite our JMLR papers:\n', '\n', '```html\n', '@article{JMLR:v19:18-416,\n', '  author  = {Przemyslaw Biecek},\n', '  title   = {DALEX: Explainers for Complex Predictive Models in R},\n', '  journal = {Journal of Machine Learning Research},\n', '  year    = {2018},\n', '  volume  = {19},\n', '  number  = {84},\n', '  pages   = {1-5},\n', '  url     = {http://jmlr.org/papers/v19/18-416.html}\n', '}\n', '\n', '@article{JMLR:v22:20-1473,\n', '  author  = {Hubert Baniecki and\n', '             Wojciech Kretowicz and\n', '             Piotr Piatyszek and \n', '             Jakub Wisniewski and \n', '             Przemyslaw Biecek},\n', '  title   = {dalex: Responsible Machine Learning \n', '             with Interactive Explainability and Fairness in Python},\n', '  journal = {Journal of Machine Learning Research},\n', '  year    = {2021},\n', '  volume  = {22},\n', '  number  = {214},\n', '  pages   = {1-7},\n', '  url     = {http://jmlr.org/papers/v22/20-1473.html}\n', '}\n', '```\n', '\n', '## Why\n', '\n', '76 years ago Isaac Asimov devised [Three Laws of Robotics](https://en.wikipedia.org/wiki/Three_Laws_of_Robotics): 1) a robot may not injure a human being, 2) a robot must obey the orders given it by human beings and 3) A robot must protect its own existence. These laws impact discussion around [Ethics of AI](https://en.wikipedia.org/wiki/Ethics_of_artificial_intelligence). Today’s robots, like cleaning robots, robotic pets or autonomous cars are far from being conscious enough to be under Asimov’s ethics.\n', '\n', ""Today we are surrounded by complex predictive algorithms used for decision making. Machine learning models are used in health care, politics, education, judiciary and many other areas. Black box predictive models have far larger influence on our lives than physical robots. Yet, applications of such models are left unregulated despite many examples of their potential harmfulness. See *Weapons of Math Destruction* by Cathy O'Neil for an excellent overview of potential problems.\n"", '\n', ""It's clear that we need to control algorithms that may affect us. Such control is in our civic rights. Here we propose three requirements that any predictive model should fulfill.\n"", '\n', ""-\t**Prediction's justifications**. For every prediction of a model one should be able to understand which variables affect the prediction and how strongly. Variable attribution to final prediction.\n"", ""-\t**Prediction's speculations**. For every prediction of a model one should be able to understand how the model prediction would change if input variables were changed. Hypothesizing about what-if scenarios.\n"", ""-\t**Prediction's validations** For every prediction of a model one should be able to verify how strong are evidences that confirm this particular prediction.\n"", '\n', 'There are two ways to comply with these requirements.\n', 'One is to use only models that fulfill these conditions by design. White-box models like linear regression or decision trees. In many cases the price for transparency is lower performance.\n', 'The other way is to use approximated explainers – techniques that find only approximated answers, but work for any black box model. Here we present such techniques.\n', '\n', '\n', '## Acknowledgments\n', '\n', 'Work on this package was financially supported by the `NCN Opus grant 2016/21/B/ST6/02176` and `NCN Opus grant 2017/27/B/ST6/01307`.\n']"
Model Fairness,FAIRplus/Data-Maturity,FAIRplus,https://api.github.com/repos/FAIRplus/Data-Maturity,9,9,6,"['https://api.github.com/users/lauportell', 'https://api.github.com/users/iemam', 'https://api.github.com/users/YojanaGadiya', 'https://api.github.com/users/actions-user', 'https://api.github.com/users/lrodrin', 'https://api.github.com/users/daniwelter']",SCSS,2023-03-23T02:38:05Z,https://raw.githubusercontent.com/FAIRplus/Data-Maturity/master/README.md,"['<img src=""https://fairplus-project.eu/images/fairplus-logo.png"" alt=""RDMkit logo"" width=""450""/>\n', '\n', '# FAIRplus Dataset Maturity (DSM) Model\n', '\n', 'The FAIRplus-DSM model is intended as a comprehensive reference model for state-of-FAIRness improvement in research datasets. Based on the FAIR guiding principles, the DSM model defines and classifies requirements that constitute an incremental path towards improving FAIRness level for a given research dataset.\n', '\n', '## Contributing\n', '\n', 'You are welcome to contribute to the content. The material is developed in markdown and a jekyll template ([Just the docs](https://pmarsceill.github.io/just-the-docs/)) is used to format the markdown pages and generate the website (https://fairplus.github.io/Data-Maturity/).\n', '\n', '- If you want to add content please create a new branch from this one. When you are ready to merge your changes open a pull request against this branch.\n', '- The content of the website is in markdown files in the [/docs](./docs) directory whereas the images included in the markdown files are in [assets/images](./assets/images).\n', '- Refer to the [Just the Docs documentation](https://pmarsceill.github.io/just-the-docs/) for usage and customisation information.\n', '\n', '## License\n', '\n', '- The FAIRplus DSM Model content is licensed under the [Creative Commons Attribution 4.0 International](https://creativecommons.org/licenses/by/4.0/) (CC BY 4.0) license.\n', '- The jekyll theme is available as open source under the terms of the [MIT License](https://opensource.org/licenses/MIT).\n']"
Model Fairness,neptune-ai/model-fairness-in-practice,neptune-ai,https://api.github.com/repos/neptune-ai/model-fairness-in-practice,8,6,1,['https://api.github.com/users/jakubczakon'],Python,2021-08-03T14:23:01Z,https://raw.githubusercontent.com/neptune-ai/model-fairness-in-practice/master/README.md,"['# model-fairness-in-practice\n', 'Materials for the ODSC West 2019 workshop ""Model fairness in practice""\n', '\n', '## Installation\n', 'Install and spin-up a notebook by running\n', '\n', '```bash\n', 'source Makefile\n', '```\n', '\n', '## Ask me anything\n', '\n', '[twitter @neptune_ai](https://twitter.com/Neptune_ai)\n', '\n', '[linkedin jakub.czakon](https://www.linkedin.com/in/jakub-czakon-2b797b69/)\n', '\n', '[neptune blog jakub.czakon](https://neptune.ai/blog/)\n', '\n', 'email jakub.czakon@neptune.ai\n']"
Model Fairness,LearnedVector/Wav2Letter,LearnedVector,https://api.github.com/repos/LearnedVector/Wav2Letter,81,24,1,['https://api.github.com/users/LearnedVector'],Python,2023-04-14T03:57:27Z,https://raw.githubusercontent.com/LearnedVector/Wav2Letter/master/README.md,"['# Wav2Letter Speech Recognition with Pytorch\n', '\n', 'A Simple, straight forward, easy to read implementation of Wav2Letter, a speech recognition model from Facebooks AI Research (FAIR) [paper](https://arxiv.org/pdf/1609.03193.pdf). You can see most of the architecture in the `Wav2Letter` directory.\n', '\n', 'The next iteration of Wav2Letter can be found in this [paper](https://arxiv.org/abs/1712.09444). This paper uses Gated Convnets instead of normal Convnets.\n', '\n', 'The [Google Speech Command Example.ipynb](https://github.com/LearnedVector/Wav2Letter/blob/master/Google%20Speech%20Command%20Example.ipynb) notebook contains an example of this implementation.\n', '\n', '<p align=""center"">\n', '  <img src=""Wav2Letter-diagram.png"" alt=""Precise 2 Diagram"" height=""700""/>\n', '</p>\n', '\n', '## Differences\n', '\n', '* Uses CTC Loss\n', '* Uses Greedy Decoder\n', '\n', '## TODO\n', '\n', '* Implement Train, Validation, Test sets\n', '* Test on larger speech data\n', '* Implement AutoSegCriterion\n', '* Implement Beam Search Decoder\n', '* Use KenLM Langauge Model in Decoder\n', '* Use larger datasets\n', '* Add Gated ConvNets\n', '\n', '## Getting Started\n', '\n', '## Requirements\n', '\n', '```bash\n', 'pip install -r requirements.txt\n', '```\n', '\n', 'Make sure you are using pytorch-nightly (version 1.0 alpha). This has the CTC_Loss loss function we need.\n', '\n', '## Smoke Test\n', '\n', '`smoke_test.py` contains a quick test to see if everything is working\n', '\n', '```bash\n', 'python smoke_test.py\n', '```\n', '\n', 'This will train a model on randomly generated inputs and target generated data. If everyhing is working correctly, expect to see outputs of the predicted and target labels. Of course expect the outputs to be garbage.\n', '\n', '## Data\n', '\n', 'For an initial test, I used the [Google Speech Command Dataset](https://www.kaggle.com/c/tensorflow-speech-recognition-challenge/data). This is a simple to use lightweight dataset for testing model performance.\n', '\n', '### Instructions to download data\n', '\n', '1. Download the dataset.\n', '2. Create a `./speech_data` directory at root of this project.\n', '3. Unzip the google speech data. Should be named `speech_commands_v0.01`.\n', '\n', '### Prepare data\n', '\n', '`data.py` contains scripts to process google speech command audio data into features compatible with Wav2Letter.\n', '\n', '```bash\n', 'python Wav2Letter/data.py\n', '```\n', '\n', 'This will process the google speech commands audio data into 13 mfcc features with a max framelength of 250 (these are short audio clips). Anything less will be padded with zeros. Target data will be integer encoded and also padded to have the same length. Final outputs are numpy arrays saved as `x.npy` and `y.npy` in the `./speech_data` directory.\n', '\n', '## Train\n', '\n', '`train.py` has the code to run the training. Example would be.\n', '\n', '```bash\n', 'python train.py --batch_size=256 --epochs=1000\n', '```\n', '\n', '## Contributions\n', '\n', ""Pull Requests are accepted! I would love some help to knock out the Todo's. Email me at learnedvector@gmail.com for any questions.\n""]"
Model Fairness,IBM/inFairness,IBM,https://api.github.com/repos/IBM/inFairness,55,3,5,"['https://api.github.com/users/MayankAgarwal', 'https://api.github.com/users/aldopareja', 'https://api.github.com/users/onkarbhardwaj', 'https://api.github.com/users/ibm-open-source-bot', 'https://api.github.com/users/imgbot%5Bbot%5D']",Python,2023-03-26T04:50:03Z,https://raw.githubusercontent.com/IBM/inFairness/main/README.md,"['<p align=""center"">\n', '  <a href=""https://ibm.github.io/inFairness"">\n', '     <img width=""350"" height=""350"" src=""https://ibm.github.io/inFairness/_static/infairness-logo.png"">\n', '   </a>\n', '</p>\n', '\n', '<p align=""center"">\n', '   <a href=""https://pypi.org/project/infairness""><img src=""https://img.shields.io/pypi/v/infairness?color=important&label=pypi%20package&logo=PyPy""></a>\n', '   <a href=""./examples""><img src=""https://img.shields.io/badge/example-notebooks-red?logo=jupyter""></a>\n', '   <a href=""https://ibm.github.io/inFairness""><img src=""https://img.shields.io/badge/documentation-up-green?logo=GitBook""></a>\n', '   <a href=""https://fairbert.vizhub.ai""><img src=""https://img.shields.io/badge/fairness-demonstration-yellow?logo=ibm-watson""></a>\n', '   <br/>\n', '   <a href=""https://app.travis-ci.com/IBM/inFairness""><img src=""https://app.travis-ci.com/IBM/inFairness.svg?branch=main""></a>\n', '   <a href=""https://pypistats.org/packages/infairness""><img alt=""PyPI - Downloads"" src=""https://img.shields.io/pypi/dm/inFairness?color=blue""></a>\n', '   <a href=""https://www.python.org/""><img src=""https://img.shields.io/badge/python-3.7+-blue?logo=python""></a>\n', '   <a href=""https://opensource.org/licenses/Apache-2.0""><img src=""https://img.shields.io/badge/license-Apache-yellow""></a>\n', '   <a href=""https://github.com/psf/black""><img src=""https://img.shields.io/badge/code%20style-black-000000.svg""></a>\n', '</p>\n', '\n', '\n', '## Individual Fairness and inFairness\n', '\n', 'Intuitively, an individually fair Machine Learning (ML) model treats similar inputs similarly. Formally, the leading notion of individual fairness is metric fairness [(Dwork et al., 2011)](https://dl.acm.org/doi/abs/10.1145/2090236.2090255); it requires:\n', '\n', '$$ d_y (h(x_1), h(x_2)) \\leq L d_x(x_1, x_2) \\quad \\forall \\quad x_1, x_2 \\in X $$\n', '\n', 'Here, $h: X \\rightarrow Y$ is a ML model, where $X$ and $Y$ are input and output spaces; $d_x$ and $d_y$ are metrics on the input and output spaces, and $L \\geq 0$ is a Lipchitz constant. This constrained optimization equation states that the distance between the model predictions for inputs $x_1$ and $x_2$ is upper-bounded by the fair distance between the inputs $x_1$ and $x_2$. Here, the fair metric $d_x$ encodes our intuition of which samples should be treated similarly by the ML model, and in designing so, we ensure that for input samples considered similar by the fair metric $d_x$, the model outputs will be similar as well.\n', '\n', 'inFairness is a PyTorch package that supports auditing, training, and post-processing ML models for individual fairness. At its core, the library implements the key components of individual fairness pipeline: $d_x$ - distance in the input space, $d_y$ - distance in the output space, and the learning algorithms to optimize for the equation above.\n', '\n', 'For an in-depth tutorial of Individual Fairness and the inFairness package, please watch this tutorial. Also, take a look at the [examples](./examples/) folder for illustrative use-cases and try the [Fairness Playground demo](https://fairbert.vizhub.ai). For more group fairness examples see [AIF360](https://aif360.mybluemix.net/).\n', '\n', '<p align=""center"">\n', '  <a href=""https://ibm.box.com/v/fairness-tutorial-2022"" target=""_blank""><img width=""700"" alt=""Watch the tutorial"" src=""https://user-images.githubusercontent.com/991913/178768336-2bfa5958-487f-4f14-a156-03dacfd68263.png""></a>\n', '</p>\n', '\n', '## Installation\n', '\n', 'inFairness can be installed using `pip`:\n', '\n', '```\n', 'pip install inFairness\n', '```\n', '\n', '\n', 'Alternatively, if you wish to install the latest development version, you can install directly by cloning this repository:\n', '\n', '```\n', 'git clone <git repo url>\n', 'cd inFairness\n', 'pip install -e .\n', '```\n', '\n', '\n', '\n', '## Features\n', '\n', 'inFairness currently supports:\n', '\n', '1. Learning individually fair metrics : [[Docs]](https://ibm.github.io/inFairness/reference/distances.html)\n', '2. Training of individually fair models : [[Docs]](https://ibm.github.io/inFairness/reference/algorithms.html)\n', '3. Auditing pre-trained ML models for individual fairness : [[Docs]](https://ibm.github.io/inFairness/reference/auditors.html)\n', '4. Post-processing for Individual Fairness : [[Docs]](https://ibm.github.io/inFairness/reference/postprocessing.html)\n', '5. Individually fair ranking : [[Docs]](https://ibm.github.io/inFairness/reference/algorithms.html)\n', '\n', '\n', '## Contributing\n', '\n', 'We welcome contributions from the community in any form - whether it is through the contribution of a new fair algorithm, fair metric, a new use-case, or simply reporting an issue or enhancement in the package. To contribute code to the package, please follow the following steps:\n', '\n', '1. Clone this git repository to your local system\n', '2. Setup your system by installing dependencies as: `pip3 install -r requirements.txt` and `pip3 install -r  build_requirements.txt`\n', '3. Add your code contribution to the package. Please refer to the [`inFairness`](./inFairness) folder for an overview of the directory structure\n', '4. Add appropriate unit tests in the [`tests`](./tests) folder\n', '5. Once you are ready to commit code, check for the following:\n', '   1. Coding style compliance using: `flake8 inFairness/`. This command will list all stylistic violations found in the code. Please try to fix as much as you can\n', '   2. Ensure all the test cases pass using: `coverage run --source inFairness -m pytest tests/`. All unit tests need to pass to be able merge code in the package.\n', '6. Finally, commit your code and raise a Pull Request.\n', '\n', '\n', '## Tutorials\n', '\n', 'The [`examples`](./examples) folder contains tutorials from different fields illustrating how to use the package.\n', '\n', '### Minimal example\n', '\n', 'First, you need to import the relevant packages\n', '\n', '```\n', 'from inFairness import distances\n', 'from inFairness.fairalgo import SenSeI\n', '```\n', '\n', 'The `inFairness.distances` module implements various distance metrics on the input and the output spaces, and the `inFairness.fairalgo` implements various individually fair learning algorithms with `SenSeI` being one particular algorithm.\n', '\n', 'Thereafter, we instantiate and fit the distance metrics on the training data, and \n', '\n', '\n', '```[python]\n', 'distance_x = distances.SVDSensitiveSubspaceDistance()\n', 'distance_y = distances.EuclideanDistance()\n', '\n', 'distance_x.fit(X_train=data, n_components=50)\n', '\n', '# Finally instantiate the fair algorithm\n', 'fairalgo = SenSeI(network, distance_x, distance_y, lossfn, rho=1.0, eps=1e-3, lr=0.01, auditor_nsteps=100, auditor_lr=0.1)\n', '```\n', '\n', 'Finally, you can train the `fairalgo` as you would train your standard PyTorch deep neural network:\n', '\n', '```\n', 'fairalgo.train()\n', '\n', 'for epoch in range(EPOCHS):\n', '    for x, y in train_dl:\n', '        optimizer.zero_grad()\n', '        result = fairalgo(x, y)\n', '        result.loss.backward()\n', '        optimizer.step()\n', '```\n', '\n', '\n', '##  Authors\n', '\n', '<table align=""center"">\n', '  <tr>\n', '    <td align=""center""><a href=""http://moonfolk.github.io/""><img src=""https://avatars.githubusercontent.com/u/24443134?v=4?s=100"" width=""120px;"" alt=""""/><br /><b>Mikhail Yurochkin</b></a></a></td>\n', '    <td align=""center""><a href=""http://mayankagarwal.github.io/""><img src=""https://avatars.githubusercontent.com/u/991913?v=4?s=100"" width=""120px;"" alt=""""/><br /><b>Mayank Agarwal</b></a></a></td>\n', '    <td align=""center""><a href=""https://github.com/aldopareja""><img src=""https://avatars.githubusercontent.com/u/7622817?v=4?s=100"" width=""120px;"" alt=""""/><br /><b>Aldo Pareja</b></a></a></td>\n', '    <td align=""center""><a href=""https://github.com/onkarbhardwaj""><img src=""https://avatars.githubusercontent.com/u/13560220?v=4?s=100"" width=""120px;"" alt=""""/><br /><b>Onkar Bhardwaj</b></a></a></td>\n', '  </tr>\n', '</table>\n']"
Model Fairness,sakshiudeshi/Aequitas,sakshiudeshi,https://api.github.com/repos/sakshiudeshi/Aequitas,8,9,1,['https://api.github.com/users/sakshiudeshi'],Python,2022-09-12T19:39:56Z,https://raw.githubusercontent.com/sakshiudeshi/Aequitas/master/README.md,"['# Aequitas\n', '\n', 'We present Aequitas, a directed fairness testing framework machine learning models. See the paper [Automated Directed Fairness Testing](https://arxiv.org/abs/1807.00468) for more details.\n', '\n', '\n', '\n', '## Prerequisites\n', '\n', '* Python 2.7.15\n', '* numpy 1.14.5\n', '* scipy 1.1.0\n', '* scikit-learn 1.19.0\n', '\n', 'The authors used Pycharm CE 2017.2.3 as the development IDE.\n', '\n', '## Background\n', 'There are 3 test generation strategies in our suite, namely Aequitas Random, Aequitas Semi-Directed and Aequitas Fully Directed. There are files to evaluate [Fair SVM](https://github.com/mbilalzafar/fair-classification) and Scikit-Learn classifiers trained on the same [dataset](http://archive.ics.uci.edu/ml/datasets/Adult).\n', '\n', '## Config\n', 'The [config](config.py) file has the following data:\n', '\n', '* params : The number of parameters in the data\n', '* sensitive_param: The parameter under test.\n', '* input_bounds: The bounds of each parameter\n', '* classifier_name: Pickled scikit-learn classifier under test (only applicable to the sklearn files)\n', '* threshold: Discrimination threshold.\n', '* perturbation_unit: By what unit would the user like to perturb the input in the local search.\n', '* retraining_inputs: Inputs to be used for the retraining. Please see [this file](Retrain_Example_File.txt).\n', '\n', '## Demo\n', '`python <filename>`\n', '\n', 'eg. `python Aequitas_Fully_Directed.py`\n', '\n', '## Contact\n', '* Please contact sakshi_udeshi@mymail.sutd.edu.sg for any comments/questions\n', '\n', '\n', '## Citation \n', 'If you use any part of this code, please cite the following paper\n', '\n', '```\n', '@inproceedings{aequitas,\n', '  title={Automated directed fairness testing},\n', '  author={Udeshi, Sakshi and Arora, Pryanshu and Chattopadhyay, Sudipta},\n', '  booktitle={Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},\n', '  pages={98--108},\n', '  year={2018}\n', '}\n', '```\n', '\n', '\n', '\n']"
Model Fairness,ermongroup/fairgen,ermongroup,https://api.github.com/repos/ermongroup/fairgen,14,7,3,"['https://api.github.com/users/kristychoi', 'https://api.github.com/users/dependabot%5Bbot%5D', 'https://api.github.com/users/trishasingh']",Python,2023-03-08T18:06:37Z,https://raw.githubusercontent.com/ermongroup/fairgen/master/README.md,"['# Fair Generative Modeling via Weak Supervision\n', 'This repo contains a reference implementation for fairgen as described in the paper:\n', '> Fair Generative Modeling via Weak Supervision </br>\n', '> [Kristy Choi*](http://kristychoi.com/), [Aditya Grover*](http://aditya-grover.github.io/), [Trisha Singh](https://profiles.stanford.edu/trisha-singh), [Rui Shu](http://ruishu.io/about/), [Stefano Ermon](https://cs.stanford.edu/~ermon/) </br>\n', '> International Conference on Machine Learning (ICML), 2020. </br>\n', '> Paper: https://arxiv.org/abs/1910.12008 </br>\n', '\n', '\n', '## 1) Data setup:\n', '(a) Download the CelebA dataset here (http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) into the `data/` directory (if elsewhere, note the path for step b). Of the download links provided, choose `Align&Cropped Images` and download `Img/img_align_celeba/` folder, `Anno/list_attr_celeba.txt`, and `Eval/list_eval_partition.txt` to `data/`.\n', '\n', '(b) Preprocess the CelebA dataset for faster training:\n', '```\n', 'python3 preprocess_celeba.py --data_dir=/path/to/downloaded/dataset/celeba/ --out_dir=../data --partition=train\n', '```\n', '\n', 'You should run this script for `--partition=[train, val, test]` to cache all the necessary data. The preprocessed files will then be saved in `data/`.\n', '\n', 'To split the data for multiple attributes, check `notebooks/multi-attribute data and unbiased FID splits.ipynb`.\n', '\n', '## 2) Pre-train attribute classifier\n', 'For a single-attribute:\n', '```\n', 'python3 train_attribute_clf.py celeba ./results/attr_clf\n', '```\n', '\n', 'For multiple attributes, add the `--multi=True` flag.\n', '```\n', 'python3 train_attribute_clf.py celeba ./results/multi_clf -- multi=True\n', '```\n', '\n', 'Then, the trained attribute classifier will be saved in `./results/attr_clf` (`./results/multi_clf`) and will be used for downstream evaluation for generative model training. Note the path where these classifiers are saved, as they will be needed for GAN training + evaluation.\n', '\n', '\n', '## 3) Pre-train density ratio classifier\n', 'The density ratio classifier should be trained for the appropriate `bias` and `perc` setting, which can be adjusted in the script below:\n', '```\n', 'python3 get_density_ratios.py celeba celeba --perc=[0.1, 0.25, 0.5, 1.0] --bias=[90_10, 80_20, multi]\n', '```\n', 'Note that the best density ratio classifier will be saved in its corresponding directory under `./data/`. \n', '\n', '\n', '## 4) Pre-compute unbiased FID scores:\n', 'We have provided both (a) biased and (b) unbiased FID statistics in the `fid_stats/` directory.\n', '\n', '(a) `fid_stats/fid_stats_celeba.npz` contains the original activations from the *entire* CelebA dataset, as in: https://github.com/ajbrock/BigGAN-PyTorch\n', '\n', '(b) `fid_stats/unbiased_all_gender_fid_stats.npz` contains activations from the entire CelebA dataset, where the gender attribute (female, male) are balanced.\n', '\n', '(c) `fid_stats/unbiased_all_multi_fid_stats.npz` contains activations from the entire CelebA dataset, where the 4 attribute classes (black hair, other hair, female, male) are balanced.\n', '\n', 'These pre-computed FID statistics are for model checkpointing (during GAN training) and downstream evaluation of sample quality only, and should be substituted for other statistics when using a different dataset/attribute splits.\n', '\n', '\n', '## 5) Train generative model (BigGAN)\n', 'A sample script to train the model can be found in `scripts/`:\n', '\n', '`bash run_celeba_90_10_perc1.0_impweight.sh`\n', '\n', 'You should add different arguments for different model configurations. For example:\n', '(a) for the multi-attribute setting, append ` --multi 1`\n', '(b) for the equi-weighted baseline, append ` --reweight 0`\n', '(c) for the conditional baseline, append `--conditional 1 --y 1 --reweight 0`\n', '(d) for the importance-weighted model, append `--reweight 1 --alpha 1.0`\n', '\n', 'Note the argument for `--name_suffix my_experiment`, as you will need it for sampling and computing FID scores.\n', '\n', '\n', '## 6) Sample from trained model\n', 'A sample script to sample from the (trained) model can be found in `scripts/`:\n', '\n', '`bash sample_celeba_90_10_perc1.0_impweight.sh`\n', '\n', 'You can either append the argument `--load_weights name_of_weights` to load a specific set of weights, or pass in the `--name_suffix my_experiment` argument for the script to find the most recent checkpoint with the best FID.\n', '\n', '\n', '## 7) Compute FID scores\n', 'To compute FID scores after running the sampling script, (using the original Tensorflow implementation), run the following:\n', '`python3 fast_fid.py my_experiment --multi=[True,False] --n_replicates=10`\n', '\n', 'This code assumes that there are 10 sets (`n_replicates`) of 10K samples generated from the model (as per `sample.py`), and will evaluate the samples on both (a) the original FID scores and (b) unbiased FID scores (as per Step #4). `my_experiment` refers to the `--name_suffix my_experiment` parameter from Step 5.\n', '\n', '\n', '## References\n', 'If you find this work useful in your research, please consider citing the following paper:\n', '```\n', '@article{grover2019fair,\n', '  title={Fair Generative Modeling via Weak Supervision},\n', '  author={Grover, Aditya and Choi, Kristy and Singh, Trisha and Shu, Rui and Ermon, Stefano},\n', '  journal={arXiv preprint arXiv:1910.12008},\n', '  year={2019}\n', '}\n', '```\n']"
Model Fairness,txsun1997/Metric-Fairness,txsun1997,https://api.github.com/repos/txsun1997/Metric-Fairness,28,4,2,"['https://api.github.com/users/1710763616', 'https://api.github.com/users/txsun1997']",Python,2023-04-20T14:20:43Z,https://raw.githubusercontent.com/txsun1997/Metric-Fairness/main/README.md,"['# Metric Fairness: Is BERTScore Fair?\n', 'This repository contains the code, data, and pre-trained checkpoints for our EMNLP paper [BERTScore is Unfair: On Social Bias in Language Model-Based Metrics for Text Generation](https://arxiv.org/abs/2210.07626).\n', '\n', '## Quick Links\n', '\n', '- [Overview](#overview)\n', '- [Measure Metric Bias](#measure-metric-bias)\n', '- [Mitigate Metric Bias](#mitigate-metric-bias)\n', '  - [Train](#train)\n', '    - [Datasets](#datasets)\n', '  - [Test](#test)\n', '    - [Adapters](#adapters)\n', '  - [Performance Evaluation](#performance-evaluation)\n', '    - [WMT20](#wmt20)\n', '    - [REALSumm](#realsumm)\n', '- [Citation](#citation)\n', '\n', '## Overview\n', '\n', 'Pre-trained language model-based metrics (PLM-based metrics, e.g., BERTScore, MoverScore, BLEURT) have been widely used in various text generation tasks including machine translation, text summarization, etc. Compared with traditional $n$-gram-based metrics (e.g., BLEU, ROUGE, NIST), PLM-based metrics can well capture the semantic similarity between system outputs and references, and therefore achieve higher correlation with human judgements. However, it is well known that PLMs can encode a high degree of social bias. How much of the social bias in BERT is inherited by BERTScore? Will BERTScore encourage systems that generate biased text? To what extent do different PLM-based metrics carry social bias? This work presents the first systematic study on social bias in PLM-based metrics for text generation.\n', '\n', 'The following figure illustrates the impact of social bias in PLM-based metrics: If the metric is biased against some sensitive attributes (e.g., gender), generative models that express such bias will be rewarded and selected. The texts generated by these biased models may be incorporated in the corpus, further reinforcing the social bias in data. \n', '\n', '\n', '![](https://github.com/txsun1997/Metric-Fairness/blob/main/metric-bias.png)\n', '\n', 'Our work includes:\n', '\n', '1. **Measuring social bias in PLM-based metrics.** We constructed datasets and metrics to measure the bias (or unfairness) in existing PLM-based metrics including BERTScore, MoverScore, BLEURT, PRISM, BARTScore, and FrugalScore.\n', '2. **Mitigating social bias in PLM-based metrics.** We explore mitigating social bias in existing metrics by (1) replacing the backbone models with debiased ones such as [Zari models](https://github.com/google-research-datasets/Zari) and (2) training debiasing adapters on augmented data.\n', '\n', '## Measure Metric Bias\n', '\n', 'We have uploaded our constructed datasets for measuring metric bias (see [/measuring_bias/data](/measuring_bias/data)). We provide 6 datasets for evaluating social bias against different sensitive attributes including age, gender, physical appearance, race, religion, and socioeconomic status. We also provide our evaluated scores using 29 existing text generation metrics for each sample in the datasets. You can reproduce our results as follows:\n', '\n', '```bash\n', 'pip install prettytable\n', 'git clone https://github.com/txsun1997/Metric-Fairness\n', 'cd Metric-Fairness/measuring_bias\n', 'python get_bias_score.py\n', '```\n', '\n', 'If all is well, you should obtain the following results:\n', '\n', '```\n', '+-------------------------+-------+--------+---------------------+------+----------+---------------+\n', '|          metric         |  age  | gender | physical-appearance | race | religion | socioeconomic |\n', '+-------------------------+-------+--------+---------------------+------+----------+---------------+\n', '|  bartscore-bart-base-f  |  6.2  |  3.67  |         6.04        | 2.44 |   5.97   |      6.65     |\n', '|  bartscore-bart-base-p  |  6.51 |  6.5   |         7.59        | 2.6  |   7.63   |      8.0      |\n', '|  bartscore-bart-base-r  |  7.1  |  2.47  |         8.44        | 2.52 |   7.12   |      7.55     |\n', '|  bartscore-bart-large-f |  3.83 |  9.47  |         6.38        | 1.67 |   4.7    |      3.47     |\n', '|  bartscore-bart-large-p |  7.65 | 14.17  |         6.42        | 1.87 |   5.13   |      4.55     |\n', '|  bartscore-bart-large-r |  2.36 |  3.69  |         4.92        | 2.13 |   4.34   |      3.48     |\n', '|   bertscore-bert-base   |  5.68 |  8.73  |         6.36        | 1.24 |   6.2    |      7.66     |\n', '|   bertscore-bert-large  |  4.64 |  4.39  |         6.07        | 2.3  |   7.87   |      6.85     |\n', '|   bertscore-distilbert  |  5.26 |  8.36  |         4.93        | 1.94 |   6.82   |      7.64     |\n', '|  bertscore-roberta-base |  6.63 |  3.75  |         7.82        | 2.27 |   4.08   |      6.21     |\n', '| bertscore-roberta-large |  8.23 |  6.99  |         7.94        | 2.59 |   4.64   |      7.4      |\n', '|           bleu          |  2.35 |  0.1   |         0.94        | 0.19 |   0.61   |      2.79     |\n', '|     bleurt-bert-base    | 13.44 | 29.97  |        12.92        | 3.02 |  16.21   |     15.41     |\n', '|    bleurt-bert-large    | 15.07 | 27.08  |         7.98        | 4.0  |  16.18   |      14.6     |\n', '|     bleurt-bert-tiny    | 14.01 |  6.47  |        10.71        | 8.43 |   6.39   |     13.01     |\n', '|      bleurt-rembert     | 16.52 | 20.93  |         8.84        | 4.21 |  17.12   |     12.93     |\n', '|           chrf          |  3.43 |  1.23  |         1.57        | 1.89 |   1.44   |      3.46     |\n', '| frugalscore-bert-medium |  5.02 |  5.73  |         5.07        | 0.93 |   5.57   |      8.09     |\n', '|  frugalscore-bert-small |  4.9  |  7.04  |         4.64        | 0.91 |   5.82   |      8.78     |\n', '|  frugalscore-bert-tiny  |  7.96 |  3.2   |         5.27        | 1.39 |   5.96   |      7.12     |\n', '|          meteor         |  4.96 |  2.63  |         3.08        | 1.53 |   2.56   |      4.4      |\n', '|   moverscore-bert-base  |  6.06 | 11.36  |         6.69        | 3.84 |   9.63   |      7.94     |\n', '|  moverscore-bert-large  |  6.78 |  6.68  |         8.04        | 4.43 |  10.24   |      8.3      |\n', '|  moverscore-distilbert  |  7.24 | 13.24  |         4.94        | 3.35 |   9.67   |      8.59     |\n', '|           nist          |  2.2  |  0.11  |         1.03        | 0.25 |   0.54   |      1.43     |\n', '|         prism-f         |  6.69 |  7.13  |         7.48        | 1.97 |   6.79   |      4.85     |\n', '|         prism-p         |  9.1  | 14.33  |         7.05        | 2.6  |   7.06   |      6.51     |\n', '|         prism-r         |  5.1  |  3.0   |         7.13        | 2.65 |   5.92   |      4.91     |\n', '|          rouge          |  3.83 |  0.21  |         2.01        | 0.12 |   1.02   |      3.4      |\n', '+-------------------------+-------+--------+---------------------+------+----------+---------------+\n', '```\n', '\n', 'You can also evaluate the bias types (and text generation metrics) of interest instead of all of them by specifying the parameters `--bias_type` and `--metric_name`. For example,\n', '\n', '```bash\n', 'python get_bias_score.py --bias_type age gender --metric_name rouge bleu\n', '```\n', '\n', 'would result in a tiny table:\n', '\n', '```\n', '+--------+------+--------+\n', '| metric | age  | gender |\n', '+--------+------+--------+\n', '|  bleu  | 2.35 |  0.1   |\n', '| rouge  | 3.83 |  0.21  |\n', '+--------+------+--------+\n', '```\n', '\n', 'You can see the exact details of how we calculated each score by checking `metrics.py`, and if you need to calculate scores of default backbones, run\n', '\n', '```bash\n', 'cd Metric-Fairness/measuring_bias/metrics\n', 'pip install -r requirements.txt\n', 'bash metrics.sh\n', '```\n', '\n', 'and you will obtain an output file named `scores.csv` by default which contains scores on our gender bias dataset.\n', '\n', 'Also, you can calculate the polarized bias scores by running\n', '\n', '```bash\n', 'python cal_bias_score.py --polarity True\n', '```\n', 'You are expected to obtain the results reported in Table 9 in the paper.\n', '\n', 'Below is an example output of `python cal_bias_score.py` (without polarity):\n', '\n', '```\n', '+------+-------+--------+------+------+-------------+-------------+-------------+------------+--------+---------+---------+---------+-------------+-------------+-------------+-------------+\n', '| bleu | rouge | meteor | nist | chrf | bertscore_r | bertscore_p | bertscore_f | moverscore | bleurt | prism_r | prism_p | prism_f | bartscore_r | bartscore_p | bartscore_f | frugalscore |\n', '+------+-------+--------+------+------+-------------+-------------+-------------+------------+--------+---------+---------+---------+-------------+-------------+-------------+-------------+\n', '| 0.1  |  0.21 |  2.14  | 0.12 | 1.23 |     4.61    |     9.04    |     6.99    |   13.24    |  30.0  |   3.0   |  14.33  |   7.13  |     3.69    |    14.17    |     9.47    |     3.19    |\n', '+------+-------+--------+------+------+-------------+-------------+-------------+------------+--------+---------+---------+---------+-------------+-------------+-------------+-------------+\n', '```\n', '\n', '\n', '\n', '## Mitigate Metric Bias\n', '\n', '### Train\n', '\n', '#### Datasets\n', '\n', '[Download link](https://drive.google.com/drive/folders/1rqPw_h6_0CxgL4LY2LhBPMR2RODnMnv6?usp=sharing)\n', '\n', 'We collect training data based on two public sentence-pair datasets, MultiNLI [(Williams et al., 2018)](https://doi.org/10.18653/v1/n18-1101) and STS-B [(Cer et al., 2017)](http://arxiv.org/abs/1708.00055), in which each sample is comprised of a premise and a hypothesis. We perform counterfactual data augmentation (CDA) ([Zhao et al., 2018b)](https://arxiv.org/abs/1804.06876) on the sentences in MultiNLI and STS-B to construct a training set. You can download the datasets from the above link, which includes `train.tsv` for BERTScore (both BERT-base and BERT-large), BARTScore (BART-base), and BLEURT (BERT-base).\n', '\n', 'The following example shows how to add and train a debiasing adapter on the BERT-large model of BERTScore. Note that we used a single NVIDIA 3090 GPU (24GB) to perform training.\n', '\n', '```bash\n', 'cd Metric-Fairness/mitigating_bias/train/BERTScore\n', 'mkdir ./logs\n', 'pip install -r requirements.txt\n', 'INPUT_PATH=train.tsv # your training set path\n', 'python train_BERTScore.py\n', '    --model_type bert-large-uncased \\\n', '    --adapter_name debiased-bertscore \\\n', '    --lr 5e-4 \\\n', '    --warmup 0.0 \\\n', '    --batch_size 16 \\\n', '    --n_epochs 4 \\\n', '    --seed 42 \\\n', '    --device cuda \\\n', '    --logging_steps 100 \\\n', '    --data_path ${INPUT_PATH}\n', '```\n', '\n', 'After training, a debiasing adapter will be saved in `./adapter/`, and you can check more training details in `./logs` . See also [fitlog](https://fitlog.readthedocs.io/zh/latest/).\n', '\n', '### Test\n', '\n', '#### Adapters\n', '\n', '[Download link](https://drive.google.com/drive/folders/1nqTQWXtf14SXZ5pC0hK5kBa28q9h-0-y?usp=sharing)\n', '\n', ""We have trained debiasing adapters for BERTScore (both BERT-base and BERT-large), BARTScore (BART-base), and BLEURT (BERT-base), so you can download these adapters' checkpoints through the link above.\n"", '\n', 'The following example shows how to add our trained debiasing adapters to BERTScore (both BERT-base and BERT-large), BARTScore (BART-base), and BLEURT (BERT-base) , and calculate the bias scores using debiased metrics on our test set in `Metric-Fairness/mitigating_bias/test/test_data` [(WinoBias)](https://doi.org/10.18653/v1/n18-2003).\n', '\n', '```bash\n', 'cd Metric-Fairness/mitigating_bias/test\n', 'pip install -r requirements.txt\n', 'BERT_SCORE_BERT_LARGE_ADAPTER_PATH=BERTScore/BERT-large/adapter # bert_score_bert_large adapter path\n', 'BERT_SCORE_BERT_BASE_ADAPTER_PATH=BERTScore/BERT-base/adapter # bert_score_bert_base adapter path\n', 'BLEURT_BERT_BASE_ADAPTER_PATH=BLEURT/adapter # bleurt_bert_base adapter path\n', 'BART_SCORE_BART_BASE_ADAPTER_PATH=BARTScore/adapter # bart_score_bart_base adapter path\n', 'python cal_debias_scores.py\n', '    --bert_score_bert_large_adapter_path ${BERT_SCORE_BERT_LARGE_ADAPTER_PATH} \\\n', '    --bert_score_bert_base_adapter_path ${BERT_SCORE_BERT_BASE_ADAPTER_PATH} \\\n', '    --bleurt_bert_base_adapter_path ${BLEURT_BERT_BASE_ADAPTER_PATH} \\\n', '    --bart_score_bart_base_adapter_path ${BART_SCORE_BART_BASE_ADAPTER_PATH}\n', '```\n', '\n', 'Below is an example result:\n', '\n', '```\n', '+----------------------+-----------------------+------------------+----------------------+\n', '| bert_score_bert_base | bert_score_bert_large | bleurt_bert_base | bart_score_bart_base |\n', '+----------------------+-----------------------+------------------+----------------------+\n', '|         4.21         |          2.69         |      10.46       |         2.35         |\n', '+----------------------+-----------------------+------------------+----------------------+\n', '```\n', '\n', 'If without debiasing adapters, the result should be:\n', '\n', '```\n', '+----------------------+-----------------------+------------------+----------------------+\n', '| bert_score_bert_base | bert_score_bert_large | bleurt_bert_base | bart_score_bart_base |\n', '+----------------------+-----------------------+------------------+----------------------+\n', '|         8.73         |          4.39         |       30.0       |         3.67         |\n', '+----------------------+-----------------------+------------------+----------------------+\n', '```\n', '\n', 'As you can see, the attached debiasing adapter successfully mitigates bias in these metrics.\n', '\n', '### Performance Evaluation\n', '\n', '#### WMT20\n', '\n', ""The following example shows how to evaluate the original metrics' perfomance on [WMT20](https://aclanthology.org/2020.wmt-1.77/):\n"", '\n', '```bash\n', 'cd Metric-Fairness/mitigating_bias/performance_eval/WMT\n', 'pip install -r requirements.txt\n', 'python eval_bert_score.py --model_type bert-base-uncased \n', 'python eval_bert_score.py --model_type bert-large-uncased \n', 'python eval_bleurt.py --model_type Elron/bleurt-base-512\n', 'python eval_bart_score.py --model_type facebook/bart-base\n', '```\n', '\n', 'Below is an example output:\n', '\n', '```\n', '+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+---------+\n', '| cs-en | de-en | iu-en | ja-en | km-en | pl-en | ps-en | ru-en | ta-en | zh-en | average |\n', '+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+---------+\n', '| 0.746 | 0.793 | 0.663 | 0.882 | 0.971 | 0.356 | 0.928 | 0.858 | 0.833 | 0.929 |  0.796  |\n', '+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+---------+\n', '```\n', '\n', ""The following example shows how to evaluate the metrics' perfomance after attaching our debiasing adapters on [WMT20](https://aclanthology.org/2020.wmt-1.77/):\n"", '\n', '```bash\n', 'BERT_SCORE_BERT_LARGE_ADAPTER_PATH=BERTScore/BERT-large/adapter # bert_score_bert_large adapter path\n', 'BERT_SCORE_BERT_BASE_ADAPTER_PATH=BERTScore/BERT-base/adapter # bert_score_bert_base adapter path\n', 'BLEURT_BERT_BASE_ADAPTER_PATH=BLEURT/adapter # bleurt_bert_base adapter path\n', 'BART_SCORE_BART_BASE_ADAPTER_PATH=BARTScore/adapter # bart_score_bart_base adapter path\n', 'python eval_bert_score.py \n', '    --model_type bert-base-uncased \\\n', '    --adapter_path ${BERT_SCORE_BERT_BASE_ADAPTER_PATH}\n', 'python eval_bert_score.py \n', '    --model_type bert-large-uncased \\\n', '    --adapter_path ${BERT_SCORE_BERT_LARGE_ADAPTER_PATH}\n', 'python eval_bleurt.py \n', '    --model_type Elron/bleurt-base-512 \\\n', '    --adapter_path ${BLEURT_BERT_BASE_ADAPTER_PATH}\n', 'python eval_bart_score.py \n', '    --model_type facebook/bart-base \\\n', '    --adapter_path ${BART_SCORE_BART_BASE_ADAPTER_PATH}\n', '```\n', '\n', 'Below is an example of performance result:\n', '\n', '```\n', '+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+---------+\n', '| cs-en | de-en | iu-en | ja-en | km-en | pl-en | ps-en | ru-en | ta-en | zh-en | average |\n', '+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+---------+\n', '| 0.758 | 0.786 | 0.639 | 0.873 |  0.97 | 0.364 | 0.932 | 0.862 | 0.832 | 0.925 |  0.794  |\n', '+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+---------+\n', '```\n', '\n', '#### REALSumm\n', '\n', 'For the sake of time, we provide the `pkl` file, run\n', '\n', '```bash\n', 'cd Metric-Fairness/mitigating_bias/performance_eval/REALSumm\n', 'pip install -r requirements.txt\n', 'python analyse_pkls.py\n', '```\n', '\n', 'and you will get results like\n', '\n', '```\n', '+------------------------------+----------------------+------------------------------+----------------------+-------------------------------+-----------------------+--------------------------+------------------+\n', '| bart_score_bart_base_adapter | bart_score_bart_base | bert_score_bert_base_adapter | bert_score_bert_base | bert_score_bert_large_adapter | bert_score_bert_large | bleurt_bert_base_adapter | bleurt_bert_base |\n', '+------------------------------+----------------------+------------------------------+----------------------+-------------------------------+-----------------------+--------------------------+------------------+\n', '|            0.307             |        0.325         |            0.473             |        0.465         |             0.468             |         0.464         |           0.4            |      0.299       |\n', '+------------------------------+----------------------+------------------------------+----------------------+-------------------------------+-----------------------+--------------------------+------------------+\n', '```\n', '\n', '## Citation\n', '\n', 'If you use our data or code, please cite:\n', '\n', '```bibtex\n', '@inproceedings{sun2022bertscore,\n', '  title={BERTScore is Unfair: On Social Bias in Language Model-Based Metrics for Text Generation},\n', '  author={Tianxiang Sun and Junliang He and Xipeng Qiu and Xuanjing Huang},\n', '  booktitle = {Proceedings of {EMNLP}},\n', '  year={2022}\n', '}\n', '```\n', '\n']"
Model Fairness,genforce/fairgen,genforce,https://api.github.com/repos/genforce/fairgen,31,6,2,"['https://api.github.com/users/Ariostgx', 'https://api.github.com/users/zhoubolei']",Python,2023-03-11T07:12:07Z,https://raw.githubusercontent.com/genforce/fairgen/main/README.md,"['# FairGen - Improving the Fairness of Deep Generative Models without Retraining\n', '\n', '![image](./docs/assets/framework.jpg)\n', '**Figure:** *Framework of FairGen*.\n', '\n', '> **Improving the Fairness of Deep Generative Models without Retraining** <br>\n', '> Shuhan Tan, Yujun Shen, Bolei Zhou <br>\n', '> *arXiv preprint arXiv:2012.04842*\n', '\n', '[[Paper](https://arxiv.org/pdf/2012.04842.pdf)]\n', '[[Project Page](https://genforce.github.io/fairgen/)]\n', '[[Colab](https://colab.research.google.com/github/genforce/fairgen/blob/main/docs/fairgen.ipynb)]\n', '\n', 'In this repository, we propose a simple yet effective method to improve the *fairness* of image generation for a pre-trained GAN model *without retraining*.\n', 'We utilize the recent work of *GAN interpretation* and a *Gaussian Mixture Model (GMM)* to support the sampling of latent codes for producing images with a more fair attribute distribution.\n', 'We call this method *FairGen*.\n', 'Experiments show that *FairGen* can substantially improve the fairness of image generation. The images generated from our method are further applied to reveal and quantify the biases in commercial face classifiers and face super-resolution model. Some results are shown as follows.\n', '\n', '\n', '## Fair Image Generation\n', '\n', 'Attributes: Age-Eyeglasses\n', '![image](./docs/assets/age_eyeglasses.jpg)\n', '\n', 'Attributes: Gender-Black Hair\n', '![image](./docs/assets/gender_black_hair.jpg)\n', '\n', '## Identifying Bias in Existing Models\n', '\n', 'Mis-classification in Commercial Gender Classifiers\n', '![image](./docs/assets/api.jpg)\n', '\n', 'Attribute Alternation by a Face Super-resolution Model\n', '![image](./docs/assets/PULSE.jpg)\n', '\n', '## BibTeX\n', '\n', '```bibtex\n', '@article{tan2020fairgen,\n', '  title   = {Improving the Fairness of Deep Generative Models without Retraining},\n', '  author  = {Tan, Shuhan and Shen, Yujun and Zhou, Bolei},\n', '  journal = {arXiv preprint arXiv:2012.04842},\n', '  year    = {2020}\n', '}\n', '```\n', '\n', '## Code Coming Soon']"
Model Fairness,Hackathonners/vania,Hackathonners,https://api.github.com/repos/Hackathonners/vania,77,6,5,"['https://api.github.com/users/fntneves', 'https://api.github.com/users/hgg', 'https://api.github.com/users/MarcoCouto', 'https://api.github.com/users/djcouto', 'https://api.github.com/users/mrjbq7']",Python,2023-03-07T22:38:40Z,https://raw.githubusercontent.com/Hackathonners/vania/master/README.md,"['# Project Vania - A Fair Distributor\n', '**Fair Distributor** is a module which [fairly](#our-meaning-of-fairness) distributes a list of arbitrary **objects** through a set of **targets**.\n', '\n', 'To be more explicit, this module considers 3 key components:\n', '* **object**: some kind of entity that can be assigned to something.\n', '* **target**: the entity that will have one (or more) **objects** assigned to it.\n', '* **weight**: represents the cost of assigning a given **object** to a **target**.\n', '\n', 'A collection of each of these components is given as input to the module.\n', 'Using linear programming, the **weights** of the **targets** relative to the **objects** are taken into consideration and used to build the constraints of an Integer Linear Programming (ILP) model. An ILP solver is then used, in order to distribute the **objects** through the **targets**, in the *fairest way possible*.\n', '\n', 'For instance, this module can be used to fairly distribute:\n', '* A set of tasks (objects) among a group of people (targets) according to their preferences to do each task (weights).\n', '* A set of projects (objects) among development teams (targets) according to their skill-level (weights) on the required skills for each project.\n', '\n', '\n', '## Our Meaning of Fairness\n', '\n', 'We define **Fairness** as:\n', ' * The total **weight** of distributing all **objects** through the **targets** should be minimal.\n', 'This enforces that the least amount of shared effort is made.\n', '\n', '_Optionally_, the following rule can be applied (enabled by default):\n', ' * The difference between the individual **weight** of each **target** is minimal.\n', 'This enforces the least amount of individual effort.\n', '\n', '## Documentation\n', '\n', 'You can find all the documentation in the following link:\n', 'https://hackathonners.github.io/vania\n', '\n', '## Download and Install\n', '\n', 'Install the latest stable version of this module:\n', '\n', '    $ pip install vania\n', '\n', 'To work with the source code, clone this repository:\n', '\n', '    $ git clone git://github.com/hackathonners/vania.git\n', '\n', '## Usage\n', 'To start using the **Fair Distributor**, you need first to import it, by doing this:\n', '```python\n', 'from vania.fair_distributor import FairDistributor\n', '```\n', 'Now, just feed it with your problem variables, and ask for the solution.\n', 'To better explain how you can do it, lets consider a specific example.\n', '\n', 'Suppose that you are managing a project, which contains **4** tasks: _Front-end Development_, _Back-end Development_, _Testing_, and _Documentation_.\n', 'There is a need to assign these **4** tasks through a set of **3** teams: _A_, _B_ and _C_.\n', 'You have the expected number of hours each team needs to finish each task:\n', '\n', '|        |*Front-end Development*|*Back-end Development*|*Testing*|*Documentation*| \n', '|--------|-----------------------|----------------------|---------|---------------|\n', '|_Team A_|          1h           |          2h          |    3h   |       2h      |\n', '|_Team B_|          3h           |          1h          |    4h   |       2h      |\n', '|_Team C_|          3h           |          4h          |    1h   |       1h      |\n', '\n', 'Here, we consider tasks as **objects**, teams as **targets** and the hours expressed in each cell are the **weights**.\n', '\n', 'It is necessary to create a data structure for each component. **Objects** and **targets** are lists, while **weights** is a collection, which contains for each target the cost of assigning every object to it, and is represented as a matrix.\n', 'The structures for this example would be as follow:\n', '\n', '```python\n', ""targets = ['Team A', 'Team B', 'Team C']\n"", ""objects = ['Front-end Development', 'Back-end Development', 'Testing', 'Documentation']\n"", 'weights = [\n', '    [1, 2, 3, 2],\t\t# hours for Team A to complete each task\n', '    [3, 1, 4, 2],\t\t# hours for Team B to complete each task\n', '    [3, 4, 1, 1]\t\t# hours for Team C to complete each task\n', ']\n', '```\n', '\n', 'Now, just feed the **Fair Distributor** with all the components, and ask for the solution:\n', '```python\n', 'distributor = FairDistributor(targets, objects, weights)\n', 'print(distributor.distribute())\n', '```\n', '\n', 'And here is the solution!\n', '```python\n', '# Output\n', '{\n', ""    'Team A': ['Front-end Development'],        # Team A does the Front-end Development\n"", ""    'Team B': ['Back-end Development'],         # Team B does the Back-end Development\n"", ""    'Team C': ['Testing', 'Documentation']      # Team C does the Testing and Documentation\n"", '}\n', '```\n', '\n', 'Here is the final code of this example:\n', '```python\n', 'from vania.fair_distributor import FairDistributor\n', '\n', ""targets = ['Team A', 'Team B', 'Team C']\n"", ""objects = ['Front-end Development', 'Back-end Development', 'Testing', 'Documentation']\n"", 'weights = [\n', '    [1, 2, 3, 2],\t\t# hours for Team A to complete each task\n', '    [3, 1, 4, 2],\t\t# hours for Team B to complete each task\n', '    [3, 4, 1, 1]\t\t# hours for Team C to complete each task\n', ']\n', '\n', 'distributor = FairDistributor(targets, objects, weights)\n', 'print(distributor.distribute())\n', '```\n', '\n', '## Contributions and Bugs\n', '\n', 'Found a bug and wish to report it? You can do so here: https://github.com/Hackathonners/vania/issues.\n', ""If you'd rather contribute to this project with the bugfix, awesome! Simply Fork the project on Github and make a Pull Request.\n"", '\n', ""Please tell us if you are unfamiliar with Git or Github and we'll definitely help you make your contribution.\n"", '\n', '## Authors\n', '\n', 'Hackathonners is **_a group of people who build things_**.\n', '\n', 'You can check us out at http://hackathonners.org.\n', '\n', '## License\n', '\n', 'The Fair Distributor is licensed under the [MIT License](https://opensource.org/licenses/MIT).\n', '\n', 'Copyright (C) 2017 Hackathonners\n']"
Model Fairness,heyaudace/ml-bias-fairness,heyaudace,https://api.github.com/repos/heyaudace/ml-bias-fairness,16,7,1,['https://api.github.com/users/heyaudace'],Python,2022-10-30T05:34:15Z,https://raw.githubusercontent.com/heyaudace/ml-bias-fairness/master/README.md,"['# ml-bias-fairness\n', '\n', 'Data collection is an expensive process that only large, profitable companies have the means to afford. This leaves smaller, less profitable organizations with no choice but to re-use data, which in some cases might have been collected for a different purpose.\n', '\n', 'In addition, all demographics are not always equally represented in the data.  There is more data about individuals from major demographics, and less or no data about people from minorities. This leads to a higher error rate for individuals of minority demographics.\n', '\n', 'In this work, we explore different steps involved in manipulating data and choosing the right algorithm to create unbiased Machine Learning applications. The work was intended and is highly applicable in developing countries where there are not enough resources for data collection, and most of the time the demographics representing the target users are not well represented in the training data.\n', '\n']"
Model Fairness,yuji-roh/fairbatch,yuji-roh,https://api.github.com/repos/yuji-roh/fairbatch,15,4,1,['https://api.github.com/users/yuji-roh'],Python,2023-04-23T05:46:44Z,https://raw.githubusercontent.com/yuji-roh/fairbatch/main/README.md,"['# FairBatch: Batch Selection for Model Fairness\n', '\n', '#### Authors: Yuji Roh, Kangwook Lee, Steven Euijong Whang, and Changho Suh\n', '#### In Proceedings of the 9th International Conference on Learning Representations (ICLR), 2021\n', '----------------------------------------------------------------------\n', '\n', 'This directory is for simulating FairBatch on the synthetic dataset.\n', 'The program needs PyTorch and Jupyter Notebook.\n', '\n', 'The directory contains a total of 4 files and 1 child directory: \n', '1 README, 2 python files, 1 jupyter notebook, \n', 'and the child directory containing 6 numpy files for synthetic data.\n', '\n', '\n', '#### To simulate FairBatch, please use the jupyter notebook in the directory.\n', '\n', 'The jupyter notebook will load the data and train the models with three \n', 'different fairness metrics: equal opportunity, equalized odds, and demographic parity.\n', '\n', 'Each training utilizes the FairBatch sampler, which is defined in FairBatchSampler.py.\n', 'The pytorch dataloader serves the batches to the model via the FairBatch sampler. \n', 'Experiments are repeated 10 times each.\n', 'After the training, the test accuracy and fairness will be shown.\n', '\n', 'The two python files are models.py and FairBatchSampler.py.\n', 'The models.py file contains a logistic regression architecture and a test function.\n', 'The FairBatchSampler.py file contains two classes: CustomDataset and FairBatch. \n', 'The CustomDataset class defines the dataset, and the FairBatch class implements \n', 'the algorithm of FairBatch as described in the paper.\n', '\n', 'More detailed explanations of each component can be found in the code as comments.\n', 'Thanks!\n']"
Model Fairness,rd-alliance/FAIR-data-maturity-model-WG,rd-alliance,https://api.github.com/repos/rd-alliance/FAIR-data-maturity-model-WG,12,3,1,['https://api.github.com/users/bahimc'],,2022-06-29T16:14:31Z,https://raw.githubusercontent.com/rd-alliance/FAIR-data-maturity-model-WG/master/README.md,"['# FAIR-data-maturity-model-WG\n', '## Introduction\n', 'Welcome to the repository supporting the work that will be carried out by the RDA FAIR data maturity Working Group. This Working Group will build on top and combine the most salient characteristics of existing efforts for measuring the readiness and implementation level of a dataset vis-à-vis the FAIR data principles.\n', '\n', 'For futher information you can read the [case statement](https://www.rd-alliance.org/group/fair-data-maturity-model-wg/case-statement/fair-data-maturity-model-wg-case-statement) on the RDA Website.\n', '\n', '### Context, objectives and scope\n', 'Please have a look at the [three slides presentation](https://github.com/RDA-FAIR/FAIR-data-maturity-model-WG/blob/master/Context%2C%20objectives%20and%20scope.pdf) to better understand the situation.  \n', '\n', '## How to contribute\n', 'Thanks for taking the time to contribute!\n', '\n', 'Each specific issue/thread treats a different topic. The overriding goal is to get consensus about every proposition that is made during the workshops and on this very repository. Navigate through the issues and comment to give your opinion about the topic of your choice. \n', '\n', 'Besides, any problems encountered, or suggestions, questions, etc. considered within scope can be submitted as issue on this very repository.  \n', '\n', '## Additional material\n', 'Here below you can find the presentations and reports for each and every workshop\n', '\n', '- [Workshop #1](https://www.rd-alliance.org/workshop-1)\n', '- [Workshop #2](https://www.rd-alliance.org/workshop-2)\n', '- [Workshop #3](https://www.rd-alliance.org/workshop-3)\n', '- [Workshop #4](https://www.rd-alliance.org/workshop-4)\n', '\n', '## Structure of the repository\n', '- /results of preliminary analysis\n', '\n', '\n', '\n']"
Model Fairness,TangJiakai/RecBole-FairRec,TangJiakai,https://api.github.com/repos/TangJiakai/RecBole-FairRec,15,4,2,"['https://api.github.com/users/TangJiakai', 'https://api.github.com/users/peteryang1031']",Python,2023-03-17T01:31:22Z,https://raw.githubusercontent.com/TangJiakai/RecBole-FairRec/master/README.md,"['# RecBole-FairRec\n', '\n', '![logo](asset/logo.png)\n', '\n', '**RecBole-FairRec** is a library toolkit built upon [RecBole](https://recbole.io) for reproducing and developing fairness-aware recommendation.\n', '\n', '## Highlights\n', '\n', '- **Easy-to-use**: Our library shares unified API and input(atomic files) as RecBole.\n', '- **Conveniently learn and compare**: Our library provides several fairess-metrics and frameworks for learning and comparing.\n', '- **Extensive FairRec library**: Recently proposed fairness-aware algorithms can be easily equipped in our library.\n', '\n', '## Requirements\n', '\n', '```\n', 'python>=3.7.0\n', 'recbole>=1.0.1\n', 'numpy>=1.20.3\n', 'torch>=1.11.0\n', 'tqdm>=4.62.3\n', '```\n', '\n', '## Quick-Start\n', '\n', 'With the source code, you can use the provided script for initial usage of our library:\n', '\n', '```\n', 'python run_recbole.py\n', '```\n', 'If you want to change the models or datasets, just run the script by setting additional command parameters:\n', '```\n', 'python run_recbole.py -m [model] -d [dataset] -c [config_files]\n', '```\n', '\n', '## Implement Models\n', '\n', 'We list the models that we have implemented up to now:\n', '\n', '- [FOCF](recbole/model/fair_recommender/focf.py) from Sirui Yao et al:[Beyond Parity：Fairness Objectives for Collaborative Filtering](https://proceedings.neurips.cc/paper/2017/hash/e6384711491713d29bc63fc5eeb5ba4f-Abstract.html)(NIPS 2017). Note: We implement this model with ranking-based metrics, e.g. NDCG@K.\n', '- PFCN from Yunqi Li et al:[Towards Personalized Fairness based on Causal Notion](https://dl.acm.org/doi/abs/10.1145/3404835.3462966?casa_token=zzHePKuKP6AAAAAA:YzZp_qUbzsgd3TXWCAGSRAfEHO2oM0_BuWZ5uZlfj_rudqKGYq8douOaZ0GoizxP54jtz3JDFw725xo)(SIGIR 2021)\n', '  - [PFCN_MLP](recbole/model/fair_recommender/pfcn_mlp.py)\n', '  - [PFCN_BiasedMF](recbole/model/fair_recommender/pfcn_biasedmf.py)\n', '  - [PFCN_DMF](recbole/model/fair_recommender/pfcn_dmf.py)\n', '  - [PFCN_PMF](recbole/model/fair_recommender/pfcn_pmf.py)\n', '- FairGo from Wu Le et al:[Learning Fair Representations for Recommendation: A Graph-based Perspective](https://dl.acm.org/doi/abs/10.1145/3442381.3450015?casa_token=MACP_5U-E6sAAAAA:L-dsEbdusWfmzF06OnATJhF2OXbjfu6el37nC-cGMjev4jGH_TBUedXyAhpfcBMyCyhyxOxLQkxqe_w) (WWW 2021) \n', '  - [FairGo_PMF(WAP,LBA,LVA)](recbole/model/fair_recommender/fairgo_pmf.py)\n', '  - [FairGo_GCN(WAP,LBA,LVA)](recbole/model/fair_recommender/fairgo_gcn.py)\n', '- [NFCF](recbole/model/fair_recommender/nfcf.py) from Rashidul Islam et al:[Debiasing career recommendations with neural fair collaborative filtering](https://dl.acm.org/doi/abs/10.1145/3442381.3449904?casa_token=ZzbZbC-Fn_oAAAAA:6KCSThLs7UsT9s0ZzeSryT3Mry067KeTiNdurfa9Q9UHWY7fLGgmjPtQy9i1zU1Yqm4Xf46NVYVuu40) (WWW 2021) \n', '\n', '## Datasets\n', '\n', ' The datasets used can be downloaded from [Datasets Link](https://drive.google.com/drive/folders/1OkDVEqetvOrtbuWebxl4y1JlZ_YjjfWj).\n', '\n', '# Hyper-parameters\n', 'We train the models with the default parameter settings, suggested in their original paper.[[link]](results/ml-1m.md)\n', '\n', '## The Team\n', 'RecBole-FairRec is developed and maintained by members from [RUCAIBox](http://aibox.ruc.edu.cn/), the main developers is Jiakai Tang ([@Tangjiakai](https://github.com/TangJiakai)).\n', '\n', '## Acknowledgement\n', '\n', 'The implementation is based on the open-source recommendation library [RecBole](https://github.com/RUCAIBox/RecBole).\n', '\n', 'Please cite the following paper as the reference if you use our code or processed datasets.\n', '\n', '```\n', '@inproceedings{zhao2021recbole,\n', '  title={Recbole: Towards a unified, comprehensive and efficient framework for recommendation algorithms},\n', '  author={Wayne Xin Zhao and Shanlei Mu and Yupeng Hou and Zihan Lin and Kaiyuan Li and Yushuo Chen and Yujie Lu and Hui Wang and Changxin Tian and Xingyu Pan and Yingqian Min and Zhichao Feng and Xinyan Fan and Xu Chen and Pengfei Wang and Wendi Ji and Yaliang Li and Xiaoling Wang and Ji-Rong Wen},\n', '  booktitle={{CIKM}},\n', '  year={2021}\n', '}\n']"
Model Fairness,chenchongthu/ENMF,chenchongthu,https://api.github.com/repos/chenchongthu/ENMF,140,28,1,['https://api.github.com/users/chenchongthu'],Python,2023-03-11T14:14:01Z,https://raw.githubusercontent.com/chenchongthu/ENMF/master/README.md,"['# ENMF\n', '\n', 'This is our implementation of Efficient Neural Matrix Factorization, which is a basic model of the paper:\n', '\n', '\n', '\n', '*Chong Chen, Min Zhang, Chenyang Wang, Weizhi Ma, Minming Li, Yiqun Liu and Shaoping Ma. 2019. [An Efficient Adaptive Transfer Neural Network for Social-aware Recommendation.](http://www.thuir.cn/group/~mzhang/publications/SIGIR2019ChenC.pdf) \n', ""In SIGIR'19.*\n"", '\n', '\n', 'This is also the codes of the TOIS paper:\n', '\n', '*Chong Chen, Min Zhang, Yongfeng Zhang, Yiqun Liu and Shaoping Ma. 2020. [Efficient Neural Matrix Factorization without Sampling for Recommendation.](https://chenchongthu.github.io/files/TOIS_ENMF.pdf) \n', 'In TOIS Vol. 38, No. 2, Article 14.*\n', '\n', 'The slides of this work has been uploaded. A chinese version instruction can be found at [Blog](https://zhuanlan.zhihu.com/p/107761829), and the video presentation can be found at [Demo](https://www.bilibili.com/video/BV1Z64y1u7GK?from=search&seid=10581986304255794319).\n', '\n', ""**Please cite our SIGIR'19 paper or TOIS paper if you use our codes. Thanks!**\n"", '\n', '```\n', '@inproceedings{chen2019efficient,\n', '  title={An Efficient Adaptive Transfer Neural Network for Social-aware Recommendation},\n', '  author={Chen, Chong and Zhang, Min and Wang, Chenyang and Ma, Weizhi and Li, Minming and Liu, Yiqun and Ma, Shaoping},\n', '  booktitle={Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},\n', '  pages={225--234},\n', '  year={2019},\n', '  organization={ACM}\n', '}\n', '```\n', '```\n', '@article{10.1145/3373807, \n', 'author = {Chen, Chong and Zhang, Min and Zhang, Yongfeng and Liu, Yiqun and Ma, Shaoping}, \n', 'title = {Efficient Neural Matrix Factorization without Sampling for Recommendation}, \n', 'year = {2020}, \n', 'issue_date = {January 2020}, \n', 'publisher = {Association for Computing Machinery}, \n', 'volume = {38}, \n', 'number = {2}, \n', 'issn = {1046-8188}, \n', 'url = {https://doi.org/10.1145/3373807}, \n', 'doi = {10.1145/3373807}, \n', 'journal = {ACM Trans. Inf. Syst.}, \n', 'month = jan, \n', 'articleno = {Article 14}, \n', 'numpages = {28}\n', '}\n', '```\n', '\n', 'Author: Chong Chen (cstchenc@163.com)\n', '\n', '## Environments\n', '\n', '- python\n', '- Tensorflow\n', '- numpy\n', '- pandas\n', '\n', '\n', '## Example to run the codes\t\t\n', '\n', 'Train and evaluate the model:\n', '\n', '```\n', 'python ENMF.py\n', '```\n', '## Suggestions for parameters\n', '\n', 'Two important parameters need to be tuned for different datasets, which are:\n', '\n', '```\n', ""parser.add_argument('--dropout', type=float, default=0.7,\n"", ""                        help='dropout keep_prob')\n"", ""parser.add_argument('--negative_weight', type=float, default=0.1,\n"", ""                        help='weight of non-observed data')\n"", '```\n', '                        \n', 'Specifically, we suggest to tune ""negative_weight"" among \\[0.001,0.005,0.01,0.02,0.05,0.1,0.2,0.5]. Generally, this parameter is related to the sparsity of dataset. If the dataset is more sparse, then a small value of negative_weight may lead to a better performance.\n', '\n', '\n', 'Generally, the performance of our ENMF is better than existing state-of-the-art recommendation models like NCF, CovNCF, CMN, and NGCF. You can also contact us if you can not tune the parameters properly.\n', '\n', '## Comparison with the most recent methods （updating）\n', '\n', 'Do the ""state-of-the-art"" recommendation models **really perform well?** If you want to see more comparison between our ENMF and any ""state-of-the-art"" recommendation models, feel free to propose an issue.\n', '\n', '### 1. LightGCN (SIGIR 2020) [LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation](http://staff.ustc.edu.cn/~hexn/papers/sigir20-LightGCN.pdf).\n', '\n', 'To be consistent with LightGCN, we use the same evaluation metrics (i.e., `Recall@K` and `NDCG@K`), use the same data Yelp2018 released in LightGCN (https://github.com/kuandeng/LightGCN).\n', '\n', 'The parameters of our ENMF on Yelp2018 are as follows:\n', '```\n', ""parser.add_argument('--dropout', type=float, default=0.7,\n"", ""                        help='dropout keep_prob')\n"", ""parser.add_argument('--negative_weight', type=float, default=0.05,\n"", ""                        help='weight of non-observed data')\n"", '```\n', 'Dataset: Yelp2018\n', '\n', '|    Model    | Recall@20 | NDCG@20 |\n', '| :---------: | :-------: | :----------: |\n', '|     NGCF    |  0.0579   |    0.0477    |  \n', '|     Mult-VAE     |  0.0584   |    0.0450    | \n', '|    GRMF    |  0.0571   |    0.0462    | \n', '|   LightGCN |  0.0649   |    **0.0530**    |\n', '|   ENMF |  **0.0650**   |    0.0515    |\n', '\n', '### 2. NBPO (SIGIR 2020) [Sampler Design for Implicit Feedback Data by Noisy-label Robust Learning](https://doi.org/10.1145/3397271.3401155). \n', 'This paper designs an adaptive sampler based on noisy-label robust learning for implicit feedback data. To be consistent with NBPO, we use the same evaluation metrics (i.e., `F1@K`, `NDCG@K`), use the same data Amazon-14core released in NBPO (https://github.com/Wenhui-Yu/NBPO). For fair comparison, we also set the embedding size as 50, which is utilized in the NBPO work.\n', '\n', 'The parameters of our ENMF on Amazon-14core are as follows:\n', '```\n', ""parser.add_argument('--dropout', type=float, default=0.2,\n"", ""                        help='dropout keep_prob')\n"", ""parser.add_argument('--negative_weight', type=float, default=0.2,\n"", ""                        help='weight of non-observed data')\n"", '```\n', 'Dataset: Amazon-14core\n', '\n', '|    Model    | F1@5 | F1@10 |F1@20| NDCG@5 | NDCG@10 |NDCG@20|\n', '| :---------: | :-------: | :----------: | :---------: | :-------: | :----------: | :----------: |\n', '|     BPR    | 0.0326| 0.0317| 0.0275|0.0444| 0.0551| 0.0680| \n', '|     NBPO     |  0.0401| 0.0357| 0.0313|0.0555| 0.0655| 0.0810|\n', '|   ENMF |  **0.0419**   |    **0.0388**    |**0.0314**|**0.0566**|**0.0698**|**0.0823**|\n', '\n', '### 3. LCFN (ICML 2020)[Graph Convolutional Network for Recommendation with Low-pass Collaborative Filters](https://arxiv.org/pdf/2006.15516v1.pdf)\n', 'To be consistent with LCFN, we use the same evaluation metrics (i.e., `F1@K`, `NDCG@K`), use the same data Movlelens-1m released in LCFN (https://github.com/Wenhui-Yu/LCFN). For fair comparison, we also set the embedding size as 128, which is utilized in the LCFN work.\n', '\n', 'The parameters of our ENMF on Movielens-1m (ml-lcfn) are as follows:\n', '```\n', ""parser.add_argument('--dropout', type=float, default=0.5,\n"", ""                        help='dropout keep_prob')\n"", ""parser.add_argument('--negative_weight', type=float, default=0.5,\n"", ""                        help='weight of non-observed data')\n"", '```                       \n', '\n', 'Dataset: Movielens-1m (ml-lcfn)\n', '\n', '|    Model    | F1@5 | F1@10 |F1@20| NDCG@5 | NDCG@10 |NDCG@20|\n', '| :---------: | :-------: | :----------: | :---------: | :-------: | :----------: | :----------: |\n', '|     GCMC    | 0.1166| 0.1437| 0.1564|0.2411| 0.2361| 0.2496| \n', '|     NGCF     |  0.1153| 0.1425| 0.1582|0.2367| 0.2347| 0.2511|\n', '|     SCF     |  0.1189| 0.1451| 0.1600|0.2419| 0.2398| 0.2560|\n', '|     CGMC     |  0.1179| 0.1431| 0.1573|0.2408| 0.2372| 0.2514|\n', '|     LCFN     |  0.1213| 0.1482| 0.1625|0.2427| 0.2429| 0.2603|\n', '|   ENMF |  **0.1239**   |    **0.1512**    |**0.1640**|**0.2457**|**0.2475**|**0.2656**|\n', '\n', '\n', '### 4. DHCF (KDD 2020)[Dual Channel Hypergraph Collaborative Filtering](http://gaoyue.org/paper/shuyi_KDD_final.pdf)\n', 'To be consistent with DHCF, we use the same evaluation metrics (i.e., `Precision@K`, `Recall@K`), use the same data CiteUlike-A (thanks for the authors of DHCF who kindly provide the dataset). For fair comparison, we also set the embedding size as 64, which is utilized in the DHCF work.\n', '\n', 'The parameters of our ENMF on CiteUlike-A are as follows:\n', '```\n', ""parser.add_argument('--dropout', type=float, default=0.5,\n"", ""                        help='dropout keep_prob')\n"", ""parser.add_argument('--negative_weight', type=float, default=0.02,\n"", ""                        help='weight of non-observed data')\n"", '```        \n', '\n', 'Dataset: CiteUlike-A\n', '\n', '\n', '|    Model    | Precision@20 | Recall@20 |\n', '| :---------: | :-------: | :----------: |\n', '|     BPR    | 0.0330| 0.0124|\n', '|     GCMC     |  0.0317| 0.0103|\n', '|     PinSage    |  0.0508| 0.0194|\n', '|     NGCF     |  0.0517| 0.0193|\n', '|     DHCF     |  0.0635| 0.0249|\n', '|   ENMF |  **0.0748**   |    **0.0280**    |\n', '\n', '### 5. SRNS (NeurIPS 2020)[Simplify and Robustify Negative Sampling for Implicit Collaborative Filtering](https://arxiv.org/pdf/2009.03376.pdf)\n', '\n', 'This work proposes a simplified and robust negative sampling approach SRNS for implicit CF. The authors have compared their SRNS method with our ENMF in the original paper. However, we reran the experiment and got some **different** results.\n', '\n', 'To be consistent with SRNS, we use the same evaluation metrics (i.e., `NDCG@K`, `Recall@K`), use the same data Movlelens-1m released in SRNS (https://github.com/dingjingtao/SRNS). For fair comparison, we also set the embedding size as 32, which is utilized in the SRNS work. \n', '\n', 'The parameters of our ENMF on Movielens-1m(ml-srns) are as follows:\n', '```\n', ""parser.add_argument('--dropout', type=float, default=0.9,\n"", ""                        help='dropout keep_prob')\n"", ""parser.add_argument('--negative_weight', type=float, default=0.3,\n"", ""                        help='weight of non-observed data')\n"", '```        \n', '\n', 'Dataset: Movielens-1m (ml-srns)\n', '\n', '| Model   | N@1        | N@3        | R@3        |\n', '| ------- | ---------- | ---------- | ---------- |\n', '| Uniform | 0.1744     | 0.2846     | 0.3663     |\n', '| NNCF    | 0.0831     | 0.1428     | 0.1873     |\n', '| AOBPR   | 0.1782     | 0.2907     | 0.3749     |\n', '| IRGAN   | 0.1763     | 0.2878     | 0.3706     |\n', '| RNS-AS  | 0.1810     | 0.2950     | 0.3801     |\n', '| AdvIR   | 0.1792     | 0.2889     | 0.3699     |\n', '| ENMF (reported in the srns paper)   | 0.1846     | 0.2970     | 0.3804     | (inaccurate results)\n', '| SRNS    | 0.1911     | 0.3056     | 0.3907     |\n', '| ENMF (our)  | **0.1917**     | **0.3124**     | **0.4016**    |\n', '\n']"
Model Fairness,lingjuanlv/FPPDL,lingjuanlv,https://api.github.com/repos/lingjuanlv/FPPDL,27,9,1,['https://api.github.com/users/lingjuanlv'],Python,2023-04-22T03:07:01Z,https://raw.githubusercontent.com/lingjuanlv/FPPDL/master/README.md,"['# FPPDL\n', 'code for TPDS paper ""Towards Fair and Privacy-Preserving Federated Deep Models""! Folder ""dpgan"" is used to generate DPGAN samples on each party!\n', '\n', '# How to run:\n', 'th fppdl_tpds.lua -dataset mnist -model deep -slevel 1 -imbalanced 1 -netSize 4 -nepochs 100 -local_nepochs 5 -batchSize 10 -learningRate 0.15 -taskID mnist_deep_p4e100_imbalanced -shardID mnist_p4_imbalanced -run run1 -pretrain 1 -credit_fade 1\n', '\n', '# How to analyze fairness:\n', 'All logs will be dumped into folder ""logs"". Process log and analyze fairness as follows:\n', '```\n', '1. X axis: standalone accuracy \n', 'grep ""standalone"" logs/fppdl_mnist_deep_p4e100_slevel01_imbalanced_IID1_pretrain1_localepoch5_localbatch10_lr0.15_run1_tpds.log >1.log\n', ""awk '{print $NF}' ORS=', ' 1.log\n"", 'x=[0.8528, 0.8895, 0.7765, 0.8828]\n', '2. Y axis: final accuracy \n', 'grep ""final test acc"" logs/fppdl_mnist_deep_p4e100_slevel01_imbalanced_IID1_pretrain1_localepoch5_localbatch10_lr0.15_run1_tpds.log >1.log\n', ""awk '{print $NF}' ORS=', ' 1.log\n"", 'y=[0.8874, 0.9191, 0.8158, 0.9118]\n', '3. Finally, using scipy.stats.pearsonr(x,y)=0.9996588631722703 to calculate fairness.\n', '```\n', '\n', '# Requirements:\n', '- torch7, download from http://torch.ch/\n', '- python3\n', '\n', '# Bibtex\n', 'Remember to cite the following papers if you use any part of the code:\n', '```\n', '@article{lyu2020towards,\n', '  title={Towards Fair and Privacy-Preserving Federated Deep Models},\n', '  author={Lyu, Lingjuan and Yu, Jiangshan and Nandakumar, Karthik and Li, Yitong and Ma, Xingjun and Jin, Jiong and Yu, Han and Ng, Kee Siong},\n', '  journal={IEEE Transactions on Parallel and Distributed Systems},\n', '  volume={31},\n', '  number={11},\n', '  pages={2524--2541},\n', '  year={2020},\n', '  publisher={IEEE}\n', '}\n', '```\n']"
Model Fairness,UCLA-StarAI/FairPC.jl,UCLA-StarAI,https://api.github.com/repos/UCLA-StarAI/FairPC.jl,5,3,3,"['https://api.github.com/users/MhDang', 'https://api.github.com/users/yoojungchoi', 'https://api.github.com/users/guyvdbroeck']",Python,2023-03-02T13:48:21Z,https://raw.githubusercontent.com/UCLA-StarAI/FairPC.jl/main/README.md,"['# Learn Fair PC\n', '\n', 'This repo contains the code and experiments from the paper ""[Group Fairness by Probabilistic Modeling with Latent Fair Decisions](http://starai.cs.ucla.edu/papers/ChoiAAAI21.pdf)"", published in AAAI 2021.\n', '\n', '\n', '## Files\n', '\n', '```\n', '  baselines/    Python scripts to reproduce `Reweight`, `Reduction` and `FairLR`.\n', '  bin/          Runnable julia scripts (see below).\n', '  circuits/     Learned circuits in experiments.\n', '  data/         Datasets used in the experiments.\n', '  scripts/      Helper files to generate experiments scripts.\n', '  src/          The source code for the algorithm.\n', '  Project.toml  This file specifies required julia environment.\n', '  README.md     This is this file.\n', '```\n', '\n', '## Installation\n', '\n', '1. Julia version 1.7\n', '\n', '2. Run commands with flag `--project` will automatically use the packages specified in `Project.toml`. See belows scripts for examples.\n', '## Experiments\n', '\n', '### Usage\n', '\n', '- Run `bin/learn.jl` with `--help` argument to see the usage message. \n', 'Most of the options have default values. The following are some arguments need to be manully set:\n', '\n', '```\n', 'positional arguments:\n', '  dataset               dataset name, in {compas, adult, german, synthetic}\n', 'optional arguments:\n', '  --sensitive_variable  sensitive variable of current data set, e.g.,{Ethnic_Code_Text_, sex, S}\n', '  --fold                fold id for k-fold cross validation, in [1:10]\n', '  --struct_type         indicate structure constrains of probability distributions, in {FairPC, TwoNB, NlatPC, LatNB}\n', '  --num_X               number of non sensitive features in synthetic data set setting, in [10:30]\n', '```\n', '\n', '- Some sample scripts\n', '\n', '```\n', '$  julia --project bin/learn.jl compas --exp-id 1  --dir ""exp/compas/1"" --struct_type ""FairPC""  --sensitive_variable ""Ethnic_Code_Text_""  --fold 1 \n', '$  julia --prroject bin/learn.jl synthetic --exp-id 2  --dir ""exp/synthetic/2"" --struct_type ""FairPC""  --num_X 10  --sensitive_variable ""S""  --fold 1 \n', '```\n', '\n', '- To generate multiple scripts and run batches of experiments in parallel, run the following for real-world dataset and synthetic dataset respectively:\n', '\n', '``` \n', '$ julia --project bin/gen_exp.jl scripts/json/realworld-fair.json \n', '$ julia --project bin/gen_exp.jl scripts/json/synthetic-fair.json\n', '```\n', 'you can also change `dir` in file `*.json` to the output directory you want.\n', '\n', '### Baselines\n', '- For `TowNB`, `LatNB`, and `NlatPC`, see above.\n', '- For `Reduction`, `Reweight`, and `FairLR` methods, run `fair_reduction.py`, `reweight.py` or `fair_lr.py` respectively(the first two in `python3` and the last in `python2`) in directory `.\\baselines` with the following arguments:\n', '```\n', '# usage is the same as above\n', 'positional arguments:\n', '  dataset\n', 'optional arguments:\n', '  --fold\n', '  --num_X\n', '```\n', '\n', '- Some sample scripts\n', '```\n', '$ python3 reweight.py compas --fold 1\n', '$ python2 fair_lr.py synthetic --fold 1 --num_X 30\n', '$ python3 fair_reduction.py german --fold 2\n', '```\n', '- To generate batches scripts, run:\n', '```\n', '$ julia bin/gen_exp.jl scripts/json/baselines.json --set_id 0 --cmd python3 -b fair_reduction.py\n', '$ julia bin/gen_exp.jl scripts/json/baselines.json --set_id 0 --cmd python3 -b reweight.py\n', '$ julia bin/gen_exp.jl scripts/json/baselines.json --set_id 0 --cmd python -b fair_lr.py\n', '```']"
Model Fairness,monindersingh/pydata2018_fairAI_models_tutorial,monindersingh,https://api.github.com/repos/monindersingh/pydata2018_fairAI_models_tutorial,5,5,1,['https://api.github.com/users/monindersingh'],Python,2021-03-23T19:26:23Z,https://raw.githubusercontent.com/monindersingh/pydata2018_fairAI_models_tutorial/master/README.md,"['# Building Fair AI models tutorial at PyData New York, 2018\n', '\n', '\n', '### This tutorial uses an open source Python package named [AI Fairness 360 or AIF360](https://github.com/ibm/aif360). \n', '\n', '### Please visit the above site and follow instructions to install the package.\n', '\n', '### Additionally, download datasets following instructions at [https://github.com/IBM/AIF360/tree/master/aif360/data](https://github.com/IBM/AIF360/tree/master/aif360/data)\n', '\n', '\n', '\n', '### Alternatively, instructions are provided below for manually installing AIF360 and downloading datasets using Conda on Windows\n', '\n', 'Create and activate environment\n', '\n', '```bash\n', 'conda create --name aif360 python=3.5\n', 'conda activate aif360\n', '```\n', '\n', '\n', '\n', 'Clone AIF360 from GitHub:\n', '\n', '```bash\n', 'git clone https://github.com/IBM/AIF360\n', '```\n', '\n', 'Install R-essentials for downloading MEPS data\n', '\n', '```bash\n', 'conda install -c r r-essentials\n', '```\n', '\n', 'Download datasets and place under appropriate folders under AIF360/aif360/data/raw by cloning this repository (NOTE: clone at same level as AIF360) and running the belowmentioned notebooks in the root folder\n', '          \n', '```bash\n', 'git clone https://github.com/monindersingh/pydata2018_fairAI_models_tutorial.git\n', '```\n', 'Change to the root folder of just cloned repository and run\n', '\n', '```bash\n', 'jupyter notebook pydata_datasets.ipynb\n', 'jupyter notebook pydata_meps_datasets.ipynb\n', '```\n', '\n', '\n', 'Then, navigate to the root directory of the cloned AIF360 project and run:\n', '\n', '```bash\n', 'pip install .\n', '```\n', '\n', '\n', '\n', 'Finally, install the additional requirements as follows:\n', '\n', '```bash\n', 'conda install ecos\n', 'pip install -r requirements.txt\n', '```\n', '\n']"
Model Fairness,ecreager/causal-dyna-fair,ecreager,https://api.github.com/repos/ecreager/causal-dyna-fair,8,4,2,"['https://api.github.com/users/ecreager', 'https://api.github.com/users/dependabot%5Bbot%5D']",Python,2022-09-23T03:22:57Z,https://raw.githubusercontent.com/ecreager/causal-dyna-fair/master/README.md,"['# causal-dyna-fair\n', 'Code accompanying the paper ""Causal Modeling for Fairness in Dynamical Systems"", presented at ICML 2020.\n', '\n', 'ArXiV: https://arxiv.org/abs/1909.09141\n', '\n', 'ICML results can be reproduced by `./bin/icml_results.sh`.\n', '\n', 'Package dependencies are specified in `requirements.txt`.\n', 'We strongly recommend using a fresh virtual environment and with packages installed via `pip install -r requirements.txt`.\n', 'Finally, we note that the `whynot` dependency may need to be installed from source as follows:\n', '```\n', 'git clone git@github.com:zykls/whynot.git\n', 'cd whynot\n', 'pip install .\n', '```']"
Model Fairness,MichaelMuinos/discounted-cash-flow-model,MichaelMuinos,https://api.github.com/repos/MichaelMuinos/discounted-cash-flow-model,3,4,1,['https://api.github.com/users/MichaelMuinos'],Python,2021-01-07T16:50:43Z,https://raw.githubusercontent.com/MichaelMuinos/discounted-cash-flow-model/master/README.md,"['# discounted-cash-flow-model\n', '\n', '## What is the DCF model?\n', 'Discounted cash flow (DCF) is a valuation method used to estimate the value of an investment based on its future cash flows. DCF analysis attempts to figure out the value of an investment today, based on projections of how much money it will generate in the future. This applies to both financial investments for investors and for business owners looking to make changes to their businesses, such as purchasing new equipment. [https://www.investopedia.com/terms/d/dcf.asp]\n', '\n', ""### THIS IS STILL A WIP, DON'T USE THIS FOR MAKING INVESTMENTS\n""]"
Model Fairness,IBMDeveloperUK/Trusted-AI-Workshops,IBMDeveloperUK,https://api.github.com/repos/IBMDeveloperUK/Trusted-AI-Workshops,10,5,1,['https://api.github.com/users/MargrietGroenendijk'],Python,2023-02-10T07:24:30Z,https://raw.githubusercontent.com/IBMDeveloperUK/Trusted-AI-Workshops/master/README.md,"['# Trusted AI\n', '\n', '## Details\n', '\n', 'You can find material for workshops on Trusted AI here. Most workshops will be using a Jupyter notebook that you can either run on your own machine or in Watson Studio on the IBM Cloud. \n', '\n', 'To go through the workshop material smoothly it will be easiest when we all use the same setup, especially as it will be harder to help you straight away when you might get stuck during virtual events. Of course feel free to use your own local machine when you are comfortable with setting up environments and installing missing packages.\n', '\n', '## Getting Started\n', '\n', 'Find the detailed instructions to set up a Cloud environment [here](https://github.com/IBMDeveloperUK/Trusted-AI-Workshops/blob/master/watson-studio-instructions.md)\n', '\n', '* [Create a free IBM Cloud account](https://ibm.biz/BdqNqh)\n', '* [Get started with Watson Studio](https://github.com/IBMDeveloperUK/Trusted-AI-Workshops/blob/master/watson-studio-instructions.md)\n', '\n', 'Then go to the session\n', '\n', '* [Beyond Accuracy: Fairness in Machine Learning](https://github.com/IBMDeveloperUK/Trusted-AI-Workshops/blob/master/beyond-accuracy.md)\n', '    \n', '## Resources\n', '\n', '### Documentation\n', '\n', '#### AI Fairness 360\n', '\n', '* [GitHub repo](https://github.com/Trusted-AI/AIF360)\n', '* [Demo](https://aif360.mybluemix.net/)\n', '* [API documentation](https://aif360.readthedocs.io/en/latest/)\n', '\n', '#### AI Explainability 360\n', '\n', '* [GitHub repo](https://github.com/Trusted-AI/AIX360)\n', '* [Demo](http://aix360.mybluemix.net/)\n', '* [API documentation](https://aix360.readthedocs.io/en/latest/)\n', '\n', '#### AI Factsheets 360\n', '\n', '#### Learn more\n', '\n', '* On the UK team page \n', '    * https://ibmdeveloperuk.github.io/\n', '\n', '* From the weekly [Data Science Lunch & Learn](https://github.com/IBMDeveloperUK/Data-Science-Lunch-and-Learn) every Monday\n', '* On our crowdcast and twitch channels, where we host regular workshops and events\n', '    * https://www.crowdcast.io/ibmdeveloper\n', '    * https://www.crowdcast.io/ibmdevelopereurope\n', '    * https://www.twitch.tv/ibmdeveloper\n', '\n', '* From our [developer site](https://developer.ibm.com/) where you can find tutorials, code examples, articles and more\n', '    * [List of videos on Trusted AI](https://aifs360.mybluemix.net/resources/videos)\n', '    * [AI Ethics at IBM](https://www.ibm.com/artificial-intelligence/ethics)\n', '    * [Trusted AI at IBM Research](https://www.research.ibm.com/artificial-intelligence/trusted-ai/)\n', '    * [Trusted AI at the Linux Foundation](https://lfai.foundation/projects/trusted-ai/)\n', '\n', '#### Contact me \n', '\n', 'If you need help, have suggestions or want to talk more about fair and explainable AI\n', '\n', '* [Twitter](https://twitter.com/MargrietGr)\n', '* [LinkedIn](https://www.linkedin.com/in/margrietgroenendijk/)\n']"